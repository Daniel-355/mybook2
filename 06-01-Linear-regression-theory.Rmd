## Simple linear regression

Simple linear regression is the most common “first model” in applied statistics. It answers a basic question:

> How does the expected value of an outcome \(Y\) change as a function of a single predictor \(X\)?

Even if your final analysis uses more sophisticated methods (mixed models, GLMs, survival models), simple linear regression remains a key building block. It teaches you how estimation works, why assumptions matter, and how uncertainty is quantified through standard errors, confidence intervals, and hypothesis tests.

In practice, regression has two equally important goals:
1) **Explanation** (estimating association or effect size), and  
2) **Prediction** (forecasting outcomes at new values of \(X\)).

---

### Linear regeression assumptions

A regression model is only as reliable as its assumptions. In real-world analysis, the assumptions are rarely perfect, but they guide diagnostics and help you understand when inference may break down.

There are four principal assumptions:

- **Linearity** of the relationship between dependent and independent variables.  
  This means the conditional mean \(E(Y|X)\) is well-approximated by a straight line. If the true relationship is curved, the linear model may still be useful as a local approximation, but interpretation can become misleading.

- **Statistical independence** of the errors with \(y\) variable.  
  Independence is often violated in longitudinal data, clustered data (e.g., patients within sites), or time series. When independence fails, standard errors are typically wrong—often **too small**, leading to overly optimistic p-values and confidence intervals that are too narrow.

- **Homoscedasticity** (constant variance) of the errors for all \(x\).  
  If variability increases with \(X\) (a “fanning out” pattern), the model may still estimate the mean trend reasonably, but standard errors and tests can be distorted unless you use robust methods or transform variables.

- **Normality** of the error distribution.  
  Normality matters mainly for *small-sample inference*. Large samples rely less on strict normality due to asymptotic approximations. Also note: normality is assumed for the **errors**, not necessarily for \(X\) or \(Y\) marginally.

`if independent assumption violated, the estimated standard errors tend to underestimate the true standard error. P value associated thus is lower.`

`only the prediction errors need to be normally distributed. but with extremely asymmetric or long-tailed, it may be hard to fit them (x and y) into a linear model whose errors will be normally distributed.`

---

### Population regression function

The **population regression function** is the ideal target we would like to know: the true conditional mean of \(Y\) given \(X\). Regression is fundamentally about modeling and estimating this conditional expectation.

Regression is to estimate and/or predict the population mean (expectation) of dependent variable (yi) by a known or a set value of explanatory variables (xi). Population regression line (PRL) is the trajectory of the conditional expectation value given Xi.

$$
E(Y|X_i)=f(X_i)=\beta_1+\beta_2X_i
$$

`This is an unknown but fixed value (can be estimated). `

A key interpretation:
- \(\beta_1\) is the expected value of \(Y\) when \(X=0\) (sometimes meaningful, sometimes not).
- \(\beta_2\) is the expected change in \(Y\) for a one-unit increase in \(X\).

---

### Population regression model  

In the population, actual observations deviate from the regression function due to randomness and unmeasured factors. We represent this deviation as an error term \(u_i\).

$$
Y_i=\beta_1+\beta_2X_i+u_i
$$

the errors \(u_i=y_i-\hat{y}_i\) have equal variance

In applied interpretation, \(u_i\) captures everything not explained by \(X\): measurement noise, omitted variables, and inherent randomness.

---

### Sample regression model 

In practice we observe data and estimate coefficients. The **sample regression model** replaces unknown population parameters with estimates (hats), and uses residuals \(e_i\) as estimated errors.

(using hat to indicate sample)

$$
Y_i=\hat{\beta}_1+\hat{\beta}_2X_i+e_i
$$

since  

$$
u_i  \sim N(0,\sigma^2)
$$
or 
$$
e_i  \sim N(0,\hat{\sigma} ^2)
$$ 

and

`i.i.d., independent identically distribution, 
the probability distributions are all the same and variables are independent of each other.`

$$
\begin{align}
u_i \sim i.i.d \ N(0,\sigma^2)  
\end{align}
$$

then 

$$
\begin{align}
Y_i- \beta_1+\beta_2X_i (\hat{Y_i}) &\sim i.i.d \ N(0,\sigma^2)\\
\end{align}
$$

This i.i.d. assumption is what allows us to derive standard errors and perform t-tests and F-tests in the classical linear regression framework.

---

### Least squares: minimize \(Q=\sum (Y_i-\hat{Y}_i)^2\)

The ordinary least squares (OLS) estimator chooses coefficients that minimize the total squared residual error. Squaring penalizes large deviations and gives a unique and mathematically convenient solution.

thence, to minimize Q \(\sum{(Y_i-\hat{Y}_i)^2}\) to solve b0 and b1. 

$$
\begin{align}
Min(Q) &=\sum{(Y_i-\hat{Y}_i)^2}\\
&=\sum{\left ( Y_i-(\hat{\beta}_1+\hat{\beta}_2X_i) \right )^2}\\
       &=f(\hat{\beta}_1,\hat{\beta}_2) 
\end{align}
$$

---

### Solve \(\hat{\beta}_1,\hat{\beta}_2\) and variance 

Once the least squares problem is solved, you get closed-form estimators for the slope and intercept. The sampling variability of these estimators depends on:
- the error variance \(\sigma^2\),
- the spread of \(X\) values (more spread → more information → smaller variance).

$$
\begin{align}
  \begin{split}
  \hat{\beta}_2 &=\frac{\sum{x_iy_i}}{\sum{x_i^2}}\\
  \hat{\beta_1} &=\bar{Y}_i-\hat{\beta}_2\bar{X}_i
  \end{split} \\
var(\hat{\beta}_2) =\sigma_{\hat{\beta}_2}^2&=\frac{1}{\sum{x_i^2}}\cdot\sigma^2&&\text{} \\
var(\hat{\beta}_1) =\sigma_{\hat{\beta}_1}^2 &=\frac{\sum{X_i^2}}{n\sum{x_i^2}}\cdot\sigma^2 
\end{align}
$$

A practical takeaway: if your \(X\) values are tightly clustered, \(\sum x_i^2\) is small and the slope becomes hard to estimate precisely.

---

### Calculate the variance \(\hat{\sigma}^2\) of error \(e_i\) 

The residual variance is estimated from the residual sum of squares. Conceptually, it measures how much unexplained variability remains after fitting the regression line.

(for sample)

$$
\begin{align}
  \hat{Y}_i &=\hat{\beta}_1+\hat{\beta}_2X_i  \\
  e_i &=Y_i-\hat{Y}_i  \\
  \hat{\sigma}^2 &=\frac{\sum{e_i^2}}{n-1}=\frac{\sum{(Y_i-\hat{Y}_i)^2}}{n-1}  
\end{align}
$$

(Practical note: in classical regression, the unbiased estimator typically uses \(n-2\) in the denominator for simple linear regression because two parameters were estimated. Your expression shows the core idea—estimating variance from squared residuals.)

---

### Sum of squares decomposition 

A central identity in regression is that total variability can be decomposed into explained and unexplained components. This is the basis of \(R^2\) and the ANOVA-style F test.

$$
\begin{align}
(Y_i-\bar{Y_i})  &= (\hat{Y_i}-\bar{Y_i}) +(Y_i-\hat{Y_i})  \\
\sum{y_i^2} &= \sum{\hat{y_i}^2} +\sum{e_i^2}  \\ 
TSS&=ESS+RSS  
\end{align}
$$

Interpretation:
- **TSS**: total sum of squares (overall variability around the mean)
- **ESS**: explained sum of squares (variability explained by the regression)
- **RSS**: residual sum of squares (unexplained variability)

---

### Coefficient of determination \(R^2\) and goodness of fit

\(R^2\) is the proportion of variability explained by the model. It is descriptive: a higher \(R^2\) means the fitted line tracks the data more closely, but it does not guarantee causality or correctness of assumptions.

$$
\begin{align}
r^2 &=\frac{ESS}{TSS}=\frac{\sum{(\hat{Y_i}-\bar{Y})^2}}{\sum{(Y_i-\bar{Y})^2}}\\
    &=1-\frac{RSS}{TSS}=1-\frac{\sum{(Y_i-\hat{Y_i})^2}}{\sum{(Y_i-\bar{Y})^2}}
\end{align}
$$

In practice, always interpret \(R^2\) alongside residual diagnostics. A model can have a decent \(R^2\) but still violate key assumptions (e.g., heteroscedasticity).

---

### Test of regression coefficients

Hypothesis testing in regression typically focuses on whether coefficients differ from zero (or another clinically meaningful value). Under classical assumptions, coefficients are normally distributed around their true values.

since  
$$
\begin{align}
\hat{\beta_2} &\sim N(\beta_2,\sigma^2_{\hat{\beta_2}})  \\
\hat{\beta_1} &\sim N(\beta_1,\sigma^2_{\hat{\beta_1}})  
\end{align}
$$

and 
$$
\begin{align}
S_{\hat{\beta}_2} &=\sqrt{\frac{1}{\sum{x_i^2}}}\cdot\hat{\sigma} 
 \\
S_{\hat{\beta}_1} &=\sqrt{\frac{\sum{X_i^2}}{n\sum{x_i^2}}}\cdot\hat{\sigma}
\end{align}
$$

therefore  
$$
\begin{align}
t_{\hat{\beta_2}}^{\ast}&=\frac{\hat{\beta_2}-\beta_2}{S_{\hat{\beta_2}}}
=\frac{\hat{\beta_2}}{S_{\hat{\beta_2}}}
=\frac{\hat{\beta_2}}{\sqrt{\frac{1}{\sum{x_i^2}}}\cdot\hat{\sigma}}
\sim t(n-2)
  \\ 
t_{\hat{\beta_1}}^{\ast}&=\frac{\hat{\beta_1}-\beta_1}{S_{\hat{\beta_1}}}
=\frac{\hat{\beta_1}}{S_{\hat{\beta_1}}}
=\frac{\hat{\beta_1}}{\sqrt{\frac{\sum{X_i^2}}{n\sum{x_i^2}}}\cdot\hat{\sigma}}
\sim t(n-2)
\end{align}
$$

In applied reporting, the slope test is often the primary focus, because it corresponds to whether \(X\) is associated with \(Y\) in a linear trend.

---

### Statistical test of model 

Beyond individual coefficients, we may want to test whether the model as a whole explains a statistically significant amount of variability compared with a null model.

since
$$
\begin{align}
Y_i&\sim i.i.d \ N(\beta_1+\beta_2X_i,\sigma^2)\\
\end{align}
$$

and   
$$
\begin{align}
ESS&=\sum{(\hat{Y_i}-\bar{Y})^2} \sim \chi^2(df_{ESS})  \\
RSS&=\sum{(Y_i-\hat{Y_i})^2} \sim \chi^2(df_{RSS})  
\end{align}
$$

therefore 
$$
\begin{align}
F^{\ast}&=\frac{ESS/df_{ESS}}{RSS/df_{RSS}}=\frac{MSS_{ESS}}{MSS_{RSS}}\\
        &=\frac{\sum{(\hat{Y_i}-\bar{Y})^2}/df_{ESS}}{\sum{(Y_i-\hat{Y_i})^2}/df_{RSS}} \\
        &=\frac{\hat{\beta_2}^2\sum{x_i^2}}{\sum{e_i^2}/{(n-2)}}\\
        &=\frac{\hat{\beta_2}^2\sum{x_i^2}}{\hat{\sigma}^2}
\end{align}
$$

The F test evaluates whether the explained variation (ESS) is large relative to residual variation (RSS), after accounting for degrees of freedom.

---

### Mean prediction

Prediction in regression has two common targets:

1) **Mean prediction**: the expected outcome for individuals with a given \(X_0\).  
2) **Individual prediction**: the outcome for a *new single individual* with \(X_0\).

Mean prediction is more precise because it estimates an average, not a single future value.

since 

$$
\begin{align}
\mu_{\hat{Y}_0}&=E(\hat{Y}_0)\\
&=E(\hat{\beta}_1+\hat{\beta}_2X_0)\\
&=\beta_1+\beta_2X_0\\
&=E(Y|X_0)
\end{align}
$$

and 
$$
\begin{align}
var(\hat{Y}_0)&=\sigma^2_{\hat{Y}_0}\\
&=E(\hat{\beta}_1+\hat{\beta}_2X_0)\\
&=\sigma^2 \left( \frac{1}{n}+ \frac{(X_0-\bar{X})^2}{\sum{x_i^2}} \right)
\end{align}
$$

therefore
$$
\begin{align}
\hat{Y}_0& \sim N(\mu_{\hat{Y}_0},\sigma^2_{\hat{Y}_0})\\
\hat{Y}_0& \sim N \left(E(Y|X_0), \sigma^2 \left( \frac{1}{n}+ \frac{(X_0-\bar{X})^2}{\sum{x_i^2}} \right) \right)
\end{align}
$$

then `construct t statistic` to estimate CI
$$
\begin{align}
t_{\hat{Y}_0}& =\frac{\hat{Y}_0-E(Y|X_0)}{S_{\hat{Y}_0}} \sim t(n-2) 
\end{align}
$$

$$
\begin{align}
\hat{Y}_0-t_{1-\alpha/2}(n-2) \cdot S_{\hat{Y}_0} \leq E(Y|X_0) \leq \hat{Y}_0+t_{1-\alpha/2}(n-2) \cdot S_{\hat{Y}_0}
\end{align}
$$

Interpretation: the CI here is for the *mean response* at \(X_0\). It is narrow when:
- \(n\) is large, and
- \(X_0\) is close to \(\bar{X}\) (more information near the center of the data).

---

### Individual prediction

Individual prediction intervals are wider because they must account for:
- uncertainty in estimating the mean trend, and
- the irreducible random error for a new observation.

since  

$$
\begin{align}
(Y_0-\hat{Y}_0)& \sim N \left(\mu_{(Y_0-\hat{Y}_0)},\sigma^2_{(Y_0-\hat{Y}_0)} \right)\\
(Y_0-\hat{Y}_0)& \sim N \left(0, \sigma^2 \left(1+ \frac{1}{n}+ \frac{(X_0-\bar{X})^2}{\sum{x_i^2}} \right) \right)
\end{align}
$$

and `Construct t statistic`
$$
\begin{align}
t_{\hat{Y}_0}& =\frac{\hat{Y}_0-E(Y|X_0)}{S_{\hat{Y}_0}} \sim t(n-2)
\end{align}
$$

and 
$$
\begin{align}
S_{\hat{Y}_0}& = \sqrt{\hat{\sigma}^2 \left( \frac{1}{n}+ \frac{(X_0-\bar{X})^2}{\sum{x_i^2}} \right)}
\end{align}
$$

therefore 

$$
\begin{align}
\hat{Y}_0-t_{1-\alpha/2}(n-2) \cdot S_{\hat{Y}_0} \leq E(Y|X_0) \leq \hat{Y}_0+t_{1-\alpha/2}(n-2) \cdot S_{\hat{Y}_0}
\end{align}
$$

`it is harder to predict your weight based on your age than to predict the mean weight of people who are your age. so, the interval of individual prediction is wider than those of mean prediction.`

A practical mental model:
- **Mean CI** answers: “What is the expected mean outcome at \(X_0\)?”
- **Prediction interval** answers: “Where might a new individual outcome fall at \(X_0\)?”

---

## Multiple linear regression

Multiple linear regression generalizes the simple model by allowing multiple predictors. The key shift is that each coefficient is interpreted as an effect **holding other variables constant**.

### Matrix format

$$
\begin{align}
Y_i&=\beta_1+\beta_2X_{2i}+\beta_3X_{3i}+\cdots+\beta_kX_{ki}+u_i 
&& \
\end{align}
$$

$$
\begin{equation}
  \begin{bmatrix}
  Y_1 \\  Y_2 \\  \cdots \\  Y_n \\
  \end{bmatrix}  =
  \begin{bmatrix}
  1 &  X_{21} & X_{31} & \cdots &  X_{k1} \\
  1 &  X_{22} & X_{32} & \cdots &  X_{k2} \\
  \cdots &  \cdots & \cdots & \cdots &  \cdots \\
  1 &  X_{2n} & X_{3n} & \cdots &  X_{kn}
  \end{bmatrix}
  \begin{bmatrix}
  \beta_1 \\  \beta_2 \\  \vdots \\  \beta_k \\
  \end{bmatrix}+
  \begin{bmatrix}
  u_1 \\  u_2 \\  \vdots \\  u_n \\
  \end{bmatrix}
\end{equation}
$$

$$
\begin{alignat}{4}
\mathbf{y} &= &\mathbf{X}&\mathbf{\beta}&+&\mathbf{u}
 \\
(n \times 1) &  &{(n \times k)} &{(k \times 1)}&+&{(n \times 1)}
\end{alignat}
$$

The matrix form is not just notation—it simplifies derivations and makes the estimator compact and general.

---

### Variance covariance matrix of random errors

The classical assumption is that errors are independent, have equal variance, and have zero covariance. This leads to a diagonal variance-covariance structure proportional to the identity matrix.

because 
$$
\mathbf{u} \sim N(\mathbf{0},\sigma^2\mathbf{I})\text{    population}\\
\mathbf{e} \sim N(\mathbf{0},\sigma^2\mathbf{I})\text{    sample}\
$$

therefore
$$
\begin{align}
var-cov(\mathbf{u})&=E(\mathbf{uu'})\\
&=
  \begin{bmatrix}
  \sigma_1^2 & \sigma_{12}^2  &\cdots &\sigma_{1n}^2\\
  \sigma_{21}^2 & \sigma_2^2  &\cdots &\sigma_{2n}^2\\
  \vdots & \vdots &\vdots &\vdots \\
  \sigma_{n1}^2 & \sigma_{n2}^2  &\cdots &\sigma_n^2\\
  \end{bmatrix}  
  && \leftarrow (E{(u_i)}=0)\\
&=
  \begin{bmatrix}
  \sigma^2 & \sigma_{12}^2  &\cdots &\sigma_{1n}^2\\
  \sigma_{21}^2 & \sigma^2  &\cdots &\sigma_{2n}^2\\
  \vdots & \vdots &\vdots &\vdots \\
  \sigma_{n1}^2 & \sigma_{n2}^2  &\cdots &\sigma^2\\
  \end{bmatrix} 
  && \leftarrow (var{(u_i)}=\sigma^2)\\
&=
  \begin{bmatrix}
  \sigma^2 & 0  &\cdots &0\\
  0 & \sigma^2  &\cdots &0\\
  \vdots & \vdots &\vdots &\vdots \\
  0 & 0  &\cdots &\sigma^2\\
  \end{bmatrix}
  && \leftarrow (cov{(u_i,u_j)}=0,i \neq j)\\
&=\sigma^2
  \begin{bmatrix}
  1 & 0  &\cdots &0\\
  0 & 1  &\cdots &0\\
  \vdots & \vdots &\vdots &\vdots \\
  0 & 0  &\cdots &1\\
  \end{bmatrix}\\
&=\sigma^2\mathbf{I}
\end{align}
$$

In applied work, this assumption is often challenged by clustering and repeated measures. When violated, analysts may move to robust standard errors, GLS, or mixed effects models.

---

### Minimize \(Q=\sum (y-\hat{y})^2\)

In matrix form, OLS still minimizes the squared residuals, but the algebra becomes compact and scalable.

$$
\begin{align}
Q&=\sum{e_i^2}\\
&=\mathbf{e'e}\\
&=\mathbf{(y-X\hat{\beta})'(y-X\hat{\beta})}\\
&=\mathbf{y'y-2\hat{\beta}'X'y+\hat{\beta}'X'X\hat{\beta}} 
\end{align}
$$

---

### Solve \(\hat{\beta}\) by derivation

Setting the derivative with respect to \(\hat{\beta}\) equal to zero yields the normal equations. Solving them gives the closed-form OLS estimator.

(population=sample)

$$
\begin{align}
\frac{\partial Q}{\partial \mathbf{\hat{\beta}}}&=0\\
\frac{\partial(\mathbf{y'y-2\hat{\beta}'X'y+\hat{\beta}'X'X\hat{\beta}})}{\partial \mathbf{\hat{\beta}}}&=0\\
-2\mathbf{X'y}+2\mathbf{X'X\hat{\beta}}&=0\\
-\mathbf{X'y}+\mathbf{X'X\hat{\beta}}&=0\\
\mathbf{X'X\hat{\beta}} &=\mathbf{X'y}
\end{align}
$$

$$
\begin{align}
\mathbf{\hat{\beta}} &=\mathbf{(X'X)^{-1}X'y} 
\end{align}
$$

Interpretation: \((X'X)^{-1}\) reflects the information in the design matrix. When predictors are highly correlated (multicollinearity), \(X'X\) becomes nearly singular and coefficient estimates become unstable.

---

### Solve \(var\text{-}cov(\mathbf{\hat{\beta}})\)

The variance-covariance matrix of \(\hat{\beta}\) is the core object for inference in multiple regression. It contains:
- variances of each coefficient (diagonal),
- covariances between coefficients (off-diagonal).

$$
\begin{align}
var-cov(\mathbf{\hat{\beta}})
&=\mathbf{E\left( \left(\hat{\beta}-E(\hat{\beta}) \right) \left( \hat{\beta}-E(\hat{\beta}) \right )' \right)}\\
&=\mathbf{E\left( \left(\hat{\beta}-{\beta} \right) \left( \hat{\beta}-\beta \right )' \right)} \\
&=\mathbf{E\left( \left((X'X)^{-1}X'u \right) \left( (X'X)^{-1}X'u \right )' \right)} \\
&=\mathbf{E\left( (X'X)^{-1}X'uu'X(X'X)^{-1}  \right)} \\
&= \mathbf{(X'X)^{-1}X'E(uu')X(X'X)^{-1}}   \\
&= \mathbf{(X'X)^{-1}X'}\sigma^2\mathbf{IX(X'X)^{-1}}   \\
&= \sigma^2\mathbf{(X'X)^{-1}X'X(X'X)^{-1}}   \\
&= \sigma^2\mathbf{(X'X)^{-1}}   \\
\end{align}
$$

---

### Solve \(S^2(\mathbf{\hat{\beta}})\) (sample)

In practice, \(\sigma^2\) is unknown. We estimate it from residuals and then plug it into the variance-covariance formula.

where 
$$
\begin{align}
\hat{\sigma}^2&=\frac{\sum{e_i^2}}{n-k}=\frac{\mathbf{e'e}}{n-k} \\
E(\hat{\sigma}^2)&=\sigma^2
\end{align}
$$

therefore
$$
\begin{align}
S^2_{ij}(\mathbf{\hat{\beta}})
&= \hat{\sigma}^2\mathbf{(X'X)^{-1}}   \\
&= \frac{\mathbf{e'e}}{n-k}\mathbf{(X'X)^{-1}}   \\
\end{align}
$$

`which is variance-covariance of coefficients`

---

### Sum of squares decomposition (matrix format)

The same TSS/ESS/RSS decomposition generalizes to multiple regression, forming the basis of \(R^2\) and the overall F test.

$$
\begin{align}
TSS&=\mathbf{y'y}-n\bar{Y}^2 \\
RSS&=\mathbf{ee'}=\mathbf{yy'-\hat{\beta}'X'y} \\
ESS&=\mathbf{\hat{\beta}'X'y}-n\bar{Y}^2 
\end{align}
$$

---

### Determination coefficient \(R^2\) and goodness of fit

$$
\begin{align}
R^2&=\frac{ESS}{TSS}\\
&=\frac{\mathbf{\hat{\beta}'X'y}-n\bar{Y}^2}{\mathbf{y'y}-n\bar{Y}^2}
\end{align}
$$

Interpretation remains the same: \(R^2\) is descriptive goodness-of-fit. In multiple regression it almost always increases as you add predictors, which is why adjusted \(R^2\) or out-of-sample validation is often preferred for model selection.

---

### Test of regression coefficients 

Multiple regression inference typically includes:
- **individual coefficient tests** (is \(\beta_j=0\)?),
- **joint tests** (are multiple coefficients simultaneously zero?).

because 
$$
\begin{align}
\mathbf{u}&\sim N(\mathbf{0},\sigma^2\mathbf{I})  \\
\mathbf{\hat{\beta}} &\sim N\left(\mathbf{\beta},\sigma^2\mathbf{X'X}^{-1} \right)   \\
\end{align}
$$

therefore 

`(for all coefficients test, vector, see above` \(S_{\hat{\beta}}^2\) `)`
$$
\begin{align}
\mathbf{t_{\hat{\beta}}}&=\mathbf{\frac{\hat{\beta}-\beta}{S_{\hat{\beta}}}} 
\sim \mathbf{t(n-k)}
\end{align}
$$

`(for individual coefficient test)`
$$
\begin{align}
\mathbf{t_{\hat{\beta}}^{\ast}}&=\frac{\mathbf{\hat{\beta}}}{\mathbf{\sqrt{S^2_{ij}(\hat{\beta}_{kk})}}} 
\end{align}
$$

where 
$$
S^2_{ij}(\hat{\beta}_{kk})=[s^2_{\hat{\beta}_1},s^2_{\hat{\beta}_2},\cdots,s^2_{\hat{\beta}_k}]'
$$

`they are on diagonal line of the matrix of` \(S^2(\mathbf{\hat{\beta}})\) 

---

### Test of model

The overall model test compares:
- an **unrestricted** model with predictors, versus
- a **restricted** model (often intercept-only).

unrestricted model
$$
\begin{align}
u_i &\sim i.i.d \ N(0,\sigma^2)\\
Y_i&\sim i.i.d \ N(\beta_1+\beta_2X_i+\cdots+\beta_kX_i,\sigma^2)\\
RSS_U&=\sum{(Y_i-\hat{Y_i})^2} \sim \chi^2(n-k) \\
\end{align}
$$

restricted model
$$
\begin{align}
u_i &\sim i.i.d \ N(0,\sigma^2)\\
Y_i&\sim i.i.d \ N(\beta_1,\sigma^2)\\
RSS_R&=\sum{(Y_i-\hat{Y_i})^2} \sim \chi^2(n-1) \\
\end{align}
$$

F test
$$
\begin{align}
F^{\ast}&=\frac{(RSS_R-RSS_U)/(k-1)}{RSS_U/(n-k)} \\
        &=\frac{ESS_U/df_{ESS_U}}{RSS_U/df_{RSS_U}} \\
        &\sim F(df_{ESS_U},df_{RSS_U})
\end{align}
$$

$$
\begin{align}
F^{\ast}&=\frac{ESS_U/df_{ESS_U}}{RSS_U/df_{RSS_U}} 
=\frac{\left(\mathbf{\hat{\beta}'X'y}-n\bar{Y}^2  \right)/{(k-1)}}{\left(\mathbf{yy'-\hat{\beta}'X'y}\right)/{(n-k)}}
\end{align}
$$

---

### Mean prediction (multiple regression)

The mean prediction generalizes naturally: you plug in a covariate vector \(X_0\). The uncertainty depends on the leverage of \(X_0\) through \(X_0(X'X)^{-1}X_0'\).

since 
$$
\begin{align}
E(\hat{Y}_0)&=E\mathbf{(X_0\hat{\beta})}=\mathbf{X_0\beta}=E\mathbf{(Y_0)}\\
var(\hat{Y}_0)&=E\mathbf{(X_0\hat{\beta}-X_0\beta)}^2\\
              &=E\mathbf{\left( X_0(\hat{\beta}-\beta)(\hat{\beta}-\beta)'X_0' \right)}\\
              &=E\mathbf{X_0\left( (\hat{\beta}-\beta)(\hat{\beta}-\beta)' \right)X_0'}\\
              &=\sigma^2\mathbf{X_0\left( X'X \right)^{-1}X_0'}\\
\end{align}
$$

and 
$$
\begin{align}
\hat{Y}_0& \sim N(\mu_{\hat{Y}_0},\sigma^2_{\hat{Y}_0})\\
\hat{Y}_0& \sim N\left(E(Y_0|X_0), \sigma^2\mathbf{X_0(X'X)^{-1}X_0'}\right)
\end{align}
$$

`construct t statistic`
$$
\begin{align}
t_{\hat{Y}_0}& =\frac{\hat{Y}_0-E(Y|X_0)}{S_{\hat{Y}_0}} 
&\sim t(n-k) 
\end{align}
$$

therefore 
$$
\begin{align}
\hat{Y}_0-t_{1-\alpha/2}(n-2) \cdot S_{\hat{Y}_0} \leq E(Y|X_0) \leq \hat{Y}_0+t_{1-\alpha/2}(n-2) \cdot S_{\hat{Y}_0}
\end{align}
$$

where 
$$
\begin{align}
\mathbf{S_{\hat{Y}_0}}
&=\sqrt{\hat{\sigma}^2X_0(X'X)^{-1}X_0'} \\
\hat{\sigma}^2&=\frac{\mathbf{ee'}}{(n-k)} 
\end{align}
$$

---

### Individual prediction (multiple regression)

Individual prediction adds the irreducible error term for a new observation, making the interval wider than the mean-response interval.

since 
$$
\begin{align}
e_0&=Y_0-\hat{Y}_0
\end{align}
$$

and 
$$
\begin{align}
E(e_0)&=E(Y_0-\hat{Y}_0)\\
      &=E(\mathbf{X_0\beta}+u_0-\mathbf{X_0\hat{\beta}})\\
      &=E\left(u_0-\mathbf{X_0 (\hat{\beta}- \beta)} \right)\\
      &=E\left(u_0-\mathbf{X_0 (X'X)^{-1}X'u} \right)\\
      &=0
\end{align}
$$

$$
\begin{align}
var(e_0)&=E(Y_0-\hat{Y}_0)^2\\
        &=E(e_0^2)\\
        &=E\left(u_0-\mathbf{X_0 (X'X)^{-1}X'u} \right)^2\\
        &=\sigma^2\left( 1+ \mathbf{X_0(X'X)^{-1}X_0'}\right)
\end{align}
$$

and  
$$
\begin{align}
e_0& \sim N(\mu_{e_0},\sigma^2_{e_0})\\
e_0& \sim N\left(0, \sigma^2\left(1+\mathbf{X_0(X'X)^{-1}X_0'}\right)\right)
\end{align}
$$

`construct a t statistic `
$$
\begin{align}
t_{e_0}& =\frac{\hat{Y}_0-Y_0}{S_{e_0}} 
\sim t(n-k) 
\end{align}
$$

therefore 
$$
\begin{align}
\hat{Y}_0-t_{1-\alpha/2}(n-2) \cdot S_{Y_0-\hat{Y}_0} \leq (Y_0|X_0) \leq \hat{Y}_0+t_{1-\alpha/2}(n-2) \cdot S_{Y_0-\hat{Y}_0}
\end{align}
$$

where 
$$
\begin{align}
S_{Y_0-\hat{Y}_0}=S_{e_0}
&=\sqrt{\hat{\sigma}^2 \left( 1+X_0(X'X)^{-1}X_0' \right) } \\
\hat{\sigma}^2&=\frac{\mathbf{ee'}}{(n-k)} 
\end{align}
$$

A final practical takeaway:
- If your goal is decision-making about the *average* response at a covariate profile, use mean-response inference.
- If your goal is forecasting an *individual* outcome, expect much wider uncertainty bands, even with a well-fitted model.
