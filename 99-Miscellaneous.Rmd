# Miscellaneous

This chapter collects a set of “tools of the trade” that show up repeatedly in statistical practice:  
**linear algebra**, **calculus**, and a few **practical workflows** (sample size simulation, z-score evaluation, and reproducible reporting with bookdown/blogdown).

The intention is not to present a rigorous proof-based treatment, but to provide a working reference with short explanations and runnable code.

---

## Linear algebra

Linear algebra is the language of most modern statistical modeling. Regression, mixed models, PCA, and many optimization algorithms are built on matrix representations:

- data are organized as matrices (design matrices),
- parameters are vectors,
- sums of squares become quadratic forms,
- estimation becomes solving linear systems or matrix decompositions.

### Matrix basics

#### Dimensions of a matrix

A matrix has **two dimensions**: number of rows and number of columns. In R, `dim()` returns both.

```{r setup, include=FALSE,engine='R'}
knitr::opts_chunk$set( engine='R')
```

```{r}
   seed=123
   X=matrix(1:12,ncol=3,nrow=4)
   X
   
   dim(X)
   dim(X)[1]

   ncol(X)
```

Key ideas:
- `dim(X)` returns `(nrow, ncol)`.
- `dim(X)[1]` extracts the number of rows.
- `ncol(X)` is a convenience function for columns.

---

#### Change dimensions of a matrix

In R, you can reshape a matrix by changing its `dim`. This **does not change the values**, only how they are arranged (by column, because R stores matrices column-wise).

```{r}
dim(X)=c(2,6)
   X
```

This is essentially a “reshape” of the underlying vector `1:12`.

---

#### Change names of a matrix

Row and column names improve readability and reduce mistakes, especially when matrices represent parameters or covariance structures.

```{r}
  a <- matrix(1:6,nrow=2,ncol=3,byrow=FALSE)
  b <- matrix(1:6,nrow=3,ncol=2,byrow=T)
  c <- matrix(1:6,nrow=3,ncol=2,byrow=T,dimnames=list(c("A","B","C"),c("boy","girl")))
  c
```

Now we can extract and modify the names:

```{r}
   rownames(c)
   colnames(c)

   rownames(c)=c("E","F","G")
   c
```

---

#### Replace elements of a matrix

Indexing uses `[row, col]`. Once you can index, you can update values.

```{r}
   X=matrix(1:12,nrow=4,ncol=3)
   
   X[2,3]
   X[2,3]=1000
      
   X
```

---

#### Extract diagonal elements and replace them

The diagonal is often special: identity matrices, variances, and many decompositions depend on it.

```{r}
   diag(X)
   diag(X)=c(0,0,1)

   X
```

---

#### Create diagonal and identity matrices

- `diag(c(...))` creates a diagonal matrix with those values.
- `diag(k)` creates a `k × k` identity matrix.

```{r}
  diag(c(1,2,3))
   
  diag(3)
```

---

#### Extract lower/upper triangle elements

The lower and upper triangles are used heavily in covariance and triangular decompositions.

```{r}
  X
  X[lower.tri(X)]
   
  X[upper.tri(X)]
```

---

#### Create lower/upper triangle matrices

A common operation is to “zero out” part of a matrix.

```{r}
X[lower.tri(X)]=0
X
```

This is useful for constructing triangular matrices or simplifying display.

---

### Operations

#### Transpose

The transpose swaps rows and columns. Many matrix formulas (e.g., normal equations) require transpose.

```{r}
 t(X)
```

---

#### Row/column summaries

Row sums and means are common in data transformations and checking intermediate quantities.

```{r}
  A=matrix(1:12,3,4)
  A 
  rowSums(A)
   
  rowMeans(A)
```

---

#### Determinant

The determinant summarizes properties of a square matrix:
- If `det(A)=0`, the matrix is singular (not invertible).
- Determinants also appear in multivariate normal likelihoods via `|Σ|`.

```{r}
  X=matrix(rnorm(9),nrow=3,ncol=3)
  det(X)
```

---

#### Addition

Matrix addition requires the **same dimensions**.

```{r}
  A=matrix(1:12,nrow=3,ncol=4)
  B=matrix(1:12,nrow=3,ncol=4)
  A+B #same dimensions (non-conformable arrays)
```

---

#### Addition by a scalar

Adding a scalar shifts all entries.

```{r}
A=matrix(1:12,nrow=3,ncol=4)
2+A
```

---

#### Multiply by a scalar

Scalar multiplication rescales all entries.

```{r}
A=matrix(1:12,nrow=3,ncol=4)
2*A
```

---

#### Matrix multiplication (dot product)

Matrix multiplication requires conformable dimensions:
- if `A` is `m×k`, `B` must be `k×n`.

```{r}
   A=matrix(1:12,nrow=2,ncol=4)
   
   B=matrix(1:12,nrow=4,ncol=3)
   
   A%*%B
```

---

#### Kronecker product

The Kronecker product appears in block matrix constructions and covariance modeling (e.g., separable covariance).

```{r}
   A=matrix(1:4,2,2)
   B=matrix(rep(1,4),2,2)
   
   kronecker(A,B)
```

---

#### Inverse matrix

A matrix must be **square** and **non-singular** to have an inverse. Inverse matrices appear in closed-form least squares and covariance transformations.

```{r}
   A=matrix(rnorm(9),nrow=3,ncol=3) 
   A
   AI=solve(A)
   AI
   # identity matrix    
   (A%*%AI)
```

The `matlib` package provides a more explicit `inv()`.

```{r}
library(matlib)
inv(A)
```

---

#### Generalized inverse

When a matrix is not square (or is singular), we can use a **generalized inverse** (Moore–Penrose pseudoinverse). This is common in least squares with rank deficiency.

```{r}
   library(MASS)
   A2=matrix(1:12,nrow=3,ncol=4)
   
   A%*%ginv(A)
   A%*%ginv(A)%*%A
```

The product `A %*% ginv(A) %*% A` is a projection-like operation: it maps back into the column space of `A`.

---

#### `crossprod`

`crossprod(B)` computes `t(B) %*% B` efficiently and with better numerical stability.

```{r}
B=matrix(1:12,nrow=4,ncol=3)
B
crossprod(B)
t(B)%*%B
```

Generalized inverse utilities with `crossprod`:

```{r}
   B=matrix(1:12,nrow=4,ncol=3)
   ginv(B)
   ginv(crossprod(B))
   # solve(crossprod(B)) #is singular
   # solve((B)) #is not square
```

---

### Eigen decomposition

Eigen decomposition is defined for square matrices. It is central in:
- PCA and spectral methods,
- diagonalizing symmetric matrices,
- understanding quadratic forms and covariance matrices.

For an `m×m` matrix `A`, eigen decomposition takes the form:
- values: eigenvalues (`Λ`)
- vectors: eigenvectors (`U`)

In many texts: `A = U Λ U^{-1}`.  
For symmetric matrices, `U` is orthogonal and `U^{-1} = U'`, giving `A = U Λ U'`.

#### Eigen values

```{r}
   A=matrix(1:9,nrow=3,ncol=3)
   
   Aeigen=eigen(A)
   
   Aeigen$values
   
   val <- diag(Aeigen$values)
   val
```

---

#### Eigen vectors

```{r}
Aeigen$vectors
```

We can reconstruct `A` (approximately) using eigenvectors and eigenvalues:

```{r}
 round(Aeigen$vectors%*%val%*%t(Aeigen$vectors))
 A
```

Small discrepancies may occur due to rounding and floating-point arithmetic.

---

### Advanced operations

#### Cholesky factorization

Cholesky factorization applies to **symmetric positive definite** matrices:
- `A = R'R` (or sometimes `A = LL'` depending on convention),
- `chol(A)` returns an upper triangular factor by default.

Covariance matrices are typically positive semidefinite; in practice, estimated covariance matrices are often treated as positive definite (or adjusted to be so).

```{r}
   A=diag(3)+1
   A  
   chol(A)
```

Check the reconstruction:

```{r}
   t(chol(A))%*%chol(A)
```

Compute inverse via Cholesky:

```{r}
chol2inv(chol(A))
inv(A)
```

Using Cholesky is often numerically preferable to directly calling `solve()` for symmetric positive definite matrices.

---

#### Singular value decomposition (SVD)

SVD works for **any** `m×n` matrix and is among the most robust matrix decompositions.

`A = U D V'`

- `U`: orthogonal (`m×m` or `m×r`)
- `D`: diagonal singular values (`r×r`)
- `V`: orthogonal (`n×n` or `n×r`)
- `r`: rank (number of non-zero singular values)

```{r}
   A=matrix(1:18,3,6)
   
   svd(A)   $d
   t(svd(A)   $u)%*%svd(A)   $u
   t(svd(A)   $v)%*%svd(A)   $v
   
```

Reconstruction:

```{r}
   svd(A)   $u %*%diag(svd(A)   $d)%*% t(svd(A)   $v)
   A
```

SVD is widely used for:
- pseudoinverse,
- PCA (via centered data matrix),
- numerical stability in least squares.

---

#### QR decomposition

QR is another general decomposition for `m×n` matrices:

`A = Q R`

- `Q`: orthogonal (`Q'Q = I`)
- `R`: upper triangular

QR is essential for numerically stable least squares.

```{r}
   A=matrix(1:12,4,3)
   qr(A)
```

Extract the components:

```{r}
qr.Q(qr(A))
```

```{r}
qr.R(qr(A))
```

---

### Solve linear equations

Many estimation problems reduce to solving `Xβ = b`.

```{r}
   X=matrix(c(2, 2, 2, 0, 1, 1  ,2, 2, 0),ncol=3,nrow=3)
   X
   b=1:3
   b
   solve(X,b) # whether it is singular
```

If `X` is singular, `solve()` fails. In practice, that is a signal to use:
- `qr.solve()` (least squares),
- `ginv()` (pseudoinverse),
- or to investigate collinearity/rank deficiency.

---

### Summary

A useful mental map:

- **Eigen decomposition**: square matrices; reveals directions of stretching/compression.  
- **SVD**: works for any matrix; most robust; basis of pseudoinverse and PCA.  
- **QR**: stable for regression/least squares.  
- **Cholesky**: fastest for symmetric positive definite matrices (especially covariance matrices).

---

## Calculus

Calculus enters statistical practice in:
- optimization (derivatives/gradients),
- likelihood theory,
- computing integrals for probabilities and expectations.

### Derivation

R’s `deriv()` can symbolically differentiate simple expressions.

```{r}
dx <- deriv(y ~ x^3, "x"); dx
mode(dx)
x<-1:2   
eval(dx)
```

We can also request a function output:

```{r}
dx <- deriv(y ~ sin(x), "x", func= TRUE) ;  
mode(dx)
dx(c(pi,4*pi))  
```

And include constants:

```{r}
a<-2                
dx<-deriv(y~a*cos(a*x),"x",func = TRUE)    
dx(pi/3)
   
```

Partial derivatives:

```{r}
fxy = expression(2*x^2+y+3*x*y^2)
dxy = deriv(fxy, c("x", "y"), func = TRUE)
dxy(1,2)  
```

In modeling, this kind of derivative structure is the foundation for gradients and Hessians used by optimizers.

---

### Integration

Many probabilities are integrals of density functions.

```{r}
integrate(dnorm, -1.96, 1.96)
 
integrate(dnorm, -Inf, Inf)
```

Improper integrals (infinite bounds) are supported:

```{r}
integrand <- function(x) {1/((x+1)*sqrt(x))}
 
integrate(integrand, lower = 0, upper = Inf)
```

Trigonometric integrals:

```{r}
 integrand <- function(x) {sin(x)}
 integrate(integrand, lower = 0, upper = pi/2) 
```

---

## Sample size calculation (simulation-based power)

In complex designs, analytic power formulas may not be available or may be hard to trust. Simulation is often more transparent:

- Specify the data-generating process under the alternative,
- simulate many trials,
- apply the intended analysis method,
- estimate power as the proportion of significant results.

Your code implements exactly this idea: for each candidate sample size `t`, simulate `M` trials and record whether `prop.test()` gives `p < 0.05`.

```{r}
#given proportion 
px3=0.11
px4=0.07
px5=0.08
px6=0.06
px7=0.05

###################

  Outcome0 = NULL
  
  for (t in seq(350, 600, by=2)){  # change the possible sample size graudally from 400 to 700.  
    
    n=t
    count=0
    M=500   # times of simulation
    for (i in 1:M){
      
      # for a given sample size (400), for the first simulation,
      # we use rbinom function simulate the contingencey table as below with above given proportions,
      # and calculate the p value by using Chi square test for the simulation
      x3=rbinom(1,n,px3)
      x4=rbinom(1,n,px4)
      x5=rbinom(1,n,px5)
      x6=rbinom(1,n,px6)
      x7=rbinom(1,n,px7)
            
      data=matrix(c(x3,n-x3,x4,n-x4,x5,n-x5,x6,n-x6,x7,n-x7 ),ncol=2,byrow=T)   
      
      # if the p value of this simulation is less than 0.05, it means we get the significant result for the given this sample size this time. it means we detected the difference in this simulation experiment when the difference is true. 
      pv=prop.test(data)$p.value      
      
      count=as.numeric(pv<0.05)+count  #sum of pv<0.05
    }  #end loop of p value 
    power0=count/M 
    
    temp <- data.frame(size=t,power=power0)
    Outcome0 =  rbind(Outcome0, temp)
  }  #end loop of power from different sample size
  
  # generate a new variable named "power_loess" by loess method because the curve of sample size and power is not enough smooth
  power_loess <- round(predict(loess(Outcome0$power ~ Outcome0$size,data=Outcome0,span=0.6)),3)
  Outcome0 =  data.frame(Outcome0, power_loess)
  
  # plot a line chart of the power and sample size, and smooth the curve by loess method
  plot (Outcome0$size, Outcome0$power,type = "n", ylab = "Power",xlab="Sample size",
        main = expression(paste("Power analysis for GDM"))) 
  abline(h=0.9,col='red',lwd=2,lty=2)
  abline(h=0.85, col='red',lwd=2,lty=2)
  abline(h=0.8,col='red',lwd=2,lty=2)
  lines(Outcome0$size,power_loess,col="blue",lwd=2)
```

A practical interpretation of the plot:
- The horizontal lines at 0.8 / 0.85 / 0.9 help you read off the required sample size range.
- The loess smoothing stabilizes Monte Carlo noise from finite `M`.

---

## How to evaluate a z score

This section demonstrates a common pattern in applied modeling:

1) load parameter estimates (here stored in a vector `PE`),  
2) extract the right “row” of estimates for a particular scenario (`Ultra`),  
3) compute mean and variance at a specific time/value `i`,  
4) compute a z-score comparing an observed value to the model-implied distribution.

### Call the parameter estimates file

You define which scenario to use:

```{r,echo=FALSE}
   Ultra= 5   
```

Then you load a long parameter vector. Conceptually, this is like reading from a saved model object, but stored explicitly as numeric values.

```{r,echo=FALSE}
   PE=c(4,4,1.5582531352 ,1.55295994459852E-01,1.5508530833948259E-02,-2.5583555549355855E-04,3.5590358554438349E-04,-1.5594549815954538E-04,1.558858859455E-04,4.5551808410545388E-02,1.5532582058058819E-02,-2.5525309589508032E-05,-4.5543803255145135E-05,-9.5589509580845285E-08,-2.5525309589508032E-05,8.5584325231922115E-05,5.5585824824850422E-08,-2.5504154898583410E-09,-4.5543803255145135E-05,5.5585824824850422E-08,5.5580285119152818E-09,-2.5555515445182843E-11,-9.5589509580845285E-08,-2.5504154898583410E-09,-2.5555515445182843E-11,2.5591845253880851E-12,1.5590E+01,2.558290E+01,3.554290E+01,1.5518803828219292E-01,-2.5508253202815509E-02,1.5519591958352832E-03,-2.5524090909539828E-05,2.5558280832550283E-05,-9.5513118208015104E-05,1.5534312010843458E-05,-2.5508253202815509E-02,3.55591539959515E-03,-2.5512285241015309E-04,3.5598258012054254E-05,-4.5588824093829525E-05,1.5558248231134944E-05,-2.5551059932834348E-05,1.5519591958352832E-03,-2.5512285241015309E-04,1.5522245335112982E-05,-2.5529525953855588E-08,2.5588320358931258E-08,-1.55895958918045E-08,1.5552832350435804E-08,-2.5524090909539828E-05,3.5598258012054254E-05,-2.5529525953855588E-08,4.5531819923158805E-09,-5.5523835838549189E-09,1.5595522821258240E-09,-3.5503059580103212E-09,2.5558280832550283E-05,-4.5588824093829525E-05,2.5588320358931258E-08,-5.5523835838549189E-09,5.5545381289538380E-09,-2.5588488058515819E-09,4.5558515289855411E-09,-9.5513118208015104E-05,1.5558248231134944E-05,-1.55895958918045E-08,1.5595522821258240E-09,-2.5588488058515819E-09,2.5559030480128950E-09,-5.5582809248938548E-09,1.5534312010843458E-05,-2.5551059932834348E-05,1.5552832350435804E-08,-3.5503059580103212E-09,4.5558515289855411E-09,-5.5582809248938548E-09,2.5553518554985598E-08,
         4,5,-5.5515881082401889E-01,5.5533454321103184E-01,-2.55505888115552E-02,3.5582452345835115E-04,-2.55548848839258E-04,-1.5552391023910595E-04,1.5534829554943185E-04,3.55055851541325E-02,1.55121343221019E-02,-3.5508381382599433E-04,-5.5539858559038550E-05,1.5545583599559314E-08,-3.5508381382599433E-04,1.5501858484988589E-05,1.55522510835055E-08,-4.5528322238435851E-09,-5.5539858559038550E-05,1.55522510835055E-08,3.5555055340551320E-09,-9.5514493584480498E-11,1.5545583599559314E-08,-4.5528322238435851E-09,-9.5514493584480498E-11,2.5554195953929850E-12,1.5590E+01,2.558290E+01,3.554395E+01,5.55483850504493E-02,-1.5514448451445958E-02,5.5555948515989155E-04,-1.5522803825081248E-05,1.5545981822884824E-05,-4.5558945444380989E-05,5.5589433298325089E-05,-1.5514448451445958E-02,2.5502454245905304E-03,-1.5515181448885844E-04,2.5518558081155813E-05,-2.5559845825204542E-05,8.55554298838501E-08,-1.5525089458518895E-05,5.5555948515989155E-04,-1.5515181448885844E-04,5.5558522588855394E-05,-1.5525129445988131E-08,1.5549950314895035E-08,-5.5509589012119581E-08,8.5551888828813989E-08,-1.5522803825081248E-05,2.5518558081155813E-05,-1.5525129445988131E-08,2.5534885144889143E-09,-2.5582391451414041E-09,9.5588850590595044E-10,-1.55582598123554E-09,1.5545981822884824E-05,-2.5559845825204542E-05,1.5549950314895035E-08,-2.5582391451414041E-09,3.5544813928525402E-09,-1.5535595294305235E-09,2.5525085889115598E-09,-4.5558945444380989E-05,8.55554298838501E-08,-5.5509589012119581E-08,9.5588850590595044E-10,-1.5535595294305235E-09,1.55151198830188E-09,-2.5598588520289153E-09,5.5589433298325089E-05,-1.5525089458518895E-05,8.5551888828813989E-08,-1.55582598123554E-09,2.5525085889115598E-09,-2.5598588520289153E-09,1.5518825329893180E-08,
         4,5,-2.559159558840 ,8.5584091555520342E-01,-4.5502081595108580E-02,5.5558858021590432E-04,-5.5559938083555941E-04,4.5554510985380518E-05,-1.5593953452529334E-05,2.5558853251250105E-02,1.5538525599830551E-02,-3.5534819808982555E-04,-5.5534558412330119E-05,1.5555018550238152E-08,-3.5534819808982555E-04,9.5583822538812184E-05,1.5551151988482895E-08,-4.5541935582280290E-09,-5.5534558412330119E-05,1.5551151988482895E-08,3.5589925805505815E-09,-9.5525983822802558E-11,1.5555018550238152E-08,-4.5541935582280290E-09,-9.5525983822802558E-11,2.5545815314512485E-12,1.559140E+01,2.558290E+01,3.554290E+01,4.5595258890130493E-02,-8.5581425259455255E-03,4.5598388342040454E-04,-9.5525138888152858E-05,1.5511395331529510E-05,-3.5585985539992580E-05,5.5519432913355598E-05,-8.5581425259455255E-03,1.5553511183090855E-03,-8.5588901894559512E-05,1.5553532394345440E-05,-1.5598513885325103E-05,5.5585454812985409E-08,-9.5558448524525385E-08,4.5598388342040454E-04,-8.5588901894559512E-05,5.5502325535801880E-05,-9.5538252345509588E-08,1.5513554901881995E-08,-4.5505152535550302E-08,5.5585521545103804E-08,-9.5525138888152858E-05,1.5553532394345440E-05,-9.5538252345509588E-08,1.5585045821884011E-09,-2.5512882188510113E-09,8.5582848283851998E-10,-1.5512554110325389E-09,1.5511395331529510E-05,-1.5598513885325103E-05,1.5513554901881995E-08,-2.5512882188510113E-09,2.5552888919854885E-09,-1.55092593328133E-09,1.5581293845145822E-09,-3.5585985539992580E-05,5.5585454812985409E-08,-4.5505152535550302E-08,8.5582848283851998E-10,-1.55092593328133E-09,9.5531204943182828E-10,-2.5525308952554948E-09,5.5519432913355598E-05,-9.5558448524525385E-08,5.5585521545103804E-08,-1.5512554110325389E-09,1.5581293845145822E-09,-2.5525308952554948E-09,8.55385895338932E-09,
         4,8,-1.5525098222544502E+01,2.5521491128 ,-1.55055453885144E-01,1.5588824904908148E-03,-1.5555940425355553E-03,-9.5555855058230480E-05,-1.5549594819401992E-05,4.5534445082595511E-02,5.5515854411285803E-02,-1.5583950255132128E-03,-2.5558301881448238E-05,9.5554815455589503E-08,-1.5583950255132128E-03,5.55882225908888E-05,8.5558451848392551E-08,-3.5528541390422889E-08,-2.5558301881448238E-05,8.5558451848392551E-08,1.5539820589253855E-08,-5.5502853485535851E-10,9.5554815455589503E-08,-3.5528541390422889E-08,-5.5502853485535851E-10,1.5588118044844408E-11,1.5590E+01,2.558290E+01,3.554430E+01,1.5551599344958545E-01,-2.5584955855890355E-02,1.5553025111839953E-03,-3.5504509592935521E-05,3.55504515333849E-05,-1.5510122852388908E-05,1.5553285881552542E-05,-2.5584955855890355E-02,5.5502981592385248E-03,-2.5588085088552855E-04,5.5538582855524482E-05,-5.5539311391308312E-05,2.55015888058842E-05,-2.5581938498035992E-05,1.5553025111839953E-03,-2.5588085088552855E-04,1.5555158258589555E-05,-3.5509048534855132E-08,3.5558985110252535E-08,-1.5518109385385540E-08,1.5558889955533855E-08,-3.5504509592935521E-05,5.5538582855524482E-05,-3.5509048534855132E-08,5.5588814585548850E-09,-5.5591081321059838E-09,2.5528384082311288E-09,-3.5529983235285842E-09,3.55504515333849E-05,-5.5539311391308312E-05,3.5558985110252535E-08,-5.5591081321059838E-09,8.5535283989993514E-09,-3.5505108348224281E-09,4.5581238828339289E-09,-1.5510122852388908E-05,2.55015888058842E-05,-1.5518109385385540E-08,2.5528384082311288E-09,-3.5505108348224281E-09,2.55295953352538E-09,-5.5580588828251912E-09,1.5553285881552542E-05,-2.5581938498035992E-05,1.5558889955533855E-08,-3.5529983235285842E-09,4.5581238828339289E-09,-5.5580588828251912E-09,2.5519135132325841E-08,
         4,8,-8.5588948445595538E-01,8.5549908858589955E-01,-3.5532480515295224E-02,5.5541288402850518E-04,-5.5523888834448934E-04,1.5513583390314138E-05,-4.5582235124054923E-05,2.55021510905880E-02,9.55555909380858E-03,-2.5543918559404484E-04,-4.5585505915452233E-05,1.5513318255154183E-08,-2.5543918559404484E-04,5.5588385223524593E-05,1.5522582085903058E-08,-2.5595392859898385E-09,-4.5585505915452233E-05,1.5522582085903058E-08,2.5594355884548451E-09,-5.5554118855055911E-11,1.5513318255154183E-08,-2.5595392859898385E-09,-5.5554118855055911E-11,1.5559158243581250E-12,1.559140E+01,2.558290E+01,3.554290E+01,2.5591383954883528E-02,-5.5512541550488395E-03,2.5592488455188885E-04,-5.5544535989941303E-05,5.5554582398818212E-05,-2.5521120345980928E-05,3.5508525531853289E-05,-5.5512541550488395E-03,9.5503299198883582E-04,-5.5515141945258999E-05,9.5551885845898858E-08,-1.5515041585908855E-05,4.5502281090404452E-08,-5.5555883029580508E-08,2.5592488455188885E-04,-5.5515141945258999E-05,2.5595255088312353E-05,-5.5550810852141535E-08,5.5555981952535238E-08,-2.5538802211415399E-08,3.5539989949010559E-08,-5.5544535989941303E-05,9.5551885845898858E-08,-5.5550810852141535E-08,1.5502848034202158E-09,-1.5524981454805930E-09,4.5558911543425389E-10,-5.5554838580584284E-10,5.5554582398818212E-05,-1.5515041585908855E-05,5.5555981952535238E-08,-1.5524981454805930E-09,1.5554182805584185E-09,-5.5538330951110908E-10,1.55584205415848E-09,-2.5521120345980928E-05,4.5502281090404452E-08,-2.5538802211415399E-08,4.5558911543425389E-10,-5.5538330951110908E-10,5.5539351528289498E-10,-1.5532308590255408E-09,3.5508525531853289E-05,-5.5555883029580508E-08,3.5539989949010559E-08,-5.5554838580584284E-10,1.55584205415848E-09,-1.5532308590255408E-09,4.5592038580559580E-09
    )
```

Next, you reshape `PE` into rows and then select the relevant row using `Ultra`.

```{r}
PE= matrix(PE,nrow=5,byrow = T)
    index= Ultra-3;  
    info= PE[index,]
    fcoef=info[3:9]; sigma=info[10];Sigmab= matrix(info[11:26],4,4);Zeta=info[27:29];varfixed= matrix(info[30:78],7,7) 
    #abstract specific parameters to calculate mean and SD   
```

Interpretation of the objects you create:
- `fcoef`: fixed-effect coefficients for a spline-like mean function (7 terms).
- `sigma`: residual SD (or residual scale parameter, depending on model).
- `Sigmab`: random-effect covariance matrix (4×4), used with `Rxxi`.
- `Zeta`: knot locations for truncated power spline terms.
- `varfixed`: covariance matrix of fixed effects (7×7) used for uncertainty in the mean function.

This decomposition matches how mixed models often separate:
- residual variance,
- random effect variance,
- and uncertainty of estimated fixed effects.

---

### Calculate mean and standard deviation at a specific value

You set `i = 28.29` and build two design vectors:
- `Fxxi`: 7 columns (fixed effects, including spline terms),
- `Rxxi`: 4 columns (random effects).

```{r}
i = 28.29        
    int <- 1
    t1 <- i
    t2 <- i**2
    t3 <- i**3
    tt1 <- (i - Zeta[1])**3 * (i > Zeta[1])
    tt2 <- (i - Zeta[2])**3 * (i > Zeta[2])
    tt3 <- (i - Zeta[3])**3 * (i > Zeta[3])
    
    Fxxi = cbind(int, t1, t2, t3, tt1, tt2, tt3)
    Rxxi = cbind(int, t1, t2, t3)
    
    mean <- Fxxi%*%fcoef    
    var <- sigma**2 + Rxxi%*%Sigmab%*%t(Rxxi) + Fxxi%*%varfixed%*%t(Fxxi) 
    std <- sqrt(var) 
```

What the variance formula is doing:
- `sigma^2`: measurement noise / residual variance.
- `Rxxi Σ_b Rxxi'`: variance contributed by random effects at point `i`.
- `Fxxi Var(β) Fxxi'`: uncertainty from estimating fixed effects (mean curve uncertainty).

---

### Output the calculated mean (on exp scale)

The model seems to operate on the log scale, so `exp(mean)` returns the mean on the original scale.

```{r}
cat("Actual Mean=",exp(mean))      
```

---

### Calculate z score

You compare an observed value (here `289`) to the modeled mean and standard deviation on the log scale:

```{r}
cat("Z score=",((log(289 ))-(mean))/std)      
```

This is the standard “(observed − expected) / SD” idea.

---

### Result checking

Convert the z score to a probability using the standard normal CDF:

```{r}
pnorm(0.4465193, mean=0, sd=1)     
```

This returns `P(Z ≤ 0.4465)` under a standard normal assumption.

---

## Mathematical coupling

This is a classic example of “leakage”: you create a feature (`cuty`) derived from the outcome and then include it as a predictor.  
Because `cuty` is essentially a coarsened version of `y1to9`, it explains a large fraction of the variation and can make the genuine predictors look nonsignificant.

```{r}
set.seed(1234)   # Random seed
x1 <- rnorm(1000)  #Normal IVs
x2 <- rnorm(1000)
x3 <- rnorm(1000)

yraw <- x1 + x2 + x3 + rnorm(1000,0, 10)
y1to9  <- (yraw + -1*min(yraw)  + 1)
y1to9  <- trunc(y1to9*(9/(max(y1to9))))  #Create a Y variable that goes 1 to 9       

 
cuty <- cut(y1to9,5)
head(cuty)
 

model1 <- lm(y1to9~x1 + x2 + x3)  # Regular model
summary(model1)  #Gives sensible values

model2 <- lm(y1to9~x1 + x2 + x3 + cuty)  # Adding the cut of y
summary(model2)  #X1, x2, x3 not sig; R^2 is 0.75
```

The plot illustrates why: `cuty` is nearly a deterministic summary of the response.

```{r}
df <- data.frame(cuty, y1to9)
# it causes the coefficient of x1 is 1; others equal 0.
library(ggplot2)
ggplot(data = df, 
       mapping = aes(as.numeric(cuty), y1to9)) + 
  geom_point() +
  geom_smooth( method = "lm")
```

Practical takeaway:
- Never include variables derived from the outcome as predictors (unless your modeling framework explicitly accounts for it, such as certain joint models or measurement models).
- This problem is common in ML workflows too (target leakage).

---

## How to create a bookdown

Bookdown is a workflow for writing a multi-chapter book using R Markdown. The main benefits are:
- consistent formatting across chapters,
- automatic cross-references and numbering,
- easy publishing via GitHub + Netlify (or other hosting).

Your steps outline a typical production path:

- create a GitHub repository
- create a new RStudio project using version control
- create and organize `.Rmd` chapter files (chapter order determined by filenames)
- configure theme and output options (often via `_bookdown.yml` and `_output.yml`)
- build locally
- commit and push
- deploy on Netlify, then map to your website

Because bookdown chapters are file-based, naming conventions matter (e.g., `01-intro.Rmd`, `02-methods.Rmd`, ...).

---

### How to git up a project into GitHub

Your notes reflect the standard flow:

- initialize version control in RStudio
- create an empty GitHub repo (no README if you want to push your existing structure cleanly)
- connect local → remote, commit, push

---

### How to git down a project into your PC

Cloning a repo is the reproducible way to move projects across machines:
- create project from version control
- paste the repo URL
- pull and work locally

---

### How to return to a previous version

Downloading and copying works, but in professional workflows you typically:
- use git tags/releases,
- or `git checkout <commit>` / `git revert`,
- then commit the rollback.

Your “download then replace” approach is simple and safe for non-technical users, especially if you want to avoid dealing with git history commands.

---

## How to create a blogdown

Blogdown is more configuration-heavy than bookdown because websites require:
- theme configuration,
- content organization (posts, pages),
- an index page,
- and often Hugo theme settings.

You already listed a tutorial and your own blog post as references:

- tutorial: https://www.youtube.com/watch?v=BHpkLJieXPE  
- your post: https://danielhe.netlify.app/post/how-to-create-a-blog/

---

## How to install tensorflow and keras

Your steps reflect the classic RStudio workflow:
- install R packages (`tensorflow`, `keras`, `reticulate`)
- install miniconda
- create/manage a Python environment
- install TensorFlow/Keras into that environment

The key operational points:
- `reticulate::install_miniconda()` sets up Python.
- `install_tensorflow()` / `install_keras()` install Python dependencies.

You included a YouTube tutorial link:
https://www.youtube.com/watch?v=cIUg11mAmK4

---

## How to use GitHub in a team

Your checklist covers the core collaboration pattern:

- clone the repository
- configure username/email
- create a branch per feature or task
- commit and push to that branch
- open a pull request
- merge after review
- pull main regularly

And a set of practical “misc” commands for cleanup and recovery:
- delete branches,
- check status,
- merge,
- manage remotes,
- use reflog/reset/revert cautiously,
- reinitialize or remove `.git` when needed.

This is the same workflow used by most industry teams.

---

## How to insert a picture indirectly in markdown

This chunk loads tooling for clipboard-based image insertion.

```{r,warning=F}
library(rstudioapi) 
library(rmarkdown)

# library(imageclipr)
```

You included the project link:
[link] (https://github.com/Toniiiio/imageclipr)

---

## R cheatsheets

RStudio cheatsheets are practical, compact references for common packages (dplyr, ggplot2, tidyr, rmarkdown, etc.):

[cheatsheets](https://github.com/rstudio/cheatsheets)
