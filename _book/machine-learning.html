<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Machine learning | Biostatistics Handbook</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Machine learning | Biostatistics Handbook" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Machine learning | Biostatistics Handbook" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="Daniel He" />


<meta name="date" content="2026-02-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-wrangling.html"/>
<link rel="next" href="deep-learning.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/table1-1.0/table1_defaults.css" rel="stylesheet" />


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biostatistics Handbook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>1</b> Data Wrangling</a>
<ul>
<li class="chapter" data-level="1.1" data-path="data-wrangling.html"><a href="data-wrangling.html#how-to-do-data-wrangling"><i class="fa fa-check"></i><b>1.1</b> How to do data wrangling</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="data-wrangling.html"><a href="data-wrangling.html#load-data-and-package"><i class="fa fa-check"></i><b>1.1.1</b> Load data and package</a></li>
<li class="chapter" data-level="1.1.2" data-path="data-wrangling.html"><a href="data-wrangling.html#select-certain-rows"><i class="fa fa-check"></i><b>1.1.2</b> Select certain rows</a></li>
<li class="chapter" data-level="1.1.3" data-path="data-wrangling.html"><a href="data-wrangling.html#select-certain-columns"><i class="fa fa-check"></i><b>1.1.3</b> Select certain columns</a></li>
<li class="chapter" data-level="1.1.4" data-path="data-wrangling.html"><a href="data-wrangling.html#rename-variables"><i class="fa fa-check"></i><b>1.1.4</b> Rename variables</a></li>
<li class="chapter" data-level="1.1.5" data-path="data-wrangling.html"><a href="data-wrangling.html#sorting-in-ascending-or-descending-order"><i class="fa fa-check"></i><b>1.1.5</b> Sorting in ascending or descending order</a></li>
<li class="chapter" data-level="1.1.6" data-path="data-wrangling.html"><a href="data-wrangling.html#transform-variables"><i class="fa fa-check"></i><b>1.1.6</b> Transform variables</a></li>
<li class="chapter" data-level="1.1.7" data-path="data-wrangling.html"><a href="data-wrangling.html#working-with-pipes"><i class="fa fa-check"></i><b>1.1.7</b> Working with pipes %&gt;%</a></li>
<li class="chapter" data-level="1.1.8" data-path="data-wrangling.html"><a href="data-wrangling.html#pivot-wider-long-to-wide"><i class="fa fa-check"></i><b>1.1.8</b> Pivot wider (long to wide)</a></li>
<li class="chapter" data-level="1.1.9" data-path="data-wrangling.html"><a href="data-wrangling.html#pivot-longer-wide-to-long"><i class="fa fa-check"></i><b>1.1.9</b> Pivot longer (wide to long)</a></li>
<li class="chapter" data-level="1.1.10" data-path="data-wrangling.html"><a href="data-wrangling.html#separate-columns"><i class="fa fa-check"></i><b>1.1.10</b> Separate columns</a></li>
<li class="chapter" data-level="1.1.11" data-path="data-wrangling.html"><a href="data-wrangling.html#recoderelabel-data"><i class="fa fa-check"></i><b>1.1.11</b> Recode/relabel data</a></li>
<li class="chapter" data-level="1.1.12" data-path="data-wrangling.html"><a href="data-wrangling.html#deduplication"><i class="fa fa-check"></i><b>1.1.12</b> deduplication</a></li>
<li class="chapter" data-level="1.1.13" data-path="data-wrangling.html"><a href="data-wrangling.html#combine-data-sets"><i class="fa fa-check"></i><b>1.1.13</b> Combine data sets</a></li>
<li class="chapter" data-level="1.1.14" data-path="data-wrangling.html"><a href="data-wrangling.html#working-with-character-strings"><i class="fa fa-check"></i><b>1.1.14</b> Working with character strings</a></li>
<li class="chapter" data-level="1.1.15" data-path="data-wrangling.html"><a href="data-wrangling.html#conditional-operations"><i class="fa fa-check"></i><b>1.1.15</b> Conditional operations</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="data-wrangling.html"><a href="data-wrangling.html#how-to-do-aggregation-summarization"><i class="fa fa-check"></i><b>1.2</b> How to do aggregation/ summarization</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="data-wrangling.html"><a href="data-wrangling.html#summarization-after-grouping"><i class="fa fa-check"></i><b>1.2.1</b> Summarization after grouping</a></li>
<li class="chapter" data-level="1.2.2" data-path="data-wrangling.html"><a href="data-wrangling.html#summarization-with-upgroup"><i class="fa fa-check"></i><b>1.2.2</b> Summarization with upgroup</a></li>
<li class="chapter" data-level="1.2.3" data-path="data-wrangling.html"><a href="data-wrangling.html#mutate-new-variables-after-grouping"><i class="fa fa-check"></i><b>1.2.3</b> Mutate new variables after grouping</a></li>
<li class="chapter" data-level="1.2.4" data-path="data-wrangling.html"><a href="data-wrangling.html#recode-and-generate-new-variables-then-value-label"><i class="fa fa-check"></i><b>1.2.4</b> Recode and generate new variables, then value label</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="data-wrangling.html"><a href="data-wrangling.html#how-to-creat-table-1-with-test"><i class="fa fa-check"></i><b>1.3</b> How to creat table 1 with test</a></li>
<li class="chapter" data-level="1.4" data-path="data-wrangling.html"><a href="data-wrangling.html#imputing-missing-data-with-mice"><i class="fa fa-check"></i><b>1.4</b> Imputing Missing Data with MICE</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>2</b> Machine learning</a></li>
<li class="chapter" data-level="3" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>3</b> Deep learning</a></li>
<li class="chapter" data-level="4" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>4</b> Data visualization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="data-visualization.html"><a href="data-visualization.html#data-visualization-introduction"><i class="fa fa-check"></i><b>4.1</b> Data visualization introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="data-visualization.html"><a href="data-visualization.html#summarization"><i class="fa fa-check"></i><b>4.1.1</b> Summarization</a></li>
<li class="chapter" data-level="4.1.2" data-path="data-visualization.html"><a href="data-visualization.html#explore-missing-values"><i class="fa fa-check"></i><b>4.1.2</b> Explore missing values</a></li>
<li class="chapter" data-level="4.1.3" data-path="data-visualization.html"><a href="data-visualization.html#add-statistical-test"><i class="fa fa-check"></i><b>4.1.3</b> Add statistical test</a></li>
<li class="chapter" data-level="4.1.4" data-path="data-visualization.html"><a href="data-visualization.html#add-texts-to-dots"><i class="fa fa-check"></i><b>4.1.4</b> Add texts to dots</a></li>
<li class="chapter" data-level="4.1.5" data-path="data-visualization.html"><a href="data-visualization.html#set-the-legend"><i class="fa fa-check"></i><b>4.1.5</b> Set the legend</a></li>
<li class="chapter" data-level="4.1.6" data-path="data-visualization.html"><a href="data-visualization.html#create-a-panel-of-plots"><i class="fa fa-check"></i><b>4.1.6</b> Create a panel of plots</a></li>
<li class="chapter" data-level="4.1.7" data-path="data-visualization.html"><a href="data-visualization.html#plots-in-regression"><i class="fa fa-check"></i><b>4.1.7</b> Plots in regression</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="data-visualization.html"><a href="data-visualization.html#scatter-plot"><i class="fa fa-check"></i><b>4.2</b> Scatter plot</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="data-visualization.html"><a href="data-visualization.html#create-a-empty-canvas"><i class="fa fa-check"></i><b>4.2.1</b> Create a empty canvas</a></li>
<li class="chapter" data-level="4.2.2" data-path="data-visualization.html"><a href="data-visualization.html#add-a-layergeom-of-points-to-the-canvas"><i class="fa fa-check"></i><b>4.2.2</b> Add a layer/geom of <code>points</code> to the canvas</a></li>
<li class="chapter" data-level="4.2.3" data-path="data-visualization.html"><a href="data-visualization.html#add-another-aesthetic"><i class="fa fa-check"></i><b>4.2.3</b> Add another aesthetic</a></li>
<li class="chapter" data-level="4.2.4" data-path="data-visualization.html"><a href="data-visualization.html#add-other-aesthetic"><i class="fa fa-check"></i><b>4.2.4</b> Add other aesthetic</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="data-visualization.html"><a href="data-visualization.html#bar-chart"><i class="fa fa-check"></i><b>4.3</b> Bar chart</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="data-visualization.html"><a href="data-visualization.html#add-some-options-for-the-whole-ggplot-rather-than-layers"><i class="fa fa-check"></i><b>4.3.1</b> Add some options for the whole ggplot rather than layers</a></li>
<li class="chapter" data-level="4.3.2" data-path="data-visualization.html"><a href="data-visualization.html#grouped-bar-chart"><i class="fa fa-check"></i><b>4.3.2</b> Grouped bar chart</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="data-visualization.html"><a href="data-visualization.html#line-charts"><i class="fa fa-check"></i><b>4.4</b> Line charts</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="data-visualization.html"><a href="data-visualization.html#grouped-by-colour-variable"><i class="fa fa-check"></i><b>4.4.1</b> Grouped by <code>colour variable</code></a></li>
<li class="chapter" data-level="4.4.2" data-path="data-visualization.html"><a href="data-visualization.html#multiple-aesthetics"><i class="fa fa-check"></i><b>4.4.2</b> Multiple aesthetics</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="data-visualization.html"><a href="data-visualization.html#ggplot2-parameters"><i class="fa fa-check"></i><b>4.5</b> ggplot2 parameters</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="data-visualization.html"><a href="data-visualization.html#components-of-plot"><i class="fa fa-check"></i><b>4.5.1</b> Components of plot</a></li>
<li class="chapter" data-level="4.5.2" data-path="data-visualization.html"><a href="data-visualization.html#create-main-title-axis-labels-caption"><i class="fa fa-check"></i><b>4.5.2</b> Create main title, axis labels, caption</a></li>
<li class="chapter" data-level="4.5.3" data-path="data-visualization.html"><a href="data-visualization.html#create-legend-title-position"><i class="fa fa-check"></i><b>4.5.3</b> Create legend title, position</a></li>
<li class="chapter" data-level="4.5.4" data-path="data-visualization.html"><a href="data-visualization.html#change-plot-colors"><i class="fa fa-check"></i><b>4.5.4</b> Change plot colors</a></li>
<li class="chapter" data-level="4.5.5" data-path="data-visualization.html"><a href="data-visualization.html#change-points-shapes-transparent-and-size"><i class="fa fa-check"></i><b>4.5.5</b> Change points shapes, transparent and size</a></li>
<li class="chapter" data-level="4.5.6" data-path="data-visualization.html"><a href="data-visualization.html#change-bars-position"><i class="fa fa-check"></i><b>4.5.6</b> Change bars position</a></li>
<li class="chapter" data-level="4.5.7" data-path="data-visualization.html"><a href="data-visualization.html#add-text-annotations"><i class="fa fa-check"></i><b>4.5.7</b> Add text annotations</a></li>
<li class="chapter" data-level="4.5.8" data-path="data-visualization.html"><a href="data-visualization.html#add-a-line-that-separates-points"><i class="fa fa-check"></i><b>4.5.8</b> Add a line that (separates points)</a></li>
<li class="chapter" data-level="4.5.9" data-path="data-visualization.html"><a href="data-visualization.html#using-scale_-function"><i class="fa fa-check"></i><b>4.5.9</b> Using scale_ function</a></li>
<li class="chapter" data-level="4.5.10" data-path="data-visualization.html"><a href="data-visualization.html#change-coordinates"><i class="fa fa-check"></i><b>4.5.10</b> Change coordinates</a></li>
<li class="chapter" data-level="4.5.11" data-path="data-visualization.html"><a href="data-visualization.html#customize-axis-ticks"><i class="fa fa-check"></i><b>4.5.11</b> Customize axis ticks</a></li>
<li class="chapter" data-level="4.5.12" data-path="data-visualization.html"><a href="data-visualization.html#flip-and-reverse-plot"><i class="fa fa-check"></i><b>4.5.12</b> Flip and reverse plot</a></li>
<li class="chapter" data-level="4.5.13" data-path="data-visualization.html"><a href="data-visualization.html#create-stats"><i class="fa fa-check"></i><b>4.5.13</b> Create stats</a></li>
<li class="chapter" data-level="4.5.14" data-path="data-visualization.html"><a href="data-visualization.html#facets"><i class="fa fa-check"></i><b>4.5.14</b> Facets</a></li>
<li class="chapter" data-level="4.5.15" data-path="data-visualization.html"><a href="data-visualization.html#theme"><i class="fa fa-check"></i><b>4.5.15</b> Theme</a></li>
<li class="chapter" data-level="4.5.16" data-path="data-visualization.html"><a href="data-visualization.html#how-to-setup-subscripts-or-superscripts"><i class="fa fa-check"></i><b>4.5.16</b> How to setup subscripts or superscripts</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="data-visualization.html"><a href="data-visualization.html#how-to-create-advanced-plots"><i class="fa fa-check"></i><b>4.6</b> How to create advanced plots</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="basic-statistics.html"><a href="basic-statistics.html"><i class="fa fa-check"></i><b>5</b> Basic statistics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="basic-statistics.html"><a href="basic-statistics.html#the-essentials-of-r"><i class="fa fa-check"></i><b>5.1</b> The essentials of R</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="basic-statistics.html"><a href="basic-statistics.html#manipulation-of-vector"><i class="fa fa-check"></i><b>5.1.1</b> Manipulation of vector</a></li>
<li class="chapter" data-level="5.1.2" data-path="basic-statistics.html"><a href="basic-statistics.html#generate-sequence-or-repeted-sequece"><i class="fa fa-check"></i><b>5.1.2</b> Generate sequence or repeted sequece</a></li>
<li class="chapter" data-level="5.1.3" data-path="basic-statistics.html"><a href="basic-statistics.html#get-directory-and-write-data-out-and-in"><i class="fa fa-check"></i><b>5.1.3</b> Get directory and write data out and in</a></li>
<li class="chapter" data-level="5.1.4" data-path="basic-statistics.html"><a href="basic-statistics.html#function"><i class="fa fa-check"></i><b>5.1.4</b> Function</a></li>
<li class="chapter" data-level="5.1.5" data-path="basic-statistics.html"><a href="basic-statistics.html#plot"><i class="fa fa-check"></i><b>5.1.5</b> Plot</a></li>
<li class="chapter" data-level="5.1.6" data-path="basic-statistics.html"><a href="basic-statistics.html#build-model-and-plot"><i class="fa fa-check"></i><b>5.1.6</b> Build model and plot</a></li>
<li class="chapter" data-level="5.1.7" data-path="basic-statistics.html"><a href="basic-statistics.html#rename-names-of-columns"><i class="fa fa-check"></i><b>5.1.7</b> Rename names of columns</a></li>
<li class="chapter" data-level="5.1.8" data-path="basic-statistics.html"><a href="basic-statistics.html#class-of-dataframe"><i class="fa fa-check"></i><b>5.1.8</b> Class of dataframe</a></li>
<li class="chapter" data-level="5.1.9" data-path="basic-statistics.html"><a href="basic-statistics.html#generate-new-variable-for-dataframe-character"><i class="fa fa-check"></i><b>5.1.9</b> Generate new variable for dataframe (character)</a></li>
<li class="chapter" data-level="5.1.10" data-path="basic-statistics.html"><a href="basic-statistics.html#create-a-new-dataframe-using-rnorm---random-number-from-distribution"><i class="fa fa-check"></i><b>5.1.10</b> Create a new dataframe using ‘rnorm’ - random number from distribution</a></li>
<li class="chapter" data-level="5.1.11" data-path="basic-statistics.html"><a href="basic-statistics.html#left-join-two-dataframes"><i class="fa fa-check"></i><b>5.1.11</b> Left join two dataframes</a></li>
<li class="chapter" data-level="5.1.12" data-path="basic-statistics.html"><a href="basic-statistics.html#select-variables"><i class="fa fa-check"></i><b>5.1.12</b> Select variables</a></li>
<li class="chapter" data-level="5.1.13" data-path="basic-statistics.html"><a href="basic-statistics.html#filter-observations"><i class="fa fa-check"></i><b>5.1.13</b> Filter observations</a></li>
<li class="chapter" data-level="5.1.14" data-path="basic-statistics.html"><a href="basic-statistics.html#append-rows"><i class="fa fa-check"></i><b>5.1.14</b> Append rows</a></li>
<li class="chapter" data-level="5.1.15" data-path="basic-statistics.html"><a href="basic-statistics.html#create-new-variables-instead-of-old-variables"><i class="fa fa-check"></i><b>5.1.15</b> Create new variables instead of old variables</a></li>
<li class="chapter" data-level="5.1.16" data-path="basic-statistics.html"><a href="basic-statistics.html#summarise-statistics"><i class="fa fa-check"></i><b>5.1.16</b> summarise statistics</a></li>
<li class="chapter" data-level="5.1.17" data-path="basic-statistics.html"><a href="basic-statistics.html#group-dataframe-then-summarise-statistics"><i class="fa fa-check"></i><b>5.1.17</b> Group dataframe then summarise statistics</a></li>
<li class="chapter" data-level="5.1.18" data-path="basic-statistics.html"><a href="basic-statistics.html#ungroup-then-summarise-statistics"><i class="fa fa-check"></i><b>5.1.18</b> Ungroup then summarise statistics</a></li>
<li class="chapter" data-level="5.1.19" data-path="basic-statistics.html"><a href="basic-statistics.html#summary-linear-regression-model"><i class="fa fa-check"></i><b>5.1.19</b> Summary linear regression model</a></li>
<li class="chapter" data-level="5.1.20" data-path="basic-statistics.html"><a href="basic-statistics.html#create-frequency-table"><i class="fa fa-check"></i><b>5.1.20</b> Create frequency table</a></li>
<li class="chapter" data-level="5.1.21" data-path="basic-statistics.html"><a href="basic-statistics.html#value-and-variable-label"><i class="fa fa-check"></i><b>5.1.21</b> Value and variable label</a></li>
<li class="chapter" data-level="5.1.22" data-path="basic-statistics.html"><a href="basic-statistics.html#recode-a-variable"><i class="fa fa-check"></i><b>5.1.22</b> Recode a variable</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="basic-statistics.html"><a href="basic-statistics.html#central-limit-theorem"><i class="fa fa-check"></i><b>5.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="5.3" data-path="basic-statistics.html"><a href="basic-statistics.html#common-statistical-distribution"><i class="fa fa-check"></i><b>5.3</b> Common statistical distribution</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistical-models.html"><a href="statistical-models.html"><i class="fa fa-check"></i><b>6</b> Statistical models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="statistical-models.html"><a href="statistical-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.1</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="statistical-models.html"><a href="statistical-models.html#linear-regeression-assumptions"><i class="fa fa-check"></i><b>6.1.1</b> Linear regeression assumptions</a></li>
<li class="chapter" data-level="6.1.2" data-path="statistical-models.html"><a href="statistical-models.html#population-regression-function"><i class="fa fa-check"></i><b>6.1.2</b> Population regression function</a></li>
<li class="chapter" data-level="6.1.3" data-path="statistical-models.html"><a href="statistical-models.html#population-regression-model"><i class="fa fa-check"></i><b>6.1.3</b> Population regression model</a></li>
<li class="chapter" data-level="6.1.4" data-path="statistical-models.html"><a href="statistical-models.html#sample-regression-model"><i class="fa fa-check"></i><b>6.1.4</b> Sample regression model</a></li>
<li class="chapter" data-level="6.1.5" data-path="statistical-models.html"><a href="statistical-models.html#least-squares-minimize-qsum-y_i-haty_i2"><i class="fa fa-check"></i><b>6.1.5</b> Least squares: minimize <span class="math inline">\(Q=\sum (Y_i-\hat{Y}_i)^2\)</span></a></li>
<li class="chapter" data-level="6.1.6" data-path="statistical-models.html"><a href="statistical-models.html#solve-hatbeta_1hatbeta_2-and-variance"><i class="fa fa-check"></i><b>6.1.6</b> Solve <span class="math inline">\(\hat{\beta}_1,\hat{\beta}_2\)</span> and variance</a></li>
<li class="chapter" data-level="6.1.7" data-path="statistical-models.html"><a href="statistical-models.html#calculate-the-variance-hatsigma2-of-error-e_i"><i class="fa fa-check"></i><b>6.1.7</b> Calculate the variance <span class="math inline">\(\hat{\sigma}^2\)</span> of error <span class="math inline">\(e_i\)</span></a></li>
<li class="chapter" data-level="6.1.8" data-path="statistical-models.html"><a href="statistical-models.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>6.1.8</b> Sum of squares decomposition</a></li>
<li class="chapter" data-level="6.1.9" data-path="statistical-models.html"><a href="statistical-models.html#coefficient-of-determination-r2-and-goodness-of-fit"><i class="fa fa-check"></i><b>6.1.9</b> Coefficient of determination <span class="math inline">\(R^2\)</span> and goodness of fit</a></li>
<li class="chapter" data-level="6.1.10" data-path="statistical-models.html"><a href="statistical-models.html#test-of-regression-coefficients"><i class="fa fa-check"></i><b>6.1.10</b> Test of regression coefficients</a></li>
<li class="chapter" data-level="6.1.11" data-path="statistical-models.html"><a href="statistical-models.html#statistical-test-of-model"><i class="fa fa-check"></i><b>6.1.11</b> Statistical test of model</a></li>
<li class="chapter" data-level="6.1.12" data-path="statistical-models.html"><a href="statistical-models.html#mean-prediction"><i class="fa fa-check"></i><b>6.1.12</b> Mean prediction</a></li>
<li class="chapter" data-level="6.1.13" data-path="statistical-models.html"><a href="statistical-models.html#individual-prediction"><i class="fa fa-check"></i><b>6.1.13</b> Individual prediction</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="statistical-models.html"><a href="statistical-models.html#multiple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Multiple linear regression</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="statistical-models.html"><a href="statistical-models.html#matrix-format"><i class="fa fa-check"></i><b>6.2.1</b> Matrix format</a></li>
<li class="chapter" data-level="6.2.2" data-path="statistical-models.html"><a href="statistical-models.html#variance-covariance-matrix-of-random-errors"><i class="fa fa-check"></i><b>6.2.2</b> Variance covariance matrix of random errors</a></li>
<li class="chapter" data-level="6.2.3" data-path="statistical-models.html"><a href="statistical-models.html#minimize-qsum-y-haty2"><i class="fa fa-check"></i><b>6.2.3</b> Minimize <span class="math inline">\(Q=\sum (y-\hat{y})^2\)</span></a></li>
<li class="chapter" data-level="6.2.4" data-path="statistical-models.html"><a href="statistical-models.html#solve-hatbeta-by-derivation"><i class="fa fa-check"></i><b>6.2.4</b> Solve <span class="math inline">\(\hat{\beta}\)</span> by derivation</a></li>
<li class="chapter" data-level="6.2.5" data-path="statistical-models.html"><a href="statistical-models.html#solve-vartext-covmathbfhatbeta"><i class="fa fa-check"></i><b>6.2.5</b> Solve <span class="math inline">\(var\text{-}cov(\mathbf{\hat{\beta}})\)</span></a></li>
<li class="chapter" data-level="6.2.6" data-path="statistical-models.html"><a href="statistical-models.html#solve-s2mathbfhatbeta-sample"><i class="fa fa-check"></i><b>6.2.6</b> Solve <span class="math inline">\(S^2(\mathbf{\hat{\beta}})\)</span> (sample)</a></li>
<li class="chapter" data-level="6.2.7" data-path="statistical-models.html"><a href="statistical-models.html#sum-of-squares-decomposition-matrix-format"><i class="fa fa-check"></i><b>6.2.7</b> Sum of squares decomposition (matrix format)</a></li>
<li class="chapter" data-level="6.2.8" data-path="statistical-models.html"><a href="statistical-models.html#determination-coefficient-r2-and-goodness-of-fit"><i class="fa fa-check"></i><b>6.2.8</b> Determination coefficient <span class="math inline">\(R^2\)</span> and goodness of fit</a></li>
<li class="chapter" data-level="6.2.9" data-path="statistical-models.html"><a href="statistical-models.html#test-of-regression-coefficients-1"><i class="fa fa-check"></i><b>6.2.9</b> Test of regression coefficients</a></li>
<li class="chapter" data-level="6.2.10" data-path="statistical-models.html"><a href="statistical-models.html#test-of-model"><i class="fa fa-check"></i><b>6.2.10</b> Test of model</a></li>
<li class="chapter" data-level="6.2.11" data-path="statistical-models.html"><a href="statistical-models.html#mean-prediction-multiple-regression"><i class="fa fa-check"></i><b>6.2.11</b> Mean prediction (multiple regression)</a></li>
<li class="chapter" data-level="6.2.12" data-path="statistical-models.html"><a href="statistical-models.html#individual-prediction-multiple-regression"><i class="fa fa-check"></i><b>6.2.12</b> Individual prediction (multiple regression)</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="statistical-models.html"><a href="statistical-models.html#multiple-linear-regression-practice"><i class="fa fa-check"></i><b>6.3</b> Multiple linear regression practice</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="statistical-models.html"><a href="statistical-models.html#load-required-packages"><i class="fa fa-check"></i><b>6.3.1</b> Load required packages</a></li>
<li class="chapter" data-level="6.3.2" data-path="statistical-models.html"><a href="statistical-models.html#loading-and-describing-data"><i class="fa fa-check"></i><b>6.3.2</b> Loading and describing data</a></li>
<li class="chapter" data-level="6.3.3" data-path="statistical-models.html"><a href="statistical-models.html#create-table-1"><i class="fa fa-check"></i><b>6.3.3</b> Create table 1</a></li>
<li class="chapter" data-level="6.3.4" data-path="statistical-models.html"><a href="statistical-models.html#missingness-checking"><i class="fa fa-check"></i><b>6.3.4</b> Missingness checking</a></li>
<li class="chapter" data-level="6.3.5" data-path="statistical-models.html"><a href="statistical-models.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>6.3.5</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="6.3.6" data-path="statistical-models.html"><a href="statistical-models.html#transformations"><i class="fa fa-check"></i><b>6.3.6</b> Transformations</a></li>
<li class="chapter" data-level="6.3.7" data-path="statistical-models.html"><a href="statistical-models.html#check-linearity-between-y-and-x"><i class="fa fa-check"></i><b>6.3.7</b> Check linearity between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span></a></li>
<li class="chapter" data-level="6.3.8" data-path="statistical-models.html"><a href="statistical-models.html#data-imputation-and-normalization"><i class="fa fa-check"></i><b>6.3.8</b> Data imputation and normalization</a></li>
<li class="chapter" data-level="6.3.9" data-path="statistical-models.html"><a href="statistical-models.html#generate-dummy-variables"><i class="fa fa-check"></i><b>6.3.9</b> Generate dummy variables</a></li>
<li class="chapter" data-level="6.3.10" data-path="statistical-models.html"><a href="statistical-models.html#splitting-data-into-training-and-test-data"><i class="fa fa-check"></i><b>6.3.10</b> Splitting data into training and test data</a></li>
<li class="chapter" data-level="6.3.11" data-path="statistical-models.html"><a href="statistical-models.html#step-regression"><i class="fa fa-check"></i><b>6.3.11</b> Step regression</a></li>
<li class="chapter" data-level="6.3.12" data-path="statistical-models.html"><a href="statistical-models.html#create-a-model-after-selecting-variables"><i class="fa fa-check"></i><b>6.3.12</b> Create a model after selecting variables</a></li>
<li class="chapter" data-level="6.3.13" data-path="statistical-models.html"><a href="statistical-models.html#multicollinearity-checking"><i class="fa fa-check"></i><b>6.3.13</b> Multicollinearity checking</a></li>
<li class="chapter" data-level="6.3.14" data-path="statistical-models.html"><a href="statistical-models.html#plot-model-to-check-assumptions"><i class="fa fa-check"></i><b>6.3.14</b> Plot model to check assumptions</a></li>
<li class="chapter" data-level="6.3.15" data-path="statistical-models.html"><a href="statistical-models.html#add-polynomial-quadratic-terms"><i class="fa fa-check"></i><b>6.3.15</b> Add polynomial (quadratic) terms</a></li>
<li class="chapter" data-level="6.3.16" data-path="statistical-models.html"><a href="statistical-models.html#add-interaction-terms"><i class="fa fa-check"></i><b>6.3.16</b> Add interaction terms</a></li>
<li class="chapter" data-level="6.3.17" data-path="statistical-models.html"><a href="statistical-models.html#robust-regression"><i class="fa fa-check"></i><b>6.3.17</b> Robust regression</a></li>
<li class="chapter" data-level="6.3.18" data-path="statistical-models.html"><a href="statistical-models.html#create-a-model-before-transforming-data"><i class="fa fa-check"></i><b>6.3.18</b> Create a model before transforming data</a></li>
<li class="chapter" data-level="6.3.19" data-path="statistical-models.html"><a href="statistical-models.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.3.19</b> K-fold cross validation</a></li>
<li class="chapter" data-level="6.3.20" data-path="statistical-models.html"><a href="statistical-models.html#nonnest-models-comparisons"><i class="fa fa-check"></i><b>6.3.20</b> Nonnest models comparisons</a></li>
<li class="chapter" data-level="6.3.21" data-path="statistical-models.html"><a href="statistical-models.html#posterior-predictive-diagnostic-checks"><i class="fa fa-check"></i><b>6.3.21</b> Posterior predictive / diagnostic checks</a></li>
<li class="chapter" data-level="6.3.22" data-path="statistical-models.html"><a href="statistical-models.html#forest-plot-for-coefficients"><i class="fa fa-check"></i><b>6.3.22</b> Forest plot for coefficients</a></li>
<li class="chapter" data-level="6.3.23" data-path="statistical-models.html"><a href="statistical-models.html#relative-importance"><i class="fa fa-check"></i><b>6.3.23</b> Relative Importance</a></li>
<li class="chapter" data-level="6.3.24" data-path="statistical-models.html"><a href="statistical-models.html#model-prediction"><i class="fa fa-check"></i><b>6.3.24</b> Model prediction</a></li>
<li class="chapter" data-level="6.3.25" data-path="statistical-models.html"><a href="statistical-models.html#compare-predictions-vs-actual-values"><i class="fa fa-check"></i><b>6.3.25</b> Compare predictions vs actual values</a></li>
<li class="chapter" data-level="6.3.26" data-path="statistical-models.html"><a href="statistical-models.html#manual-computation-haty-and-confidence-interval"><i class="fa fa-check"></i><b>6.3.26</b> Manual computation: <span class="math inline">\(\hat{y}\)</span> and confidence interval</a></li>
<li class="chapter" data-level="6.3.27" data-path="statistical-models.html"><a href="statistical-models.html#external-data-validation"><i class="fa fa-check"></i><b>6.3.27</b> External data validation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="statistical-models.html"><a href="statistical-models.html#variable-selection"><i class="fa fa-check"></i><b>6.4</b> Variable selection</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="statistical-models.html"><a href="statistical-models.html#chapter-takeaways"><i class="fa fa-check"></i><b>6.4.1</b> Chapter takeaways</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="statistical-models.html"><a href="statistical-models.html#linear-mixed-model-theory"><i class="fa fa-check"></i><b>6.5</b> Linear mixed model theory</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="statistical-models.html"><a href="statistical-models.html#matrix-format-1"><i class="fa fa-check"></i><b>6.5.1</b> Matrix format</a></li>
<li class="chapter" data-level="6.5.2" data-path="statistical-models.html"><a href="statistical-models.html#why-random-effects-create-correlation"><i class="fa fa-check"></i><b>6.5.2</b> Why random effects create correlation</a></li>
<li class="chapter" data-level="6.5.3" data-path="statistical-models.html"><a href="statistical-models.html#example-reducing-the-work-stress-of-nurses"><i class="fa fa-check"></i><b>6.5.3</b> Example: reducing the work stress of nurses</a></li>
<li class="chapter" data-level="6.5.4" data-path="statistical-models.html"><a href="statistical-models.html#the-dependent-variable-mathbfy"><i class="fa fa-check"></i><b>6.5.4</b> The dependent variable <span class="math inline">\(\mathbf{y}\)</span></a></li>
<li class="chapter" data-level="6.5.5" data-path="statistical-models.html"><a href="statistical-models.html#fixed-effects-design-matrix-mathbfx"><i class="fa fa-check"></i><b>6.5.5</b> Fixed effects design matrix <span class="math inline">\(\mathbf{X}\)</span></a></li>
<li class="chapter" data-level="6.5.6" data-path="statistical-models.html"><a href="statistical-models.html#fixed-effect-coefficients-boldsymbolhatbeta"><i class="fa fa-check"></i><b>6.5.6</b> Fixed effect coefficients <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span></a></li>
<li class="chapter" data-level="6.5.7" data-path="statistical-models.html"><a href="statistical-models.html#variancecovariance-matrix-of-hatbeta"><i class="fa fa-check"></i><b>6.5.7</b> Variance–covariance matrix of <span class="math inline">\(\hat{\beta}\)</span></a></li>
<li class="chapter" data-level="6.5.8" data-path="statistical-models.html"><a href="statistical-models.html#random-effects-design-matrix-mathbfz"><i class="fa fa-check"></i><b>6.5.8</b> Random effects design matrix <span class="math inline">\(\mathbf{Z}\)</span></a></li>
<li class="chapter" data-level="6.5.9" data-path="statistical-models.html"><a href="statistical-models.html#random-effects-boldsymbolu-and-blup-intuition"><i class="fa fa-check"></i><b>6.5.9</b> Random effects <span class="math inline">\(\boldsymbol{u}\)</span> and BLUP intuition</a></li>
<li class="chapter" data-level="6.5.10" data-path="statistical-models.html"><a href="statistical-models.html#covariance-structures-mathbfg-mathbfr-and-mathbfv"><i class="fa fa-check"></i><b>6.5.10</b> Covariance structures: <span class="math inline">\(\mathbf{G}\)</span>, <span class="math inline">\(\mathbf{R}\)</span>, and <span class="math inline">\(\mathbf{V}\)</span></a></li>
<li class="chapter" data-level="6.5.11" data-path="statistical-models.html"><a href="statistical-models.html#estimating-variance-parameters"><i class="fa fa-check"></i><b>6.5.11</b> Estimating variance parameters</a></li>
<li class="chapter" data-level="6.5.12" data-path="statistical-models.html"><a href="statistical-models.html#model-statement-and-interpretation"><i class="fa fa-check"></i><b>6.5.12</b> Model statement and interpretation</a></li>
<li class="chapter" data-level="6.5.13" data-path="statistical-models.html"><a href="statistical-models.html#testing-and-model-comparison"><i class="fa fa-check"></i><b>6.5.13</b> Testing and model comparison</a></li>
<li class="chapter" data-level="6.5.14" data-path="statistical-models.html"><a href="statistical-models.html#diagnostics-what-to-actually-check"><i class="fa fa-check"></i><b>6.5.14</b> Diagnostics (what to actually check)</a></li>
<li class="chapter" data-level="6.5.15" data-path="statistical-models.html"><a href="statistical-models.html#practical-takeaway-for-readers"><i class="fa fa-check"></i><b>6.5.15</b> Practical takeaway for readers</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="statistical-models.html"><a href="statistical-models.html#linear-mixed-model-practice"><i class="fa fa-check"></i><b>6.6</b> Linear mixed model practice</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="statistical-models.html"><a href="statistical-models.html#loading-data-and-library"><i class="fa fa-check"></i><b>6.6.1</b> Loading data and library</a></li>
<li class="chapter" data-level="6.6.2" data-path="statistical-models.html"><a href="statistical-models.html#general-linear-regression-ols"><i class="fa fa-check"></i><b>6.6.2</b> General linear regression (OLS)</a></li>
<li class="chapter" data-level="6.6.3" data-path="statistical-models.html"><a href="statistical-models.html#generalized-linear-regression-glm"><i class="fa fa-check"></i><b>6.6.3</b> Generalized linear regression (GLM)</a></li>
<li class="chapter" data-level="6.6.4" data-path="statistical-models.html"><a href="statistical-models.html#varying-intercept-by-adding-a-stratum-variable-as-fixed-effect-glm"><i class="fa fa-check"></i><b>6.6.4</b> Varying intercept by adding a stratum variable as fixed effect (GLM)</a></li>
<li class="chapter" data-level="6.6.5" data-path="statistical-models.html"><a href="statistical-models.html#comparisons-of-models-fixed-effect-approach"><i class="fa fa-check"></i><b>6.6.5</b> Comparisons of models (fixed effect approach)</a></li>
<li class="chapter" data-level="6.6.6" data-path="statistical-models.html"><a href="statistical-models.html#adding-class-and-school-as-fixed-effects-glm"><i class="fa fa-check"></i><b>6.6.6</b> Adding class and school as fixed effects (GLM)</a></li>
<li class="chapter" data-level="6.6.7" data-path="statistical-models.html"><a href="statistical-models.html#considering-different-slopes-by-stratum-glm"><i class="fa fa-check"></i><b>6.6.7</b> Considering different slopes by stratum (GLM)</a></li>
<li class="chapter" data-level="6.6.8" data-path="statistical-models.html"><a href="statistical-models.html#varying-intercept-with-lmm"><i class="fa fa-check"></i><b>6.6.8</b> Varying intercept with LMM</a></li>
<li class="chapter" data-level="6.6.9" data-path="statistical-models.html"><a href="statistical-models.html#add-class-and-school-as-random-effects"><i class="fa fa-check"></i><b>6.6.9</b> Add class and school as random effects</a></li>
<li class="chapter" data-level="6.6.10" data-path="statistical-models.html"><a href="statistical-models.html#nested-terms-schoolclass"><i class="fa fa-check"></i><b>6.6.10</b> Nested terms: <code>school/class</code></a></li>
<li class="chapter" data-level="6.6.11" data-path="statistical-models.html"><a href="statistical-models.html#varying-slope-with-lmm"><i class="fa fa-check"></i><b>6.6.11</b> Varying slope with LMM</a></li>
<li class="chapter" data-level="6.6.12" data-path="statistical-models.html"><a href="statistical-models.html#summarize-model-8"><i class="fa fa-check"></i><b>6.6.12</b> Summarize model 8</a></li>
<li class="chapter" data-level="6.6.13" data-path="statistical-models.html"><a href="statistical-models.html#testing-random-effects-lrt-logic"><i class="fa fa-check"></i><b>6.6.13</b> Testing random effects (LRT logic)</a></li>
<li class="chapter" data-level="6.6.14" data-path="statistical-models.html"><a href="statistical-models.html#visualizing-grouping-structure"><i class="fa fa-check"></i><b>6.6.14</b> Visualizing grouping structure</a></li>
<li class="chapter" data-level="6.6.15" data-path="statistical-models.html"><a href="statistical-models.html#residual-checks-by-grouping-variables"><i class="fa fa-check"></i><b>6.6.15</b> Residual checks by grouping variables</a></li>
<li class="chapter" data-level="6.6.16" data-path="statistical-models.html"><a href="statistical-models.html#residuals-vs-explanatory-variables"><i class="fa fa-check"></i><b>6.6.16</b> Residuals vs explanatory variables</a></li>
<li class="chapter" data-level="6.6.17" data-path="statistical-models.html"><a href="statistical-models.html#normality-checks"><i class="fa fa-check"></i><b>6.6.17</b> Normality checks</a></li>
<li class="chapter" data-level="6.6.18" data-path="statistical-models.html"><a href="statistical-models.html#extracting-elements-parameters"><i class="fa fa-check"></i><b>6.6.18</b> Extracting elements (parameters)</a></li>
<li class="chapter" data-level="6.6.19" data-path="statistical-models.html"><a href="statistical-models.html#fitted-values-and-residuals"><i class="fa fa-check"></i><b>6.6.19</b> Fitted values and residuals</a></li>
<li class="chapter" data-level="6.6.20" data-path="statistical-models.html"><a href="statistical-models.html#fitted-lines-by-group"><i class="fa fa-check"></i><b>6.6.20</b> Fitted lines by group</a></li>
<li class="chapter" data-level="6.6.21" data-path="statistical-models.html"><a href="statistical-models.html#model-diagnostics-second-dataset-nurses"><i class="fa fa-check"></i><b>6.6.21</b> Model diagnostics (second dataset: Nurses)</a></li>
<li class="chapter" data-level="6.6.22" data-path="statistical-models.html"><a href="statistical-models.html#intra-class-correlation-icc"><i class="fa fa-check"></i><b>6.6.22</b> Intra class correlation (ICC)</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="statistical-models.html"><a href="statistical-models.html#linear-mixed-model-covariance-decomposition-with-random-intercept-lme4"><i class="fa fa-check"></i><b>6.7</b> Linear mixed model covariance decomposition with random intercept — lme4</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="statistical-models.html"><a href="statistical-models.html#load-data"><i class="fa fa-check"></i><b>6.7.1</b> Load data</a></li>
<li class="chapter" data-level="6.7.2" data-path="statistical-models.html"><a href="statistical-models.html#plot-means-and-variances-by-higher-level-variable-grouping"><i class="fa fa-check"></i><b>6.7.2</b> Plot means and variances by higher level variable (grouping)</a></li>
<li class="chapter" data-level="6.7.3" data-path="statistical-models.html"><a href="statistical-models.html#using-glm-style-fixed-effects-baseline-reference"><i class="fa fa-check"></i><b>6.7.3</b> Using glm-style fixed effects (baseline reference)</a></li>
<li class="chapter" data-level="6.7.4" data-path="statistical-models.html"><a href="statistical-models.html#using-lmm-random-intercept-in-lme4"><i class="fa fa-check"></i><b>6.7.4</b> Using LMM (random intercept) in lme4</a></li>
<li class="chapter" data-level="6.7.5" data-path="statistical-models.html"><a href="statistical-models.html#get-mathbfx-mathbfy-mathbfz"><i class="fa fa-check"></i><b>6.7.5</b> Get <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{y}\)</span>, <span class="math inline">\(\mathbf{Z}\)</span></a></li>
<li class="chapter" data-level="6.7.6" data-path="statistical-models.html"><a href="statistical-models.html#get-fixed-and-random-effect-coefficients"><i class="fa fa-check"></i><b>6.7.6</b> Get fixed and random effect coefficients</a></li>
<li class="chapter" data-level="6.7.7" data-path="statistical-models.html"><a href="statistical-models.html#random-effect-covariance-structure-and-the-role-of-theta"><i class="fa fa-check"></i><b>6.7.7</b> Random effect covariance structure and the role of <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="6.7.8" data-path="statistical-models.html"><a href="statistical-models.html#residual-variance-and-mathbfrsigma2mathbfi"><i class="fa fa-check"></i><b>6.7.8</b> Residual variance and <span class="math inline">\(\mathbf{R}=\sigma^2\mathbf{I}\)</span></a></li>
<li class="chapter" data-level="6.7.9" data-path="statistical-models.html"><a href="statistical-models.html#marginal-covariance-matrix-mathbfv-of-mathbfy"><i class="fa fa-check"></i><b>6.7.9</b> Marginal covariance matrix <span class="math inline">\(\mathbf{V}\)</span> of <span class="math inline">\(\mathbf{y}\)</span></a></li>
<li class="chapter" data-level="6.7.10" data-path="statistical-models.html"><a href="statistical-models.html#fixed-effect-coefficient-covariance-matrix"><i class="fa fa-check"></i><b>6.7.10</b> Fixed effect coefficient covariance matrix</a></li>
<li class="chapter" data-level="6.7.11" data-path="statistical-models.html"><a href="statistical-models.html#random-effect-covariance-matrix-standard-deviation-scale"><i class="fa fa-check"></i><b>6.7.11</b> Random effect covariance matrix (standard deviation scale)</a></li>
<li class="chapter" data-level="6.7.12" data-path="statistical-models.html"><a href="statistical-models.html#compute-fixed-effect-coefficients-manually-gls"><i class="fa fa-check"></i><b>6.7.12</b> Compute fixed effect coefficients manually (GLS)</a></li>
<li class="chapter" data-level="6.7.13" data-path="statistical-models.html"><a href="statistical-models.html#compute-covariance-of-fixed-effect-coefficients-manually"><i class="fa fa-check"></i><b>6.7.13</b> Compute covariance of fixed effect coefficients manually</a></li>
<li class="chapter" data-level="6.7.14" data-path="statistical-models.html"><a href="statistical-models.html#compute-random-effect-coefficients-manually-blups"><i class="fa fa-check"></i><b>6.7.14</b> Compute random effect coefficients manually (BLUPs)</a></li>
<li class="chapter" data-level="6.7.15" data-path="statistical-models.html"><a href="statistical-models.html#compute-predicted-values"><i class="fa fa-check"></i><b>6.7.15</b> Compute predicted values</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="statistical-models.html"><a href="statistical-models.html#linear-mixed-model-covariance-decomposition-with-random-slopes-lme4"><i class="fa fa-check"></i><b>6.8</b> Linear Mixed Model Covariance Decomposition with Random Slopes (lme4)</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="statistical-models.html"><a href="statistical-models.html#load-data-1"><i class="fa fa-check"></i><b>6.8.1</b> Load data</a></li>
<li class="chapter" data-level="6.8.2" data-path="statistical-models.html"><a href="statistical-models.html#using-a-linear-mixed-model-with-random-slopes"><i class="fa fa-check"></i><b>6.8.2</b> Using a linear mixed model with random slopes</a></li>
<li class="chapter" data-level="6.8.3" data-path="statistical-models.html"><a href="statistical-models.html#extracting-the-design-matrices-x-y-and-z"><i class="fa fa-check"></i><b>6.8.3</b> Extracting the design matrices X, y, and Z</a></li>
<li class="chapter" data-level="6.8.4" data-path="statistical-models.html"><a href="statistical-models.html#fixed-and-random-effect-coefficients"><i class="fa fa-check"></i><b>6.8.4</b> Fixed and random effect coefficients</a></li>
<li class="chapter" data-level="6.8.5" data-path="statistical-models.html"><a href="statistical-models.html#random-effect-covariance-structure-matrix-expansion"><i class="fa fa-check"></i><b>6.8.5</b> Random-effect covariance structure (matrix expansion)</a></li>
<li class="chapter" data-level="6.8.6" data-path="statistical-models.html"><a href="statistical-models.html#residual-variance-and-identity-matrix"><i class="fa fa-check"></i><b>6.8.6</b> Residual variance and identity matrix</a></li>
<li class="chapter" data-level="6.8.7" data-path="statistical-models.html"><a href="statistical-models.html#covariance-matrix-of-the-response-vector-y"><i class="fa fa-check"></i><b>6.8.7</b> Covariance matrix of the response vector y</a></li>
<li class="chapter" data-level="6.8.8" data-path="statistical-models.html"><a href="statistical-models.html#covariance-matrix-of-fixed-effect-coefficients"><i class="fa fa-check"></i><b>6.8.8</b> Covariance matrix of fixed-effect coefficients</a></li>
<li class="chapter" data-level="6.8.9" data-path="statistical-models.html"><a href="statistical-models.html#random-effect-covariance-matrix"><i class="fa fa-check"></i><b>6.8.9</b> Random-effect covariance matrix</a></li>
<li class="chapter" data-level="6.8.10" data-path="statistical-models.html"><a href="statistical-models.html#computing-fixed-effect-coefficients-manually"><i class="fa fa-check"></i><b>6.8.10</b> Computing fixed-effect coefficients manually</a></li>
<li class="chapter" data-level="6.8.11" data-path="statistical-models.html"><a href="statistical-models.html#covariance-of-fixed-effect-coefficients"><i class="fa fa-check"></i><b>6.8.11</b> Covariance of fixed-effect coefficients</a></li>
<li class="chapter" data-level="6.8.12" data-path="statistical-models.html"><a href="statistical-models.html#computing-random-effect-coefficients"><i class="fa fa-check"></i><b>6.8.12</b> Computing random-effect coefficients</a></li>
<li class="chapter" data-level="6.8.13" data-path="statistical-models.html"><a href="statistical-models.html#predicted-values"><i class="fa fa-check"></i><b>6.8.13</b> Predicted values</a></li>
<li class="chapter" data-level="6.8.14" data-path="statistical-models.html"><a href="statistical-models.html#model-with-two-grouping-factors"><i class="fa fa-check"></i><b>6.8.14</b> Model with two grouping factors</a></li>
<li class="chapter" data-level="6.8.15" data-path="statistical-models.html"><a href="statistical-models.html#extracting-z-for-the-two-factor-model"><i class="fa fa-check"></i><b>6.8.15</b> Extracting Z for the two-factor model</a></li>
<li class="chapter" data-level="6.8.16" data-path="statistical-models.html"><a href="statistical-models.html#nested-versus-crossed-random-effects"><i class="fa fa-check"></i><b>6.8.16</b> Nested versus crossed random effects</a></li>
<li class="chapter" data-level="6.8.17" data-path="statistical-models.html"><a href="statistical-models.html#remark-nested-vs.-crossed-random-effects"><i class="fa fa-check"></i><b>6.8.17</b> Remark: Nested vs. crossed random effects</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="statistical-models.html"><a href="statistical-models.html#linear-mixed-model-covariance-decomposition-nlme"><i class="fa fa-check"></i><b>6.9</b> Linear Mixed Model Covariance Decomposition (nlme)</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="statistical-models.html"><a href="statistical-models.html#load-data-2"><i class="fa fa-check"></i><b>6.9.1</b> Load data</a></li>
<li class="chapter" data-level="6.9.2" data-path="statistical-models.html"><a href="statistical-models.html#exploratory-visualization"><i class="fa fa-check"></i><b>6.9.2</b> Exploratory visualization</a></li>
<li class="chapter" data-level="6.9.3" data-path="statistical-models.html"><a href="statistical-models.html#using-lme-nlme"><i class="fa fa-check"></i><b>6.9.3</b> Using lme (nlme)</a></li>
<li class="chapter" data-level="6.9.4" data-path="statistical-models.html"><a href="statistical-models.html#inference-for-fixed-effects"><i class="fa fa-check"></i><b>6.9.4</b> Inference for fixed effects</a></li>
<li class="chapter" data-level="6.9.5" data-path="statistical-models.html"><a href="statistical-models.html#model-diagnosing"><i class="fa fa-check"></i><b>6.9.5</b> Model diagnosing</a></li>
<li class="chapter" data-level="6.9.6" data-path="statistical-models.html"><a href="statistical-models.html#get-x-y-z-matrices-using-lme4"><i class="fa fa-check"></i><b>6.9.6</b> Get X, y, Z matrices (using lme4)</a></li>
<li class="chapter" data-level="6.9.7" data-path="statistical-models.html"><a href="statistical-models.html#fixed-effect-coefficient"><i class="fa fa-check"></i><b>6.9.7</b> Fixed effect coefficient</a></li>
<li class="chapter" data-level="6.9.8" data-path="statistical-models.html"><a href="statistical-models.html#random-effect-coefficient"><i class="fa fa-check"></i><b>6.9.8</b> Random effect coefficient</a></li>
<li class="chapter" data-level="6.9.9" data-path="statistical-models.html"><a href="statistical-models.html#fixed-effect-coefficients-covariance"><i class="fa fa-check"></i><b>6.9.9</b> Fixed effect coefficients covariance</a></li>
<li class="chapter" data-level="6.9.10" data-path="statistical-models.html"><a href="statistical-models.html#random-effect-covariance-and-correlation"><i class="fa fa-check"></i><b>6.9.10</b> Random effect covariance and correlation</a></li>
<li class="chapter" data-level="6.9.11" data-path="statistical-models.html"><a href="statistical-models.html#get-y-covariance-directly"><i class="fa fa-check"></i><b>6.9.11</b> Get y covariance directly</a></li>
<li class="chapter" data-level="6.9.12" data-path="statistical-models.html"><a href="statistical-models.html#residual-variance"><i class="fa fa-check"></i><b>6.9.12</b> Residual variance</a></li>
<li class="chapter" data-level="6.9.13" data-path="statistical-models.html"><a href="statistical-models.html#compute-fixed-effect-coefficients-by-gls"><i class="fa fa-check"></i><b>6.9.13</b> Compute fixed effect coefficients (by GLS)</a></li>
<li class="chapter" data-level="6.9.14" data-path="statistical-models.html"><a href="statistical-models.html#compute-covariance-of-fixed-effect-coefficients"><i class="fa fa-check"></i><b>6.9.14</b> Compute covariance of fixed effect coefficients</a></li>
<li class="chapter" data-level="6.9.15" data-path="statistical-models.html"><a href="statistical-models.html#compute-random-effect-coefficients-blup"><i class="fa fa-check"></i><b>6.9.15</b> Compute random effect coefficients (BLUP)</a></li>
<li class="chapter" data-level="6.9.16" data-path="statistical-models.html"><a href="statistical-models.html#compute-predicted-values-1"><i class="fa fa-check"></i><b>6.9.16</b> Compute predicted values</a></li>
<li class="chapter" data-level="6.9.17" data-path="statistical-models.html"><a href="statistical-models.html#the-marginal-distribution"><i class="fa fa-check"></i><b>6.9.17</b> The marginal distribution</a></li>
<li class="chapter" data-level="6.9.18" data-path="statistical-models.html"><a href="statistical-models.html#extending-the-covariance-structure-correlated-residual-errors"><i class="fa fa-check"></i><b>6.9.18</b> Extending the covariance structure: correlated residual errors</a></li>
<li class="chapter" data-level="6.9.19" data-path="statistical-models.html"><a href="statistical-models.html#using-lme-with-gaussian-correlation"><i class="fa fa-check"></i><b>6.9.19</b> Using lme with Gaussian correlation</a></li>
<li class="chapter" data-level="6.9.20" data-path="statistical-models.html"><a href="statistical-models.html#using-lme-with-autoregressive-ar1"><i class="fa fa-check"></i><b>6.9.20</b> Using lme with autoregressive (AR1)</a></li>
<li class="chapter" data-level="6.9.21" data-path="statistical-models.html"><a href="statistical-models.html#using-lme-with-exponential-correlation"><i class="fa fa-check"></i><b>6.9.21</b> Using lme with exponential correlation</a></li>
<li class="chapter" data-level="6.9.22" data-path="statistical-models.html"><a href="statistical-models.html#using-lme-with-unstructured-residual-correlation-corsymm"><i class="fa fa-check"></i><b>6.9.22</b> Using lme with unstructured residual correlation (corSymm)</a></li>
<li class="chapter" data-level="6.9.23" data-path="statistical-models.html"><a href="statistical-models.html#using-lme-with-compound-symmetry-residual-correlation-corcompsymm"><i class="fa fa-check"></i><b>6.9.23</b> Using lme with compound symmetry residual correlation (corCompSymm)</a></li>
<li class="chapter" data-level="6.9.24" data-path="statistical-models.html"><a href="statistical-models.html#gls-models-correlation-in-residuals-without-random-effects"><i class="fa fa-check"></i><b>6.9.24</b> GLS models: correlation in residuals without random effects</a></li>
<li class="chapter" data-level="6.9.25" data-path="statistical-models.html"><a href="statistical-models.html#gls-with-unstructured-correlation"><i class="fa fa-check"></i><b>6.9.25</b> GLS with unstructured correlation</a></li>
<li class="chapter" data-level="6.9.26" data-path="statistical-models.html"><a href="statistical-models.html#gls-with-compound-symmetry"><i class="fa fa-check"></i><b>6.9.26</b> GLS with compound symmetry</a></li>
<li class="chapter" data-level="6.9.27" data-path="statistical-models.html"><a href="statistical-models.html#practical-limitation-of-lme4"><i class="fa fa-check"></i><b>6.9.27</b> Practical limitation of lme4</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="statistical-models.html"><a href="statistical-models.html#manual-simulating-data-for-linear-mix-model"><i class="fa fa-check"></i><b>6.10</b> Manual simulating data for linear mix model</a></li>
<li class="chapter" data-level="6.11" data-path="statistical-models.html"><a href="statistical-models.html#how-to-calculate-the-prediction-interval-for-lmm"><i class="fa fa-check"></i><b>6.11</b> How to calculate the prediction interval for LMM</a></li>
<li class="chapter" data-level="6.12" data-path="statistical-models.html"><a href="statistical-models.html#least-squares-means-with-interaction-effect"><i class="fa fa-check"></i><b>6.12</b> Least-squares means with interaction effect</a></li>
<li class="chapter" data-level="6.13" data-path="statistical-models.html"><a href="statistical-models.html#spline-regression"><i class="fa fa-check"></i><b>6.13</b> Spline regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>7</b> Probability</a>
<ul>
<li class="chapter" data-level="7.1" data-path="probability.html"><a href="probability.html#probability-basics"><i class="fa fa-check"></i><b>7.1</b> Probability basics</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="probability.html"><a href="probability.html#events"><i class="fa fa-check"></i><b>7.1.1</b> Events</a></li>
<li class="chapter" data-level="7.1.2" data-path="probability.html"><a href="probability.html#probability-formulas"><i class="fa fa-check"></i><b>7.1.2</b> Probability formulas</a></li>
<li class="chapter" data-level="7.1.3" data-path="probability.html"><a href="probability.html#calculation-of-probability-operations"><i class="fa fa-check"></i><b>7.1.3</b> Calculation of probability (operations)</a></li>
<li class="chapter" data-level="7.1.4" data-path="probability.html"><a href="probability.html#bayess-theorem"><i class="fa fa-check"></i><b>7.1.4</b> Bayes’s theorem</a></li>
<li class="chapter" data-level="7.1.5" data-path="probability.html"><a href="probability.html#random-variables-and-distribution-functions"><i class="fa fa-check"></i><b>7.1.5</b> Random variables and distribution functions</a></li>
<li class="chapter" data-level="7.1.6" data-path="probability.html"><a href="probability.html#probability-distributions-joint-marginal-conditional"><i class="fa fa-check"></i><b>7.1.6</b> Probability distributions (joint, marginal, conditional)</a></li>
<li class="chapter" data-level="7.1.7" data-path="probability.html"><a href="probability.html#conditional-expectation"><i class="fa fa-check"></i><b>7.1.7</b> Conditional expectation</a></li>
<li class="chapter" data-level="7.1.8" data-path="probability.html"><a href="probability.html#conditional-variance"><i class="fa fa-check"></i><b>7.1.8</b> Conditional variance</a></li>
<li class="chapter" data-level="7.1.9" data-path="probability.html"><a href="probability.html#sampling"><i class="fa fa-check"></i><b>7.1.9</b> Sampling</a></li>
<li class="chapter" data-level="7.1.10" data-path="probability.html"><a href="probability.html#central-limit-theorem-and-law-of-large-numbers"><i class="fa fa-check"></i><b>7.1.10</b> Central limit theorem and law of large numbers</a></li>
<li class="chapter" data-level="7.1.11" data-path="probability.html"><a href="probability.html#confidence-interval"><i class="fa fa-check"></i><b>7.1.11</b> Confidence interval</a></li>
<li class="chapter" data-level="7.1.12" data-path="probability.html"><a href="probability.html#introduction-to-hypothesis-testing"><i class="fa fa-check"></i><b>7.1.12</b> Introduction to hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="probability.html"><a href="probability.html#probability-r-practice"><i class="fa fa-check"></i><b>7.2</b> Probability R practice</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="probability.html"><a href="probability.html#integrate"><i class="fa fa-check"></i><b>7.2.1</b> Integrate</a></li>
<li class="chapter" data-level="7.2.2" data-path="probability.html"><a href="probability.html#derivation-symbolic-differentiation"><i class="fa fa-check"></i><b>7.2.2</b> Derivation (symbolic differentiation)</a></li>
<li class="chapter" data-level="7.2.3" data-path="probability.html"><a href="probability.html#create-random-variables-with-specific-distributions"><i class="fa fa-check"></i><b>7.2.3</b> Create random variables with specific distributions</a></li>
<li class="chapter" data-level="7.2.4" data-path="probability.html"><a href="probability.html#probability-function-examples"><i class="fa fa-check"></i><b>7.2.4</b> Probability function examples</a></li>
<li class="chapter" data-level="7.2.5" data-path="probability.html"><a href="probability.html#vector-and-operations"><i class="fa fa-check"></i><b>7.2.5</b> Vector and operations</a></li>
<li class="chapter" data-level="7.2.6" data-path="probability.html"><a href="probability.html#select-and-substitute-elements-of-vector"><i class="fa fa-check"></i><b>7.2.6</b> Select and substitute elements of vector</a></li>
<li class="chapter" data-level="7.2.7" data-path="probability.html"><a href="probability.html#matrix-and-operations"><i class="fa fa-check"></i><b>7.2.7</b> Matrix and operations</a></li>
<li class="chapter" data-level="7.2.8" data-path="probability.html"><a href="probability.html#compute-inverse-determinant-and-eigenvalues"><i class="fa fa-check"></i><b>7.2.8</b> Compute inverse, determinant and eigenvalues</a></li>
<li class="chapter" data-level="7.2.9" data-path="probability.html"><a href="probability.html#dataframe"><i class="fa fa-check"></i><b>7.2.9</b> Dataframe</a></li>
<li class="chapter" data-level="7.2.10" data-path="probability.html"><a href="probability.html#solve-problems-using-simulation"><i class="fa fa-check"></i><b>7.2.10</b> Solve problems using simulation</a></li>
<li class="chapter" data-level="7.2.11" data-path="probability.html"><a href="probability.html#permutations-and-combinations"><i class="fa fa-check"></i><b>7.2.11</b> Permutations and combinations</a></li>
<li class="chapter" data-level="7.2.12" data-path="probability.html"><a href="probability.html#search-value-position-in-vector"><i class="fa fa-check"></i><b>7.2.12</b> Search value position in vector</a></li>
<li class="chapter" data-level="7.2.13" data-path="probability.html"><a href="probability.html#solve-directly-and-optimize"><i class="fa fa-check"></i><b>7.2.13</b> Solve directly and optimize</a></li>
<li class="chapter" data-level="7.2.14" data-path="probability.html"><a href="probability.html#calculate-probability-using-simulation-method"><i class="fa fa-check"></i><b>7.2.14</b> Calculate probability using simulation method</a></li>
<li class="chapter" data-level="7.2.15" data-path="probability.html"><a href="probability.html#discrete-random-variable"><i class="fa fa-check"></i><b>7.2.15</b> Discrete random variable</a></li>
<li class="chapter" data-level="7.2.16" data-path="probability.html"><a href="probability.html#exponent-distribution"><i class="fa fa-check"></i><b>7.2.16</b> Exponent distribution</a></li>
<li class="chapter" data-level="7.2.17" data-path="probability.html"><a href="probability.html#normal-distribution-plot"><i class="fa fa-check"></i><b>7.2.17</b> Normal distribution plot</a></li>
<li class="chapter" data-level="7.2.18" data-path="probability.html"><a href="probability.html#distribution-of-random-variable-function"><i class="fa fa-check"></i><b>7.2.18</b> Distribution of random variable function</a></li>
<li class="chapter" data-level="7.2.19" data-path="probability.html"><a href="probability.html#joint-and-marginal-probability-simulation-approach"><i class="fa fa-check"></i><b>7.2.19</b> Joint and marginal probability (simulation approach)</a></li>
<li class="chapter" data-level="7.2.20" data-path="probability.html"><a href="probability.html#multiple-random-variables-derived-distributions"><i class="fa fa-check"></i><b>7.2.20</b> Multiple random variables: derived distributions</a></li>
<li class="chapter" data-level="7.2.21" data-path="probability.html"><a href="probability.html#sum-of-two-independent-normal-variables"><i class="fa fa-check"></i><b>7.2.21</b> Sum of two independent normal variables</a></li>
<li class="chapter" data-level="7.2.22" data-path="probability.html"><a href="probability.html#generate-a-circle-using-simulated-random-dots"><i class="fa fa-check"></i><b>7.2.22</b> Generate a circle using simulated random dots</a></li>
<li class="chapter" data-level="7.2.23" data-path="probability.html"><a href="probability.html#expectation-simulation"><i class="fa fa-check"></i><b>7.2.23</b> Expectation (simulation)</a></li>
<li class="chapter" data-level="7.2.24" data-path="probability.html"><a href="probability.html#central-limit-theorem-simulation"><i class="fa fa-check"></i><b>7.2.24</b> Central Limit Theorem (simulation)</a></li>
<li class="chapter" data-level="7.2.25" data-path="probability.html"><a href="probability.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.25</b> Law of large numbers</a></li>
<li class="chapter" data-level="7.2.26" data-path="probability.html"><a href="probability.html#empirical-distribution-function-ecdf"><i class="fa fa-check"></i><b>7.2.26</b> Empirical distribution function (ECDF)</a></li>
<li class="chapter" data-level="7.2.27" data-path="probability.html"><a href="probability.html#probability-of-mean-3-simulation"><i class="fa fa-check"></i><b>7.2.27</b> Probability of mean &gt; 3 (simulation)</a></li>
<li class="chapter" data-level="7.2.28" data-path="probability.html"><a href="probability.html#maximum-likelihood-estimate-mle"><i class="fa fa-check"></i><b>7.2.28</b> Maximum likelihood estimate (MLE)</a></li>
<li class="chapter" data-level="7.2.29" data-path="probability.html"><a href="probability.html#t-distribution-f-distribution-plots-and-common-distributions"><i class="fa fa-check"></i><b>7.2.29</b> t distribution, F distribution plots, and common distributions</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#closing-perspective"><i class="fa fa-check"></i>Closing perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="algorithms.html"><a href="algorithms.html"><i class="fa fa-check"></i><b>8</b> Algorithms</a>
<ul>
<li class="chapter" data-level="8.1" data-path="algorithms.html"><a href="algorithms.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>8.1</b> Maximum likelihood estimation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="algorithms.html"><a href="algorithms.html#likelihood-estimation"><i class="fa fa-check"></i><b>8.1.1</b> Likelihood estimation</a></li>
<li class="chapter" data-level="8.1.2" data-path="algorithms.html"><a href="algorithms.html#negative-log-likelihood-scalar-form"><i class="fa fa-check"></i><b>8.1.2</b> Negative log-likelihood (scalar form)</a></li>
<li class="chapter" data-level="8.1.3" data-path="algorithms.html"><a href="algorithms.html#matrix-formulation"><i class="fa fa-check"></i><b>8.1.3</b> Matrix formulation</a></li>
<li class="chapter" data-level="8.1.4" data-path="algorithms.html"><a href="algorithms.html#minimizing-the-negative-log-likelihood"><i class="fa fa-check"></i><b>8.1.4</b> Minimizing the negative log-likelihood</a></li>
<li class="chapter" data-level="8.1.5" data-path="algorithms.html"><a href="algorithms.html#taking-derivatives"><i class="fa fa-check"></i><b>8.1.5</b> Taking derivatives</a></li>
<li class="chapter" data-level="8.1.6" data-path="algorithms.html"><a href="algorithms.html#reml-versus-ml"><i class="fa fa-check"></i><b>8.1.6</b> REML versus ML</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="algorithms.html"><a href="algorithms.html#r-demonstration"><i class="fa fa-check"></i><b>8.2</b> R demonstration</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="algorithms.html"><a href="algorithms.html#toy-data"><i class="fa fa-check"></i><b>8.2.1</b> Toy data</a></li>
<li class="chapter" data-level="8.2.2" data-path="algorithms.html"><a href="algorithms.html#linear-regression-closed-form-mle"><i class="fa fa-check"></i><b>8.2.2</b> Linear regression (closed-form MLE)</a></li>
<li class="chapter" data-level="8.2.3" data-path="algorithms.html"><a href="algorithms.html#mle-via-explicit-negative-log-likelihood"><i class="fa fa-check"></i><b>8.2.3</b> MLE via explicit negative log-likelihood</a></li>
<li class="chapter" data-level="8.2.4" data-path="algorithms.html"><a href="algorithms.html#maximum-likelihood-via-maxlik"><i class="fa fa-check"></i><b>8.2.4</b> Maximum likelihood via <code>maxLik</code></a></li>
<li class="chapter" data-level="8.2.5" data-path="algorithms.html"><a href="algorithms.html#another-example-real-data"><i class="fa fa-check"></i><b>8.2.5</b> Another example (real data)</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="algorithms.html"><a href="algorithms.html#confidence-intervals-from-likelihood"><i class="fa fa-check"></i><b>8.3</b> Confidence intervals from likelihood</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="algorithms.html"><a href="algorithms.html#fisher-information-and-hessian"><i class="fa fa-check"></i><b>8.3.1</b> Fisher information and Hessian</a></li>
<li class="chapter" data-level="8.3.2" data-path="algorithms.html"><a href="algorithms.html#likelihood-ratio-confidence-intervals"><i class="fa fa-check"></i><b>8.3.2</b> Likelihood ratio confidence intervals</a></li>
<li class="chapter" data-level="8.3.3" data-path="algorithms.html"><a href="algorithms.html#profile-likelihood"><i class="fa fa-check"></i><b>8.3.3</b> Profile likelihood</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="algorithms.html"><a href="algorithms.html#maximum-likelihood-estimate-practice"><i class="fa fa-check"></i><b>8.4</b> Maximum likelihood estimate practice</a></li>
<li class="chapter" data-level="8.5" data-path="algorithms.html"><a href="algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>8.5</b> Gradient descent</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="algorithms.html"><a href="algorithms.html#linear-regression-example"><i class="fa fa-check"></i><b>8.5.1</b> Linear regression example</a></li>
<li class="chapter" data-level="8.5.2" data-path="algorithms.html"><a href="algorithms.html#cost-function-and-gradient-descent"><i class="fa fa-check"></i><b>8.5.2</b> Cost function and gradient descent</a></li>
<li class="chapter" data-level="8.5.3" data-path="algorithms.html"><a href="algorithms.html#cost-function-convergence"><i class="fa fa-check"></i><b>8.5.3</b> Cost function convergence</a></li>
<li class="chapter" data-level="8.5.4" data-path="algorithms.html"><a href="algorithms.html#comparing-gradient-descent-and-linear-regression"><i class="fa fa-check"></i><b>8.5.4</b> Comparing gradient descent and linear regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="sasmarkdown.html"><a href="sasmarkdown.html"><i class="fa fa-check"></i><b>9</b> SASmarkdown</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sasmarkdown.html"><a href="sasmarkdown.html#how-to-install-sasmarkdown"><i class="fa fa-check"></i><b>9.1</b> How to install SASmarkdown</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="sasmarkdown.html"><a href="sasmarkdown.html#use-an-engine"><i class="fa fa-check"></i><b>9.1.1</b> Use an engine</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="sasmarkdown.html"><a href="sasmarkdown.html#common-statements"><i class="fa fa-check"></i><b>9.2</b> Common statements</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="sasmarkdown.html"><a href="sasmarkdown.html#read-in-data-using-informats-date-data"><i class="fa fa-check"></i><b>9.2.1</b> Read in data using informats (date data)</a></li>
<li class="chapter" data-level="9.2.2" data-path="sasmarkdown.html"><a href="sasmarkdown.html#compute-mean-and-frequency"><i class="fa fa-check"></i><b>9.2.2</b> Compute mean and frequency</a></li>
<li class="chapter" data-level="9.2.3" data-path="sasmarkdown.html"><a href="sasmarkdown.html#sort-a-dataset"><i class="fa fa-check"></i><b>9.2.3</b> Sort a dataset</a></li>
<li class="chapter" data-level="9.2.4" data-path="sasmarkdown.html"><a href="sasmarkdown.html#transpose-or-reshape"><i class="fa fa-check"></i><b>9.2.4</b> Transpose or reshape</a></li>
<li class="chapter" data-level="9.2.5" data-path="sasmarkdown.html"><a href="sasmarkdown.html#conditional-statements"><i class="fa fa-check"></i><b>9.2.5</b> Conditional statements</a></li>
<li class="chapter" data-level="9.2.6" data-path="sasmarkdown.html"><a href="sasmarkdown.html#like-operation-to-select-rows-containing-a-pattern"><i class="fa fa-check"></i><b>9.2.6</b> LIKE operation to select rows containing a pattern</a></li>
<li class="chapter" data-level="9.2.7" data-path="sasmarkdown.html"><a href="sasmarkdown.html#change-format-of-a-variable"><i class="fa fa-check"></i><b>9.2.7</b> Change format of a variable</a></li>
<li class="chapter" data-level="9.2.8" data-path="sasmarkdown.html"><a href="sasmarkdown.html#basic-operations"><i class="fa fa-check"></i><b>9.2.8</b> Basic operations</a></li>
<li class="chapter" data-level="9.2.9" data-path="sasmarkdown.html"><a href="sasmarkdown.html#rename-variables-1"><i class="fa fa-check"></i><b>9.2.9</b> Rename variables</a></li>
<li class="chapter" data-level="9.2.10" data-path="sasmarkdown.html"><a href="sasmarkdown.html#text-manipulation"><i class="fa fa-check"></i><b>9.2.10</b> Text manipulation</a></li>
<li class="chapter" data-level="9.2.11" data-path="sasmarkdown.html"><a href="sasmarkdown.html#create-a-report"><i class="fa fa-check"></i><b>9.2.11</b> Create a report</a></li>
<li class="chapter" data-level="9.2.12" data-path="sasmarkdown.html"><a href="sasmarkdown.html#random-variables"><i class="fa fa-check"></i><b>9.2.12</b> Random variables</a></li>
<li class="chapter" data-level="9.2.13" data-path="sasmarkdown.html"><a href="sasmarkdown.html#combine-two-texts-compress-spaces-locate-a-substring-change-case"><i class="fa fa-check"></i><b>9.2.13</b> Combine two texts, compress spaces, locate a substring, change case</a></li>
<li class="chapter" data-level="9.2.14" data-path="sasmarkdown.html"><a href="sasmarkdown.html#deduplication-1"><i class="fa fa-check"></i><b>9.2.14</b> Deduplication</a></li>
<li class="chapter" data-level="9.2.15" data-path="sasmarkdown.html"><a href="sasmarkdown.html#select-a-subset-of-rows"><i class="fa fa-check"></i><b>9.2.15</b> Select a subset of rows</a></li>
<li class="chapter" data-level="9.2.16" data-path="sasmarkdown.html"><a href="sasmarkdown.html#create-macros-with-do-loops"><i class="fa fa-check"></i><b>9.2.16</b> Create macros with DO loops</a></li>
<li class="chapter" data-level="9.2.17" data-path="sasmarkdown.html"><a href="sasmarkdown.html#output-intermediate-tables-with-ods"><i class="fa fa-check"></i><b>9.2.17</b> Output intermediate tables with ODS</a></li>
<li class="chapter" data-level="9.2.18" data-path="sasmarkdown.html"><a href="sasmarkdown.html#create-sequence-numbers"><i class="fa fa-check"></i><b>9.2.18</b> Create sequence numbers</a></li>
<li class="chapter" data-level="9.2.19" data-path="sasmarkdown.html"><a href="sasmarkdown.html#merge-datasets"><i class="fa fa-check"></i><b>9.2.19</b> Merge datasets</a></li>
<li class="chapter" data-level="9.2.20" data-path="sasmarkdown.html"><a href="sasmarkdown.html#create-table-1-1"><i class="fa fa-check"></i><b>9.2.20</b> Create Table 1</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="sasmarkdown.html"><a href="sasmarkdown.html#using-macros"><i class="fa fa-check"></i><b>9.3</b> Using macros</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="sasmarkdown.html"><a href="sasmarkdown.html#proc-report-for-flexible-summaries"><i class="fa fa-check"></i><b>9.3.1</b> PROC REPORT for flexible summaries</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="sasmarkdown.html"><a href="sasmarkdown.html#use-sas-formats"><i class="fa fa-check"></i><b>9.4</b> Use SAS formats</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="sasmarkdown.html"><a href="sasmarkdown.html#storing-formats-to-a-library"><i class="fa fa-check"></i><b>9.4.1</b> Storing formats to a library</a></li>
<li class="chapter" data-level="9.4.2" data-path="sasmarkdown.html"><a href="sasmarkdown.html#modify-formats"><i class="fa fa-check"></i><b>9.4.2</b> Modify formats</a></li>
<li class="chapter" data-level="9.4.3" data-path="sasmarkdown.html"><a href="sasmarkdown.html#transform-missing-to-character-then-to-numeric-again"><i class="fa fa-check"></i><b>9.4.3</b> Transform missing to character then to numeric again</a></li>
<li class="chapter" data-level="9.4.4" data-path="sasmarkdown.html"><a href="sasmarkdown.html#output-format-definition-details"><i class="fa fa-check"></i><b>9.4.4</b> Output format definition details</a></li>
<li class="chapter" data-level="9.4.5" data-path="sasmarkdown.html"><a href="sasmarkdown.html#a-macro-to-copy-an-existing-proc-format"><i class="fa fa-check"></i><b>9.4.5</b> A macro to copy an existing PROC FORMAT</a></li>
<li class="chapter" data-level="9.4.6" data-path="sasmarkdown.html"><a href="sasmarkdown.html#a-macro-to-view-the-list-of-variables"><i class="fa fa-check"></i><b>9.4.6</b> A macro to view the list of variables</a></li>
<li class="chapter" data-level="9.4.7" data-path="sasmarkdown.html"><a href="sasmarkdown.html#practical-workflow-recommendations-for-sasmarkdown"><i class="fa fa-check"></i><b>9.4.7</b> Practical workflow recommendations for SASmarkdown</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>10</b> Causal inference</a>
<ul>
<li class="chapter" data-level="10.1" data-path="causal-inference.html"><a href="causal-inference.html#causal-inference-introduction"><i class="fa fa-check"></i><b>10.1</b> Causal inference introduction</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="causal-inference.html"><a href="causal-inference.html#definitions-and-directed-acyclic-graph-dag"><i class="fa fa-check"></i><b>10.1.1</b> Definitions and directed acyclic graph (DAG)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="causal-inference.html"><a href="causal-inference.html#inverse-probability-weighting"><i class="fa fa-check"></i><b>10.2</b> Inverse probability weighting</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="causal-inference.html"><a href="causal-inference.html#matching-and-weighting-wo-imputation"><i class="fa fa-check"></i><b>10.2.1</b> Matching and weighting w/o imputation</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="causal-inference.html"><a href="causal-inference.html#g-formula"><i class="fa fa-check"></i><b>10.3</b> G formula</a></li>
<li class="chapter" data-level="10.4" data-path="causal-inference.html"><a href="causal-inference.html#double-robust-estimators"><i class="fa fa-check"></i><b>10.4</b> Double Robust Estimators</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="causal-inference.html"><a href="causal-inference.html#superlearner"><i class="fa fa-check"></i><b>10.4.1</b> SuperLearner</a></li>
<li class="chapter" data-level="10.4.2" data-path="causal-inference.html"><a href="causal-inference.html#double-robust-estimators-using-superlearner_sl3"><i class="fa fa-check"></i><b>10.4.2</b> Double robust estimators using SuperLearner_sl3</a></li>
<li class="chapter" data-level="10.4.3" data-path="causal-inference.html"><a href="causal-inference.html#double-robust-estimators-using-tmle"><i class="fa fa-check"></i><b>10.4.3</b> Double robust estimators using TMLE</a></li>
<li class="chapter" data-level="10.4.4" data-path="causal-inference.html"><a href="causal-inference.html#longitudinal-targeted-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>10.4.4</b> Longitudinal targeted maximum likelihood estimation</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="causal-inference.html"><a href="causal-inference.html#instrumental-variable-regression"><i class="fa fa-check"></i><b>10.5</b> Instrumental variable regression</a></li>
<li class="chapter" data-level="10.6" data-path="causal-inference.html"><a href="causal-inference.html#mediation-analysis"><i class="fa fa-check"></i><b>10.6</b> Mediation analysis</a></li>
<li class="chapter" data-level="10.7" data-path="causal-inference.html"><a href="causal-inference.html#confounding-and-effect-measure-modification"><i class="fa fa-check"></i><b>10.7</b> Confounding and effect measure modification</a></li>
<li class="chapter" data-level="10.8" data-path="causal-inference.html"><a href="causal-inference.html#causal-inference-and-associated-regression"><i class="fa fa-check"></i><b>10.8</b> Causal inference and associated regression</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="clinical-trial.html"><a href="clinical-trial.html"><i class="fa fa-check"></i><b>11</b> Clinical Trial</a>
<ul>
<li class="chapter" data-level="11.1" data-path="clinical-trial.html"><a href="clinical-trial.html#statistics-in-clinical-trial"><i class="fa fa-check"></i><b>11.1</b> Statistics in clinical trial</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="clinical-trial.html"><a href="clinical-trial.html#sample-size-interim-data-reports-and-randomization-of-assignment"><i class="fa fa-check"></i><b>11.1.1</b> Sample size, interim data reports and randomization of assignment</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="common-statistical-models.html"><a href="common-statistical-models.html"><i class="fa fa-check"></i><b>12</b> Common statistical models</a>
<ul>
<li class="chapter" data-level="12.1" data-path="common-statistical-models.html"><a href="common-statistical-models.html#survival-analysis"><i class="fa fa-check"></i><b>12.1</b> Survival analysis</a></li>
<li class="chapter" data-level="12.2" data-path="common-statistical-models.html"><a href="common-statistical-models.html#logistical-regression"><i class="fa fa-check"></i><b>12.2</b> Logistical regression</a></li>
<li class="chapter" data-level="12.3" data-path="common-statistical-models.html"><a href="common-statistical-models.html#poisson-regression"><i class="fa fa-check"></i><b>12.3</b> Poisson regression</a></li>
<li class="chapter" data-level="12.4" data-path="common-statistical-models.html"><a href="common-statistical-models.html#quantile-regression"><i class="fa fa-check"></i><b>12.4</b> Quantile regression</a></li>
<li class="chapter" data-level="12.5" data-path="common-statistical-models.html"><a href="common-statistical-models.html#principle-components-analysis"><i class="fa fa-check"></i><b>12.5</b> Principle components analysis</a></li>
<li class="chapter" data-level="12.6" data-path="common-statistical-models.html"><a href="common-statistical-models.html#which-covariates-should-be-adjusted"><i class="fa fa-check"></i><b>12.6</b> Which covariates should be adjusted</a></li>
<li class="chapter" data-level="12.7" data-path="common-statistical-models.html"><a href="common-statistical-models.html#variable-selection-1"><i class="fa fa-check"></i><b>12.7</b> Variable selection</a></li>
<li class="chapter" data-level="12.8" data-path="common-statistical-models.html"><a href="common-statistical-models.html#fit-regression-model-with-a-fan-shaped-relation"><i class="fa fa-check"></i><b>12.8</b> Fit regression model with a fan-shaped relation</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html"><i class="fa fa-check"></i><b>13</b> Bayesian statistics</a>
<ul>
<li class="chapter" data-level="13.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayesian-inference-in-r"><i class="fa fa-check"></i><b>13.1</b> Bayesian Inference in R</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="epidemiolgy.html"><a href="epidemiolgy.html"><i class="fa fa-check"></i><b>14</b> Epidemiolgy</a>
<ul>
<li class="chapter" data-level="14.1" data-path="epidemiolgy.html"><a href="epidemiolgy.html#introduction"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="epidemiolgy.html"><a href="epidemiolgy.html#bias-analysis-and-control-in-r"><i class="fa fa-check"></i><b>14.2</b> Bias analysis and control IN R</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="bioinformation.html"><a href="bioinformation.html"><i class="fa fa-check"></i><b>15</b> Bioinformation</a>
<ul>
<li class="chapter" data-level="15.1" data-path="bioinformation.html"><a href="bioinformation.html#sequence-analysis"><i class="fa fa-check"></i><b>15.1</b> Sequence Analysis</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="common-issues-in-statistics.html"><a href="common-issues-in-statistics.html"><i class="fa fa-check"></i><b>16</b> Common issues in Statistics</a></li>
<li class="chapter" data-level="17" data-path="miscellaneous.html"><a href="miscellaneous.html"><i class="fa fa-check"></i><b>17</b> Miscellaneous</a>
<ul>
<li class="chapter" data-level="17.1" data-path="miscellaneous.html"><a href="miscellaneous.html#linear-algebra"><i class="fa fa-check"></i><b>17.1</b> Linear algebra</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="miscellaneous.html"><a href="miscellaneous.html#matrix-basics"><i class="fa fa-check"></i><b>17.1.1</b> Matrix basics</a></li>
<li class="chapter" data-level="17.1.2" data-path="miscellaneous.html"><a href="miscellaneous.html#operations"><i class="fa fa-check"></i><b>17.1.2</b> Operations</a></li>
<li class="chapter" data-level="17.1.3" data-path="miscellaneous.html"><a href="miscellaneous.html#eigen-decomposition"><i class="fa fa-check"></i><b>17.1.3</b> Eigen decomposition</a></li>
<li class="chapter" data-level="17.1.4" data-path="miscellaneous.html"><a href="miscellaneous.html#advanced-operations"><i class="fa fa-check"></i><b>17.1.4</b> Advanced operations</a></li>
<li class="chapter" data-level="17.1.5" data-path="miscellaneous.html"><a href="miscellaneous.html#solve-linear-equations"><i class="fa fa-check"></i><b>17.1.5</b> Solve linear equations</a></li>
<li class="chapter" data-level="17.1.6" data-path="miscellaneous.html"><a href="miscellaneous.html#summary"><i class="fa fa-check"></i><b>17.1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="miscellaneous.html"><a href="miscellaneous.html#calculus"><i class="fa fa-check"></i><b>17.2</b> Calculus</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="miscellaneous.html"><a href="miscellaneous.html#derivation"><i class="fa fa-check"></i><b>17.2.1</b> Derivation</a></li>
<li class="chapter" data-level="17.2.2" data-path="miscellaneous.html"><a href="miscellaneous.html#integration"><i class="fa fa-check"></i><b>17.2.2</b> Integration</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="miscellaneous.html"><a href="miscellaneous.html#sample-size-calculation-simulation-based-power"><i class="fa fa-check"></i><b>17.3</b> Sample size calculation (simulation-based power)</a></li>
<li class="chapter" data-level="17.4" data-path="miscellaneous.html"><a href="miscellaneous.html#how-to-evaluate-a-z-score"><i class="fa fa-check"></i><b>17.4</b> How to evaluate a z score</a>
<ul>
<li class="chapter" data-level="17.4.1" data-path="miscellaneous.html"><a href="miscellaneous.html#call-the-parameter-estimates-file"><i class="fa fa-check"></i><b>17.4.1</b> Call the parameter estimates file</a></li>
<li class="chapter" data-level="17.4.2" data-path="miscellaneous.html"><a href="miscellaneous.html#calculate-mean-and-standard-deviation-at-a-specific-value"><i class="fa fa-check"></i><b>17.4.2</b> Calculate mean and standard deviation at a specific value</a></li>
<li class="chapter" data-level="17.4.3" data-path="miscellaneous.html"><a href="miscellaneous.html#output-the-calculated-mean-on-exp-scale"><i class="fa fa-check"></i><b>17.4.3</b> Output the calculated mean (on exp scale)</a></li>
<li class="chapter" data-level="17.4.4" data-path="miscellaneous.html"><a href="miscellaneous.html#calculate-z-score"><i class="fa fa-check"></i><b>17.4.4</b> Calculate z score</a></li>
<li class="chapter" data-level="17.4.5" data-path="miscellaneous.html"><a href="miscellaneous.html#result-checking"><i class="fa fa-check"></i><b>17.4.5</b> Result checking</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="miscellaneous.html"><a href="miscellaneous.html#mathematical-coupling"><i class="fa fa-check"></i><b>17.5</b> Mathematical coupling</a></li>
<li class="chapter" data-level="17.6" data-path="miscellaneous.html"><a href="miscellaneous.html#how-to-create-a-bookdown"><i class="fa fa-check"></i><b>17.6</b> How to create a bookdown</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="miscellaneous.html"><a href="miscellaneous.html#how-to-git-up-a-project-into-github"><i class="fa fa-check"></i><b>17.6.1</b> How to git up a project into GitHub</a></li>
<li class="chapter" data-level="17.6.2" data-path="miscellaneous.html"><a href="miscellaneous.html#how-to-git-down-a-project-into-your-pc"><i class="fa fa-check"></i><b>17.6.2</b> How to git down a project into your PC</a></li>
<li class="chapter" data-level="17.6.3" data-path="miscellaneous.html"><a href="miscellaneous.html#how-to-return-to-a-previous-version"><i class="fa fa-check"></i><b>17.6.3</b> How to return to a previous version</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="miscellaneous.html"><a href="miscellaneous.html#how-to-create-a-blogdown"><i class="fa fa-check"></i><b>17.7</b> How to create a blogdown</a></li>
<li class="chapter" data-level="17.8" data-path="miscellaneous.html"><a href="miscellaneous.html#how-to-install-tensorflow-and-keras"><i class="fa fa-check"></i><b>17.8</b> How to install tensorflow and keras</a></li>
<li class="chapter" data-level="17.9" data-path="miscellaneous.html"><a href="miscellaneous.html#how-to-use-github-in-a-team"><i class="fa fa-check"></i><b>17.9</b> How to use GitHub in a team</a></li>
<li class="chapter" data-level="17.10" data-path="miscellaneous.html"><a href="miscellaneous.html#how-to-insert-a-picture-indirectly-in-markdown"><i class="fa fa-check"></i><b>17.10</b> How to insert a picture indirectly in markdown</a></li>
<li class="chapter" data-level="17.11" data-path="miscellaneous.html"><a href="miscellaneous.html#r-cheatsheets"><i class="fa fa-check"></i><b>17.11</b> R cheatsheets</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biostatistics Handbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Machine learning<a href="machine-learning.html#machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Machine learning (ML) is a collection of methods that learn patterns from data and use those patterns to make predictions or classifications. In practice, ML is most useful when (1) the relationship between predictors and outcome is complex, (2) there are many predictors and potential interactions, or (3) prediction performance is the primary goal.</p>
<p>This chapter focuses on a practical, end-to-end workflow using the <code>caret</code> ecosystem. The emphasis is on <strong>reproducible steps</strong> that appear repeatedly in real work:<br />
1) load data and inspect structure,<br />
2) split data into training and test sets,<br />
3) perform preprocessing (imputation, encoding, normalization),<br />
4) explore features visually,<br />
5) train models and tune hyperparameters,<br />
6) evaluate performance using a confusion matrix and cross-validation, and<br />
7) compare/ensemble multiple models.</p>
<p>We will use the Pima Indians Diabetes dataset (<code>PimaIndiansDiabetes</code>) as a standard binary classification example. The outcome variable is <code>diabetes</code>, and the predictors are clinical measurements. The goal is to predict whether a subject has diabetes.</p>
<p>For more details, please read <a href="https://rpubs.com/Daniel_He/1396669">here</a>.</p>
<!-- ## Machine learning workflow -->
<!-- A common mistake in ML is to “peek” at the test data during preprocessing or tuning. In a strict workflow, the test set is held out until the end. All preprocessing models (imputation, encoding, scaling) should be built **on training data only**, then applied to the test data using the same transformation objects. This prevents information leakage and gives a more honest estimate of performance. -->
<!-- ### Loading packages and datasets -->
<!-- We first load the dataset. It is good practice to keep dataset names short and consistent, especially in teaching code or iterative modeling scripts.   -->
<!-- ```{r} -->
<!-- # load the Pima Indians dataset from the mlbench dataset -->
<!-- library(mlbench) -->
<!-- data(PimaIndiansDiabetes) -->
<!-- # rename dataset to have shorter name because lazy -->
<!-- diabetes <- PimaIndiansDiabetes -->
<!-- ``` -->
<!-- - look at the data set   -->
<!-- Before modeling, always inspect the dataset: check variable types, outcome coding, and whether any variables have unexpected distributions. This early check often prevents downstream errors.   -->
<!-- ```{r} -->
<!-- # install.packages(c('caret', 'skimr', 'RANN', 'randomForest',   'gbm', 'xgboost', 'caretEnsemble', 'C50', 'earth')) -->
<!-- # Load the caret package -->
<!-- library(caret) -->
<!-- # Structure of the dataframe -->
<!-- str(diabetes) -->
<!-- # See top 6 rows -->
<!-- head(diabetes ) -->
<!-- ``` -->
<!-- ### Spliting the dataset into training and test data sets -->
<!-- Splitting data is essential for evaluating generalization. The training set is used to learn model parameters and tuning, and the test set is used for final evaluation.   -->
<!-- Here we use an 80/20 split. `createDataPartition()` performs a stratified split when possible, which is important for classification tasks because it helps preserve the class proportions in the training and test sets.   -->
<!-- ```{r} -->
<!-- # Create the training and test datasets -->
<!-- set.seed(100) -->
<!-- # Step 1: Get row numbers for the training data -->
<!-- trainRowNumbers <- createDataPartition(diabetes$diabetes, p=0.8, list=FALSE) -->
<!-- # Step 2: Create the training  dataset -->
<!-- trainData <- diabetes[trainRowNumbers,] -->
<!-- # Step 3: Create the test dataset -->
<!-- testData <- diabetes[-trainRowNumbers,] -->
<!-- # Store X and Y for later use. -->
<!-- # x = trainData[, -1] -->
<!-- y = trainData$diabetes -->
<!-- ``` -->
<!-- - have a look training data set   -->
<!-- A quick summary of missingness, distributions, and variable types helps decide which preprocessing steps are needed. `skimr::skim()` is a convenient way to get a compact overview.   -->
<!-- ```{r} -->
<!-- library(skimr) -->
<!-- skimmed <- skim (trainData) -->
<!-- skimmed -->
<!-- ``` -->
<!-- ### Implement data imputation -->
<!-- Missing data are common in real-world clinical and EHR datasets. Many ML algorithms cannot handle missing values directly, so imputation is a practical preprocessing step.   -->
<!-- Important practice note: the imputation model must be fit on the training data only, then applied to both training and test sets using the same fitted object. -->
<!-- - compiling knnimpute model   -->
<!-- Here we use k-nearest neighbors imputation (`knnImpute`). This method imputes missing values based on similar observations in the feature space.   -->
<!-- ```{r} -->
<!-- # Create the knn imputation model on the training data -->
<!-- preProcess_missingdata_model <- preProcess(trainData, method='knnImpute') -->
<!-- preProcess_missingdata_model -->
<!-- ``` -->
<!-- - check missingness   -->
<!-- After applying imputation, we check if any missing values remain. If `anyNA(trainData)` is still TRUE, we need to investigate which variables were not imputed and why.   -->
<!-- ```{r} -->
<!-- # Use the imputation model to predict the values of missing data points -->
<!-- library(RANN)  # required for knnInpute -->
<!-- trainData <- predict(preProcess_missingdata_model, newdata = trainData) -->
<!-- anyNA(trainData) -->
<!-- ``` -->
<!-- ### One-hot-endcoding -->
<!-- Many ML algorithms require numeric inputs. Categorical predictors must be converted into numeric features. One-hot encoding (dummy variables) is a standard approach: it converts a categorical variable with K levels into K (or K-1) binary indicators.   -->
<!-- - Y (dependent) will not be encoded as one-hot-encoding   -->
<!-- A practical rule: **do not one-hot encode the outcome** in typical classification workflows. Keep the outcome as a factor with levels representing classes.   -->
<!-- This code creates a dummy-variable model based on the training data, then applies it to generate a numeric feature matrix.   -->
<!-- ```{r} -->
<!-- # One-Hot Encoding -->
<!-- # Creating dummy variables is converting a categorical variable to as many binary variables as here are categories. -->
<!-- dummies_model <- dummyVars(diabetes ~ ., data=trainData) -->
<!-- # Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat. -->
<!-- trainData_mat <- predict(dummies_model, newdata = trainData) -->
<!-- # # Convert to dataframe -->
<!-- trainData <- data.frame(trainData_mat) -->
<!-- # # See the structure of the new dataset -->
<!-- str(trainData) -->
<!-- ``` -->
<!-- ### Normalizing features -->
<!-- Many ML algorithms are sensitive to feature scales (e.g., KNN, SVM). Normalization ensures predictors are on comparable scales. Here we use range scaling to transform features to [0, 1].   -->
<!-- Key practice note: scaling parameters must be learned on training data only, then applied to test data using the same fitted object.   -->
<!-- ```{r} -->
<!-- preProcess_range_model <- preProcess(trainData, method='range') -->
<!-- trainData <- predict(preProcess_range_model, newdata = trainData) -->
<!-- # Append the Y variable instead of normalized data -->
<!-- trainData$diabetes <- y -->
<!-- # Look the dataset -->
<!-- apply(trainData[, -1], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))}) -->
<!-- ``` -->
<!-- After preprocessing, always confirm the structure. This helps catch unexpected type changes or column ordering issues.   -->
<!-- ```{r} -->
<!-- str(trainData) -->
<!-- ``` -->
<!-- ### Plot features -->
<!-- Exploratory data analysis (EDA) is still valuable in ML workflows. Even when prediction is the goal, feature plots can reveal outliers, skewness, separation between classes, and variables that may be uninformative.   -->
<!-- Boxplots show distribution and outliers by class.   -->
<!-- ```{r} -->
<!-- featurePlot(x = trainData[, 1:8], -->
<!--             y = trainData$diabetes, -->
<!--             plot = "box", -->
<!--             strip=strip.custom(par.strip.text=list(cex=.7)), -->
<!--             scales = list(x = list(relation="free"), -->
<!--                           y = list(relation="free"))) -->
<!-- ``` -->
<!-- Density plots show the entire distribution and allow visual comparison between classes (e.g., “pos” vs “neg”).   -->
<!-- ```{r} -->
<!-- featurePlot(x = trainData[, 1:8], -->
<!--             y = trainData$diabetes, -->
<!--             plot = "density", -->
<!--             strip=strip.custom(par.strip.text=list(cex=.7)), -->
<!--             scales = list(x = list(relation="free"), -->
<!--                           y = list(relation="free"))) -->
<!-- ``` -->
<!-- Correlation plots help detect highly correlated predictors. Strong correlations can affect some models (e.g., linear models) and can also reduce interpretability. In tree-based models, correlation is less problematic, but it still matters for feature importance interpretation.   -->
<!-- ```{r} -->
<!--  library(corrplot) -->
<!-- corrplot(cor((trainData[,-9] ))) -->
<!-- ``` -->
<!-- ### **Recursive feature elimination (rfe)** -->
<!-- Feature selection is sometimes used to reduce dimensionality, improve interpretability, and possibly improve generalization. However, removing predictors can also discard useful signals. In many applied settings, we use feature selection as a diagnostic tool rather than a hard rule.   -->
<!-- - In some scenarios, we just have to include the significant features into the following model. A good choice of selecting the important features is the recursive feature elimination (RFE).   -->
<!-- RFE iteratively trains a model and removes the least important predictors, then evaluates performance for different subset sizes. -->
<!-- - the final subset model is marked with a `starisk` in the last column, here it is 8th.   -->
<!-- In the printed RFE output, the chosen subset size is marked. This indicates which number of predictors produced the best resampling performance under the chosen control settings. -->
<!-- - though it is not wise to neglect the other predictors.   -->
<!-- This is a practical warning: “important” features can change depending on resampling splits and model type. If the goal is prediction, do not remove variables unless you have a clear reason (e.g., cost, feasibility, regulatory constraints, or strong collinearity concerns). -->
<!-- Below we run RFE using random forest functions (`rfFuncs`) with repeated cross-validation.   -->
<!-- ```{r} -->
<!-- set.seed(100) -->
<!-- options(warn=-1) -->
<!-- subsets <- c(1:8) -->
<!-- ctrl <- rfeControl(functions = rfFuncs,  #random forest algorithm -->
<!--                    method = "repeatedcv", #k fold cross validation repeated 5 times -->
<!--                    repeats = 5, -->
<!--                    verbose = FALSE) -->
<!-- lmProfile <- rfe(x=trainData[, 1:8], y=trainData$diabetes, -->
<!--                  sizes = subsets, -->
<!--                  rfeControl = ctrl) -->
<!-- lmProfile -->
<!-- ``` -->
<!-- `look up features of all models in R`   -->
<!-- `caret` supports many model types through a unified interface. You can list all available model methods, and you can look up details for a specific model. This is helpful when selecting candidate algorithms for a given problem.   -->
<!-- ```{r} -->
<!-- # See available algorithms in caret -->
<!-- modelnames <- paste(names(getModelInfo()), collapse=',  ') -->
<!-- ``` -->
<!-- This shows tuning parameters and required packages for the `xgbTree` method.   -->
<!-- ```{r} -->
<!-- modelLookup('xgbTree') -->
<!-- ``` -->
<!-- ### Training a model `Multivariate Adaptive Regression Splines (MARS)` -->
<!-- MARS is a flexible regression/classification approach that can model nonlinear relationships using piecewise linear basis functions. It is often more interpretable than many black-box models while still capturing nonlinearity.   -->
<!-- We train the model using `caret::train()`. The formula interface `diabetes ~ .` uses all predictors in `trainData` to predict `diabetes`.   -->
<!-- ```{r} -->
<!-- # Set the seed for reproducibility -->
<!-- set.seed(100) -->
<!-- # Train the model using randomForest and predict on the training data itself. -->
<!-- model_mars = train(diabetes ~ ., data=trainData, method='earth') -->
<!-- fitted <- predict(model_mars) -->
<!-- ``` -->
<!-- - the default of resampling (Bootstrapped) is 25 reps   -->
<!-- The `train()` function uses resampling to estimate performance during training. If you do not specify `trControl`, caret uses a default resampling method (often bootstrap). This matters because reported accuracy during training is a resampling estimate, not the final test performance.   -->
<!-- ```{r} -->
<!-- model_mars -->
<!-- ``` -->
<!-- - plot the Accuracy of various combinations of the hyper parameters - `interaction.depth and n.trees`.   -->
<!-- In practice, always plot performance across tuning parameters to see stability. Even when a “best” tuning is selected, nearby settings may perform similarly.   -->
<!-- ```{r} -->
<!-- plot(model_mars, main="Model Accuracies with MARS") -->
<!-- ``` -->
<!-- - calculate the importance of variable   -->
<!-- Variable importance provides a rough summary of which predictors the model relies on. Interpret importance with caution, especially when predictors are correlated.   -->
<!-- ```{r} -->
<!-- varimp_mars <- varImp(model_mars) -->
<!-- plot(varimp_mars, main="Variable Importance with MARS") -->
<!-- ``` -->
<!-- ### Prepare the test data set -->
<!-- - `imputation,dummy, and normalization`   -->
<!-- This is a critical step: we must apply the **same** transformations learned from training data to the test data. We do not refit preprocessing on test data.   -->
<!-- The following code applies (1) imputation, (2) dummy encoding, and (3) range normalization to the test set, using the objects already fitted on training data.   -->
<!-- ```{r} -->
<!-- # Step 1: Impute missing values -->
<!-- testData2 <- predict(preProcess_missingdata_model, testData) -->
<!-- # Step 2: Create one-hot encodings (dummy variables) -->
<!-- testData3 <- predict(dummies_model, testData2) -->
<!-- # Step 3: Transform the features to range between 0 and 1 -->
<!-- testData4 <- predict(preProcess_range_model, testData3) -->
<!-- # View -->
<!-- head(testData4 ) -->
<!-- ``` -->
<!-- ### Prediction uisng testdata -->
<!-- Now we generate predictions on the final test feature matrix. In classification, predictions are typically class labels; in other settings, we may predict probabilities as well.   -->
<!-- ```{r} -->
<!-- # Predict on testData -->
<!-- predicted <- predict(model_mars, testData4) -->
<!-- head(predicted) -->
<!-- ``` -->
<!-- ### Compute confusion matrix -->
<!-- The confusion matrix summarizes classification performance by comparing predicted labels with true labels. It includes sensitivity, specificity, and overall accuracy. In medical or clinical contexts, sensitivity/specificity are often more informative than accuracy alone, especially when classes are imbalanced.   -->
<!-- ```{r} -->
<!-- # Compute the confusion matrix -->
<!-- confusionMatrix(reference = as.factor(testData$diabetes), data = as.factor(predicted )) -->
<!-- ``` -->
<!-- ### Tuning hyperparameter to optimize the model -->
<!-- Hyperparameter tuning searches over model settings to improve performance. The key is to tune using cross-validation on the training set, then evaluate final tuned performance on the test set.   -->
<!-- - setting up hyper parameter `tuneLength, tuneGrid`   -->
<!-- Here we define cross-validation settings and specify that class probabilities and ROC-based summary metrics should be computed. ROC/AUC is often preferred when class imbalance exists or when threshold choice matters.   -->
<!-- ```{r} -->
<!-- # Define the training control -->
<!-- fitControl <- trainControl( -->
<!--     method = 'cv',                   # k-fold cross validation -->
<!--     number = 5,                      # number of folds -->
<!--     savePredictions = 'final',       # saves predictions for optimal tuning parameter -->
<!--     classProbs = T,                  # should class probabilities be returned -->
<!--     summaryFunction=twoClassSummary  # results summary function -->
<!-- ) -->
<!-- ``` -->
<!-- We define a tuning grid for MARS:   -->
<!-- - `nprune` controls the number of terms retained (model complexity),   -->
<!-- - `degree` controls interaction degree (nonlinearity/interaction flexibility).   -->
<!-- Then we train using ROC as the optimization metric.   -->
<!-- ```{r} -->
<!-- # Step 1: Define the tuneGrid -->
<!-- marsGrid <-  expand.grid(nprune = c(2, 4, 6, 8, 10), -->
<!--                          degree = c(1, 2, 3)) -->
<!-- # Step 2: Tune hyper parameters by setting tuneGrid -->
<!-- set.seed(100) -->
<!-- model_mars3 = train(diabetes ~ ., data=trainData, method='earth', metric='ROC', tuneGrid = marsGrid, trControl = fitControl) -->
<!-- model_mars3 -->
<!-- ``` -->
<!-- After tuning, evaluate on the test data. This gives a more realistic estimate of out-of-sample performance than training resampling metrics.   -->
<!-- ```{r} -->
<!-- # Step 3: Predict on testData and Compute the confusion matrix -->
<!-- predicted3 <- predict(model_mars3, testData4) -->
<!-- confusionMatrix(reference = as.factor(testData$diabetes), data = as.factor(predicted3   )) -->
<!-- ``` -->
<!-- ### Other marchine learning algorithms -->
<!-- It is good practice to compare multiple algorithms. Different models have different bias-variance profiles and handle nonlinearities and interactions differently. Here we train several common classifiers using the same training control settings. -->
<!-- #### **adaboost algorithm** -->
<!-- AdaBoost is an ensemble method that combines weak learners into a stronger classifier. It can perform well but may be sensitive to noise and outliers.   -->
<!-- ```{r} -->
<!-- set.seed(100) -->
<!-- # Train the model using adaboost -->
<!-- model_adaboost = train(diabetes ~ ., data=trainData, method='gbm', tuneLength=2, trControl = fitControl) -->
<!-- model_adaboost -->
<!-- ``` -->
<!-- #### **random forest** -->
<!-- Random forest is a robust tree-based ensemble method. It handles nonlinearities and interactions well and is often a strong baseline model.   -->
<!-- ```{r} -->
<!-- set.seed(100) -->
<!-- # Train the model using rf -->
<!-- model_rf = train(diabetes ~ ., data=trainData, method='rf', tuneLength=5, trControl = fitControl) -->
<!-- model_rf -->
<!-- ``` -->
<!-- #### **xgbDART algorithm** -->
<!-- This section shows an example template for XGBoost DART, which is a boosted-tree approach with dropout. The code is commented out, which is a common way to keep a placeholder for future extensions.   -->
<!-- ```{r} -->
<!-- # set.seed(100) -->
<!-- # -->
<!-- # # Train the model using MARS -->
<!-- # model_xgbDART = train(Purchase ~ ., data=trainData, method='xgbDART', tuneLength=5, trControl = fitControl, verbose=F) -->
<!-- # model_xgbDART -->
<!-- ``` -->
<!-- #### **Support Vector Machines (SVM)** -->
<!-- SVMs can be very effective in classification, especially with nonlinear kernels. They are sensitive to feature scaling, which is why normalization earlier is important.   -->
<!-- ```{r} -->
<!-- set.seed(100) -->
<!-- # Train the model using MARS -->
<!-- model_svmRadial = train(diabetes ~ ., data=trainData, method='svmRadial', tuneLength=15, trControl = fitControl) -->
<!-- model_svmRadial -->
<!-- ``` -->
<!-- #### **K-Nearest Neighbors** -->
<!-- KNN is a simple and intuitive method: classify based on the majority class of the nearest neighbors. It is strongly affected by scaling and by the choice of K.   -->
<!-- ```{r} -->
<!-- set.seed(100) -->
<!-- # Train the model using MARS -->
<!-- model_knn = train(diabetes ~ ., data=trainData, method='knn', tuneLength=15, trControl = fitControl) -->
<!-- model_knn -->
<!-- ``` -->
<!-- ### Comparisons of different models -->
<!-- A common workflow is to train multiple models under the same resampling settings and then compare their performance distributions. `resamples()` collects resampling results across models for summary and visualization.   -->
<!-- ```{r} -->
<!-- # Compare model performances using resample() -->
<!-- models_compare <- resamples(list(ADABOOST=model_adaboost, RF=model_rf, knn=model_knn, MARS=model_mars3, SVM=model_svmRadial)) -->
<!-- # Summary of the models performances -->
<!-- summary(models_compare) -->
<!-- ``` -->
<!-- ### Plot comparisons of models -->
<!-- Visualization helps compare stability and variability across resampling folds. A model with slightly lower mean performance but more stability might be preferable in production settings.   -->
<!-- ```{r} -->
<!-- # Draw box plots to compare models -->
<!-- scales <- list(x=list(relation="free"), y=list(relation="free")) -->
<!-- bwplot(models_compare, scales=scales) -->
<!-- ``` -->
<!-- ### Ensemble predictions from multiple models -->
<!-- Ensembling combines multiple models to potentially improve performance and robustness. The idea is that different models capture different aspects of the signal; combining them can reduce variance and improve generalization. -->
<!-- - create multiple models   -->
<!-- Here we use `caretEnsemble` to train multiple algorithms under repeated cross-validation and store predictions for stacking.   -->
<!-- ```{r} -->
<!-- library(caretEnsemble) -->
<!-- # Stacking Algorithms - Run multiple algos in one call. -->
<!-- trainControl <- trainControl(method="repeatedcv", -->
<!--                              number=10, -->
<!--                              repeats=3, -->
<!--                              savePredictions=TRUE, -->
<!--                              classProbs=TRUE) -->
<!-- algorithmList <- c('rf', 'gbm', 'earth', 'knn', 'svmRadial') -->
<!-- set.seed(100) -->
<!-- models <- caretList(diabetes ~ ., data=trainData, trControl=trainControl, methodList=algorithmList) -->
<!-- results <- resamples(models) -->
<!-- summary(results) -->
<!-- ``` -->
<!-- - comparison by visualization   -->
<!-- Boxplots summarize performance across resampling repeats. This helps identify consistently strong models.   -->
<!-- ```{r} -->
<!-- # Box plots to compare models -->
<!-- scales <- list(x=list(relation="free"), y=list(relation="free")) -->
<!-- bwplot(results, scales=scales) -->
<!-- ``` -->
<!-- - ensemble predictions on testdata   -->
<!-- Stacking trains a meta-model (here a GLM) on the predictions from base models. This is a practical and widely used ensembling strategy.   -->
<!-- ```{r} -->
<!-- # Create the trainControl -->
<!-- set.seed(101) -->
<!-- stackControl <- trainControl(method="repeatedcv", -->
<!--                              number=10, -->
<!--                              repeats=3, -->
<!--                              savePredictions=TRUE, -->
<!--                              classProbs=TRUE) -->
<!-- # Ensemble the predictions of `models` to form a new combined prediction based on glm -->
<!-- stack.glm <- caretStack(models, method="glm", metric="Accuracy", trControl=stackControl) -->
<!-- (stack.glm) -->
<!-- ``` -->
<!-- - compute confusion matrix   -->
<!-- Finally, evaluate the stacked model on the test data. This provides a direct comparison against individual models.   -->
<!-- ```{r} -->
<!-- # Predict on testData -->
<!-- stack_predicteds <- predict(stack.glm, newdata=testData4) -->
<!-- stack_predicteds$label <- ifelse(stack_predicteds$neg > stack_predicteds$pos, "neg", "pos") -->
<!-- confusionMatrix(reference = as.factor(testData$diabetes), data = as.factor(stack_predicteds$label ) ) -->
<!-- ``` -->
<!-- <!-- =================== -->
<p>–&gt;</p>
<!-- ##  KNN Classifier -->
<!-- This section demonstrates KNN in a more direct way using the `class::knn()` function. This is useful for learning and for understanding how KNN works under the hood, separate from the caret wrapper.   -->
<!-- A key reminder: KNN requires feature scaling because it relies on distance calculations. -->
<!-- ```{r,message=FALSE} -->
<!-- # Loading package -->
<!-- # library(e1071) -->
<!-- library(caTools) -->
<!-- library(class) -->
<!-- ``` -->
<!-- ### Splitting data -->
<!-- We reload the dataset and split it again. In real projects, you would typically reuse the same split. Here the goal is to demonstrate a standalone KNN pipeline.   -->
<!-- ```{r} -->
<!-- # load the Pima Indians dataset from the mlbench dataset -->
<!-- library(mlbench) -->
<!-- data(PimaIndiansDiabetes) -->
<!-- # rename dataset to have shorter name because lazy -->
<!-- diabetes <- PimaIndiansDiabetes -->
<!-- ``` -->
<!-- We create a train/test split using `caTools::sample.split()`. This function is commonly used for quick splits.   -->
<!-- ```{r} -->
<!-- # Splitting data into train and test data -->
<!-- set.seed(100) -->
<!-- split <- sample.split(diabetes, SplitRatio = 0.8) -->
<!-- train_cl <- subset(diabetes, split == "TRUE") -->
<!-- test_cl <- subset(diabetes, split == "FALSE") -->
<!-- ``` -->
<!-- Feature scaling is required for KNN. Here we scale predictors to have mean 0 and standard deviation 1.   -->
<!-- This is different from range scaling earlier; both are common. The key is to apply the same scaling logic consistently.   -->
<!-- ```{r} -->
<!-- # Feature Scaling -->
<!-- train_scale <- scale(train_cl[, 1:8]) -->
<!-- test_scale <- scale(test_cl[, 1:8]) -->
<!-- # train_y <- scale(train_cl[, 5]) -->
<!-- # test_y <- scale(test_cl[, 5]) -->
<!-- ``` -->
<!-- ### Creating KNN model -->
<!-- KNN predicts the class of each test observation based on the majority class among its K nearest neighbors in the training set. Smaller K tends to fit more locally (higher variance), while larger K smooths more (higher bias).   -->
<!-- ```{r} -->
<!-- # Fitting KNN Model to training dataset -->
<!-- classifier_knn <- knn(train = train_scale, -->
<!--                       cl = train_cl$diabetes, -->
<!--                       test = test_scale, -->
<!--                       k = 1) -->
<!-- classifier_knn -->
<!-- ``` -->
<!-- ### Model Evaluation -->
<!-- - Creat confusion matrix   -->
<!-- A confusion matrix is the most basic evaluation tool for classifiers. It shows counts of correct/incorrect predictions by class.   -->
<!-- ```{r} -->
<!-- # Confusion Matrix -->
<!-- cm <- table(test_cl$diabetes, classifier_knn) -->
<!-- cm -->
<!-- ``` -->
<!-- ### Calculate accuracy with different K -->
<!-- Accuracy is easy to compute, but do not rely on accuracy alone if classes are imbalanced. In applied medical problems, sensitivity/specificity (or ROC/AUC) often matter more.   -->
<!-- ```{r} -->
<!-- # Model Evaluation - Choosing K =1 -->
<!-- # Calculate out of Sample error -->
<!-- misClassError <- mean(classifier_knn != test_cl$diabetes) -->
<!-- print(paste('Accuracy =', 1-misClassError)) -->
<!-- ``` -->
<!-- This example uses a larger K. In practice, you tune K by evaluating a range of values and selecting the best under cross-validation or a validation set.   -->
<!-- ```{r} -->
<!-- # K = 7 -->
<!-- classifier_knn <- knn(train = train_scale, -->
<!--                       test = test_scale, -->
<!--                       cl = train_cl$diabetes, -->
<!--                       k = 23) -->
<!-- misClassError <- mean(classifier_knn != test_cl$diabetes) -->
<!-- print(paste('Accuracy =', 1-misClassError)) -->
<!-- ``` -->
<!-- ###  Optimization -->
<!-- - search better k parameter   -->
<!-- Here we try a sequence of K values and compute accuracy for each. This gives a quick sensitivity analysis of performance versus K.   -->
<!-- Note: In production, prefer cross-validation rather than a single split to reduce randomness.   -->
<!-- ```{r} -->
<!-- i=1 -->
<!-- k.optm=1 -->
<!-- for (i in 1:39){ -->
<!--  y_pred = knn(train = train_scale, -->
<!--              test = test_scale, -->
<!--              cl = train_cl$diabetes, -->
<!--              k = i ) -->
<!--  k.optm[i] <-   1- mean(y_pred != test_cl$diabetes) -->
<!--  k=i -->
<!--  cat(k,'=',k.optm[i],'') -->
<!--  } -->
<!-- ``` -->
<!-- - Accuracy plot `k=15`   -->
<!-- Plotting performance across K helps identify a stable region rather than a single “lucky” optimum.   -->
<!-- ```{r} -->
<!-- plot(k.optm, type="b", xlab="K- Value",ylab="RMSE level") -->
<!-- ``` -->
<!-- ### Visualization -->
<!-- This section is a placeholder for visualization of results. In practice, you might visualize:   -->
<!-- - decision boundaries (for 2D cases),   -->
<!-- - ROC curves,   -->
<!-- - or distributions of predicted probabilities.   -->
<!-- ```{r} -->
<!-- # Visualising the Training set results -->
<!-- # Install ElemStatLearn if not present -->
<!-- ``` -->
<!-- ## KNN regression -->
<!-- KNN can also be used for regression problems, where the prediction is a continuous value. Instead of majority vote, regression KNN typically averages the outcomes of nearest neighbors.   -->
<!-- This section uses the Boston housing dataset (`MASS::Boston`) and focuses on predicting `medv` (median value of owner-occupied homes). -->
<!-- ### Data exploring -->
<!-- We first inspect missingness and correlation structure. For KNN regression, feature scaling is especially important because distance computations drive the model.   -->
<!-- ```{r, message=FALSE} -->
<!-- library("Amelia") -->
<!-- ``` -->
<!-- The missingness map provides a quick visual summary of missing patterns.   -->
<!-- ```{r} -->
<!-- data("Boston", package = "MASS") -->
<!-- missmap(Boston,col=c('yellow','black'),y.at=1,y.labels='',legend=TRUE) -->
<!-- ``` -->
<!-- Correlation plots help identify redundant predictors and strong relationships with the outcome.   -->
<!-- ```{r} -->
<!-- library(corrplot) -->
<!-- corrplot(cor((Boston))) -->
<!-- ``` -->
<!-- Descriptive summaries help confirm variable ranges and distributions before selecting predictors.   -->
<!-- ```{r,message=F} -->
<!-- library(Hmisc) -->
<!-- describe(Boston) -->
<!-- ``` -->
<!-- ### Prepareing data -->
<!-- We select a subset of variables for a simpler demonstration. In practice, variable selection may be based on domain knowledge, correlation screening, or model-based importance.   -->
<!-- ```{r} -->
<!-- Boston <-    dplyr::select (Boston ,medv , crim , rm , tax , lstat) -->
<!-- ``` -->
<!-- We split the dataset into training and test sets. This is necessary to evaluate out-of-sample RMSE (or other regression metrics).   -->
<!-- ```{r} -->
<!-- # Splitting the dataset into -->
<!-- # the Training set and Test set -->
<!-- # install.packages('caTools') -->
<!-- library(caTools) -->
<!-- set.seed(123) -->
<!-- split = sample.split(Boston$medv, -->
<!--                      SplitRatio = 0.75) -->
<!-- training_set_origi = subset(Boston, -->
<!--                       split == TRUE) -->
<!-- test_set_origi = subset(Boston, -->
<!--                   split == FALSE) -->
<!-- ``` -->
<!-- Feature scaling is applied to predictors. This is critical for distance-based methods like KNN.   -->
<!-- ```{r} -->
<!-- # Feature Scaling -->
<!-- training_set = scale(training_set_origi[,-1] ) -->
<!-- test_set = scale(test_set_origi [,-1]) -->
<!-- ``` -->
<!-- ### Creating model -->
<!-- This code fits a KNN model and predicts on the test set.   -->
<!-- Note: The `class::knn()` function is primarily designed for classification, but this code demonstrates a simplified approach for regression-style prediction. In applied regression settings, you would typically use a KNN regression implementation (e.g., from specialized packages) to avoid forced categorical conversion.   -->
<!-- ```{r} -->
<!-- # Fitting K-NN to the Training set -->
<!-- # and Predicting the Test set results -->
<!-- # library(class) -->
<!-- y_pred = knn(train = training_set[, -1], -->
<!--              test = test_set[, -1], -->
<!--              cl = training_set_origi[, 1], -->
<!--              k = 15 ) -->
<!-- # -->
<!-- ``` -->
<!-- ### Evaluation -->
<!-- We compute prediction error and a simple RMSE-like metric. In standard regression, RMSE is computed as `sqrt(mean(error^2))`. Here we follow the structure in the code and inspect results.   -->
<!-- ```{r} -->
<!-- # converting factor into character then into numeric -->
<!-- error <- test_set_origi[,1]-as.numeric (as.character(y_pred)) -->
<!-- head(error) -->
<!-- rmse <- sqrt(mean(error)^2) -->
<!-- rmse -->
<!-- ``` -->
<!-- Plotting the error helps detect systematic bias or extreme outliers.   -->
<!-- ```{r} -->
<!-- plot(error) -->
<!-- ``` -->
<!-- Viewing observed vs predicted values is a quick sanity check.   -->
<!-- ```{r} -->
<!-- head(cbind(test_set_origi[,1], as.numeric (as.character(y_pred)))) -->
<!-- ``` -->
<!-- ###  Optimization -->
<!-- - search better k parameter   -->
<!-- As in classification, K controls the bias-variance tradeoff. We search over a range of K values and compute the metric stored in `k.optm`.   -->
<!-- ```{r} -->
<!-- i=1 -->
<!-- k.optm=1 -->
<!-- for (i in 1:29){ -->
<!--  y_pred = knn(train = training_set[, -1], -->
<!--              test = test_set[, -1], -->
<!--              cl = training_set_origi[, 1], -->
<!--              k = i ) -->
<!--  k.optm[i] <-  sqrt(mean(   test_set_origi[,1]-as.numeric (as.character(y_pred))   )^2) -->
<!--  k=i -->
<!--  cat(k,'=',k.optm[i],'') -->
<!--  } -->
<!-- ``` -->
<!-- - Accuracy plot `k=15`   -->
<!-- Plotting the metric versus K helps identify the region where performance stabilizes and avoid choosing K based on a single noisy optimum.   -->
<!-- ```{r} -->
<!-- plot(k.optm, type="b", xlab="K- Value",ylab="RMSE level") -->
<!-- ``` -->

</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-wrangling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="deep-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/USERNAME/REPO/edit/BRANCH/02-machinelearning.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["_main.pdf", "_main.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
