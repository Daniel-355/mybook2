[["index.html", "Biostatistics Handbook Preface", " Biostatistics Handbook Daniel He 2026-02-20 Preface This book is a practice-oriented handbook in biostatistics, written for readers who want to apply statistical methods to real data rather than focus solely on mathematical derivations. In modern biomedical, epidemiological, and clinical research, data are increasingly complex, high-dimensional, and imperfect. Real-world datasets often contain missing values, measurement errors, and intricate correlation structures. As a result, biostatisticians and data analysts must go beyond theoretical knowledge and develop the ability to implement methods correctly, interpret results responsibly, and communicate findings clearly. This book is written to address these practical challenges. A practice-first approach to biostatistics The central philosophy of this book is to bridge statistical theory and applied data analysis. Statistics is treated not as an abstract mathematical discipline, but as a set of tools designed to answer scientific questions. The book primarily uses R as the computational platform, with selected chapters incorporating SAS where it remains widely used in practice. Topics are motivated by common analytical tasks encountered in biomedical research, including: data wrangling, aggregation, and restructuring descriptive statistics and data visualization construction of publication-ready summary tables (e.g., Table 1) handling missing data using principled approaches model selection, estimation, and interpretation integration of machine learning and deep learning methods statistical reasoning in causal inference and clinical trials Statistical theory is introduced only to the extent necessary to support correct application, clarify assumptions, and guide interpretation. Organization of the book The chapters are organized to reflect a realistic data analysis workflow: Data Wrangling and Visualization introduce essential tools for preparing, summarizing, and exploring data. Basic Statistics, Probability, and Algorithms provide the foundational concepts required to understand statistical methods. Statistical Models form the core of the book, with extensive coverage of linear regression, mixed-effects models, spline regression, and practical inference. Machine Learning and Deep Learning emphasize workflow, intuition, and implementation rather than black-box usage. Causal Inference and Clinical Trials focus on estimation targets, bias control, and scientific interpretation. Bayesian Statistics, Epidemiology, and Bioinformatics offer focused introductions for specialized applications. Miscellaneous Topics address common practical issues such as simulation, sample size calculation, linear algebra, reproducible reporting, and collaborative tools. Each chapter is designed to be readable on its own and useful as a long-term reference. Intended audience This book is intended for: students in biostatistics, statistics, or related disciplines clinical researchers and epidemiologists working with real data data scientists in biomedical and health-related fields practitioners seeking a practical reference rather than a purely theoretical text A basic background in statistics and familiarity with R are helpful, but the book emphasizes applied understanding over formal prerequisites. Scope and philosophy This book does not aim to be an exhaustive theoretical treatise. Instead, it is meant to serve as a working handbook for applied statistical analysis. In real research settings, data limitations, study design constraints, and practical considerations often matter more than elegant formulas. By emphasizing implementation, interpretation, and reproducibility, this book aims to help readers apply statistical methods thoughtfully, rigorously, and responsibly in real-world biomedical research. “In God we Trust, all others bring data and a statistician who believes in God.” "],["data-wrangling.html", "1 Data Wrangling 1.1 How to do data wrangling 1.2 How to do aggregation/ summarization 1.3 How to creat table 1 with test 1.4 Imputing Missing Data with MICE", " 1 Data Wrangling 1.1 How to do data wrangling Data wrangling is the process of turning raw data into an analysis-ready dataset. In practice, this includes (1) selecting the right observations and variables, (2) creating or transforming variables, (3) cleaning and standardizing values, and (4) reshaping the dataset into the structure required by downstream analyses and reporting. In this chapter, we use functions from the tidyverse (mainly dplyr and tidyr) because they provide a consistent grammar for data manipulation. The goal here is not only to learn syntax, but also to understand common patterns that appear repeatedly in real projects. We will use the built-in iris dataset for simple examples, and then create small simulated datasets (e.g., drug_trial, babies, blood_pressure) to demonstrate typical workflows. 1.1.1 Load data and package Before doing any wrangling, it is good practice to quickly inspect the dataset. Looking at the first few rows helps confirm variable names, data types, and whether the values look reasonable. head (iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Load the tidyverse package. This automatically loads common packages for wrangling and visualization, including dplyr, tidyr, tibble, and ggplot2. library(tidyverse) ## Warning: package &#39;ggplot2&#39; was built under R version 4.4.3 ## Warning: package &#39;readr&#39; was built under R version 4.4.3 ## ── Attaching core tidyverse packages ────────────────── ## ✔ dplyr 1.1.4 ✔ readr 2.1.6 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 4.0.2 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ───────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors 1.1.2 Select certain rows Row selection is typically used to (1) create analysis populations, (2) subset by treatment, time window, or eligibility criteria, or (3) debug issues using a small subset. The filter() function keeps rows that satisfy a condition. Here, we select only the setosa species. setosa &lt;- filter(iris, Species == &#39;setosa&#39;) setosa ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## 11 5.4 3.7 1.5 0.2 setosa ## 12 4.8 3.4 1.6 0.2 setosa ## 13 4.8 3.0 1.4 0.1 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 15 5.8 4.0 1.2 0.2 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 17 5.4 3.9 1.3 0.4 setosa ## 18 5.1 3.5 1.4 0.3 setosa ## 19 5.7 3.8 1.7 0.3 setosa ## 20 5.1 3.8 1.5 0.3 setosa ## 21 5.4 3.4 1.7 0.2 setosa ## 22 5.1 3.7 1.5 0.4 setosa ## 23 4.6 3.6 1.0 0.2 setosa ## 24 5.1 3.3 1.7 0.5 setosa ## 25 4.8 3.4 1.9 0.2 setosa ## 26 5.0 3.0 1.6 0.2 setosa ## 27 5.0 3.4 1.6 0.4 setosa ## 28 5.2 3.5 1.5 0.2 setosa ## 29 5.2 3.4 1.4 0.2 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 31 4.8 3.1 1.6 0.2 setosa ## 32 5.4 3.4 1.5 0.4 setosa ## 33 5.2 4.1 1.5 0.1 setosa ## 34 5.5 4.2 1.4 0.2 setosa ## 35 4.9 3.1 1.5 0.2 setosa ## 36 5.0 3.2 1.2 0.2 setosa ## 37 5.5 3.5 1.3 0.2 setosa ## 38 4.9 3.6 1.4 0.1 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 40 5.1 3.4 1.5 0.2 setosa ## 41 5.0 3.5 1.3 0.3 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 44 5.0 3.5 1.6 0.6 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 47 5.1 3.8 1.6 0.2 setosa ## 48 4.6 3.2 1.4 0.2 setosa ## 49 5.3 3.7 1.5 0.2 setosa ## 50 5.0 3.3 1.4 0.2 setosa filter the first 5 rows When you need a small subset for quick checking, you can take the first few rows. slice() selects rows by position rather than by condition. iris %&gt;% slice(1:5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa 1.1.3 Select certain columns Selecting columns is useful for (1) focusing on variables needed for an analysis, (2) reducing clutter when printing tables, and (3) preparing a dataset before joining or reshaping. select() keeps only specified variables. select(iris, Sepal.Length, Species) ## Sepal.Length Species ## 1 5.1 setosa ## 2 4.9 setosa ## 3 4.7 setosa ## 4 4.6 setosa ## 5 5.0 setosa ## 6 5.4 setosa ## 7 4.6 setosa ## 8 5.0 setosa ## 9 4.4 setosa ## 10 4.9 setosa ## 11 5.4 setosa ## 12 4.8 setosa ## 13 4.8 setosa ## 14 4.3 setosa ## 15 5.8 setosa ## 16 5.7 setosa ## 17 5.4 setosa ## 18 5.1 setosa ## 19 5.7 setosa ## 20 5.1 setosa ## 21 5.4 setosa ## 22 5.1 setosa ## 23 4.6 setosa ## 24 5.1 setosa ## 25 4.8 setosa ## 26 5.0 setosa ## 27 5.0 setosa ## 28 5.2 setosa ## 29 5.2 setosa ## 30 4.7 setosa ## 31 4.8 setosa ## 32 5.4 setosa ## 33 5.2 setosa ## 34 5.5 setosa ## 35 4.9 setosa ## 36 5.0 setosa ## 37 5.5 setosa ## 38 4.9 setosa ## 39 4.4 setosa ## 40 5.1 setosa ## 41 5.0 setosa ## 42 4.5 setosa ## 43 4.4 setosa ## 44 5.0 setosa ## 45 5.1 setosa ## 46 4.8 setosa ## 47 5.1 setosa ## 48 4.6 setosa ## 49 5.3 setosa ## 50 5.0 setosa ## 51 7.0 versicolor ## 52 6.4 versicolor ## 53 6.9 versicolor ## 54 5.5 versicolor ## 55 6.5 versicolor ## 56 5.7 versicolor ## 57 6.3 versicolor ## 58 4.9 versicolor ## 59 6.6 versicolor ## 60 5.2 versicolor ## 61 5.0 versicolor ## 62 5.9 versicolor ## 63 6.0 versicolor ## 64 6.1 versicolor ## 65 5.6 versicolor ## 66 6.7 versicolor ## 67 5.6 versicolor ## 68 5.8 versicolor ## 69 6.2 versicolor ## 70 5.6 versicolor ## 71 5.9 versicolor ## 72 6.1 versicolor ## 73 6.3 versicolor ## 74 6.1 versicolor ## 75 6.4 versicolor ## 76 6.6 versicolor ## 77 6.8 versicolor ## 78 6.7 versicolor ## 79 6.0 versicolor ## 80 5.7 versicolor ## 81 5.5 versicolor ## 82 5.5 versicolor ## 83 5.8 versicolor ## 84 6.0 versicolor ## 85 5.4 versicolor ## 86 6.0 versicolor ## 87 6.7 versicolor ## 88 6.3 versicolor ## 89 5.6 versicolor ## 90 5.5 versicolor ## 91 5.5 versicolor ## 92 6.1 versicolor ## 93 5.8 versicolor ## 94 5.0 versicolor ## 95 5.6 versicolor ## 96 5.7 versicolor ## 97 5.7 versicolor ## 98 6.2 versicolor ## 99 5.1 versicolor ## 100 5.7 versicolor ## 101 6.3 virginica ## 102 5.8 virginica ## 103 7.1 virginica ## 104 6.3 virginica ## 105 6.5 virginica ## 106 7.6 virginica ## 107 4.9 virginica ## 108 7.3 virginica ## 109 6.7 virginica ## 110 7.2 virginica ## 111 6.5 virginica ## 112 6.4 virginica ## 113 6.8 virginica ## 114 5.7 virginica ## 115 5.8 virginica ## 116 6.4 virginica ## 117 6.5 virginica ## 118 7.7 virginica ## 119 7.7 virginica ## 120 6.0 virginica ## 121 6.9 virginica ## 122 5.6 virginica ## 123 7.7 virginica ## 124 6.3 virginica ## 125 6.7 virginica ## 126 7.2 virginica ## 127 6.2 virginica ## 128 6.1 virginica ## 129 6.4 virginica ## 130 7.2 virginica ## 131 7.4 virginica ## 132 7.9 virginica ## 133 6.4 virginica ## 134 6.3 virginica ## 135 6.1 virginica ## 136 7.7 virginica ## 137 6.3 virginica ## 138 6.4 virginica ## 139 6.0 virginica ## 140 6.9 virginica ## 141 6.7 virginica ## 142 6.9 virginica ## 143 5.8 virginica ## 144 6.8 virginica ## 145 6.7 virginica ## 146 6.7 virginica ## 147 6.3 virginica ## 148 6.5 virginica ## 149 6.2 virginica ## 150 5.9 virginica You can also exclude variables using a minus sign. This is helpful when you want to drop only a few variables while keeping most columns. select(iris, -Sepal.Length, -Species) ## Sepal.Width Petal.Length Petal.Width ## 1 3.5 1.4 0.2 ## 2 3.0 1.4 0.2 ## 3 3.2 1.3 0.2 ## 4 3.1 1.5 0.2 ## 5 3.6 1.4 0.2 ## 6 3.9 1.7 0.4 ## 7 3.4 1.4 0.3 ## 8 3.4 1.5 0.2 ## 9 2.9 1.4 0.2 ## 10 3.1 1.5 0.1 ## 11 3.7 1.5 0.2 ## 12 3.4 1.6 0.2 ## 13 3.0 1.4 0.1 ## 14 3.0 1.1 0.1 ## 15 4.0 1.2 0.2 ## 16 4.4 1.5 0.4 ## 17 3.9 1.3 0.4 ## 18 3.5 1.4 0.3 ## 19 3.8 1.7 0.3 ## 20 3.8 1.5 0.3 ## 21 3.4 1.7 0.2 ## 22 3.7 1.5 0.4 ## 23 3.6 1.0 0.2 ## 24 3.3 1.7 0.5 ## 25 3.4 1.9 0.2 ## 26 3.0 1.6 0.2 ## 27 3.4 1.6 0.4 ## 28 3.5 1.5 0.2 ## 29 3.4 1.4 0.2 ## 30 3.2 1.6 0.2 ## 31 3.1 1.6 0.2 ## 32 3.4 1.5 0.4 ## 33 4.1 1.5 0.1 ## 34 4.2 1.4 0.2 ## 35 3.1 1.5 0.2 ## 36 3.2 1.2 0.2 ## 37 3.5 1.3 0.2 ## 38 3.6 1.4 0.1 ## 39 3.0 1.3 0.2 ## 40 3.4 1.5 0.2 ## 41 3.5 1.3 0.3 ## 42 2.3 1.3 0.3 ## 43 3.2 1.3 0.2 ## 44 3.5 1.6 0.6 ## 45 3.8 1.9 0.4 ## 46 3.0 1.4 0.3 ## 47 3.8 1.6 0.2 ## 48 3.2 1.4 0.2 ## 49 3.7 1.5 0.2 ## 50 3.3 1.4 0.2 ## 51 3.2 4.7 1.4 ## 52 3.2 4.5 1.5 ## 53 3.1 4.9 1.5 ## 54 2.3 4.0 1.3 ## 55 2.8 4.6 1.5 ## 56 2.8 4.5 1.3 ## 57 3.3 4.7 1.6 ## 58 2.4 3.3 1.0 ## 59 2.9 4.6 1.3 ## 60 2.7 3.9 1.4 ## 61 2.0 3.5 1.0 ## 62 3.0 4.2 1.5 ## 63 2.2 4.0 1.0 ## 64 2.9 4.7 1.4 ## 65 2.9 3.6 1.3 ## 66 3.1 4.4 1.4 ## 67 3.0 4.5 1.5 ## 68 2.7 4.1 1.0 ## 69 2.2 4.5 1.5 ## 70 2.5 3.9 1.1 ## 71 3.2 4.8 1.8 ## 72 2.8 4.0 1.3 ## 73 2.5 4.9 1.5 ## 74 2.8 4.7 1.2 ## 75 2.9 4.3 1.3 ## 76 3.0 4.4 1.4 ## 77 2.8 4.8 1.4 ## 78 3.0 5.0 1.7 ## 79 2.9 4.5 1.5 ## 80 2.6 3.5 1.0 ## 81 2.4 3.8 1.1 ## 82 2.4 3.7 1.0 ## 83 2.7 3.9 1.2 ## 84 2.7 5.1 1.6 ## 85 3.0 4.5 1.5 ## 86 3.4 4.5 1.6 ## 87 3.1 4.7 1.5 ## 88 2.3 4.4 1.3 ## 89 3.0 4.1 1.3 ## 90 2.5 4.0 1.3 ## 91 2.6 4.4 1.2 ## 92 3.0 4.6 1.4 ## 93 2.6 4.0 1.2 ## 94 2.3 3.3 1.0 ## 95 2.7 4.2 1.3 ## 96 3.0 4.2 1.2 ## 97 2.9 4.2 1.3 ## 98 2.9 4.3 1.3 ## 99 2.5 3.0 1.1 ## 100 2.8 4.1 1.3 ## 101 3.3 6.0 2.5 ## 102 2.7 5.1 1.9 ## 103 3.0 5.9 2.1 ## 104 2.9 5.6 1.8 ## 105 3.0 5.8 2.2 ## 106 3.0 6.6 2.1 ## 107 2.5 4.5 1.7 ## 108 2.9 6.3 1.8 ## 109 2.5 5.8 1.8 ## 110 3.6 6.1 2.5 ## 111 3.2 5.1 2.0 ## 112 2.7 5.3 1.9 ## 113 3.0 5.5 2.1 ## 114 2.5 5.0 2.0 ## 115 2.8 5.1 2.4 ## 116 3.2 5.3 2.3 ## 117 3.0 5.5 1.8 ## 118 3.8 6.7 2.2 ## 119 2.6 6.9 2.3 ## 120 2.2 5.0 1.5 ## 121 3.2 5.7 2.3 ## 122 2.8 4.9 2.0 ## 123 2.8 6.7 2.0 ## 124 2.7 4.9 1.8 ## 125 3.3 5.7 2.1 ## 126 3.2 6.0 1.8 ## 127 2.8 4.8 1.8 ## 128 3.0 4.9 1.8 ## 129 2.8 5.6 2.1 ## 130 3.0 5.8 1.6 ## 131 2.8 6.1 1.9 ## 132 3.8 6.4 2.0 ## 133 2.8 5.6 2.2 ## 134 2.8 5.1 1.5 ## 135 2.6 5.6 1.4 ## 136 3.0 6.1 2.3 ## 137 3.4 5.6 2.4 ## 138 3.1 5.5 1.8 ## 139 3.0 4.8 1.8 ## 140 3.1 5.4 2.1 ## 141 3.1 5.6 2.4 ## 142 3.1 5.1 2.3 ## 143 2.7 5.1 1.9 ## 144 3.2 5.9 2.3 ## 145 3.3 5.7 2.5 ## 146 3.0 5.2 2.3 ## 147 2.5 5.0 1.9 ## 148 3.0 5.2 2.0 ## 149 3.4 5.4 2.3 ## 150 3.0 5.1 1.8 reorder the variables Reordering columns is often done for reporting or for readability (e.g., keep key identifiers first). everything() selects all remaining variables in their current order. iris %&gt;% select(Species, everything()) %&gt;% print() ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 setosa 5.1 3.5 1.4 0.2 ## 2 setosa 4.9 3.0 1.4 0.2 ## 3 setosa 4.7 3.2 1.3 0.2 ## 4 setosa 4.6 3.1 1.5 0.2 ## 5 setosa 5.0 3.6 1.4 0.2 ## 6 setosa 5.4 3.9 1.7 0.4 ## 7 setosa 4.6 3.4 1.4 0.3 ## 8 setosa 5.0 3.4 1.5 0.2 ## 9 setosa 4.4 2.9 1.4 0.2 ## 10 setosa 4.9 3.1 1.5 0.1 ## 11 setosa 5.4 3.7 1.5 0.2 ## 12 setosa 4.8 3.4 1.6 0.2 ## 13 setosa 4.8 3.0 1.4 0.1 ## 14 setosa 4.3 3.0 1.1 0.1 ## 15 setosa 5.8 4.0 1.2 0.2 ## 16 setosa 5.7 4.4 1.5 0.4 ## 17 setosa 5.4 3.9 1.3 0.4 ## 18 setosa 5.1 3.5 1.4 0.3 ## 19 setosa 5.7 3.8 1.7 0.3 ## 20 setosa 5.1 3.8 1.5 0.3 ## 21 setosa 5.4 3.4 1.7 0.2 ## 22 setosa 5.1 3.7 1.5 0.4 ## 23 setosa 4.6 3.6 1.0 0.2 ## 24 setosa 5.1 3.3 1.7 0.5 ## 25 setosa 4.8 3.4 1.9 0.2 ## 26 setosa 5.0 3.0 1.6 0.2 ## 27 setosa 5.0 3.4 1.6 0.4 ## 28 setosa 5.2 3.5 1.5 0.2 ## 29 setosa 5.2 3.4 1.4 0.2 ## 30 setosa 4.7 3.2 1.6 0.2 ## 31 setosa 4.8 3.1 1.6 0.2 ## 32 setosa 5.4 3.4 1.5 0.4 ## 33 setosa 5.2 4.1 1.5 0.1 ## 34 setosa 5.5 4.2 1.4 0.2 ## 35 setosa 4.9 3.1 1.5 0.2 ## 36 setosa 5.0 3.2 1.2 0.2 ## 37 setosa 5.5 3.5 1.3 0.2 ## 38 setosa 4.9 3.6 1.4 0.1 ## 39 setosa 4.4 3.0 1.3 0.2 ## 40 setosa 5.1 3.4 1.5 0.2 ## 41 setosa 5.0 3.5 1.3 0.3 ## 42 setosa 4.5 2.3 1.3 0.3 ## 43 setosa 4.4 3.2 1.3 0.2 ## 44 setosa 5.0 3.5 1.6 0.6 ## 45 setosa 5.1 3.8 1.9 0.4 ## 46 setosa 4.8 3.0 1.4 0.3 ## 47 setosa 5.1 3.8 1.6 0.2 ## 48 setosa 4.6 3.2 1.4 0.2 ## 49 setosa 5.3 3.7 1.5 0.2 ## 50 setosa 5.0 3.3 1.4 0.2 ## 51 versicolor 7.0 3.2 4.7 1.4 ## 52 versicolor 6.4 3.2 4.5 1.5 ## 53 versicolor 6.9 3.1 4.9 1.5 ## 54 versicolor 5.5 2.3 4.0 1.3 ## 55 versicolor 6.5 2.8 4.6 1.5 ## 56 versicolor 5.7 2.8 4.5 1.3 ## 57 versicolor 6.3 3.3 4.7 1.6 ## 58 versicolor 4.9 2.4 3.3 1.0 ## 59 versicolor 6.6 2.9 4.6 1.3 ## 60 versicolor 5.2 2.7 3.9 1.4 ## 61 versicolor 5.0 2.0 3.5 1.0 ## 62 versicolor 5.9 3.0 4.2 1.5 ## 63 versicolor 6.0 2.2 4.0 1.0 ## 64 versicolor 6.1 2.9 4.7 1.4 ## 65 versicolor 5.6 2.9 3.6 1.3 ## 66 versicolor 6.7 3.1 4.4 1.4 ## 67 versicolor 5.6 3.0 4.5 1.5 ## 68 versicolor 5.8 2.7 4.1 1.0 ## 69 versicolor 6.2 2.2 4.5 1.5 ## 70 versicolor 5.6 2.5 3.9 1.1 ## 71 versicolor 5.9 3.2 4.8 1.8 ## 72 versicolor 6.1 2.8 4.0 1.3 ## 73 versicolor 6.3 2.5 4.9 1.5 ## 74 versicolor 6.1 2.8 4.7 1.2 ## 75 versicolor 6.4 2.9 4.3 1.3 ## 76 versicolor 6.6 3.0 4.4 1.4 ## 77 versicolor 6.8 2.8 4.8 1.4 ## 78 versicolor 6.7 3.0 5.0 1.7 ## 79 versicolor 6.0 2.9 4.5 1.5 ## 80 versicolor 5.7 2.6 3.5 1.0 ## 81 versicolor 5.5 2.4 3.8 1.1 ## 82 versicolor 5.5 2.4 3.7 1.0 ## 83 versicolor 5.8 2.7 3.9 1.2 ## 84 versicolor 6.0 2.7 5.1 1.6 ## 85 versicolor 5.4 3.0 4.5 1.5 ## 86 versicolor 6.0 3.4 4.5 1.6 ## 87 versicolor 6.7 3.1 4.7 1.5 ## 88 versicolor 6.3 2.3 4.4 1.3 ## 89 versicolor 5.6 3.0 4.1 1.3 ## 90 versicolor 5.5 2.5 4.0 1.3 ## 91 versicolor 5.5 2.6 4.4 1.2 ## 92 versicolor 6.1 3.0 4.6 1.4 ## 93 versicolor 5.8 2.6 4.0 1.2 ## 94 versicolor 5.0 2.3 3.3 1.0 ## 95 versicolor 5.6 2.7 4.2 1.3 ## 96 versicolor 5.7 3.0 4.2 1.2 ## 97 versicolor 5.7 2.9 4.2 1.3 ## 98 versicolor 6.2 2.9 4.3 1.3 ## 99 versicolor 5.1 2.5 3.0 1.1 ## 100 versicolor 5.7 2.8 4.1 1.3 ## 101 virginica 6.3 3.3 6.0 2.5 ## 102 virginica 5.8 2.7 5.1 1.9 ## 103 virginica 7.1 3.0 5.9 2.1 ## 104 virginica 6.3 2.9 5.6 1.8 ## 105 virginica 6.5 3.0 5.8 2.2 ## 106 virginica 7.6 3.0 6.6 2.1 ## 107 virginica 4.9 2.5 4.5 1.7 ## 108 virginica 7.3 2.9 6.3 1.8 ## 109 virginica 6.7 2.5 5.8 1.8 ## 110 virginica 7.2 3.6 6.1 2.5 ## 111 virginica 6.5 3.2 5.1 2.0 ## 112 virginica 6.4 2.7 5.3 1.9 ## 113 virginica 6.8 3.0 5.5 2.1 ## 114 virginica 5.7 2.5 5.0 2.0 ## 115 virginica 5.8 2.8 5.1 2.4 ## 116 virginica 6.4 3.2 5.3 2.3 ## 117 virginica 6.5 3.0 5.5 1.8 ## 118 virginica 7.7 3.8 6.7 2.2 ## 119 virginica 7.7 2.6 6.9 2.3 ## 120 virginica 6.0 2.2 5.0 1.5 ## 121 virginica 6.9 3.2 5.7 2.3 ## 122 virginica 5.6 2.8 4.9 2.0 ## 123 virginica 7.7 2.8 6.7 2.0 ## 124 virginica 6.3 2.7 4.9 1.8 ## 125 virginica 6.7 3.3 5.7 2.1 ## 126 virginica 7.2 3.2 6.0 1.8 ## 127 virginica 6.2 2.8 4.8 1.8 ## 128 virginica 6.1 3.0 4.9 1.8 ## 129 virginica 6.4 2.8 5.6 2.1 ## 130 virginica 7.2 3.0 5.8 1.6 ## 131 virginica 7.4 2.8 6.1 1.9 ## 132 virginica 7.9 3.8 6.4 2.0 ## 133 virginica 6.4 2.8 5.6 2.2 ## 134 virginica 6.3 2.8 5.1 1.5 ## 135 virginica 6.1 2.6 5.6 1.4 ## 136 virginica 7.7 3.0 6.1 2.3 ## 137 virginica 6.3 3.4 5.6 2.4 ## 138 virginica 6.4 3.1 5.5 1.8 ## 139 virginica 6.0 3.0 4.8 1.8 ## 140 virginica 6.9 3.1 5.4 2.1 ## 141 virginica 6.7 3.1 5.6 2.4 ## 142 virginica 6.9 3.1 5.1 2.3 ## 143 virginica 5.8 2.7 5.1 1.9 ## 144 virginica 6.8 3.2 5.9 2.3 ## 145 virginica 6.7 3.3 5.7 2.5 ## 146 virginica 6.7 3.0 5.2 2.3 ## 147 virginica 6.3 2.5 5.0 1.9 ## 148 virginica 6.5 3.0 5.2 2.0 ## 149 virginica 6.2 3.4 5.4 2.3 ## 150 virginica 5.9 3.0 5.1 1.8 You can also select a range of variables using the order they appear in the dataset. This is a quick way to take a contiguous block of variables. # using the direction of vector iris %&gt;% select(Species:Sepal.Length ) %&gt;% print() ## Species Petal.Width Petal.Length Sepal.Width Sepal.Length ## 1 setosa 0.2 1.4 3.5 5.1 ## 2 setosa 0.2 1.4 3.0 4.9 ## 3 setosa 0.2 1.3 3.2 4.7 ## 4 setosa 0.2 1.5 3.1 4.6 ## 5 setosa 0.2 1.4 3.6 5.0 ## 6 setosa 0.4 1.7 3.9 5.4 ## 7 setosa 0.3 1.4 3.4 4.6 ## 8 setosa 0.2 1.5 3.4 5.0 ## 9 setosa 0.2 1.4 2.9 4.4 ## 10 setosa 0.1 1.5 3.1 4.9 ## 11 setosa 0.2 1.5 3.7 5.4 ## 12 setosa 0.2 1.6 3.4 4.8 ## 13 setosa 0.1 1.4 3.0 4.8 ## 14 setosa 0.1 1.1 3.0 4.3 ## 15 setosa 0.2 1.2 4.0 5.8 ## 16 setosa 0.4 1.5 4.4 5.7 ## 17 setosa 0.4 1.3 3.9 5.4 ## 18 setosa 0.3 1.4 3.5 5.1 ## 19 setosa 0.3 1.7 3.8 5.7 ## 20 setosa 0.3 1.5 3.8 5.1 ## 21 setosa 0.2 1.7 3.4 5.4 ## 22 setosa 0.4 1.5 3.7 5.1 ## 23 setosa 0.2 1.0 3.6 4.6 ## 24 setosa 0.5 1.7 3.3 5.1 ## 25 setosa 0.2 1.9 3.4 4.8 ## 26 setosa 0.2 1.6 3.0 5.0 ## 27 setosa 0.4 1.6 3.4 5.0 ## 28 setosa 0.2 1.5 3.5 5.2 ## 29 setosa 0.2 1.4 3.4 5.2 ## 30 setosa 0.2 1.6 3.2 4.7 ## 31 setosa 0.2 1.6 3.1 4.8 ## 32 setosa 0.4 1.5 3.4 5.4 ## 33 setosa 0.1 1.5 4.1 5.2 ## 34 setosa 0.2 1.4 4.2 5.5 ## 35 setosa 0.2 1.5 3.1 4.9 ## 36 setosa 0.2 1.2 3.2 5.0 ## 37 setosa 0.2 1.3 3.5 5.5 ## 38 setosa 0.1 1.4 3.6 4.9 ## 39 setosa 0.2 1.3 3.0 4.4 ## 40 setosa 0.2 1.5 3.4 5.1 ## 41 setosa 0.3 1.3 3.5 5.0 ## 42 setosa 0.3 1.3 2.3 4.5 ## 43 setosa 0.2 1.3 3.2 4.4 ## 44 setosa 0.6 1.6 3.5 5.0 ## 45 setosa 0.4 1.9 3.8 5.1 ## 46 setosa 0.3 1.4 3.0 4.8 ## 47 setosa 0.2 1.6 3.8 5.1 ## 48 setosa 0.2 1.4 3.2 4.6 ## 49 setosa 0.2 1.5 3.7 5.3 ## 50 setosa 0.2 1.4 3.3 5.0 ## 51 versicolor 1.4 4.7 3.2 7.0 ## 52 versicolor 1.5 4.5 3.2 6.4 ## 53 versicolor 1.5 4.9 3.1 6.9 ## 54 versicolor 1.3 4.0 2.3 5.5 ## 55 versicolor 1.5 4.6 2.8 6.5 ## 56 versicolor 1.3 4.5 2.8 5.7 ## 57 versicolor 1.6 4.7 3.3 6.3 ## 58 versicolor 1.0 3.3 2.4 4.9 ## 59 versicolor 1.3 4.6 2.9 6.6 ## 60 versicolor 1.4 3.9 2.7 5.2 ## 61 versicolor 1.0 3.5 2.0 5.0 ## 62 versicolor 1.5 4.2 3.0 5.9 ## 63 versicolor 1.0 4.0 2.2 6.0 ## 64 versicolor 1.4 4.7 2.9 6.1 ## 65 versicolor 1.3 3.6 2.9 5.6 ## 66 versicolor 1.4 4.4 3.1 6.7 ## 67 versicolor 1.5 4.5 3.0 5.6 ## 68 versicolor 1.0 4.1 2.7 5.8 ## 69 versicolor 1.5 4.5 2.2 6.2 ## 70 versicolor 1.1 3.9 2.5 5.6 ## 71 versicolor 1.8 4.8 3.2 5.9 ## 72 versicolor 1.3 4.0 2.8 6.1 ## 73 versicolor 1.5 4.9 2.5 6.3 ## 74 versicolor 1.2 4.7 2.8 6.1 ## 75 versicolor 1.3 4.3 2.9 6.4 ## 76 versicolor 1.4 4.4 3.0 6.6 ## 77 versicolor 1.4 4.8 2.8 6.8 ## 78 versicolor 1.7 5.0 3.0 6.7 ## 79 versicolor 1.5 4.5 2.9 6.0 ## 80 versicolor 1.0 3.5 2.6 5.7 ## 81 versicolor 1.1 3.8 2.4 5.5 ## 82 versicolor 1.0 3.7 2.4 5.5 ## 83 versicolor 1.2 3.9 2.7 5.8 ## 84 versicolor 1.6 5.1 2.7 6.0 ## 85 versicolor 1.5 4.5 3.0 5.4 ## 86 versicolor 1.6 4.5 3.4 6.0 ## 87 versicolor 1.5 4.7 3.1 6.7 ## 88 versicolor 1.3 4.4 2.3 6.3 ## 89 versicolor 1.3 4.1 3.0 5.6 ## 90 versicolor 1.3 4.0 2.5 5.5 ## 91 versicolor 1.2 4.4 2.6 5.5 ## 92 versicolor 1.4 4.6 3.0 6.1 ## 93 versicolor 1.2 4.0 2.6 5.8 ## 94 versicolor 1.0 3.3 2.3 5.0 ## 95 versicolor 1.3 4.2 2.7 5.6 ## 96 versicolor 1.2 4.2 3.0 5.7 ## 97 versicolor 1.3 4.2 2.9 5.7 ## 98 versicolor 1.3 4.3 2.9 6.2 ## 99 versicolor 1.1 3.0 2.5 5.1 ## 100 versicolor 1.3 4.1 2.8 5.7 ## 101 virginica 2.5 6.0 3.3 6.3 ## 102 virginica 1.9 5.1 2.7 5.8 ## 103 virginica 2.1 5.9 3.0 7.1 ## 104 virginica 1.8 5.6 2.9 6.3 ## 105 virginica 2.2 5.8 3.0 6.5 ## 106 virginica 2.1 6.6 3.0 7.6 ## 107 virginica 1.7 4.5 2.5 4.9 ## 108 virginica 1.8 6.3 2.9 7.3 ## 109 virginica 1.8 5.8 2.5 6.7 ## 110 virginica 2.5 6.1 3.6 7.2 ## 111 virginica 2.0 5.1 3.2 6.5 ## 112 virginica 1.9 5.3 2.7 6.4 ## 113 virginica 2.1 5.5 3.0 6.8 ## 114 virginica 2.0 5.0 2.5 5.7 ## 115 virginica 2.4 5.1 2.8 5.8 ## 116 virginica 2.3 5.3 3.2 6.4 ## 117 virginica 1.8 5.5 3.0 6.5 ## 118 virginica 2.2 6.7 3.8 7.7 ## 119 virginica 2.3 6.9 2.6 7.7 ## 120 virginica 1.5 5.0 2.2 6.0 ## 121 virginica 2.3 5.7 3.2 6.9 ## 122 virginica 2.0 4.9 2.8 5.6 ## 123 virginica 2.0 6.7 2.8 7.7 ## 124 virginica 1.8 4.9 2.7 6.3 ## 125 virginica 2.1 5.7 3.3 6.7 ## 126 virginica 1.8 6.0 3.2 7.2 ## 127 virginica 1.8 4.8 2.8 6.2 ## 128 virginica 1.8 4.9 3.0 6.1 ## 129 virginica 2.1 5.6 2.8 6.4 ## 130 virginica 1.6 5.8 3.0 7.2 ## 131 virginica 1.9 6.1 2.8 7.4 ## 132 virginica 2.0 6.4 3.8 7.9 ## 133 virginica 2.2 5.6 2.8 6.4 ## 134 virginica 1.5 5.1 2.8 6.3 ## 135 virginica 1.4 5.6 2.6 6.1 ## 136 virginica 2.3 6.1 3.0 7.7 ## 137 virginica 2.4 5.6 3.4 6.3 ## 138 virginica 1.8 5.5 3.1 6.4 ## 139 virginica 1.8 4.8 3.0 6.0 ## 140 virginica 2.1 5.4 3.1 6.9 ## 141 virginica 2.4 5.6 3.1 6.7 ## 142 virginica 2.3 5.1 3.1 6.9 ## 143 virginica 1.9 5.1 2.7 5.8 ## 144 virginica 2.3 5.9 3.2 6.8 ## 145 virginica 2.5 5.7 3.3 6.7 ## 146 virginica 2.3 5.2 3.0 6.7 ## 147 virginica 1.9 5.0 2.5 6.3 ## 148 virginica 2.0 5.2 3.0 6.5 ## 149 virginica 2.3 5.4 3.4 6.2 ## 150 virginica 1.8 5.1 3.0 5.9 1.1.4 Rename variables Renaming variables improves clarity and standardization, especially when variable names will be used in tables, plots, or models. A common practice is to use consistent naming conventions (e.g., snake_case) and avoid special characters. rename() changes variable names explicitly. rename(iris, Sepal_Width= Sepal.Width, Sepal_Length= Sepal.Length ) ## Sepal_Length Sepal_Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## 11 5.4 3.7 1.5 0.2 setosa ## 12 4.8 3.4 1.6 0.2 setosa ## 13 4.8 3.0 1.4 0.1 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 15 5.8 4.0 1.2 0.2 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 17 5.4 3.9 1.3 0.4 setosa ## 18 5.1 3.5 1.4 0.3 setosa ## 19 5.7 3.8 1.7 0.3 setosa ## 20 5.1 3.8 1.5 0.3 setosa ## 21 5.4 3.4 1.7 0.2 setosa ## 22 5.1 3.7 1.5 0.4 setosa ## 23 4.6 3.6 1.0 0.2 setosa ## 24 5.1 3.3 1.7 0.5 setosa ## 25 4.8 3.4 1.9 0.2 setosa ## 26 5.0 3.0 1.6 0.2 setosa ## 27 5.0 3.4 1.6 0.4 setosa ## 28 5.2 3.5 1.5 0.2 setosa ## 29 5.2 3.4 1.4 0.2 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 31 4.8 3.1 1.6 0.2 setosa ## 32 5.4 3.4 1.5 0.4 setosa ## 33 5.2 4.1 1.5 0.1 setosa ## 34 5.5 4.2 1.4 0.2 setosa ## 35 4.9 3.1 1.5 0.2 setosa ## 36 5.0 3.2 1.2 0.2 setosa ## 37 5.5 3.5 1.3 0.2 setosa ## 38 4.9 3.6 1.4 0.1 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 40 5.1 3.4 1.5 0.2 setosa ## 41 5.0 3.5 1.3 0.3 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 44 5.0 3.5 1.6 0.6 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 47 5.1 3.8 1.6 0.2 setosa ## 48 4.6 3.2 1.4 0.2 setosa ## 49 5.3 3.7 1.5 0.2 setosa ## 50 5.0 3.3 1.4 0.2 setosa ## 51 7.0 3.2 4.7 1.4 versicolor ## 52 6.4 3.2 4.5 1.5 versicolor ## 53 6.9 3.1 4.9 1.5 versicolor ## 54 5.5 2.3 4.0 1.3 versicolor ## 55 6.5 2.8 4.6 1.5 versicolor ## 56 5.7 2.8 4.5 1.3 versicolor ## 57 6.3 3.3 4.7 1.6 versicolor ## 58 4.9 2.4 3.3 1.0 versicolor ## 59 6.6 2.9 4.6 1.3 versicolor ## 60 5.2 2.7 3.9 1.4 versicolor ## 61 5.0 2.0 3.5 1.0 versicolor ## 62 5.9 3.0 4.2 1.5 versicolor ## 63 6.0 2.2 4.0 1.0 versicolor ## 64 6.1 2.9 4.7 1.4 versicolor ## 65 5.6 2.9 3.6 1.3 versicolor ## 66 6.7 3.1 4.4 1.4 versicolor ## 67 5.6 3.0 4.5 1.5 versicolor ## 68 5.8 2.7 4.1 1.0 versicolor ## 69 6.2 2.2 4.5 1.5 versicolor ## 70 5.6 2.5 3.9 1.1 versicolor ## 71 5.9 3.2 4.8 1.8 versicolor ## 72 6.1 2.8 4.0 1.3 versicolor ## 73 6.3 2.5 4.9 1.5 versicolor ## 74 6.1 2.8 4.7 1.2 versicolor ## 75 6.4 2.9 4.3 1.3 versicolor ## 76 6.6 3.0 4.4 1.4 versicolor ## 77 6.8 2.8 4.8 1.4 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 79 6.0 2.9 4.5 1.5 versicolor ## 80 5.7 2.6 3.5 1.0 versicolor ## 81 5.5 2.4 3.8 1.1 versicolor ## 82 5.5 2.4 3.7 1.0 versicolor ## 83 5.8 2.7 3.9 1.2 versicolor ## 84 6.0 2.7 5.1 1.6 versicolor ## 85 5.4 3.0 4.5 1.5 versicolor ## 86 6.0 3.4 4.5 1.6 versicolor ## 87 6.7 3.1 4.7 1.5 versicolor ## 88 6.3 2.3 4.4 1.3 versicolor ## 89 5.6 3.0 4.1 1.3 versicolor ## 90 5.5 2.5 4.0 1.3 versicolor ## 91 5.5 2.6 4.4 1.2 versicolor ## 92 6.1 3.0 4.6 1.4 versicolor ## 93 5.8 2.6 4.0 1.2 versicolor ## 94 5.0 2.3 3.3 1.0 versicolor ## 95 5.6 2.7 4.2 1.3 versicolor ## 96 5.7 3.0 4.2 1.2 versicolor ## 97 5.7 2.9 4.2 1.3 versicolor ## 98 6.2 2.9 4.3 1.3 versicolor ## 99 5.1 2.5 3.0 1.1 versicolor ## 100 5.7 2.8 4.1 1.3 versicolor ## 101 6.3 3.3 6.0 2.5 virginica ## 102 5.8 2.7 5.1 1.9 virginica ## 103 7.1 3.0 5.9 2.1 virginica ## 104 6.3 2.9 5.6 1.8 virginica ## 105 6.5 3.0 5.8 2.2 virginica ## 106 7.6 3.0 6.6 2.1 virginica ## 107 4.9 2.5 4.5 1.7 virginica ## 108 7.3 2.9 6.3 1.8 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 110 7.2 3.6 6.1 2.5 virginica ## 111 6.5 3.2 5.1 2.0 virginica ## 112 6.4 2.7 5.3 1.9 virginica ## 113 6.8 3.0 5.5 2.1 virginica ## 114 5.7 2.5 5.0 2.0 virginica ## 115 5.8 2.8 5.1 2.4 virginica ## 116 6.4 3.2 5.3 2.3 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 120 6.0 2.2 5.0 1.5 virginica ## 121 6.9 3.2 5.7 2.3 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 124 6.3 2.7 4.9 1.8 virginica ## 125 6.7 3.3 5.7 2.1 virginica ## 126 7.2 3.2 6.0 1.8 virginica ## 127 6.2 2.8 4.8 1.8 virginica ## 128 6.1 3.0 4.9 1.8 virginica ## 129 6.4 2.8 5.6 2.1 virginica ## 130 7.2 3.0 5.8 1.6 virginica ## 131 7.4 2.8 6.1 1.9 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 133 6.4 2.8 5.6 2.2 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 virginica ## 136 7.7 3.0 6.1 2.3 virginica ## 137 6.3 3.4 5.6 2.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica ## 139 6.0 3.0 4.8 1.8 virginica ## 140 6.9 3.1 5.4 2.1 virginica ## 141 6.7 3.1 5.6 2.4 virginica ## 142 6.9 3.1 5.1 2.3 virginica ## 143 5.8 2.7 5.1 1.9 virginica ## 144 6.8 3.2 5.9 2.3 virginica ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica rename_with() applies a function to multiple variable names. For example, converting to lowercase can help standardize variable names when merging datasets from different sources. iris %&gt;% rename_with(tolower) %&gt;% head() ## sepal.length sepal.width petal.length petal.width species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa 1.1.5 Sorting in ascending or descending order Sorting is commonly used for (1) identifying extreme values, (2) reviewing data quality, and (3) preparing ordered outputs for tables. put a minus in front of a variable for descending order Here we sort by Petal.Length ascending, and within that, by Petal.Width descending. arrange(iris, Petal.Length, -Petal.Width) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 4.6 3.6 1.0 0.2 setosa ## 2 4.3 3.0 1.1 0.1 setosa ## 3 5.8 4.0 1.2 0.2 setosa ## 4 5.0 3.2 1.2 0.2 setosa ## 5 5.4 3.9 1.3 0.4 setosa ## 6 5.0 3.5 1.3 0.3 setosa ## 7 4.5 2.3 1.3 0.3 setosa ## 8 4.7 3.2 1.3 0.2 setosa ## 9 5.5 3.5 1.3 0.2 setosa ## 10 4.4 3.0 1.3 0.2 setosa ## 11 4.4 3.2 1.3 0.2 setosa ## 12 4.6 3.4 1.4 0.3 setosa ## 13 5.1 3.5 1.4 0.3 setosa ## 14 4.8 3.0 1.4 0.3 setosa ## 15 5.1 3.5 1.4 0.2 setosa ## 16 4.9 3.0 1.4 0.2 setosa ## 17 5.0 3.6 1.4 0.2 setosa ## 18 4.4 2.9 1.4 0.2 setosa ## 19 5.2 3.4 1.4 0.2 setosa ## 20 5.5 4.2 1.4 0.2 setosa ## 21 4.6 3.2 1.4 0.2 setosa ## 22 5.0 3.3 1.4 0.2 setosa ## 23 4.8 3.0 1.4 0.1 setosa ## 24 4.9 3.6 1.4 0.1 setosa ## 25 5.7 4.4 1.5 0.4 setosa ## 26 5.1 3.7 1.5 0.4 setosa ## 27 5.4 3.4 1.5 0.4 setosa ## 28 5.1 3.8 1.5 0.3 setosa ## 29 4.6 3.1 1.5 0.2 setosa ## 30 5.0 3.4 1.5 0.2 setosa ## 31 5.4 3.7 1.5 0.2 setosa ## 32 5.2 3.5 1.5 0.2 setosa ## 33 4.9 3.1 1.5 0.2 setosa ## 34 5.1 3.4 1.5 0.2 setosa ## 35 5.3 3.7 1.5 0.2 setosa ## 36 4.9 3.1 1.5 0.1 setosa ## 37 5.2 4.1 1.5 0.1 setosa ## 38 5.0 3.5 1.6 0.6 setosa ## 39 5.0 3.4 1.6 0.4 setosa ## 40 4.8 3.4 1.6 0.2 setosa ## 41 5.0 3.0 1.6 0.2 setosa ## 42 4.7 3.2 1.6 0.2 setosa ## 43 4.8 3.1 1.6 0.2 setosa ## 44 5.1 3.8 1.6 0.2 setosa ## 45 5.1 3.3 1.7 0.5 setosa ## 46 5.4 3.9 1.7 0.4 setosa ## 47 5.7 3.8 1.7 0.3 setosa ## 48 5.4 3.4 1.7 0.2 setosa ## 49 5.1 3.8 1.9 0.4 setosa ## 50 4.8 3.4 1.9 0.2 setosa ## 51 5.1 2.5 3.0 1.1 versicolor ## 52 4.9 2.4 3.3 1.0 versicolor ## 53 5.0 2.3 3.3 1.0 versicolor ## 54 5.0 2.0 3.5 1.0 versicolor ## 55 5.7 2.6 3.5 1.0 versicolor ## 56 5.6 2.9 3.6 1.3 versicolor ## 57 5.5 2.4 3.7 1.0 versicolor ## 58 5.5 2.4 3.8 1.1 versicolor ## 59 5.2 2.7 3.9 1.4 versicolor ## 60 5.8 2.7 3.9 1.2 versicolor ## 61 5.6 2.5 3.9 1.1 versicolor ## 62 5.5 2.3 4.0 1.3 versicolor ## 63 6.1 2.8 4.0 1.3 versicolor ## 64 5.5 2.5 4.0 1.3 versicolor ## 65 5.8 2.6 4.0 1.2 versicolor ## 66 6.0 2.2 4.0 1.0 versicolor ## 67 5.6 3.0 4.1 1.3 versicolor ## 68 5.7 2.8 4.1 1.3 versicolor ## 69 5.8 2.7 4.1 1.0 versicolor ## 70 5.9 3.0 4.2 1.5 versicolor ## 71 5.6 2.7 4.2 1.3 versicolor ## 72 5.7 2.9 4.2 1.3 versicolor ## 73 5.7 3.0 4.2 1.2 versicolor ## 74 6.4 2.9 4.3 1.3 versicolor ## 75 6.2 2.9 4.3 1.3 versicolor ## 76 6.7 3.1 4.4 1.4 versicolor ## 77 6.6 3.0 4.4 1.4 versicolor ## 78 6.3 2.3 4.4 1.3 versicolor ## 79 5.5 2.6 4.4 1.2 versicolor ## 80 4.9 2.5 4.5 1.7 virginica ## 81 6.0 3.4 4.5 1.6 versicolor ## 82 6.4 3.2 4.5 1.5 versicolor ## 83 5.6 3.0 4.5 1.5 versicolor ## 84 6.2 2.2 4.5 1.5 versicolor ## 85 6.0 2.9 4.5 1.5 versicolor ## 86 5.4 3.0 4.5 1.5 versicolor ## 87 5.7 2.8 4.5 1.3 versicolor ## 88 6.5 2.8 4.6 1.5 versicolor ## 89 6.1 3.0 4.6 1.4 versicolor ## 90 6.6 2.9 4.6 1.3 versicolor ## 91 6.3 3.3 4.7 1.6 versicolor ## 92 6.7 3.1 4.7 1.5 versicolor ## 93 7.0 3.2 4.7 1.4 versicolor ## 94 6.1 2.9 4.7 1.4 versicolor ## 95 6.1 2.8 4.7 1.2 versicolor ## 96 5.9 3.2 4.8 1.8 versicolor ## 97 6.2 2.8 4.8 1.8 virginica ## 98 6.0 3.0 4.8 1.8 virginica ## 99 6.8 2.8 4.8 1.4 versicolor ## 100 5.6 2.8 4.9 2.0 virginica ## 101 6.3 2.7 4.9 1.8 virginica ## 102 6.1 3.0 4.9 1.8 virginica ## 103 6.9 3.1 4.9 1.5 versicolor ## 104 6.3 2.5 4.9 1.5 versicolor ## 105 5.7 2.5 5.0 2.0 virginica ## 106 6.3 2.5 5.0 1.9 virginica ## 107 6.7 3.0 5.0 1.7 versicolor ## 108 6.0 2.2 5.0 1.5 virginica ## 109 5.8 2.8 5.1 2.4 virginica ## 110 6.9 3.1 5.1 2.3 virginica ## 111 6.5 3.2 5.1 2.0 virginica ## 112 5.8 2.7 5.1 1.9 virginica ## 113 5.8 2.7 5.1 1.9 virginica ## 114 5.9 3.0 5.1 1.8 virginica ## 115 6.0 2.7 5.1 1.6 versicolor ## 116 6.3 2.8 5.1 1.5 virginica ## 117 6.7 3.0 5.2 2.3 virginica ## 118 6.5 3.0 5.2 2.0 virginica ## 119 6.4 3.2 5.3 2.3 virginica ## 120 6.4 2.7 5.3 1.9 virginica ## 121 6.2 3.4 5.4 2.3 virginica ## 122 6.9 3.1 5.4 2.1 virginica ## 123 6.8 3.0 5.5 2.1 virginica ## 124 6.5 3.0 5.5 1.8 virginica ## 125 6.4 3.1 5.5 1.8 virginica ## 126 6.3 3.4 5.6 2.4 virginica ## 127 6.7 3.1 5.6 2.4 virginica ## 128 6.4 2.8 5.6 2.2 virginica ## 129 6.4 2.8 5.6 2.1 virginica ## 130 6.3 2.9 5.6 1.8 virginica ## 131 6.1 2.6 5.6 1.4 virginica ## 132 6.7 3.3 5.7 2.5 virginica ## 133 6.9 3.2 5.7 2.3 virginica ## 134 6.7 3.3 5.7 2.1 virginica ## 135 6.5 3.0 5.8 2.2 virginica ## 136 6.7 2.5 5.8 1.8 virginica ## 137 7.2 3.0 5.8 1.6 virginica ## 138 6.8 3.2 5.9 2.3 virginica ## 139 7.1 3.0 5.9 2.1 virginica ## 140 6.3 3.3 6.0 2.5 virginica ## 141 7.2 3.2 6.0 1.8 virginica ## 142 7.2 3.6 6.1 2.5 virginica ## 143 7.7 3.0 6.1 2.3 virginica ## 144 7.4 2.8 6.1 1.9 virginica ## 145 7.3 2.9 6.3 1.8 virginica ## 146 7.9 3.8 6.4 2.0 virginica ## 147 7.6 3.0 6.6 2.1 virginica ## 148 7.7 3.8 6.7 2.2 virginica ## 149 7.7 2.8 6.7 2.0 virginica ## 150 7.7 2.6 6.9 2.3 virginica 1.1.6 Transform variables Creating new variables is one of the most common wrangling tasks. Typical use cases include unit conversion, categorization, deriving endpoints, and creating analysis flags. mutate() creates new variables (or overwrites existing variables). Here, we create a new variable and also rescale an existing variable. mutate(iris, newvar= Sepal.Width*10, Petal.Length=Petal.Length/100 ) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species newvar ## 1 5.1 3.5 0.014 0.2 setosa 35 ## 2 4.9 3.0 0.014 0.2 setosa 30 ## 3 4.7 3.2 0.013 0.2 setosa 32 ## 4 4.6 3.1 0.015 0.2 setosa 31 ## 5 5.0 3.6 0.014 0.2 setosa 36 ## 6 5.4 3.9 0.017 0.4 setosa 39 ## 7 4.6 3.4 0.014 0.3 setosa 34 ## 8 5.0 3.4 0.015 0.2 setosa 34 ## 9 4.4 2.9 0.014 0.2 setosa 29 ## 10 4.9 3.1 0.015 0.1 setosa 31 ## 11 5.4 3.7 0.015 0.2 setosa 37 ## 12 4.8 3.4 0.016 0.2 setosa 34 ## 13 4.8 3.0 0.014 0.1 setosa 30 ## 14 4.3 3.0 0.011 0.1 setosa 30 ## 15 5.8 4.0 0.012 0.2 setosa 40 ## 16 5.7 4.4 0.015 0.4 setosa 44 ## 17 5.4 3.9 0.013 0.4 setosa 39 ## 18 5.1 3.5 0.014 0.3 setosa 35 ## 19 5.7 3.8 0.017 0.3 setosa 38 ## 20 5.1 3.8 0.015 0.3 setosa 38 ## 21 5.4 3.4 0.017 0.2 setosa 34 ## 22 5.1 3.7 0.015 0.4 setosa 37 ## 23 4.6 3.6 0.010 0.2 setosa 36 ## 24 5.1 3.3 0.017 0.5 setosa 33 ## 25 4.8 3.4 0.019 0.2 setosa 34 ## 26 5.0 3.0 0.016 0.2 setosa 30 ## 27 5.0 3.4 0.016 0.4 setosa 34 ## 28 5.2 3.5 0.015 0.2 setosa 35 ## 29 5.2 3.4 0.014 0.2 setosa 34 ## 30 4.7 3.2 0.016 0.2 setosa 32 ## 31 4.8 3.1 0.016 0.2 setosa 31 ## 32 5.4 3.4 0.015 0.4 setosa 34 ## 33 5.2 4.1 0.015 0.1 setosa 41 ## 34 5.5 4.2 0.014 0.2 setosa 42 ## 35 4.9 3.1 0.015 0.2 setosa 31 ## 36 5.0 3.2 0.012 0.2 setosa 32 ## 37 5.5 3.5 0.013 0.2 setosa 35 ## 38 4.9 3.6 0.014 0.1 setosa 36 ## 39 4.4 3.0 0.013 0.2 setosa 30 ## 40 5.1 3.4 0.015 0.2 setosa 34 ## 41 5.0 3.5 0.013 0.3 setosa 35 ## 42 4.5 2.3 0.013 0.3 setosa 23 ## 43 4.4 3.2 0.013 0.2 setosa 32 ## 44 5.0 3.5 0.016 0.6 setosa 35 ## 45 5.1 3.8 0.019 0.4 setosa 38 ## 46 4.8 3.0 0.014 0.3 setosa 30 ## 47 5.1 3.8 0.016 0.2 setosa 38 ## 48 4.6 3.2 0.014 0.2 setosa 32 ## 49 5.3 3.7 0.015 0.2 setosa 37 ## 50 5.0 3.3 0.014 0.2 setosa 33 ## 51 7.0 3.2 0.047 1.4 versicolor 32 ## 52 6.4 3.2 0.045 1.5 versicolor 32 ## 53 6.9 3.1 0.049 1.5 versicolor 31 ## 54 5.5 2.3 0.040 1.3 versicolor 23 ## 55 6.5 2.8 0.046 1.5 versicolor 28 ## 56 5.7 2.8 0.045 1.3 versicolor 28 ## 57 6.3 3.3 0.047 1.6 versicolor 33 ## 58 4.9 2.4 0.033 1.0 versicolor 24 ## 59 6.6 2.9 0.046 1.3 versicolor 29 ## 60 5.2 2.7 0.039 1.4 versicolor 27 ## 61 5.0 2.0 0.035 1.0 versicolor 20 ## 62 5.9 3.0 0.042 1.5 versicolor 30 ## 63 6.0 2.2 0.040 1.0 versicolor 22 ## 64 6.1 2.9 0.047 1.4 versicolor 29 ## 65 5.6 2.9 0.036 1.3 versicolor 29 ## 66 6.7 3.1 0.044 1.4 versicolor 31 ## 67 5.6 3.0 0.045 1.5 versicolor 30 ## 68 5.8 2.7 0.041 1.0 versicolor 27 ## 69 6.2 2.2 0.045 1.5 versicolor 22 ## 70 5.6 2.5 0.039 1.1 versicolor 25 ## 71 5.9 3.2 0.048 1.8 versicolor 32 ## 72 6.1 2.8 0.040 1.3 versicolor 28 ## 73 6.3 2.5 0.049 1.5 versicolor 25 ## 74 6.1 2.8 0.047 1.2 versicolor 28 ## 75 6.4 2.9 0.043 1.3 versicolor 29 ## 76 6.6 3.0 0.044 1.4 versicolor 30 ## 77 6.8 2.8 0.048 1.4 versicolor 28 ## 78 6.7 3.0 0.050 1.7 versicolor 30 ## 79 6.0 2.9 0.045 1.5 versicolor 29 ## 80 5.7 2.6 0.035 1.0 versicolor 26 ## 81 5.5 2.4 0.038 1.1 versicolor 24 ## 82 5.5 2.4 0.037 1.0 versicolor 24 ## 83 5.8 2.7 0.039 1.2 versicolor 27 ## 84 6.0 2.7 0.051 1.6 versicolor 27 ## 85 5.4 3.0 0.045 1.5 versicolor 30 ## 86 6.0 3.4 0.045 1.6 versicolor 34 ## 87 6.7 3.1 0.047 1.5 versicolor 31 ## 88 6.3 2.3 0.044 1.3 versicolor 23 ## 89 5.6 3.0 0.041 1.3 versicolor 30 ## 90 5.5 2.5 0.040 1.3 versicolor 25 ## 91 5.5 2.6 0.044 1.2 versicolor 26 ## 92 6.1 3.0 0.046 1.4 versicolor 30 ## 93 5.8 2.6 0.040 1.2 versicolor 26 ## 94 5.0 2.3 0.033 1.0 versicolor 23 ## 95 5.6 2.7 0.042 1.3 versicolor 27 ## 96 5.7 3.0 0.042 1.2 versicolor 30 ## 97 5.7 2.9 0.042 1.3 versicolor 29 ## 98 6.2 2.9 0.043 1.3 versicolor 29 ## 99 5.1 2.5 0.030 1.1 versicolor 25 ## 100 5.7 2.8 0.041 1.3 versicolor 28 ## 101 6.3 3.3 0.060 2.5 virginica 33 ## 102 5.8 2.7 0.051 1.9 virginica 27 ## 103 7.1 3.0 0.059 2.1 virginica 30 ## 104 6.3 2.9 0.056 1.8 virginica 29 ## 105 6.5 3.0 0.058 2.2 virginica 30 ## 106 7.6 3.0 0.066 2.1 virginica 30 ## 107 4.9 2.5 0.045 1.7 virginica 25 ## 108 7.3 2.9 0.063 1.8 virginica 29 ## 109 6.7 2.5 0.058 1.8 virginica 25 ## 110 7.2 3.6 0.061 2.5 virginica 36 ## 111 6.5 3.2 0.051 2.0 virginica 32 ## 112 6.4 2.7 0.053 1.9 virginica 27 ## 113 6.8 3.0 0.055 2.1 virginica 30 ## 114 5.7 2.5 0.050 2.0 virginica 25 ## 115 5.8 2.8 0.051 2.4 virginica 28 ## 116 6.4 3.2 0.053 2.3 virginica 32 ## 117 6.5 3.0 0.055 1.8 virginica 30 ## 118 7.7 3.8 0.067 2.2 virginica 38 ## 119 7.7 2.6 0.069 2.3 virginica 26 ## 120 6.0 2.2 0.050 1.5 virginica 22 ## 121 6.9 3.2 0.057 2.3 virginica 32 ## 122 5.6 2.8 0.049 2.0 virginica 28 ## 123 7.7 2.8 0.067 2.0 virginica 28 ## 124 6.3 2.7 0.049 1.8 virginica 27 ## 125 6.7 3.3 0.057 2.1 virginica 33 ## 126 7.2 3.2 0.060 1.8 virginica 32 ## 127 6.2 2.8 0.048 1.8 virginica 28 ## 128 6.1 3.0 0.049 1.8 virginica 30 ## 129 6.4 2.8 0.056 2.1 virginica 28 ## 130 7.2 3.0 0.058 1.6 virginica 30 ## 131 7.4 2.8 0.061 1.9 virginica 28 ## 132 7.9 3.8 0.064 2.0 virginica 38 ## 133 6.4 2.8 0.056 2.2 virginica 28 ## 134 6.3 2.8 0.051 1.5 virginica 28 ## 135 6.1 2.6 0.056 1.4 virginica 26 ## 136 7.7 3.0 0.061 2.3 virginica 30 ## 137 6.3 3.4 0.056 2.4 virginica 34 ## 138 6.4 3.1 0.055 1.8 virginica 31 ## 139 6.0 3.0 0.048 1.8 virginica 30 ## 140 6.9 3.1 0.054 2.1 virginica 31 ## 141 6.7 3.1 0.056 2.4 virginica 31 ## 142 6.9 3.1 0.051 2.3 virginica 31 ## 143 5.8 2.7 0.051 1.9 virginica 27 ## 144 6.8 3.2 0.059 2.3 virginica 32 ## 145 6.7 3.3 0.057 2.5 virginica 33 ## 146 6.7 3.0 0.052 2.3 virginica 30 ## 147 6.3 2.5 0.050 1.9 virginica 25 ## 148 6.5 3.0 0.052 2.0 virginica 30 ## 149 6.2 3.4 0.054 2.3 virginica 34 ## 150 5.9 3.0 0.051 1.8 virginica 30 # use mutate and ifelse to categorize the varible rep, sample In practice, you often need to test code before real study data are available. Simulated toy datasets are useful for prototyping pipelines, checking assumptions, and training new team members. The following example creates a simple longitudinal dataset with three records per subject (year = 0, 1, 2). We also simulate a treatment assignment and a binary adverse event (se_headache). set.seed(123) drug_trial &lt;- tibble( # each id has three id = rep(1:20, each = 3), # all has 20 year = rep(0:2, times = 20), # sample 20 and each repeat 3 age = sample(35:75, 20, TRUE) %&gt;% rep(each = 3), # drug = sample(c(&quot;Placebo&quot;, &quot;Active&quot;), 20, TRUE) %&gt;% rep(each = 3), # se_headache = if_else( drug == &quot;Placebo&quot;, # the possibility that outcome is 1 sample(0:1, 60, TRUE, c(.95,.05)), sample(0:1, 60, TRUE, c(.10, .90)) ) # ) head(drug_trial) ## # A tibble: 6 × 5 ## id year age drug se_headache ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 0 65 Active 0 ## 2 1 1 65 Active 1 ## 3 1 2 65 Active 1 ## 4 2 0 49 Active 1 ## 5 2 1 49 Active 0 ## 6 2 2 49 Active 1 A quick check with head() helps confirm the dataset structure and that variables were created as expected. # drug_trial %&gt;% mutate(complete = c(1)) ## # A tibble: 60 × 6 ## id year age drug se_headache complete ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 0 65 Active 0 1 ## 2 1 1 65 Active 1 1 ## 3 1 2 65 Active 1 1 ## 4 2 0 49 Active 1 1 ## 5 2 1 49 Active 0 1 ## 6 2 2 49 Active 1 1 ## 7 3 0 48 Placebo 0 1 ## 8 3 1 48 Placebo 0 1 ## 9 3 2 48 Placebo 0 1 ## 10 4 0 37 Placebo 0 1 ## # ℹ 50 more rows factor is just for factor (labeling the value; whilerenamelabeling the variable) and recode is for different types whose outcome depends on the right side of the equation sign. factor can change the reference group. This is an important concept in statistical work: - rename() changes the variable name (column name). - factor() changes the data type and attaches value labels to categories. Factors also have an ordering of levels, which can affect summaries and regression reference categories. - recode() maps old values to new values (useful for harmonizing coding across sources, or mapping numeric codes to analytic codes). Below we (1) create labeled factors for the headache indicator, (2) create a reversed reference order to demonstrate how factor level order changes the reference group, and (3) create a recoded numeric variable. drug_trial=drug_trial %&gt;% mutate(mi_f = factor(se_headache , c(0, 1), c(&quot;No&quot;, &quot;Yes&quot;)))%&gt;% mutate(mi_f_reverse = factor(se_headache , c(1, 0), c(&quot;yes&quot;, &quot;no&quot;)))%&gt;% # change the reference group by using factor mutate(mi = recode(se_headache , &quot;0&quot;=2, &quot;1&quot;=1)) str(drug_trial) ## tibble [60 × 8] (S3: tbl_df/tbl/data.frame) ## $ id : int [1:60] 1 1 1 2 2 2 3 3 3 4 ... ## $ year : int [1:60] 0 1 2 0 1 2 0 1 2 0 ... ## $ age : int [1:60] 65 65 65 49 49 49 48 48 48 37 ... ## $ drug : chr [1:60] &quot;Active&quot; &quot;Active&quot; &quot;Active&quot; &quot;Active&quot; ... ## $ se_headache : int [1:60] 0 1 1 1 0 1 0 0 0 0 ... ## $ mi_f : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 2 2 1 2 1 1 1 1 ... ## $ mi_f_reverse: Factor w/ 2 levels &quot;yes&quot;,&quot;no&quot;: 2 1 1 1 2 1 2 2 2 2 ... ## $ mi : num [1:60] 2 1 1 1 2 1 2 2 2 2 ... table (drug_trial$mi_f) ## ## No Yes ## 40 20 table (drug_trial$mi_f_reverse) ## ## yes no ## 20 40 Row-wise operations apply calculations to each row independently. This is helpful when you need to combine multiple columns within a row (e.g., “any adverse event occurred”, “max lab value within row”, etc.). However, for most group-level summaries (e.g., by subject), group_by() is usually more efficient and more common. drug_trial %&gt;% rowwise() %&gt;% mutate(any_se_year = sum(se_headache, year) &gt; 0) ## # A tibble: 60 × 9 ## # Rowwise: ## id year age drug se_headache mi_f mi_f_reverse mi any_se_year ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 1 0 65 Active 0 No no 2 FALSE ## 2 1 1 65 Active 1 Yes yes 1 TRUE ## 3 1 2 65 Active 1 Yes yes 1 TRUE ## 4 2 0 49 Active 1 Yes yes 1 TRUE ## 5 2 1 49 Active 0 No no 2 TRUE ## 6 2 2 49 Active 1 Yes yes 1 TRUE ## 7 3 0 48 Placebo 0 No no 2 FALSE ## 8 3 1 48 Placebo 0 No no 2 TRUE ## 9 3 2 48 Placebo 0 No no 2 TRUE ## 10 4 0 37 Placebo 0 No no 2 FALSE ## # ℹ 50 more rows what is the difference of rowwise and group_by - group_by(id) means operations are performed within each subject group. This is typically used for subject-level summaries, longitudinal flags, and group totals. - rowwise() means operations are performed for each row. This is best when combining multiple variables across columns within the same row. In the examples below, compare what happens when you compute sum(se_headache, year) under different grouping structures. drug_trial %&gt;% group_by(id) %&gt;% mutate(any_se_year = sum(se_headache, year) ) ## # A tibble: 60 × 9 ## # Groups: id [20] ## id year age drug se_headache mi_f mi_f_reverse mi any_se_year ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 0 65 Active 0 No no 2 5 ## 2 1 1 65 Active 1 Yes yes 1 5 ## 3 1 2 65 Active 1 Yes yes 1 5 ## 4 2 0 49 Active 1 Yes yes 1 5 ## 5 2 1 49 Active 0 No no 2 5 ## 6 2 2 49 Active 1 Yes yes 1 5 ## 7 3 0 48 Placebo 0 No no 2 3 ## 8 3 1 48 Placebo 0 No no 2 3 ## 9 3 2 48 Placebo 0 No no 2 3 ## 10 4 0 37 Placebo 0 No no 2 3 ## # ℹ 50 more rows drug_trial %&gt;% rowwise() %&gt;% mutate(any_se_year = sum(se_headache, year) ) ## # A tibble: 60 × 9 ## # Rowwise: ## id year age drug se_headache mi_f mi_f_reverse mi any_se_year ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 0 65 Active 0 No no 2 0 ## 2 1 1 65 Active 1 Yes yes 1 2 ## 3 1 2 65 Active 1 Yes yes 1 3 ## 4 2 0 49 Active 1 Yes yes 1 1 ## 5 2 1 49 Active 0 No no 2 1 ## 6 2 2 49 Active 1 Yes yes 1 3 ## 7 3 0 48 Placebo 0 No no 2 0 ## 8 3 1 48 Placebo 0 No no 2 1 ## 9 3 2 48 Placebo 0 No no 2 2 ## 10 4 0 37 Placebo 0 No no 2 0 ## # ℹ 50 more rows starts_with and end_with Selector helpers such as starts_with() are useful when working with datasets that have many related variables with consistent naming patterns (e.g., adverse event indicators, lab variables, visit-level measurements). drug_trial_sub &lt;- drug_trial %&gt;% select(id, year, starts_with(&quot;se&quot;)) %&gt;% print() ## # A tibble: 60 × 3 ## id year se_headache ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 0 0 ## 2 1 1 1 ## 3 1 2 1 ## 4 2 0 1 ## 5 2 1 0 ## 6 2 2 1 ## 7 3 0 0 ## 8 3 1 0 ## 9 3 2 0 ## 10 4 0 0 ## # ℹ 50 more rows keep all combination levels even 0 times When summarizing counts for categorical combinations, you sometimes want to display categories even if the count is zero. This is particularly important in reporting (e.g., safety tables, disposition tables). Here, .drop = FALSE keeps all factor combinations in the grouped output. drug_trial %&gt;% filter(year == 0) %&gt;% filter(age &lt; 65) %&gt;% group_by(drug, se_headache, .drop = FALSE) %&gt;% summarise(n = n()) ## `summarise()` has grouped output by &#39;drug&#39;. You can ## override using the `.groups` argument. ## # A tibble: 3 × 3 ## # Groups: drug [2] ## drug se_headache n ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Active 1 5 ## 2 Placebo 0 11 ## 3 Placebo 1 1 group_by A very common pattern is: filter to the analytic dataset, group by a key variable (e.g., treatment), then summarize. drug_trial %&gt;% filter(!is.na(age)) %&gt;% group_by(drug) %&gt;% summarise(mean_age = mean(age)) ## # A tibble: 2 × 2 ## drug mean_age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Active 54.7 ## 2 Placebo 53.2 1.1.7 Working with pipes %&gt;% The pipe operator %&gt;% is used to chain steps in a readable sequence. This style matches how we typically describe data processing: “filter, then transform, then select, then sort.” In practice, pipes help build reproducible and auditable wrangling pipelines. iris %&gt;% filter(Species==&quot;setosa&quot;) %&gt;% mutate (newvar=Sepal.Width*10) %&gt;% select (-Sepal.Width, -Petal.Width) %&gt;% arrange(-Sepal.Length, newvar) ## Sepal.Length Petal.Length Species newvar ## 1 5.8 1.2 setosa 40 ## 2 5.7 1.7 setosa 38 ## 3 5.7 1.5 setosa 44 ## 4 5.5 1.3 setosa 35 ## 5 5.5 1.4 setosa 42 ## 6 5.4 1.7 setosa 34 ## 7 5.4 1.5 setosa 34 ## 8 5.4 1.5 setosa 37 ## 9 5.4 1.7 setosa 39 ## 10 5.4 1.3 setosa 39 ## 11 5.3 1.5 setosa 37 ## 12 5.2 1.4 setosa 34 ## 13 5.2 1.5 setosa 35 ## 14 5.2 1.5 setosa 41 ## 15 5.1 1.7 setosa 33 ## 16 5.1 1.5 setosa 34 ## 17 5.1 1.4 setosa 35 ## 18 5.1 1.4 setosa 35 ## 19 5.1 1.5 setosa 37 ## 20 5.1 1.5 setosa 38 ## 21 5.1 1.9 setosa 38 ## 22 5.1 1.6 setosa 38 ## 23 5.0 1.6 setosa 30 ## 24 5.0 1.2 setosa 32 ## 25 5.0 1.4 setosa 33 ## 26 5.0 1.5 setosa 34 ## 27 5.0 1.6 setosa 34 ## 28 5.0 1.3 setosa 35 ## 29 5.0 1.6 setosa 35 ## 30 5.0 1.4 setosa 36 ## 31 4.9 1.4 setosa 30 ## 32 4.9 1.5 setosa 31 ## 33 4.9 1.5 setosa 31 ## 34 4.9 1.4 setosa 36 ## 35 4.8 1.4 setosa 30 ## 36 4.8 1.4 setosa 30 ## 37 4.8 1.6 setosa 31 ## 38 4.8 1.6 setosa 34 ## 39 4.8 1.9 setosa 34 ## 40 4.7 1.3 setosa 32 ## 41 4.7 1.6 setosa 32 ## 42 4.6 1.5 setosa 31 ## 43 4.6 1.4 setosa 32 ## 44 4.6 1.4 setosa 34 ## 45 4.6 1.0 setosa 36 ## 46 4.5 1.3 setosa 23 ## 47 4.4 1.4 setosa 29 ## 48 4.4 1.3 setosa 30 ## 49 4.4 1.3 setosa 32 ## 50 4.3 1.1 setosa 30 1.1.8 Pivot wider (long to wide) Reshaping data is common in longitudinal analysis and reporting. - Long format (one row per subject-visit) is common for modeling and plotting. - Wide format (one row per subject, with multiple columns for repeated measures) is sometimes required for specific reporting formats or certain algorithms. If no unique identifier row in each group doesn’t work Pivoting wider requires a unique key for each output row. If the key is not unique, values can collide and pivoting may fail or require aggregation. iris %&gt;% pivot_wider( names_from=Species, values_from= c(Sepal.Length)) ## Warning: Values from `Sepal.Length` are not uniquely ## identified; output will contain list-cols. ## • Use `values_fn = list` to suppress this warning. ## • Use `values_fn = {summary_fun}` to summarise ## duplicates. ## • Use the following dplyr code to identify duplicates. ## {data} |&gt; ## dplyr::summarise(n = dplyr::n(), .by = ## c(Sepal.Width, Petal.Length, Petal.Width, Species)) ## |&gt; ## dplyr::filter(n &gt; 1L) ## # A tibble: 143 × 6 ## Sepal.Width Petal.Length Petal.Width setosa versicolor virginica ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 3.5 1.4 0.2 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 2 3 1.4 0.2 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 3 3.2 1.3 0.2 &lt;dbl [2]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 4 3.1 1.5 0.2 &lt;dbl [2]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 5 3.6 1.4 0.2 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 6 3.9 1.7 0.4 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 7 3.4 1.4 0.3 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 8 3.4 1.5 0.2 &lt;dbl [2]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 9 2.9 1.4 0.2 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 10 3.1 1.5 0.1 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## # ℹ 133 more rows Create a unique identifier row for each name and then use pivot_wider Here we create a row number within each Species group, which becomes part of the unique key. widedata &lt;- iris %&gt;% # create groups then assign unique identifier row number in each group group_by(Species) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from=Species, values_from= c(Petal.Length,Sepal.Length,Petal.Width,Sepal.Width)) widedata ## # A tibble: 50 × 13 ## row Petal.Length_setosa Petal.Length_versicolor Petal.Length_virginica ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.4 4.7 6 ## 2 2 1.4 4.5 5.1 ## 3 3 1.3 4.9 5.9 ## 4 4 1.5 4 5.6 ## 5 5 1.4 4.6 5.8 ## 6 6 1.7 4.5 6.6 ## 7 7 1.4 4.7 4.5 ## 8 8 1.5 3.3 6.3 ## 9 9 1.4 4.6 5.8 ## 10 10 1.5 3.9 6.1 ## # ℹ 40 more rows ## # ℹ 9 more variables: Sepal.Length_setosa &lt;dbl&gt;, Sepal.Length_versicolor &lt;dbl&gt;, ## # Sepal.Length_virginica &lt;dbl&gt;, Petal.Width_setosa &lt;dbl&gt;, ## # Petal.Width_versicolor &lt;dbl&gt;, Petal.Width_virginica &lt;dbl&gt;, ## # Sepal.Width_setosa &lt;dbl&gt;, Sepal.Width_versicolor &lt;dbl&gt;, ## # Sepal.Width_virginica &lt;dbl&gt; This is a smaller example, pivoting only a subset of variables. iris %&gt;% group_by(Species) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider( names_from=Species, values_from= c(Petal.Length, Petal.Width)) ## # A tibble: 150 × 9 ## Sepal.Length Sepal.Width row Petal.Length_setosa Petal.Length_versicolor ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1 1.4 NA ## 2 4.9 3 2 1.4 NA ## 3 4.7 3.2 3 1.3 NA ## 4 4.6 3.1 4 1.5 NA ## 5 5 3.6 5 1.4 NA ## 6 5.4 3.9 6 1.7 NA ## 7 4.6 3.4 7 1.4 NA ## 8 5 3.4 8 1.5 NA ## 9 4.4 2.9 9 1.4 NA ## 10 4.9 3.1 10 1.5 NA ## # ℹ 140 more rows ## # ℹ 4 more variables: Petal.Length_virginica &lt;dbl&gt;, Petal.Width_setosa &lt;dbl&gt;, ## # Petal.Width_versicolor &lt;dbl&gt;, Petal.Width_virginica &lt;dbl&gt; 1.1.9 Pivot longer (wide to long) why long format? there are many analytic techniques that require our data to be in this format, like plotting. In clinical trial and observational research, long format is often the default because it supports: - longitudinal models (e.g., MMRM, mixed models) - visit-based summaries - plotting repeated measures over time - tidy workflows where “one observation per row” is preferred The next example converts selected wide columns into a long structure. longdata = pivot_longer(widedata, - c( &quot;row&quot; , &quot;Petal.Length_setosa&quot; , &quot;Petal.Length_versicolor&quot;, &quot;Petal.Length_virginica&quot;, &quot;Sepal.Length_setosa&quot; , &quot;Sepal.Length_versicolor&quot;, &quot;Sepal.Length_virginica&quot; , &quot;Petal.Width_setosa&quot; , &quot;Petal.Width_versicolor&quot; , &quot;Petal.Width_virginica&quot; ) , names_to=&quot;Sepal.Width&quot;, values_to=&quot;Sepal.Width.value&quot;) longdata ## # A tibble: 150 × 12 ## row Petal.Length_setosa Petal.Length_versicolor Petal.Length_virginica ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.4 4.7 6 ## 2 1 1.4 4.7 6 ## 3 1 1.4 4.7 6 ## 4 2 1.4 4.5 5.1 ## 5 2 1.4 4.5 5.1 ## 6 2 1.4 4.5 5.1 ## 7 3 1.3 4.9 5.9 ## 8 3 1.3 4.9 5.9 ## 9 3 1.3 4.9 5.9 ## 10 4 1.5 4 5.6 ## # ℹ 140 more rows ## # ℹ 8 more variables: Sepal.Length_setosa &lt;dbl&gt;, Sepal.Length_versicolor &lt;dbl&gt;, ## # Sepal.Length_virginica &lt;dbl&gt;, Petal.Width_setosa &lt;dbl&gt;, ## # Petal.Width_versicolor &lt;dbl&gt;, Petal.Width_virginica &lt;dbl&gt;, ## # Sepal.Width &lt;chr&gt;, Sepal.Width.value &lt;dbl&gt; The following babies dataset is a simple example of repeated measurements stored in wide format (e.g., weight_3, weight_6). This naming pattern is common in real datasets: variable name + underscore + timepoint. babies &lt;- tibble( id = 1001:1008, sex = c(&quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;), weight_3 = c(9, 11, 17, 16, 11, 17, 16, 15), weight_6 = c(13, 16, 20, 18, 15, 21, 17, 16), weight_9 = c(16, 17, 23, 21, 16, 25, 19, 18), weight_12 = c(17, 20, 24, 22, 18, 26, 21, 19) ) %&gt;% print() ## # A tibble: 8 × 6 ## id sex weight_3 weight_6 weight_9 weight_12 ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 F 9 13 16 17 ## 2 1002 F 11 16 17 20 ## 3 1003 M 17 20 23 24 ## 4 1004 F 16 18 21 22 ## 5 1005 M 11 15 16 18 ## 6 1006 M 17 21 25 26 ## 7 1007 M 16 17 19 21 ## 8 1008 F 15 16 18 19 select which cols pivot_longer() converts multiple columns into key-value pairs. Here, we reshape all columns that start with \"weight\". babies %&gt;% pivot_longer( cols = starts_with(&quot;weight&quot;), #which cols names_to = &quot;months&quot;, names_prefix = &quot;weight_&quot;, #remove prefix values_to = &quot;weight&quot; ) ## # A tibble: 32 × 4 ## id sex months weight ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1001 F 3 9 ## 2 1001 F 6 13 ## 3 1001 F 9 16 ## 4 1001 F 12 17 ## 5 1002 F 3 11 ## 6 1002 F 6 16 ## 7 1002 F 9 17 ## 8 1002 F 12 20 ## 9 1003 M 3 17 ## 10 1003 M 6 20 ## # ℹ 22 more rows using names_prefix argument names_prefix removes a fixed prefix from the variable names. This is useful to keep only the meaningful portion (e.g., the timepoint). “it is a regular expression used to remove matching text from the start of each variable name.” babies %&gt;% pivot_longer( cols = starts_with(&quot;weight&quot;), names_to = &quot;months&quot;, names_prefix = &quot;\\\\w+_&quot; ) ## # A tibble: 32 × 4 ## id sex months value ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1001 F 3 9 ## 2 1001 F 6 13 ## 3 1001 F 9 16 ## 4 1001 F 12 17 ## 5 1002 F 3 11 ## 6 1002 F 6 16 ## 7 1002 F 9 17 ## 8 1002 F 12 20 ## 9 1003 M 3 17 ## 10 1003 M 6 20 ## # ℹ 22 more rows using default argument If you do not specify names_to or values_to, pivot_longer() uses defaults. This is convenient for quick exploration, but in practice you usually want explicit names for clarity. babies %&gt;% pivot_longer( cols = starts_with(&quot;weight&quot;) ) ## # A tibble: 32 × 4 ## id sex name value ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1001 F weight_3 9 ## 2 1001 F weight_6 13 ## 3 1001 F weight_9 16 ## 4 1001 F weight_12 17 ## 5 1002 F weight_3 11 ## 6 1002 F weight_6 16 ## 7 1002 F weight_9 17 ## 8 1002 F weight_12 20 ## 9 1003 M weight_3 17 ## 10 1003 M weight_6 20 ## # ℹ 22 more rows This extended example includes both weight and length measured at multiple timepoints. We will demonstrate how to reshape multiple measurement types in one step. set.seed(123) babies &lt;- tibble( id = 1001:1008, sex = c(&quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;), weight_3 = c(9, 11, 17, 16, 11, 17, 16, 15), weight_6 = c(13, 16, 20, 18, 15, 21, 17, 16), weight_9 = c(16, 17, 23, 21, 16, 25, 19, 18), weight_12 = c(17, 20, 24, 22, 18, 26, 21, 19), length_3 = c(17, 19, 23, 20, 18, 22, 21, 18), length_6 = round(length_3 + rnorm(8, 2, 1)), length_9 = round(length_6 + rnorm(8, 2, 1)), length_12 = round(length_9 + rnorm(8, 2, 1)), ) %&gt;% print() ## # A tibble: 8 × 10 ## id sex weight_3 weight_6 weight_9 weight_12 length_3 length_6 length_9 ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 F 9 13 16 17 17 18 19 ## 2 1002 F 11 16 17 20 19 21 23 ## 3 1003 M 17 20 23 24 23 27 30 ## 4 1004 F 16 18 21 22 20 22 24 ## 5 1005 M 11 15 16 18 18 20 22 ## 6 1006 M 17 21 25 26 22 26 28 ## 7 1007 M 16 17 19 21 21 23 24 ## 8 1008 F 15 16 18 19 18 19 23 ## # ℹ 1 more variable: length_12 &lt;dbl&gt; pivoting multiple sets of cols When variable names contain multiple components (e.g., weight_3), you can split them into separate columns using names_sep. Using .value is a powerful technique: it tells pivot_longer() to create a separate output column for each measurement type (e.g., weight and length). This format is especially helpful for longitudinal plotting and modeling where you want one row per subject-month. babies %&gt;% pivot_longer( cols = c(-id, -sex), names_to = c(&quot;.value&quot;, &quot;months&quot;), names_sep = &quot;_&quot; ) ## # A tibble: 32 × 5 ## id sex months weight length ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 F 3 9 17 ## 2 1001 F 6 13 18 ## 3 1001 F 9 16 19 ## 4 1001 F 12 17 21 ## 5 1002 F 3 11 19 ## 6 1002 F 6 16 21 ## 7 1002 F 9 17 23 ## 8 1002 F 12 20 23 ## 9 1003 M 3 17 23 ## 10 1003 M 6 20 27 ## # ℹ 22 more rows This alternative example reshapes into a simpler long format with one value column. This is useful when you want a single measurement (e.g., weight only). babies %&gt;% pivot_longer( cols = c(-id, -sex), names_to = &quot;months&quot;, values_to = &quot;weight&quot; ) %&gt;% print() ## # A tibble: 64 × 4 ## id sex months weight ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1001 F weight_3 9 ## 2 1001 F weight_6 13 ## 3 1001 F weight_9 16 ## 4 1001 F weight_12 17 ## 5 1001 F length_3 17 ## 6 1001 F length_6 18 ## 7 1001 F length_9 19 ## 8 1001 F length_12 21 ## 9 1002 F weight_3 11 ## 10 1002 F weight_6 16 ## # ℹ 54 more rows using names_sep and .value argument Here we keep both parts of the name as separate columns: measure and months. This creates a “tidy” dataset with one value column and a measure identifier. babies %&gt;% pivot_longer( cols = c(-id, -sex), names_to = c(&quot;measure&quot;, &quot;months&quot;), names_sep = &quot;_&quot; ) ## # A tibble: 64 × 5 ## id sex measure months value ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1001 F weight 3 9 ## 2 1001 F weight 6 13 ## 3 1001 F weight 9 16 ## 4 1001 F weight 12 17 ## 5 1001 F length 3 17 ## 6 1001 F length 6 18 ## 7 1001 F length 9 19 ## 8 1001 F length 12 21 ## 9 1002 F weight 3 11 ## 10 1002 F weight 6 16 ## # ℹ 54 more rows .value tells pivot_longer() to create a new column for each unique character string that is in front of the underscore This is often the most practical output when you have multiple repeated-measure variables collected at the same timepoints. babies %&gt;% pivot_longer( cols = c(-id, -sex), names_to = c( &quot;.value&quot;,&quot;months&quot; ), names_sep = &quot;_&quot; ) ## # A tibble: 32 × 5 ## id sex months weight length ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 F 3 9 17 ## 2 1001 F 6 13 18 ## 3 1001 F 9 16 19 ## 4 1001 F 12 17 21 ## 5 1002 F 3 11 19 ## 6 1002 F 6 16 21 ## 7 1002 F 9 17 23 ## 8 1002 F 12 20 23 ## 9 1003 M 3 17 23 ## 10 1003 M 6 20 27 ## # ℹ 22 more rows Swapping the order changes the interpretation of the split name pieces. This example is mainly to show that the mapping is positional. babies %&gt;% pivot_longer( cols = c(-id, -sex), names_to = c( &quot;months&quot;, &quot;.value&quot;), names_sep = &quot;_&quot; ) ## # A tibble: 16 × 7 ## id sex months `3` `6` `9` `12` ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 F weight 9 13 16 17 ## 2 1001 F length 17 18 19 21 ## 3 1002 F weight 11 16 17 20 ## 4 1002 F length 19 21 23 23 ## 5 1003 M weight 17 20 23 24 ## 6 1003 M length 23 27 30 33 ## 7 1004 F weight 16 18 21 22 ## 8 1004 F length 20 22 24 26 ## 9 1005 M weight 11 15 16 18 ## 10 1005 M length 18 20 22 23 ## 11 1006 M weight 17 21 25 26 ## 12 1006 M length 22 26 28 30 ## 13 1007 M weight 16 17 19 21 ## 14 1007 M length 21 23 24 25 ## 15 1008 F weight 15 16 18 19 ## 16 1008 F length 18 19 23 24 Pivot wider again (long to wide) After processing data in long format (e.g., computing summaries by month), you may want to convert back to wide format for reporting. pivoting to multiple sets of cols This demonstrates a common workflow: wide → long (for analysis) → wide (for reporting). babies %&gt;% pivot_longer( cols = c(-id, -sex), names_to = c( &quot;.value&quot;,&quot;months&quot; ), names_sep = &quot;_&quot; ) %&gt;% pivot_wider( names_from=months, values_from= c(weight, length )) ## # A tibble: 8 × 10 ## id sex weight_3 weight_6 weight_9 weight_12 length_3 length_6 length_9 ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 F 9 13 16 17 17 18 19 ## 2 1002 F 11 16 17 20 19 21 23 ## 3 1003 M 17 20 23 24 23 27 30 ## 4 1004 F 16 18 21 22 20 22 24 ## 5 1005 M 11 15 16 18 18 20 22 ## 6 1006 M 17 21 25 26 22 26 28 ## 7 1007 M 16 17 19 21 21 23 24 ## 8 1008 F 15 16 18 19 18 19 23 ## # ℹ 1 more variable: length_12 &lt;dbl&gt; This example pivots the previously created longdata object back to wide. pivot_wider(longdata, names_from=Sepal.Width, values_from= c(Sepal.Width.value)) ## # A tibble: 50 × 13 ## row Petal.Length_setosa Petal.Length_versicolor Petal.Length_virginica ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.4 4.7 6 ## 2 2 1.4 4.5 5.1 ## 3 3 1.3 4.9 5.9 ## 4 4 1.5 4 5.6 ## 5 5 1.4 4.6 5.8 ## 6 6 1.7 4.5 6.6 ## 7 7 1.4 4.7 4.5 ## 8 8 1.5 3.3 6.3 ## 9 9 1.4 4.6 5.8 ## 10 10 1.5 3.9 6.1 ## # ℹ 40 more rows ## # ℹ 9 more variables: Sepal.Length_setosa &lt;dbl&gt;, Sepal.Length_versicolor &lt;dbl&gt;, ## # Sepal.Length_virginica &lt;dbl&gt;, Petal.Width_setosa &lt;dbl&gt;, ## # Petal.Width_versicolor &lt;dbl&gt;, Petal.Width_virginica &lt;dbl&gt;, ## # Sepal.Width_setosa &lt;dbl&gt;, Sepal.Width_versicolor &lt;dbl&gt;, ## # Sepal.Width_virginica &lt;dbl&gt; 1.1.10 Separate columns Separating columns is useful when a single string variable contains multiple components. In practice, this appears in fields like codes (e.g., lab test codes), composite identifiers, or delimited values. Here we split Species into three parts using \"o\" as the separator. This is a toy example to demonstrate the function. separate(iris, Species, into = c(&quot;integer&quot;,&quot;decimal&quot;,&quot;third&quot;), sep=&quot;o&quot;) ## Warning: Expected 3 pieces. Missing pieces filled with `NA` in ## 100 rows [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, ## 14, 15, 16, 17, 18, 19, 20, ...]. ## Sepal.Length Sepal.Width Petal.Length Petal.Width integer decimal third ## 1 5.1 3.5 1.4 0.2 set sa &lt;NA&gt; ## 2 4.9 3.0 1.4 0.2 set sa &lt;NA&gt; ## 3 4.7 3.2 1.3 0.2 set sa &lt;NA&gt; ## 4 4.6 3.1 1.5 0.2 set sa &lt;NA&gt; ## 5 5.0 3.6 1.4 0.2 set sa &lt;NA&gt; ## 6 5.4 3.9 1.7 0.4 set sa &lt;NA&gt; ## 7 4.6 3.4 1.4 0.3 set sa &lt;NA&gt; ## 8 5.0 3.4 1.5 0.2 set sa &lt;NA&gt; ## 9 4.4 2.9 1.4 0.2 set sa &lt;NA&gt; ## 10 4.9 3.1 1.5 0.1 set sa &lt;NA&gt; ## 11 5.4 3.7 1.5 0.2 set sa &lt;NA&gt; ## 12 4.8 3.4 1.6 0.2 set sa &lt;NA&gt; ## 13 4.8 3.0 1.4 0.1 set sa &lt;NA&gt; ## 14 4.3 3.0 1.1 0.1 set sa &lt;NA&gt; ## 15 5.8 4.0 1.2 0.2 set sa &lt;NA&gt; ## 16 5.7 4.4 1.5 0.4 set sa &lt;NA&gt; ## 17 5.4 3.9 1.3 0.4 set sa &lt;NA&gt; ## 18 5.1 3.5 1.4 0.3 set sa &lt;NA&gt; ## 19 5.7 3.8 1.7 0.3 set sa &lt;NA&gt; ## 20 5.1 3.8 1.5 0.3 set sa &lt;NA&gt; ## 21 5.4 3.4 1.7 0.2 set sa &lt;NA&gt; ## 22 5.1 3.7 1.5 0.4 set sa &lt;NA&gt; ## 23 4.6 3.6 1.0 0.2 set sa &lt;NA&gt; ## 24 5.1 3.3 1.7 0.5 set sa &lt;NA&gt; ## 25 4.8 3.4 1.9 0.2 set sa &lt;NA&gt; ## 26 5.0 3.0 1.6 0.2 set sa &lt;NA&gt; ## 27 5.0 3.4 1.6 0.4 set sa &lt;NA&gt; ## 28 5.2 3.5 1.5 0.2 set sa &lt;NA&gt; ## 29 5.2 3.4 1.4 0.2 set sa &lt;NA&gt; ## 30 4.7 3.2 1.6 0.2 set sa &lt;NA&gt; ## 31 4.8 3.1 1.6 0.2 set sa &lt;NA&gt; ## 32 5.4 3.4 1.5 0.4 set sa &lt;NA&gt; ## 33 5.2 4.1 1.5 0.1 set sa &lt;NA&gt; ## 34 5.5 4.2 1.4 0.2 set sa &lt;NA&gt; ## 35 4.9 3.1 1.5 0.2 set sa &lt;NA&gt; ## 36 5.0 3.2 1.2 0.2 set sa &lt;NA&gt; ## 37 5.5 3.5 1.3 0.2 set sa &lt;NA&gt; ## 38 4.9 3.6 1.4 0.1 set sa &lt;NA&gt; ## 39 4.4 3.0 1.3 0.2 set sa &lt;NA&gt; ## 40 5.1 3.4 1.5 0.2 set sa &lt;NA&gt; ## 41 5.0 3.5 1.3 0.3 set sa &lt;NA&gt; ## 42 4.5 2.3 1.3 0.3 set sa &lt;NA&gt; ## 43 4.4 3.2 1.3 0.2 set sa &lt;NA&gt; ## 44 5.0 3.5 1.6 0.6 set sa &lt;NA&gt; ## 45 5.1 3.8 1.9 0.4 set sa &lt;NA&gt; ## 46 4.8 3.0 1.4 0.3 set sa &lt;NA&gt; ## 47 5.1 3.8 1.6 0.2 set sa &lt;NA&gt; ## 48 4.6 3.2 1.4 0.2 set sa &lt;NA&gt; ## 49 5.3 3.7 1.5 0.2 set sa &lt;NA&gt; ## 50 5.0 3.3 1.4 0.2 set sa &lt;NA&gt; ## 51 7.0 3.2 4.7 1.4 versic l r ## 52 6.4 3.2 4.5 1.5 versic l r ## 53 6.9 3.1 4.9 1.5 versic l r ## 54 5.5 2.3 4.0 1.3 versic l r ## 55 6.5 2.8 4.6 1.5 versic l r ## 56 5.7 2.8 4.5 1.3 versic l r ## 57 6.3 3.3 4.7 1.6 versic l r ## 58 4.9 2.4 3.3 1.0 versic l r ## 59 6.6 2.9 4.6 1.3 versic l r ## 60 5.2 2.7 3.9 1.4 versic l r ## 61 5.0 2.0 3.5 1.0 versic l r ## 62 5.9 3.0 4.2 1.5 versic l r ## 63 6.0 2.2 4.0 1.0 versic l r ## 64 6.1 2.9 4.7 1.4 versic l r ## 65 5.6 2.9 3.6 1.3 versic l r ## 66 6.7 3.1 4.4 1.4 versic l r ## 67 5.6 3.0 4.5 1.5 versic l r ## 68 5.8 2.7 4.1 1.0 versic l r ## 69 6.2 2.2 4.5 1.5 versic l r ## 70 5.6 2.5 3.9 1.1 versic l r ## 71 5.9 3.2 4.8 1.8 versic l r ## 72 6.1 2.8 4.0 1.3 versic l r ## 73 6.3 2.5 4.9 1.5 versic l r ## 74 6.1 2.8 4.7 1.2 versic l r ## 75 6.4 2.9 4.3 1.3 versic l r ## 76 6.6 3.0 4.4 1.4 versic l r ## 77 6.8 2.8 4.8 1.4 versic l r ## 78 6.7 3.0 5.0 1.7 versic l r ## 79 6.0 2.9 4.5 1.5 versic l r ## 80 5.7 2.6 3.5 1.0 versic l r ## 81 5.5 2.4 3.8 1.1 versic l r ## 82 5.5 2.4 3.7 1.0 versic l r ## 83 5.8 2.7 3.9 1.2 versic l r ## 84 6.0 2.7 5.1 1.6 versic l r ## 85 5.4 3.0 4.5 1.5 versic l r ## 86 6.0 3.4 4.5 1.6 versic l r ## 87 6.7 3.1 4.7 1.5 versic l r ## 88 6.3 2.3 4.4 1.3 versic l r ## 89 5.6 3.0 4.1 1.3 versic l r ## 90 5.5 2.5 4.0 1.3 versic l r ## 91 5.5 2.6 4.4 1.2 versic l r ## 92 6.1 3.0 4.6 1.4 versic l r ## 93 5.8 2.6 4.0 1.2 versic l r ## 94 5.0 2.3 3.3 1.0 versic l r ## 95 5.6 2.7 4.2 1.3 versic l r ## 96 5.7 3.0 4.2 1.2 versic l r ## 97 5.7 2.9 4.2 1.3 versic l r ## 98 6.2 2.9 4.3 1.3 versic l r ## 99 5.1 2.5 3.0 1.1 versic l r ## 100 5.7 2.8 4.1 1.3 versic l r ## 101 6.3 3.3 6.0 2.5 virginica &lt;NA&gt; &lt;NA&gt; ## 102 5.8 2.7 5.1 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 103 7.1 3.0 5.9 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 104 6.3 2.9 5.6 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 105 6.5 3.0 5.8 2.2 virginica &lt;NA&gt; &lt;NA&gt; ## 106 7.6 3.0 6.6 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 107 4.9 2.5 4.5 1.7 virginica &lt;NA&gt; &lt;NA&gt; ## 108 7.3 2.9 6.3 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 109 6.7 2.5 5.8 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 110 7.2 3.6 6.1 2.5 virginica &lt;NA&gt; &lt;NA&gt; ## 111 6.5 3.2 5.1 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 112 6.4 2.7 5.3 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 113 6.8 3.0 5.5 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 114 5.7 2.5 5.0 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 115 5.8 2.8 5.1 2.4 virginica &lt;NA&gt; &lt;NA&gt; ## 116 6.4 3.2 5.3 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 117 6.5 3.0 5.5 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 118 7.7 3.8 6.7 2.2 virginica &lt;NA&gt; &lt;NA&gt; ## 119 7.7 2.6 6.9 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 120 6.0 2.2 5.0 1.5 virginica &lt;NA&gt; &lt;NA&gt; ## 121 6.9 3.2 5.7 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 122 5.6 2.8 4.9 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 123 7.7 2.8 6.7 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 124 6.3 2.7 4.9 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 125 6.7 3.3 5.7 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 126 7.2 3.2 6.0 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 127 6.2 2.8 4.8 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 128 6.1 3.0 4.9 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 129 6.4 2.8 5.6 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 130 7.2 3.0 5.8 1.6 virginica &lt;NA&gt; &lt;NA&gt; ## 131 7.4 2.8 6.1 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 132 7.9 3.8 6.4 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 133 6.4 2.8 5.6 2.2 virginica &lt;NA&gt; &lt;NA&gt; ## 134 6.3 2.8 5.1 1.5 virginica &lt;NA&gt; &lt;NA&gt; ## 135 6.1 2.6 5.6 1.4 virginica &lt;NA&gt; &lt;NA&gt; ## 136 7.7 3.0 6.1 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 137 6.3 3.4 5.6 2.4 virginica &lt;NA&gt; &lt;NA&gt; ## 138 6.4 3.1 5.5 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 139 6.0 3.0 4.8 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 140 6.9 3.1 5.4 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 141 6.7 3.1 5.6 2.4 virginica &lt;NA&gt; &lt;NA&gt; ## 142 6.9 3.1 5.1 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 143 5.8 2.7 5.1 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 144 6.8 3.2 5.9 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 145 6.7 3.3 5.7 2.5 virginica &lt;NA&gt; &lt;NA&gt; ## 146 6.7 3.0 5.2 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 147 6.3 2.5 5.0 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 148 6.5 3.0 5.2 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 149 6.2 3.4 5.4 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 150 5.9 3.0 5.1 1.8 virginica &lt;NA&gt; &lt;NA&gt; 1.1.11 Recode/relabel data Recoding is commonly used to standardize categories, shorten labels, or map source system codes to analysis-friendly values. mutate(iris, Species2 = recode(Species, &quot;setosa&quot;=&quot;seto&quot;, &quot;versicolor&quot;=&quot;versi&quot;)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Species2 ## 1 5.1 3.5 1.4 0.2 setosa seto ## 2 4.9 3.0 1.4 0.2 setosa seto ## 3 4.7 3.2 1.3 0.2 setosa seto ## 4 4.6 3.1 1.5 0.2 setosa seto ## 5 5.0 3.6 1.4 0.2 setosa seto ## 6 5.4 3.9 1.7 0.4 setosa seto ## 7 4.6 3.4 1.4 0.3 setosa seto ## 8 5.0 3.4 1.5 0.2 setosa seto ## 9 4.4 2.9 1.4 0.2 setosa seto ## 10 4.9 3.1 1.5 0.1 setosa seto ## 11 5.4 3.7 1.5 0.2 setosa seto ## 12 4.8 3.4 1.6 0.2 setosa seto ## 13 4.8 3.0 1.4 0.1 setosa seto ## 14 4.3 3.0 1.1 0.1 setosa seto ## 15 5.8 4.0 1.2 0.2 setosa seto ## 16 5.7 4.4 1.5 0.4 setosa seto ## 17 5.4 3.9 1.3 0.4 setosa seto ## 18 5.1 3.5 1.4 0.3 setosa seto ## 19 5.7 3.8 1.7 0.3 setosa seto ## 20 5.1 3.8 1.5 0.3 setosa seto ## 21 5.4 3.4 1.7 0.2 setosa seto ## 22 5.1 3.7 1.5 0.4 setosa seto ## 23 4.6 3.6 1.0 0.2 setosa seto ## 24 5.1 3.3 1.7 0.5 setosa seto ## 25 4.8 3.4 1.9 0.2 setosa seto ## 26 5.0 3.0 1.6 0.2 setosa seto ## 27 5.0 3.4 1.6 0.4 setosa seto ## 28 5.2 3.5 1.5 0.2 setosa seto ## 29 5.2 3.4 1.4 0.2 setosa seto ## 30 4.7 3.2 1.6 0.2 setosa seto ## 31 4.8 3.1 1.6 0.2 setosa seto ## 32 5.4 3.4 1.5 0.4 setosa seto ## 33 5.2 4.1 1.5 0.1 setosa seto ## 34 5.5 4.2 1.4 0.2 setosa seto ## 35 4.9 3.1 1.5 0.2 setosa seto ## 36 5.0 3.2 1.2 0.2 setosa seto ## 37 5.5 3.5 1.3 0.2 setosa seto ## 38 4.9 3.6 1.4 0.1 setosa seto ## 39 4.4 3.0 1.3 0.2 setosa seto ## 40 5.1 3.4 1.5 0.2 setosa seto ## 41 5.0 3.5 1.3 0.3 setosa seto ## 42 4.5 2.3 1.3 0.3 setosa seto ## 43 4.4 3.2 1.3 0.2 setosa seto ## 44 5.0 3.5 1.6 0.6 setosa seto ## 45 5.1 3.8 1.9 0.4 setosa seto ## 46 4.8 3.0 1.4 0.3 setosa seto ## 47 5.1 3.8 1.6 0.2 setosa seto ## 48 4.6 3.2 1.4 0.2 setosa seto ## 49 5.3 3.7 1.5 0.2 setosa seto ## 50 5.0 3.3 1.4 0.2 setosa seto ## 51 7.0 3.2 4.7 1.4 versicolor versi ## 52 6.4 3.2 4.5 1.5 versicolor versi ## 53 6.9 3.1 4.9 1.5 versicolor versi ## 54 5.5 2.3 4.0 1.3 versicolor versi ## 55 6.5 2.8 4.6 1.5 versicolor versi ## 56 5.7 2.8 4.5 1.3 versicolor versi ## 57 6.3 3.3 4.7 1.6 versicolor versi ## 58 4.9 2.4 3.3 1.0 versicolor versi ## 59 6.6 2.9 4.6 1.3 versicolor versi ## 60 5.2 2.7 3.9 1.4 versicolor versi ## 61 5.0 2.0 3.5 1.0 versicolor versi ## 62 5.9 3.0 4.2 1.5 versicolor versi ## 63 6.0 2.2 4.0 1.0 versicolor versi ## 64 6.1 2.9 4.7 1.4 versicolor versi ## 65 5.6 2.9 3.6 1.3 versicolor versi ## 66 6.7 3.1 4.4 1.4 versicolor versi ## 67 5.6 3.0 4.5 1.5 versicolor versi ## 68 5.8 2.7 4.1 1.0 versicolor versi ## 69 6.2 2.2 4.5 1.5 versicolor versi ## 70 5.6 2.5 3.9 1.1 versicolor versi ## 71 5.9 3.2 4.8 1.8 versicolor versi ## 72 6.1 2.8 4.0 1.3 versicolor versi ## 73 6.3 2.5 4.9 1.5 versicolor versi ## 74 6.1 2.8 4.7 1.2 versicolor versi ## 75 6.4 2.9 4.3 1.3 versicolor versi ## 76 6.6 3.0 4.4 1.4 versicolor versi ## 77 6.8 2.8 4.8 1.4 versicolor versi ## 78 6.7 3.0 5.0 1.7 versicolor versi ## 79 6.0 2.9 4.5 1.5 versicolor versi ## 80 5.7 2.6 3.5 1.0 versicolor versi ## 81 5.5 2.4 3.8 1.1 versicolor versi ## 82 5.5 2.4 3.7 1.0 versicolor versi ## 83 5.8 2.7 3.9 1.2 versicolor versi ## 84 6.0 2.7 5.1 1.6 versicolor versi ## 85 5.4 3.0 4.5 1.5 versicolor versi ## 86 6.0 3.4 4.5 1.6 versicolor versi ## 87 6.7 3.1 4.7 1.5 versicolor versi ## 88 6.3 2.3 4.4 1.3 versicolor versi ## 89 5.6 3.0 4.1 1.3 versicolor versi ## 90 5.5 2.5 4.0 1.3 versicolor versi ## 91 5.5 2.6 4.4 1.2 versicolor versi ## 92 6.1 3.0 4.6 1.4 versicolor versi ## 93 5.8 2.6 4.0 1.2 versicolor versi ## 94 5.0 2.3 3.3 1.0 versicolor versi ## 95 5.6 2.7 4.2 1.3 versicolor versi ## 96 5.7 3.0 4.2 1.2 versicolor versi ## 97 5.7 2.9 4.2 1.3 versicolor versi ## 98 6.2 2.9 4.3 1.3 versicolor versi ## 99 5.1 2.5 3.0 1.1 versicolor versi ## 100 5.7 2.8 4.1 1.3 versicolor versi ## 101 6.3 3.3 6.0 2.5 virginica virginica ## 102 5.8 2.7 5.1 1.9 virginica virginica ## 103 7.1 3.0 5.9 2.1 virginica virginica ## 104 6.3 2.9 5.6 1.8 virginica virginica ## 105 6.5 3.0 5.8 2.2 virginica virginica ## 106 7.6 3.0 6.6 2.1 virginica virginica ## 107 4.9 2.5 4.5 1.7 virginica virginica ## 108 7.3 2.9 6.3 1.8 virginica virginica ## 109 6.7 2.5 5.8 1.8 virginica virginica ## 110 7.2 3.6 6.1 2.5 virginica virginica ## 111 6.5 3.2 5.1 2.0 virginica virginica ## 112 6.4 2.7 5.3 1.9 virginica virginica ## 113 6.8 3.0 5.5 2.1 virginica virginica ## 114 5.7 2.5 5.0 2.0 virginica virginica ## 115 5.8 2.8 5.1 2.4 virginica virginica ## 116 6.4 3.2 5.3 2.3 virginica virginica ## 117 6.5 3.0 5.5 1.8 virginica virginica ## 118 7.7 3.8 6.7 2.2 virginica virginica ## 119 7.7 2.6 6.9 2.3 virginica virginica ## 120 6.0 2.2 5.0 1.5 virginica virginica ## 121 6.9 3.2 5.7 2.3 virginica virginica ## 122 5.6 2.8 4.9 2.0 virginica virginica ## 123 7.7 2.8 6.7 2.0 virginica virginica ## 124 6.3 2.7 4.9 1.8 virginica virginica ## 125 6.7 3.3 5.7 2.1 virginica virginica ## 126 7.2 3.2 6.0 1.8 virginica virginica ## 127 6.2 2.8 4.8 1.8 virginica virginica ## 128 6.1 3.0 4.9 1.8 virginica virginica ## 129 6.4 2.8 5.6 2.1 virginica virginica ## 130 7.2 3.0 5.8 1.6 virginica virginica ## 131 7.4 2.8 6.1 1.9 virginica virginica ## 132 7.9 3.8 6.4 2.0 virginica virginica ## 133 6.4 2.8 5.6 2.2 virginica virginica ## 134 6.3 2.8 5.1 1.5 virginica virginica ## 135 6.1 2.6 5.6 1.4 virginica virginica ## 136 7.7 3.0 6.1 2.3 virginica virginica ## 137 6.3 3.4 5.6 2.4 virginica virginica ## 138 6.4 3.1 5.5 1.8 virginica virginica ## 139 6.0 3.0 4.8 1.8 virginica virginica ## 140 6.9 3.1 5.4 2.1 virginica virginica ## 141 6.7 3.1 5.6 2.4 virginica virginica ## 142 6.9 3.1 5.1 2.3 virginica virginica ## 143 5.8 2.7 5.1 1.9 virginica virginica ## 144 6.8 3.2 5.9 2.3 virginica virginica ## 145 6.7 3.3 5.7 2.5 virginica virginica ## 146 6.7 3.0 5.2 2.3 virginica virginica ## 147 6.3 2.5 5.0 1.9 virginica virginica ## 148 6.5 3.0 5.2 2.0 virginica virginica ## 149 6.2 3.4 5.4 2.3 virginica virginica ## 150 5.9 3.0 5.1 1.8 virginica virginica 1.1.12 deduplication Duplicate rows can occur due to data entry issues, merges that unintentionally replicate records, or repeated extracts. A typical cleaning step is to either remove duplicates or flag them for review. 1.1.12.1 Complete duplicate row Complete duplicates are rows that match across all variables. distinct() keeps only unique rows. df &lt;- tribble( ~id, ~day, ~x, 1, 1, 1, 1, 2, 11, 2, 1, 12, 2, 2, 13, 2, 2, 14, 3, 1, 12, 3, 1, 12, 3, 2, 13, 4, 1, 13, 5, 1, 10, 5, 2, 11, 5, 1, 10 ) df %&gt;% distinct() ## # A tibble: 10 × 3 ## id day x ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 ## 2 1 2 11 ## 3 2 1 12 ## 4 2 2 13 ## 5 2 2 14 ## 6 3 1 12 ## 7 3 2 13 ## 8 4 1 13 ## 9 5 1 10 ## 10 5 2 11 mark duplication The duplicated() function identifies whether each row has appeared previously. This is useful when you want to keep the full dataset but flag duplicates for investigation. df %&gt;% mutate(dup = duplicated(df)) ## # A tibble: 12 × 4 ## id day x dup ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 1 1 1 FALSE ## 2 1 2 11 FALSE ## 3 2 1 12 FALSE ## 4 2 2 13 FALSE ## 5 2 2 14 FALSE ## 6 3 1 12 FALSE ## 7 3 1 12 TRUE ## 8 3 2 13 FALSE ## 9 4 1 13 FALSE ## 10 5 1 10 FALSE ## 11 5 2 11 FALSE ## 12 5 1 10 TRUE This approach creates a row counter within identical rows and flags the second and later occurrences as duplicates. df %&gt;% group_by_all() %&gt;% mutate( n_row = row_number(), dup = n_row &gt; 1 ) ## # A tibble: 12 × 5 ## # Groups: id, day, x [10] ## id day x n_row dup ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; ## 1 1 1 1 1 FALSE ## 2 1 2 11 1 FALSE ## 3 2 1 12 1 FALSE ## 4 2 2 13 1 FALSE ## 5 2 2 14 1 FALSE ## 6 3 1 12 1 FALSE ## 7 3 1 12 2 TRUE ## 8 3 2 13 1 FALSE ## 9 4 1 13 1 FALSE ## 10 5 1 10 1 FALSE ## 11 5 2 11 1 FALSE ## 12 5 1 10 2 TRUE 1.1.12.2 Partial duplicate rows Sometimes duplication is defined only by a key (e.g., subject + visit), not by all columns. Here we keep only the first record for each id, day pair and keep all columns (.keep_all = TRUE). df %&gt;% distinct(id, day, .keep_all = TRUE) ## # A tibble: 9 × 3 ## id day x ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 ## 2 1 2 11 ## 3 2 1 12 ## 4 2 2 13 ## 5 3 1 12 ## 6 3 2 13 ## 7 4 1 13 ## 8 5 1 10 ## 9 5 2 11 Another way is to explicitly count duplicates within the key groups. This is useful if you want to decide how to handle duplicates (e.g., keep latest, average, or review manually). df %&gt;% group_by(id, day) %&gt;% mutate( count = row_number(), # Counts rows by group dup = count &gt; 1 # TRUE if there is more than one row per group ) ## # A tibble: 12 × 5 ## # Groups: id, day [9] ## id day x count dup ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; ## 1 1 1 1 1 FALSE ## 2 1 2 11 1 FALSE ## 3 2 1 12 1 FALSE ## 4 2 2 13 1 FALSE ## 5 2 2 14 2 TRUE ## 6 3 1 12 1 FALSE ## 7 3 1 12 2 TRUE ## 8 3 2 13 1 FALSE ## 9 4 1 13 1 FALSE ## 10 5 1 10 1 FALSE ## 11 5 2 11 1 FALSE ## 12 5 1 10 2 TRUE select duplicates This returns only the rows that are duplicated (including both the first and later occurrences). df %&gt;% mutate(dup = duplicated(.) | duplicated(., fromLast = TRUE)) %&gt;% filter(dup==T) ## # A tibble: 4 × 4 ## id day x dup ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 3 1 12 TRUE ## 2 3 1 12 TRUE ## 3 5 1 10 TRUE ## 4 5 1 10 TRUE 1.1.13 Combine data sets Combining datasets is a core step in analysis workflows. Common scenarios include: - merging demographics with outcomes - attaching treatment assignment to longitudinal records - linking subject-level and visit-level datasets Below we demonstrate common join types in dplyr. The key concept is to be explicit about which table is the “left” table and which key variables define a match. prepare data sets data1 &lt;- data.frame(ID = 1:4, X1 = c(&quot;a1&quot;, &quot;a2&quot;,&quot;a3&quot;, &quot;a4&quot;), stringsAsFactors = FALSE) data2 &lt;- data.frame(ID = 2:5, X2 = c(&quot;b1&quot;, &quot;b2&quot;,&quot;b3&quot;, &quot;b4&quot;), stringsAsFactors = FALSE) inner join Keeps only IDs that appear in both datasets. inner_join(data1, data2, by = &quot;ID&quot;) ## ID X1 X2 ## 1 2 a2 b1 ## 2 3 a3 b2 ## 3 4 a4 b3 left join Keeps all rows from data1 and attaches matches from data2. Unmatched rows get missing values for data2 variables. there is not ID 1 in data2, thence, x2 is missing. left_join(data1, data2, by = &quot;ID&quot;) ## ID X1 X2 ## 1 1 a1 &lt;NA&gt; ## 2 2 a2 b1 ## 3 3 a3 b2 ## 4 4 a4 b3 differing key col names When key columns have different names across datasets, you can map them with by = c(\"left_key\"=\"right_key\"). left_join(data1, data2, by = c(&quot;ID&quot;=&quot;ID&quot;)) ## ID X1 X2 ## 1 1 a1 &lt;NA&gt; ## 2 2 a2 b1 ## 3 3 a3 b2 ## 4 4 a4 b3 One-to-many relationship has the same statement When the right table has multiple rows per key, the join will expand rows in the left table accordingly. This is expected but should be checked carefully in real analyses. left_join(data1, data2, by = c(&quot;ID&quot; )) ## ID X1 X2 ## 1 1 a1 &lt;NA&gt; ## 2 2 a2 b1 ## 3 3 a3 b2 ## 4 4 a4 b3 multiple data frames You can chain joins to attach multiple datasets step by step. In practice, verify keys and row counts after each join. data1 %&gt;% left_join(data2, by = &quot;ID&quot;) %&gt;% left_join(data2, by = &quot;ID&quot;) ## ID X1 X2.x X2.y ## 1 1 a1 &lt;NA&gt; &lt;NA&gt; ## 2 2 a2 b1 b1 ## 3 3 a3 b2 b2 ## 4 4 a4 b3 b3 multiple key values In many clinical datasets, joins use multiple keys such as subject ID and visit number. # demographics %&gt;% # left_join(ultra, by = c(&quot;id&quot;, &quot;visit&quot;)) right join Keeps all rows from the right table. This is less common in practice, because most workflows start from a primary “analysis dataset” on the left. right_join(data1, data2, by = &quot;ID&quot;) ## ID X1 X2 ## 1 2 a2 b1 ## 2 3 a3 b2 ## 3 4 a4 b3 ## 4 5 &lt;NA&gt; b4 full join Keeps all rows from both tables. This is useful for reconciliation or when building a master key list. full_join(data1, data2, by = &quot;ID&quot;) ## ID X1 X2 ## 1 1 a1 &lt;NA&gt; ## 2 2 a2 b1 ## 3 3 a3 b2 ## 4 4 a4 b3 ## 5 5 &lt;NA&gt; b4 keep cases of left data table without in right data table anti_join() is useful for checking which keys did not match, which is a common QC step after merges. anti_join(data1, data2, by = &quot;ID&quot;) ## ID X1 ## 1 1 a1 keep cases of left data table in right data table semi_join() keeps rows from the left table that have a match in the right table, without bringing in right-table columns. semi_join(data1, data2, by = &quot;ID&quot;) ## ID X1 ## 1 2 a2 ## 2 3 a3 ## 3 4 a4 multiple full join This demonstrates that joins can be chained. In practice, consider whether repeated joins are intended and check for row multiplication. full_join(data1, data2, by = &quot;ID&quot;) %&gt;% full_join(., data2, by = &quot;ID&quot;) ## ID X1 X2.x X2.y ## 1 1 a1 &lt;NA&gt; &lt;NA&gt; ## 2 2 a2 b1 b1 ## 3 3 a3 b2 b2 ## 4 4 a4 b3 b3 ## 5 5 &lt;NA&gt; b4 b4 append two data tables by using join and merge Base R merge() is still commonly seen in legacy code. all = TRUE corresponds to a full join. data_frame1 &lt;- data.frame(col1 = c(6:8), col2 = letters[1:3], col3 = c(1,4,NA)) data_frame2 &lt;- data.frame(col1 = c(5:6), col5 = letters[7:8]) data_frame_merge &lt;- merge(data_frame1, data_frame2, by = &#39;col1&#39;, all = TRUE) print (data_frame_merge) ## col1 col2 col3 col5 ## 1 5 &lt;NA&gt; NA g ## 2 6 a 1 h ## 3 7 b 4 &lt;NA&gt; ## 4 8 c NA &lt;NA&gt; This is the tidyverse equivalent of the merge above. full_join(data_frame1,data_frame2, by=c(&quot;col1&quot;),) ## col1 col2 col3 col5 ## 1 6 a 1 h ## 2 7 b 4 &lt;NA&gt; ## 3 8 c NA &lt;NA&gt; ## 4 5 &lt;NA&gt; NA g adding rows using bind_ function Row-binding is used when two datasets have the same meaning (same variable definitions) and you want to stack them. In practice, this is common when appending data from multiple sites, multiple batches, or multiple extracts. rbind doesn’t work Base R rbind() requires the same column names and order. bind_ function conbines the data framesbased on column names; having our columns in a different order isn’t a problem. df1 &lt;- data.frame(col1 = LETTERS[1:6], col2a = c(5:10), col3a = TRUE) df2 &lt;- data.frame(col1 = LETTERS[4:8], col2b= c(4:8), col3b = FALSE) # rbind(df1,df2) df1 %&gt;% bind_rows(df2) ## col1 col2a col3a col2b col3b ## 1 A 5 TRUE NA NA ## 2 B 6 TRUE NA NA ## 3 C 7 TRUE NA NA ## 4 D 8 TRUE NA NA ## 5 E 9 TRUE NA NA ## 6 F 10 TRUE NA NA ## 7 D NA NA 4 FALSE ## 8 E NA NA 5 FALSE ## 9 F NA NA 6 FALSE ## 10 G NA NA 7 FALSE ## 11 H NA NA 8 FALSE rename the differing cols Before binding rows, make sure variable names represent the same concept. Here we align names so that columns stack correctly. df1 %&gt;% bind_rows(df2 %&gt;% rename(col2a=col2b, col3a=col3b) ) ## col1 col2a col3a ## 1 A 5 TRUE ## 2 B 6 TRUE ## 3 C 7 TRUE ## 4 D 8 TRUE ## 5 E 9 TRUE ## 6 F 10 TRUE ## 7 D 4 FALSE ## 8 E 5 FALSE ## 9 F 6 FALSE ## 10 G 7 FALSE ## 11 H 8 FALSE combining more than 2 data frames You can bind multiple datasets at once. In practice, you may bind a list of datasets after validating that they share the same structure. df1 %&gt;% bind_rows(df2,df2) ## col1 col2a col3a col2b col3b ## 1 A 5 TRUE NA NA ## 2 B 6 TRUE NA NA ## 3 C 7 TRUE NA NA ## 4 D 8 TRUE NA NA ## 5 E 9 TRUE NA NA ## 6 F 10 TRUE NA NA ## 7 D NA NA 4 FALSE ## 8 E NA NA 5 FALSE ## 9 F NA NA 6 FALSE ## 10 G NA NA 7 FALSE ## 11 H NA NA 8 FALSE ## 12 D NA NA 4 FALSE ## 13 E NA NA 5 FALSE ## 14 F NA NA 6 FALSE ## 15 G NA NA 7 FALSE ## 16 H NA NA 8 FALSE 1.1.14 Working with character strings Character string cleaning is common in real datasets: names, sites, lab test labels, free-text fields, and coded fields may contain inconsistent capitalization, extra spaces, or punctuation. The stringr package provides consistent and vectorized string operations, which fit naturally in dplyr pipelines. library(stringr) library(readxl) Look at the values A good first step is to inspect the raw values and identify patterns (e.g., trailing commas, mixed case, inconsistent coding). ehr &lt;- read_excel(&quot;C:\\\\Users\\\\hed2\\\\Downloads\\\\others\\\\mybook2\\\\mybook2/excel.xlsx&quot;) ehr %&gt;% arrange(sex) %&gt;% pull(sex) ## [1] &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; Coerce to lowercase Standardizing case helps reduce duplication (e.g., \"Male\" vs \"MALE\"). ehr %&gt;% arrange(sex) %&gt;% pull(sex) %&gt;% str_to_lower() ## [1] &quot;female&quot; &quot;female&quot; &quot;male&quot; &quot;male&quot; Coerce to upper case Uppercase is sometimes used for code lists or standardized labels. ehr %&gt;% arrange(sex) %&gt;% pull(sex) %&gt;% str_to_upper() ## [1] &quot;FEMALE&quot; &quot;FEMALE&quot; &quot;MALE&quot; &quot;MALE&quot; Title case Title case improves readability for display fields such as names. ehr %&gt;% arrange(sex) %&gt;% pull(sex) %&gt;% str_to_title() ## [1] &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; Sentence case Sentence case is useful for free text or display labels. ehr %&gt;% arrange(sex) %&gt;% pull(sex) %&gt;% str_to_sentence() ## [1] &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; Trim white space from the beginning and end Extra spaces are very common and can break joins or comparisons if not removed. str_trim(&quot;Ryan Edwards &quot;) ## [1] &quot;Ryan Edwards&quot; Remove the comma This shows a simple string replacement. str_replace( string = &quot;weston fox,&quot;, pattern = &quot;,&quot;, replacement = &quot;&quot; ) ## [1] &quot;weston fox&quot; Applying the replacement to a dataset column is a typical cleaning step. ehr %&gt;% mutate(sex = str_replace(sex, &quot;,&quot;, &quot;&quot;)) ## # A tibble: 4 × 4 ## ID sex ht_in wgt_lbs ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 001 Male 71 190 ## 2 002 Male 69 176 ## 3 003 Female 64 130 ## 4 004 Female 65 154 Separate values into component parts Regular expressions allow flexible extraction of patterns within strings. This is powerful for cleaning IDs, parsing codes, or extracting components from composite variables. regular express, ^\\\\w+ look for one or more consecutive word characters at the start of the character string and extract them. str_extract(&quot;zariah hernandez&quot;, &quot;^\\\\w+&quot;) ## [1] &quot;zariah&quot; str_extract(&quot;zariah hernandez&quot;, &quot;\\\\w+$&quot;) ## [1] &quot;hernandez&quot; str_match() returns captured groups based on parentheses in the pattern. This is useful when you need to parse structured expressions. stringr::str_match(&quot;mean(weight_3)&quot;, &quot;(\\\\()(\\\\w+)(\\\\_)(\\\\d+)(\\\\))&quot;) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] &quot;(weight_3)&quot; &quot;(&quot; &quot;weight&quot; &quot;_&quot; &quot;3&quot; &quot;)&quot; Detect a special string to category str_detect() returns TRUE/FALSE based on whether the pattern is present. This is commonly used to create indicator variables. index a special string ehr %&gt;% mutate( man = str_detect(sex, &quot;F&quot;), woman = str_detect(sex, &quot;M&quot;), all = str_detect(sex, &quot;ale&quot;) ) %&gt;% mutate(man_dummy = as.numeric(man)) ## # A tibble: 4 × 8 ## ID sex ht_in wgt_lbs man woman all man_dummy ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 001 Male 71 190 FALSE TRUE TRUE 0 ## 2 002 Male 69 176 FALSE TRUE TRUE 0 ## 3 003 Female 64 130 TRUE FALSE TRUE 1 ## 4 004 Female 65 154 TRUE FALSE TRUE 1 This is a similar example using ifelse() to create a numeric dummy variable. In practice, if_else() is preferred because it is stricter about types, but both patterns are frequently seen. ehr %&gt;% mutate( dummy = ifelse(str_detect(sex, &quot;F&quot;),0,1) ) ## # A tibble: 4 × 5 ## ID sex ht_in wgt_lbs dummy ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 001 Male 71 190 1 ## 2 002 Male 69 176 1 ## 3 003 Female 64 130 0 ## 4 004 Female 65 154 0 1.1.15 Conditional operations Conditional logic is used to define categories, flags, and derived endpoints. In clinical and observational data, this is often required for defining analysis populations, endpoint derivations, or clinical classifications. Testing multiple conditions simultaneously Here we construct a toy blood pressure dataset and classify observations based on systolic and diastolic thresholds. blood_pressure &lt;- tibble( id = 1:10, sysbp = c(152, 120, 119, 123, 135, 83, 191, 147, 209, 166), diasbp = c(78, 60, 88, 76, 85, 54, 116, 95, 100, 106) ) %&gt;% print() ## # A tibble: 10 × 3 ## id sysbp diasbp ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 152 78 ## 2 2 120 60 ## 3 3 119 88 ## 4 4 123 76 ## 5 5 135 85 ## 6 6 83 54 ## 7 7 191 116 ## 8 8 147 95 ## 9 9 209 100 ## 10 10 166 106 if_else() is a tidyverse version of ifelse() and is often preferred for clearer type handling. blood_pressure %&gt;% mutate(bp = if_else(sysbp &lt; 120 &amp; diasbp &lt; 80, &quot;Normal&quot;, &quot;Not Normal&quot;)) ## # A tibble: 10 × 4 ## id sysbp diasbp bp ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 152 78 Not Normal ## 2 2 120 60 Not Normal ## 3 3 119 88 Not Normal ## 4 4 123 76 Not Normal ## 5 5 135 85 Not Normal ## 6 6 83 54 Normal ## 7 7 191 116 Not Normal ## 8 8 147 95 Not Normal ## 9 9 209 100 Not Normal ## 10 10 166 106 Not Normal Using case_when function case_when() is the standard approach for multiple mutually exclusive conditions. It improves readability compared with nested if_else() statements. blood_pressure %&gt;% mutate( bp = case_when( sysbp &lt; 120 &amp; diasbp &lt; 80 ~ &quot;Normal&quot;, sysbp &gt;= 120 &amp; sysbp &lt; 130 &amp; diasbp &lt; 80 ~ &quot;Elevated&quot;, sysbp &gt;= 130 &amp; sysbp &lt; 140 | diasbp &gt;= 80 &amp; diasbp &lt; 90 ~ &quot;Hypertension Stage 1&quot;, sysbp &gt;= 140 | diasbp &gt;= 90 ~ &quot;Hypertension Stage 2&quot; ) ) ## # A tibble: 10 × 4 ## id sysbp diasbp bp ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 152 78 Hypertension Stage 2 ## 2 2 120 60 Elevated ## 3 3 119 88 Hypertension Stage 1 ## 4 4 123 76 Elevated ## 5 5 135 85 Hypertension Stage 1 ## 6 6 83 54 Normal ## 7 7 191 116 Hypertension Stage 2 ## 8 8 147 95 Hypertension Stage 2 ## 9 9 209 100 Hypertension Stage 2 ## 10 10 166 106 Hypertension Stage 2 Recoding variables Sometimes you want a numeric coding for modeling, and a labeled factor for interpretability in tables. Here we create both in one step. blood_pressure %&gt;% mutate( bp= case_when( sysbp &lt; 120 &amp; diasbp &lt; 80 ~ 1, sysbp &gt;= 120 &amp; sysbp &lt; 130 &amp; diasbp &lt; 80 ~ 2, sysbp &gt;= 130 &amp; sysbp &lt; 140 | diasbp &gt;= 80 &amp; diasbp &lt; 90 ~ 3, sysbp &gt;= 140 | diasbp &gt;= 90 ~ 4 ) , bp_f = factor( bp, labels = c( &quot;normal&quot;, &quot;elevated&quot;, &quot;stage 1&quot;, &quot;stage 2&quot;) ) ) ## # A tibble: 10 × 5 ## id sysbp diasbp bp bp_f ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 152 78 4 stage 2 ## 2 2 120 60 2 elevated ## 3 3 119 88 3 stage 1 ## 4 4 123 76 2 elevated ## 5 5 135 85 3 stage 1 ## 6 6 83 54 1 normal ## 7 7 191 116 4 stage 2 ## 8 8 147 95 4 stage 2 ## 9 9 209 100 4 stage 2 ## 10 10 166 106 4 stage 2 Recoding missing Missing values are common in real datasets. A typical step is to recode special numeric values (e.g., 7, 9, 99) to missing. using NA_real instead of NA. NA_real_ ensures the result stays numeric (double). This avoids unintended type conversion. demographics &lt;- tibble( race = c(1, 2, 1, 4, 7, 1, 2, 9, 1, 3), hispanic = c(7, 0, 1, 0, 1, 0, 1, 9, 0, 1) ) demographics %&gt;% mutate( # Recode 7 and 9 to missing race_recode = if_else(race == 7 | race == 9, NA_real_, race), hispanic_recode = if_else(hispanic == 7 | hispanic == 9, NA_real_, hispanic) ) ## # A tibble: 10 × 4 ## race hispanic race_recode hispanic_recode ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 7 1 NA ## 2 2 0 2 0 ## 3 1 1 1 1 ## 4 4 0 4 0 ## 5 7 1 NA 1 ## 6 1 0 1 0 ## 7 2 1 2 1 ## 8 9 9 NA NA ## 9 1 0 1 0 ## 10 3 1 3 1 check whether or not Missing values can propagate through logical checks. This pattern shows one way to replace missing logical values with FALSE. blood_pressure %&gt;% mutate( match = sysbp == sysbp, n_match = if_else(is.na(match), FALSE, match) ) ## # A tibble: 10 × 5 ## id sysbp diasbp match n_match ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1 152 78 TRUE TRUE ## 2 2 120 60 TRUE TRUE ## 3 3 119 88 TRUE TRUE ## 4 4 123 76 TRUE TRUE ## 5 5 135 85 TRUE TRUE ## 6 6 83 54 TRUE TRUE ## 7 7 191 116 TRUE TRUE ## 8 8 147 95 TRUE TRUE ## 9 9 209 100 TRUE TRUE ## 10 10 166 106 TRUE TRUE 1.2 How to do aggregation/ summarization Aggregation is used to produce summary statistics for reporting and exploratory analysis. Typical examples include baseline summaries by treatment, counts by category, and descriptive statistics by subgroup. 1.2.1 Summarization after grouping We use group_by() to define groups, then summarise() (or summarize()) to compute summary statistics within each group. library(tidyverse) This chunk is hidden but ensures the package is available for the following code blocks in some knitted settings. This example calculates the mean Sepal.Length for each species, then sorts from highest to lowest. iris %&gt;% group_by(Species) %&gt;% summarize(Support = mean(Sepal.Length)) %&gt;% # average arrange(-Support) # sort ## # A tibble: 3 × 2 ## Species Support ## &lt;fct&gt; &lt;dbl&gt; ## 1 virginica 6.59 ## 2 versicolor 5.94 ## 3 setosa 5.01 This example shows how to compute multiple summary statistics at once. Creating intermediate metrics (like diff) is useful for quick comparisons across groups. iris %&gt;% group_by(Species) %&gt;% summarize(mean_s = mean(Sepal.Width), meas_p = mean(Petal.Length), diff = mean(Sepal.Width-Petal.Length)) %&gt;% arrange(-diff) ## # A tibble: 3 × 4 ## Species mean_s meas_p diff ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 3.43 1.46 1.97 ## 2 versicolor 2.77 4.26 -1.49 ## 3 virginica 2.97 5.55 -2.58 This example adds sample size (n) and standard deviation (sd). These are standard descriptive summaries for continuous variables. iris %&gt;% group_by(Species) %&gt;% summarize(n = n(), meas_p = mean(Petal.Length), sd = sd(Petal.Length)) ## # A tibble: 3 × 4 ## Species n meas_p sd ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 50 1.46 0.174 ## 2 versicolor 50 4.26 0.470 ## 3 virginica 50 5.55 0.552 1.2.2 Summarization with upgroup If you have grouped data but want overall summaries across all rows, you can remove the grouping structure using ungroup(). iris %&gt;% ungroup( ) %&gt;% summarize(n = n(), meas_p = mean(Petal.Length), sd = sd(Petal.Length)) ## n meas_p sd ## 1 150 3.758 1.765298 1.2.3 Mutate new variables after grouping mutate() after group_by() adds group-level summaries back to each row. This is useful when you want both row-level values and group-level context in the same dataset (e.g., for plotting or QC checks). iris %&gt;% group_by(Species) %&gt;% mutate(n = n(), meas_p = mean(Petal.Length), sd = sd(Petal.Length)) ## # A tibble: 150 × 8 ## # Groups: Species [3] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species n meas_p sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 setosa 50 1.46 0.174 ## 2 4.9 3 1.4 0.2 setosa 50 1.46 0.174 ## 3 4.7 3.2 1.3 0.2 setosa 50 1.46 0.174 ## 4 4.6 3.1 1.5 0.2 setosa 50 1.46 0.174 ## 5 5 3.6 1.4 0.2 setosa 50 1.46 0.174 ## 6 5.4 3.9 1.7 0.4 setosa 50 1.46 0.174 ## 7 4.6 3.4 1.4 0.3 setosa 50 1.46 0.174 ## 8 5 3.4 1.5 0.2 setosa 50 1.46 0.174 ## 9 4.4 2.9 1.4 0.2 setosa 50 1.46 0.174 ## 10 4.9 3.1 1.5 0.1 setosa 50 1.46 0.174 ## # ℹ 140 more rows This example demonstrates: - computing group-level summaries with missing handling (na.rm = T), and - then summarizing again to produce one row per group. In practice, this pattern appears in QC workflows and layered summary derivations. iris %&gt;% group_by(Species) %&gt;% mutate(n = n(), meas_p = mean(Petal.Length, na.rm = T), sd = sd(Petal.Length)) %&gt;% summarize (n_mean = paste (&quot;sample size:&quot;,mean(n)), meas_p = mean(Petal.Length), sd = sd(Petal.Length)) ## # A tibble: 3 × 4 ## Species n_mean meas_p sd ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa sample size: 50 1.46 0.174 ## 2 versicolor sample size: 50 4.26 0.470 ## 3 virginica sample size: 50 5.55 0.552 1.2.4 Recode and generate new variables, then value label This is a common pattern: create a derived variable with conditional logic, then attach labels for interpretability. Here, we set one category to missing and then relabel remaining categories using a factor. irisifelse &lt;- iris %&gt;% mutate(Species2 = ifelse(Species == &quot;setosa&quot;, NA, Species)) # relabel values irisifelse$Species2 &lt;- factor(irisifelse$Species2,labels = c( &quot;versi&quot;,&quot;virg&quot;)) irisifelse ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Species2 ## 1 5.1 3.5 1.4 0.2 setosa &lt;NA&gt; ## 2 4.9 3.0 1.4 0.2 setosa &lt;NA&gt; ## 3 4.7 3.2 1.3 0.2 setosa &lt;NA&gt; ## 4 4.6 3.1 1.5 0.2 setosa &lt;NA&gt; ## 5 5.0 3.6 1.4 0.2 setosa &lt;NA&gt; ## 6 5.4 3.9 1.7 0.4 setosa &lt;NA&gt; ## 7 4.6 3.4 1.4 0.3 setosa &lt;NA&gt; ## 8 5.0 3.4 1.5 0.2 setosa &lt;NA&gt; ## 9 4.4 2.9 1.4 0.2 setosa &lt;NA&gt; ## 10 4.9 3.1 1.5 0.1 setosa &lt;NA&gt; ## 11 5.4 3.7 1.5 0.2 setosa &lt;NA&gt; ## 12 4.8 3.4 1.6 0.2 setosa &lt;NA&gt; ## 13 4.8 3.0 1.4 0.1 setosa &lt;NA&gt; ## 14 4.3 3.0 1.1 0.1 setosa &lt;NA&gt; ## 15 5.8 4.0 1.2 0.2 setosa &lt;NA&gt; ## 16 5.7 4.4 1.5 0.4 setosa &lt;NA&gt; ## 17 5.4 3.9 1.3 0.4 setosa &lt;NA&gt; ## 18 5.1 3.5 1.4 0.3 setosa &lt;NA&gt; ## 19 5.7 3.8 1.7 0.3 setosa &lt;NA&gt; ## 20 5.1 3.8 1.5 0.3 setosa &lt;NA&gt; ## 21 5.4 3.4 1.7 0.2 setosa &lt;NA&gt; ## 22 5.1 3.7 1.5 0.4 setosa &lt;NA&gt; ## 23 4.6 3.6 1.0 0.2 setosa &lt;NA&gt; ## 24 5.1 3.3 1.7 0.5 setosa &lt;NA&gt; ## 25 4.8 3.4 1.9 0.2 setosa &lt;NA&gt; ## 26 5.0 3.0 1.6 0.2 setosa &lt;NA&gt; ## 27 5.0 3.4 1.6 0.4 setosa &lt;NA&gt; ## 28 5.2 3.5 1.5 0.2 setosa &lt;NA&gt; ## 29 5.2 3.4 1.4 0.2 setosa &lt;NA&gt; ## 30 4.7 3.2 1.6 0.2 setosa &lt;NA&gt; ## 31 4.8 3.1 1.6 0.2 setosa &lt;NA&gt; ## 32 5.4 3.4 1.5 0.4 setosa &lt;NA&gt; ## 33 5.2 4.1 1.5 0.1 setosa &lt;NA&gt; ## 34 5.5 4.2 1.4 0.2 setosa &lt;NA&gt; ## 35 4.9 3.1 1.5 0.2 setosa &lt;NA&gt; ## 36 5.0 3.2 1.2 0.2 setosa &lt;NA&gt; ## 37 5.5 3.5 1.3 0.2 setosa &lt;NA&gt; ## 38 4.9 3.6 1.4 0.1 setosa &lt;NA&gt; ## 39 4.4 3.0 1.3 0.2 setosa &lt;NA&gt; ## 40 5.1 3.4 1.5 0.2 setosa &lt;NA&gt; ## 41 5.0 3.5 1.3 0.3 setosa &lt;NA&gt; ## 42 4.5 2.3 1.3 0.3 setosa &lt;NA&gt; ## 43 4.4 3.2 1.3 0.2 setosa &lt;NA&gt; ## 44 5.0 3.5 1.6 0.6 setosa &lt;NA&gt; ## 45 5.1 3.8 1.9 0.4 setosa &lt;NA&gt; ## 46 4.8 3.0 1.4 0.3 setosa &lt;NA&gt; ## 47 5.1 3.8 1.6 0.2 setosa &lt;NA&gt; ## 48 4.6 3.2 1.4 0.2 setosa &lt;NA&gt; ## 49 5.3 3.7 1.5 0.2 setosa &lt;NA&gt; ## 50 5.0 3.3 1.4 0.2 setosa &lt;NA&gt; ## 51 7.0 3.2 4.7 1.4 versicolor versi ## 52 6.4 3.2 4.5 1.5 versicolor versi ## 53 6.9 3.1 4.9 1.5 versicolor versi ## 54 5.5 2.3 4.0 1.3 versicolor versi ## 55 6.5 2.8 4.6 1.5 versicolor versi ## 56 5.7 2.8 4.5 1.3 versicolor versi ## 57 6.3 3.3 4.7 1.6 versicolor versi ## 58 4.9 2.4 3.3 1.0 versicolor versi ## 59 6.6 2.9 4.6 1.3 versicolor versi ## 60 5.2 2.7 3.9 1.4 versicolor versi ## 61 5.0 2.0 3.5 1.0 versicolor versi ## 62 5.9 3.0 4.2 1.5 versicolor versi ## 63 6.0 2.2 4.0 1.0 versicolor versi ## 64 6.1 2.9 4.7 1.4 versicolor versi ## 65 5.6 2.9 3.6 1.3 versicolor versi ## 66 6.7 3.1 4.4 1.4 versicolor versi ## 67 5.6 3.0 4.5 1.5 versicolor versi ## 68 5.8 2.7 4.1 1.0 versicolor versi ## 69 6.2 2.2 4.5 1.5 versicolor versi ## 70 5.6 2.5 3.9 1.1 versicolor versi ## 71 5.9 3.2 4.8 1.8 versicolor versi ## 72 6.1 2.8 4.0 1.3 versicolor versi ## 73 6.3 2.5 4.9 1.5 versicolor versi ## 74 6.1 2.8 4.7 1.2 versicolor versi ## 75 6.4 2.9 4.3 1.3 versicolor versi ## 76 6.6 3.0 4.4 1.4 versicolor versi ## 77 6.8 2.8 4.8 1.4 versicolor versi ## 78 6.7 3.0 5.0 1.7 versicolor versi ## 79 6.0 2.9 4.5 1.5 versicolor versi ## 80 5.7 2.6 3.5 1.0 versicolor versi ## 81 5.5 2.4 3.8 1.1 versicolor versi ## 82 5.5 2.4 3.7 1.0 versicolor versi ## 83 5.8 2.7 3.9 1.2 versicolor versi ## 84 6.0 2.7 5.1 1.6 versicolor versi ## 85 5.4 3.0 4.5 1.5 versicolor versi ## 86 6.0 3.4 4.5 1.6 versicolor versi ## 87 6.7 3.1 4.7 1.5 versicolor versi ## 88 6.3 2.3 4.4 1.3 versicolor versi ## 89 5.6 3.0 4.1 1.3 versicolor versi ## 90 5.5 2.5 4.0 1.3 versicolor versi ## 91 5.5 2.6 4.4 1.2 versicolor versi ## 92 6.1 3.0 4.6 1.4 versicolor versi ## 93 5.8 2.6 4.0 1.2 versicolor versi ## 94 5.0 2.3 3.3 1.0 versicolor versi ## 95 5.6 2.7 4.2 1.3 versicolor versi ## 96 5.7 3.0 4.2 1.2 versicolor versi ## 97 5.7 2.9 4.2 1.3 versicolor versi ## 98 6.2 2.9 4.3 1.3 versicolor versi ## 99 5.1 2.5 3.0 1.1 versicolor versi ## 100 5.7 2.8 4.1 1.3 versicolor versi ## 101 6.3 3.3 6.0 2.5 virginica virg ## 102 5.8 2.7 5.1 1.9 virginica virg ## 103 7.1 3.0 5.9 2.1 virginica virg ## 104 6.3 2.9 5.6 1.8 virginica virg ## 105 6.5 3.0 5.8 2.2 virginica virg ## 106 7.6 3.0 6.6 2.1 virginica virg ## 107 4.9 2.5 4.5 1.7 virginica virg ## 108 7.3 2.9 6.3 1.8 virginica virg ## 109 6.7 2.5 5.8 1.8 virginica virg ## 110 7.2 3.6 6.1 2.5 virginica virg ## 111 6.5 3.2 5.1 2.0 virginica virg ## 112 6.4 2.7 5.3 1.9 virginica virg ## 113 6.8 3.0 5.5 2.1 virginica virg ## 114 5.7 2.5 5.0 2.0 virginica virg ## 115 5.8 2.8 5.1 2.4 virginica virg ## 116 6.4 3.2 5.3 2.3 virginica virg ## 117 6.5 3.0 5.5 1.8 virginica virg ## 118 7.7 3.8 6.7 2.2 virginica virg ## 119 7.7 2.6 6.9 2.3 virginica virg ## 120 6.0 2.2 5.0 1.5 virginica virg ## 121 6.9 3.2 5.7 2.3 virginica virg ## 122 5.6 2.8 4.9 2.0 virginica virg ## 123 7.7 2.8 6.7 2.0 virginica virg ## 124 6.3 2.7 4.9 1.8 virginica virg ## 125 6.7 3.3 5.7 2.1 virginica virg ## 126 7.2 3.2 6.0 1.8 virginica virg ## 127 6.2 2.8 4.8 1.8 virginica virg ## 128 6.1 3.0 4.9 1.8 virginica virg ## 129 6.4 2.8 5.6 2.1 virginica virg ## 130 7.2 3.0 5.8 1.6 virginica virg ## 131 7.4 2.8 6.1 1.9 virginica virg ## 132 7.9 3.8 6.4 2.0 virginica virg ## 133 6.4 2.8 5.6 2.2 virginica virg ## 134 6.3 2.8 5.1 1.5 virginica virg ## 135 6.1 2.6 5.6 1.4 virginica virg ## 136 7.7 3.0 6.1 2.3 virginica virg ## 137 6.3 3.4 5.6 2.4 virginica virg ## 138 6.4 3.1 5.5 1.8 virginica virg ## 139 6.0 3.0 4.8 1.8 virginica virg ## 140 6.9 3.1 5.4 2.1 virginica virg ## 141 6.7 3.1 5.6 2.4 virginica virg ## 142 6.9 3.1 5.1 2.3 virginica virg ## 143 5.8 2.7 5.1 1.9 virginica virg ## 144 6.8 3.2 5.9 2.3 virginica virg ## 145 6.7 3.3 5.7 2.5 virginica virg ## 146 6.7 3.0 5.2 2.3 virginica virg ## 147 6.3 2.5 5.0 1.9 virginica virg ## 148 6.5 3.0 5.2 2.0 virginica virg ## 149 6.2 3.4 5.4 2.3 virginica virg ## 150 5.9 3.0 5.1 1.8 virginica virg Always check structure after transformations, especially when factors and missing values are involved. str(irisifelse) ## &#39;data.frame&#39;: 150 obs. of 6 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Species2 : Factor w/ 2 levels &quot;versi&quot;,&quot;virg&quot;: NA NA NA NA NA NA NA NA NA NA ... 1.3 How to creat table 1 with test In clinical trial reporting, “Table 1” typically refers to baseline characteristics (e.g., age, sex, race, key labs) summarized by treatment group. This often includes statistical tests (or standardized differences) to describe balance between groups. The link below provides a full example workflow. see here 1.4 Imputing Missing Data with MICE Missing data are common in clinical and observational studies. Multiple imputation (e.g., using MICE) is a practical approach under missing-at-random assumptions, and it allows uncertainty due to missingness to be propagated into final inference. The link below provides an example implementation. see here "],["machine-learning.html", "2 Machine learning", " 2 Machine learning Machine learning (ML) is a collection of methods that learn patterns from data and use those patterns to make predictions or classifications. In practice, ML is most useful when (1) the relationship between predictors and outcome is complex, (2) there are many predictors and potential interactions, or (3) prediction performance is the primary goal. This chapter focuses on a practical, end-to-end workflow using the caret ecosystem. The emphasis is on reproducible steps that appear repeatedly in real work: 1) load data and inspect structure, 2) split data into training and test sets, 3) perform preprocessing (imputation, encoding, normalization), 4) explore features visually, 5) train models and tune hyperparameters, 6) evaluate performance using a confusion matrix and cross-validation, and 7) compare/ensemble multiple models. We will use the Pima Indians Diabetes dataset (PimaIndiansDiabetes) as a standard binary classification example. The outcome variable is diabetes, and the predictors are clinical measurements. The goal is to predict whether a subject has diabetes. For more details, please read here. –&gt; "],["deep-learning.html", "3 Deep learning", " 3 Deep learning Deep learning is a subset of machine learning that uses neural networks with multiple layers (“deep” architectures). Compared with many classical ML models, deep learning can capture complex nonlinear relationships and can be especially powerful for unstructured data such as images and text. In practice, the deep learning workflow still follows the same discipline as standard ML: define the prediction target and the feature set, split data into training and test sets, preprocess features (scaling, encoding, reshaping), build and compile a model, train the model and monitor overfitting, evaluate on the test set, and review predictions and performance metrics. In this chapter, we demonstrate deep neural networks for: - binary classification (Pima Indians Diabetes), - regression (Boston housing), - image classification using a convolutional neural network (CNN) (MNIST). We use the keras package in R, which provides a user-friendly interface to define and train neural networks. For more details, please read here. "],["data-visualization.html", "4 Data visualization 4.1 Data visualization introduction 4.2 Scatter plot 4.3 Bar chart 4.4 Line charts 4.5 ggplot2 parameters 4.6 How to create advanced plots", " 4 Data visualization Data visualization is not only about making plots look “pretty.” In real projects, visualization has three practical jobs: Understand the data: distributions, outliers, missingness, and relationships. Communicate findings: help others quickly grasp patterns and results. Validate assumptions: especially before modeling (linearity, normality, separation, influential points, etc.). This chapter focuses on ggplot2 (in the tidyverse ecosystem) and a few practical companion tools. The goal is to build a workflow that is repeatable: summarize → reshape → visualize → refine → annotate → communicate. 4.1 Data visualization introduction Before plotting, it is often useful to summarize, reshape, and clean your data. Many plotting tasks become straightforward once the dataset is in a tidy (“long”) format. The section below shows common operations that prepare your data for visualization. 4.1.1 Summarization A typical first step is to compute group-level summaries. In practice, these summaries help you: - sanity-check units and ranges, - compare groups quickly, - decide which plots are appropriate (boxplots, line charts, etc.). The following example converts miles-per-gallon (mpg) into a rough metric unit and summarizes by cylinder group. library(tidyverse) library(dplyr) mtcars %&gt;% mutate( kml = mpg * 0.42) %&gt;% group_by(cyl) %&gt;% summarise(avg_US = mean(mpg), avg_metric = mean(kml)) ## # A tibble: 3 × 3 ## cyl avg_US avg_metric ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 26.7 11.2 ## 2 6 19.7 8.29 ## 3 8 15.1 6.34 The next summary computes average city and highway mileage by manufacturer and year. In reporting, this kind of grouped summary often becomes the basis for trend plots and comparisons. mpg %&gt;% group_by(manufacturer, year) %&gt;% summarise_at(vars(cty, hwy), mean) ## # A tibble: 30 × 4 ## # Groups: manufacturer [15] ## manufacturer year cty hwy ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi 1999 17.1 26.1 ## 2 audi 2008 18.1 26.8 ## 3 chevrolet 1999 15.1 21.6 ## 4 chevrolet 2008 14.9 22.1 ## 5 dodge 1999 13.4 18.4 ## 6 dodge 2008 13.0 17.6 ## 7 ford 1999 13.9 18.6 ## 8 ford 2008 14.1 20.5 ## 9 honda 1999 24.8 31.6 ## 10 honda 2008 24 33.8 ## # ℹ 20 more rows change layout Sometimes you want a table for quick comparison rather than a plot. Here we count cars by class and year, then spread into a wide layout (years as rows, classes as columns). This is helpful for producing “Table 1 style” counts or for feeding into heatmaps. mpg %&gt;% count(class, year)%&gt;% spread(class, n) ## # A tibble: 2 × 8 ## year `2seater` compact midsize minivan pickup subcompact suv ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1999 2 25 20 6 16 19 29 ## 2 2008 3 22 21 5 17 16 33 change all characters into factors Many plotting functions treat characters as discrete categories anyway, but converting to factor can make your intent explicit and allows you to control ordering and labeling more deliberately. This is especially useful for facets and legends. mpg &lt;- mpg %&gt;% mutate_if(is.character, as.factor) #if a column is a character, change to a factor wide to long data A common visualization trick is to stack multiple measurement columns into a long format. Here we convert cty and hwy into a single numeric column with a key indicating which metric it came from. Long format is the natural input format for ggplot because it plays well with color, linetype, and facets. mpg1 &lt;- mpg %&gt;% gather(&quot;key&quot;, &quot;value&quot;, cty, hwy) convert wide data to long data using pivot_longer pivot_longer() is the modern tidyverse replacement for gather(). This example: pivots cty and hwy into long form, recodes the indicator variable into friendly labels, overlays points and smooth curves for each measurement type. This is a classic real-world pattern: put the data in long format, then map the “type” to color. ## Your code here. Naming choices for 1 and 2 are yours dta &lt;- mpg %&gt;% pivot_longer(cty:hwy, names_to = &quot;var&quot;, values_to = &quot;value&quot;) %&gt;% # Both of those are # value label mutate(var = ifelse( var == &#39;cty&#39;, &#39;city&#39;,&#39;highway&#39;)) ggplot(dta, aes(x = displ, y = value)) + geom_point(aes(color = var)) + geom_smooth(aes(color = var), se = F) ## `geom_smooth()` using method = &#39;loess&#39; and formula = ## &#39;y ~ x&#39; explore distribution Exploratory distribution plots help you quickly identify: skewed variables, implausible values, heavy tails, and whether transformations (log, sqrt) might be helpful. Packages like DataExplorer and psych provide rapid EDA utilities. (Here, riskfactors is assumed to exist in your environment.) library(DataExplorer) library(psych) ## ## Attaching package: &#39;psych&#39; ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha library(naniar) ## Warning: package &#39;naniar&#39; was built under R version 4.4.3 plot_histogram(riskfactors) explore relationship/correlation Pairwise plots are very useful early in analysis because they reveal: correlations, nonlinear patterns, clustering, and outliers that may dominate a model. These are especially valuable before regression or machine learning. library(psych) pairs.panels(riskfactors[,1:10]) ## Warning in cor(x, y, use = &quot;pairwise&quot;, method = method): the standard deviation ## is zero ## Warning in cor(x, y, use = &quot;pairwise&quot;, method = method): the standard deviation ## is zero create a individual theme In a book or internal training material, consistent formatting matters. Creating your own theme function lets you apply the same style across all figures: consistent font sizes, consistent background, consistent axis formatting. This is a practical “quality of life” improvement when you generate many plots. my_theme &lt;- function(){ theme_bw() + theme(axis.title = element_text(size=16), axis.text = element_text(size=14), text = element_text(size = 14)) } 4.1.2 Explore missing values Missingness is not just a data cleaning issue—it can be informative. Before you impute or drop data, you should understand: - which variables have missingness, - whether missingness co-occurs across variables, - and whether missingness is related to groups (e.g., sites, time periods). The following blocks use naniar and related tools to visualize missing patterns. # install.packages(&quot;naniar&quot;) library(naniar) # head(riskfactors) riskfactors &lt;- riskfactors gg_miss_upset() is useful for seeing combinations of missing variables (like an Upset plot). This helps identify patterns such as “these three labs are often missing together.” gg_miss_upset(riskfactors,nsets=10) ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation idioms with `aes()`. ## ℹ See also `vignette(&quot;ggplot2-in-packages&quot;)` for more ## information. ## ℹ The deprecated feature was likely used in the UpSetR ## package. ## Please report the issue to the authors. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see ## where this warning was generated. ## Warning: Using `size` aesthetic for lines was deprecated in ## ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## ℹ The deprecated feature was likely used in the UpSetR ## package. ## Please report the issue to the authors. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see ## where this warning was generated. ## Warning: The `size` argument of `element_line()` is deprecated ## as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## ℹ The deprecated feature was likely used in the UpSetR ## package. ## Please report the issue to the authors. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see ## where this warning was generated. plot_missing() gives a quick overall missingness view, often used at the start of EDA. # install.packages(&quot;DataExplorer&quot;) plot_missing(riskfactors) visdat::vis_dat() is a fast way to inspect column types and missingness at once. It’s particularly helpful to detect: - columns accidentally read as character, - columns with mixed types, - or unexpected missing patterns. # take a quick look at the data types of each column visdat::vis_dat(riskfactors) 4.1.3 Add statistical test Plots often become more persuasive when paired with a simple statistical comparison—but you should add tests carefully: - avoid fishing/over-testing, - ensure assumptions are reasonable, - and interpret p-values in context. Here we build a faceted boxplot and add a comparison between years (1999 vs 2008) for each manufacturer. This is a practical template for “small multiple” visual comparisons with annotated tests. library(ggpubr) plt &lt;- ggplot( data=mpg, mapping= aes(x = as.factor(year), y = cty, color = as.factor(year) ) )+ geom_boxplot() + geom_jitter(width=0.1)+ labs(x = &#39;Year&#39;, y = &quot;City mpg&quot;) + my_theme()+ facet_wrap( ~ manufacturer,nrow = 2) # add statistical test my_comparisons &lt;- list(c(&#39;1999&#39;,&#39;2008&#39;)) plt + stat_compare_means() + stat_compare_means(comparisons = my_comparisons) 4.1.4 Add texts to dots When plotting many points, labels can clutter quickly. But for small datasets (or when identifying key outliers), labels add interpretability. This example labels each state for a scatter plot of urban population vs murder arrests. USArrests &lt;- USArrests %&gt;% rownames_to_column(&#39;State&#39;) ggplot(USArrests, aes( x=UrbanPop,y=Murder))+ geom_point() + labs(x = &quot;Percent of population that is urban&quot;, y = &quot;Murder arrests (per 100,000)&quot;, caption = &quot;McNeil (1997). Interactive Data Analysis&quot;)+ geom_text(aes(label=State),size=3) 4.1.5 Set the legend Legends are often the difference between a confusing figure and a publishable one. Here we demonstrate manual control of: - fill colors, - linetypes, - legend title and labels. In practice, manual scales are useful when you want consistent colors across plots in a report or book. ggplot(iris, aes(x= Sepal.Length , fill= as.factor( Species)) ) + #whole plot&#39;s option geom_histogram(aes(y=..density..),alpha=0.5, position=&quot;identity&quot; , bins = 50)+ geom_density(aes(linetype=as.factor(Species)),alpha=.1 )+ #aesthetic&#39;s option scale_fill_manual( name = &quot;Groups&quot;,values = c(&quot;grey&quot;, &quot;black&quot;, &quot;skyblue&quot;),labels = c(&quot;setosa&quot;, &quot;versicolor&quot; , &quot;virginica&quot; ))+ scale_linetype_manual( name = &quot;Groups&quot; ,values = c(1,3,5),labels = c(&quot;setosa&quot;, &quot;versicolor&quot; , &quot;virginica&quot;) )+ # common legend labs(x = &quot;Sepal.Length&quot;, y = &quot;Density&quot;, title = &quot;&quot;) ## Warning: The dot-dot notation (`..density..`) was deprecated in ## ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see ## where this warning was generated. 4.1.6 Create a panel of plots A panel (multi-plot layout) is a very effective way to summarize multiple variables at once. In exploratory analysis, it helps you quickly see: - distributions (histogram, bar), - group differences (boxplot), - and composition (stacked percent bars). Here we build four plots and arrange them into a 2×2 panel. p1=ggplot(data=riskfactors,aes(x=age))+ geom_histogram(bins = 30 ) p2=ggplot(data=riskfactors,aes(x=sex))+ geom_bar (aes(x=sex) ) p3=ggplot(riskfactors,aes(x = education, y = bmi))+ geom_boxplot ( ) p4=ggplot(riskfactors, aes(x = marital )) + geom_bar(aes(group = education, y = (..count..)/sum(..count..),fill = education)) + scale_y_continuous(labels=scales::percent) # install.packages(&quot;ggpubr&quot;) library(ggpubr) ggarrange(p1, p2, p3, p4, ncol = 2, nrow=2) ## Warning: Removed 11 rows containing non-finite outside the ## scale range (`stat_boxplot()`). 4.1.7 Plots in regression Visualization is a powerful companion to regression: - diagnostic plots check assumptions, - coefficient/OR plots communicate model results, - and fitted relationships can be compared across groups. create linear regression model We fit a linear regression on Boston housing as a demonstration. In practice, you would check residuals, leverage, and outliers; this section mainly sets up the model for coefficient visualization. data(&quot;Boston&quot;, package = &quot;MASS&quot;) linear_reg &lt;- glm(medv ~ ., data=Boston , family = gaussian()) summary(linear_reg) ## ## Call: ## glm(formula = medv ~ ., family = gaussian(), data = Boston) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.646e+01 5.103e+00 7.144 3.28e-12 *** ## crim -1.080e-01 3.286e-02 -3.287 0.001087 ** ## zn 4.642e-02 1.373e-02 3.382 0.000778 *** ## indus 2.056e-02 6.150e-02 0.334 0.738288 ## chas 2.687e+00 8.616e-01 3.118 0.001925 ** ## nox -1.777e+01 3.820e+00 -4.651 4.25e-06 *** ## rm 3.810e+00 4.179e-01 9.116 &lt; 2e-16 *** ## age 6.922e-04 1.321e-02 0.052 0.958229 ## dis -1.476e+00 1.995e-01 -7.398 6.01e-13 *** ## rad 3.060e-01 6.635e-02 4.613 5.07e-06 *** ## tax -1.233e-02 3.760e-03 -3.280 0.001112 ** ## ptratio -9.527e-01 1.308e-01 -7.283 1.31e-12 *** ## black 9.312e-03 2.686e-03 3.467 0.000573 *** ## lstat -5.248e-01 5.072e-02 -10.347 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 22.51785) ## ## Null deviance: 42716 on 505 degrees of freedom ## Residual deviance: 11079 on 492 degrees of freedom ## AIC: 3027.6 ## ## Number of Fisher Scoring iterations: 2 summary broom::tidy() produces a clean summary table that is easy to include in reports. Using knitr::kable() formats it neatly in HTML/PDF output. knitr::kable(broom::tidy(linear_reg)) term estimate std.error statistic p.value (Intercept) 36.4594884 5.1034588 7.1440742 0.0000000 crim -0.1080114 0.0328650 -3.2865169 0.0010868 zn 0.0464205 0.0137275 3.3815763 0.0007781 indus 0.0205586 0.0614957 0.3343100 0.7382881 chas 2.6867338 0.8615798 3.1183809 0.0019250 nox -17.7666112 3.8197437 -4.6512574 0.0000042 rm 3.8098652 0.4179253 9.1161402 0.0000000 age 0.0006922 0.0132098 0.0524024 0.9582293 dis -1.4755668 0.1994547 -7.3980036 0.0000000 rad 0.3060495 0.0663464 4.6128998 0.0000051 tax -0.0123346 0.0037605 -3.2800091 0.0011116 ptratio -0.9527472 0.1308268 -7.2825106 0.0000000 black 0.0093117 0.0026860 3.4667926 0.0005729 lstat -0.5247584 0.0507153 -10.3471458 0.0000000 create logistical regression We fit logistic regression on the diabetes dataset. This is a classic example where coefficient plots and OR plots become very helpful for communication. # load the Pima Indians dataset from the mlbench dataset library(mlbench) ## Warning: package &#39;mlbench&#39; was built under R version 4.4.3 data(PimaIndiansDiabetes) # rename dataset to have shorter name because lazy diabetes &lt;- PimaIndiansDiabetes logistic_reg &lt;- glm(diabetes ~ ., data=diabetes, family = binomial) summary(logistic_reg) ## ## Call: ## glm(formula = diabetes ~ ., family = binomial, data = diabetes) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.4046964 0.7166359 -11.728 &lt; 2e-16 *** ## pregnant 0.1231823 0.0320776 3.840 0.000123 *** ## glucose 0.0351637 0.0037087 9.481 &lt; 2e-16 *** ## pressure -0.0132955 0.0052336 -2.540 0.011072 * ## triceps 0.0006190 0.0068994 0.090 0.928515 ## insulin -0.0011917 0.0009012 -1.322 0.186065 ## mass 0.0897010 0.0150876 5.945 2.76e-09 *** ## pedigree 0.9451797 0.2991475 3.160 0.001580 ** ## age 0.0148690 0.0093348 1.593 0.111192 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 993.48 on 767 degrees of freedom ## Residual deviance: 723.45 on 759 degrees of freedom ## AIC: 741.45 ## ## Number of Fisher Scoring iterations: 5 summary Again, broom::tidy() provides a clear coefficient table. knitr::kable(broom::tidy(logistic_reg)) term estimate std.error statistic p.value (Intercept) -8.4046964 0.7166359 -11.7279870 0.0000000 pregnant 0.1231823 0.0320776 3.8401403 0.0001230 glucose 0.0351637 0.0037087 9.4813935 0.0000000 pressure -0.0132955 0.0052336 -2.5404160 0.0110721 triceps 0.0006190 0.0068994 0.0897131 0.9285152 insulin -0.0011917 0.0009012 -1.3223094 0.1860652 mass 0.0897010 0.0150876 5.9453340 0.0000000 pedigree 0.9451797 0.2991475 3.1595780 0.0015800 age 0.0148690 0.0093348 1.5928584 0.1111920 4.1.7.1 Create forest plots for coefficients or OR Forest plots are one of the most practical ways to present regression results: - For linear regression, coefficients are on the original outcome scale. - For logistic regression, coefficients are usually transformed to odds ratios (OR). sjPlot::plot_model() offers fast, clean coefficient plots. library(sjPlot) ## Warning: package &#39;sjPlot&#39; was built under R version 4.4.3 ## ## Attaching package: &#39;sjPlot&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## set_theme plot_model(linear_reg, show.values = TRUE, value.offset = 0.5) Here we plot logistic regression results similarly. In practice, you may want to ensure the plot displays OR (exp(beta)) depending on the function defaults and your audience. plot_model(logistic_reg, show.values = TRUE, value.offset = .5, vline.color = &quot;black&quot;) another way finalfit provides publication-friendly coefficient/OR plots with tables. This is especially useful for clinical or epidemiology style reporting. library(finalfit) ## Warning: package &#39;finalfit&#39; was built under R version 4.4.3 explanatory = c( &quot;crim&quot; , &quot;zn&quot; , &quot;indus&quot; , &quot;nox&quot; , &quot;rm&quot; , &quot;age&quot; , &quot;dis&quot; , &quot;rad&quot; , &quot;tax&quot; ,&quot;ptratio&quot; ,&quot;black&quot; , &quot;lstat&quot; ) dependent = &quot;medv&quot; Boston %&gt;% coefficient_plot(dependent, explanatory, table_text_size=3, title_text_size=12, plot_opts=list(xlab(&quot;Beta, 95% CI&quot;), theme(axis.title = element_text(size=12)))) ## `height` was translated to `width`. And the OR plot for logistic regression. This is a practical template to create a clean “OR (95% CI)” figure plus table in one call. library(finalfit) explanatory = c( &quot;pregnant&quot;, &quot;glucose&quot; , &quot;pressure&quot;, &quot;triceps&quot; ,&quot;insulin&quot; , &quot;mass&quot; , &quot;pedigree&quot;, &quot;age&quot; ) dependent = &quot;diabetes&quot; diabetes %&gt;% or_plot(dependent, explanatory, table_text_size=3, title_text_size=12, plot_opts=list(xlab(&quot;OR, 95% CI&quot;), theme(axis.title = element_text(size=12)))) ## Waiting for profiling to be done... ## Waiting for profiling to be done... ## Waiting for profiling to be done... ## `height` was translated to `width`. qq plot A QQ plot is a quick check of distributional assumptions. Here we show a QQ plot for the Boston target variable. In practice, you might also check residuals from the fitted model, not only the raw outcome. ggqqplot( (Boston$medv)) ## Warning: The `size` argument of `element_rect()` is deprecated ## as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## ℹ The deprecated feature was likely used in the ggpubr ## package. ## Please report the issue at ## &lt;https://github.com/kassambara/ggpubr/issues&gt;. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see ## where this warning was generated. Loading data set We now shift into ggplot2 fundamentals using the classic iris dataset. This section builds intuition: start from an empty canvas, add layers, add mappings, then customize. library(printr) ## Warning: package &#39;printr&#39; was built under R version 4.4.3 ## Registered S3 method overwritten by &#39;printr&#39;: ## method from ## knit_print.data.frame rmarkdown library(tidyverse) head(iris) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.2 Scatter plot Scatter plots are ideal for exploring relationships between two continuous variables. In practice, they are often your first step before fitting correlation/regression models. 4.2.1 Create a empty canvas then create aesthetic mapping tell the function which dataset and variables to use This creates a ggplot “object” (canvas) but does not draw anything until a geom layer is added. ggplot(data = iris, # which data set? canvas? aes(x=Sepal.Length , y=Petal.Length )) # which variables as aesthetics? x and y are mapped to columns of the data; different geoms can have different aesthetics (different variables). 4.2.2 Add a layer/geom of points to the canvas A plot becomes visible once we add geom_point(). This is the standard pattern in ggplot: define the canvas + add one or more layers. ggplot(data = iris, mapping = aes(x=Sepal.Length , y=Petal.Length )) + geom_point() # adding the geometrical representation This alternative style places the mapping in the geom. It is useful when different layers use different mappings. # same plot as above ggplot(data = iris) + geom_point( aes(x=Sepal.Length , y=Petal.Length )) 4.2.3 Add another aesthetic add a curve/straight line to fit these points geom provides the aesthetic to ggplot A smooth curve helps reveal the trend. By default, geom_smooth() uses a loess smoother for small/medium datasets. # Loess curve ggplot(data = iris, mapping = aes(x=Sepal.Length , y=Petal.Length )) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula = ## &#39;y ~ x&#39; If you want an interpretable linear trend line, specify method = \"lm\". # Linear regression line ggplot(data = iris, mapping = aes(x=Sepal.Length , y=Petal.Length)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 4.2.4 Add other aesthetic set other aesthetics colour, alpha (transparency), and size of points Aesthetic mapping is powerful: - mapping inside aes() means “vary by data,” - setting outside aes() means “fixed constant.” Here we map point size to Sepal.Width and set a fixed color + alpha. ggplot(data = iris) + geom_point(aes(x=Sepal.Length , y=Petal.Length, size = Sepal.Width ), alpha = .5, colour = &quot;red&quot;) Here we map color to Species, producing a grouped scatter plot. ggplot(data = iris) + geom_point(aes(x=Sepal.Length , y=Petal.Length, size = Sepal.Width , colour=Species), #white is a variable here alpha=.9) categorize Petal.Width then map colour to this new variable Creating derived categorical variables is common in applied analysis (e.g., risk categories). Then you can visualize patterns by category. iris &lt;- iris %&gt;% mutate(growth = ifelse(Petal.Width &gt; 1.5, &quot;Wide&quot;, &quot;Normal&quot;)) ggplot(data=iris) + geom_point(aes(x=Sepal.Length , y=Petal.Length, size = Sepal.Width , colour=growth), alpha=.9) 4.3 Bar chart Bar charts summarize counts for categorical variables. Here we count the number of observations in each growth category. ggplot(data = iris) + geom_bar(aes(x = growth)) bar chart after group_by then use stat='identity' If you want bars to represent pre-computed values (means, totals, proportions), you must provide y explicitly and set stat = \"identity\". Here we summarize mean Sepal.Length by Species and growth category. library(dplyr) results &lt;- iris %&gt;% group_by(Species, growth) %&gt;% summarise(Sepal.Length.mean=mean (Sepal.Length )) ## `summarise()` has grouped output by &#39;Species&#39;. You can ## override using the `.groups` argument. gop &lt;- results %&gt;% filter(Species != &quot;setosa_null&quot; ) gop ## # A tibble: 5 × 3 ## # Groups: Species [3] ## Species growth Sepal.Length.mean ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 setosa Normal 5.01 ## 2 versicolor Normal 5.91 ## 3 versicolor Wide 6.18 ## 4 virginica Normal 6.13 ## 5 virginica Wide 6.62 though meaningless below until line chart (just use the mean as the sum for demonstration) This shows the mechanics of an identity bar chart. # We can also store parts of a plot in an object plot1 &lt;- ggplot(gop) + geom_bar(aes(x=growth , y=Sepal.Length.mean), stat=&#39;identity&#39;) plot1 4.3.1 Add some options for the whole ggplot rather than layers switch the x and y axes coord_flip() is frequently used when category labels are long. plot1 + coord_flip() reorder x categories (-means descending) Ordering categories by a statistic is one of the easiest ways to improve readability. ggplot( gop) + geom_bar(aes(x=reorder(growth, -Sepal.Length.mean), y=Sepal.Length.mean, fill=growth), stat=&#39;identity&#39;) + coord_flip() add x axis label and a theme This is a typical “publication polish” pattern: label axes, remove redundant legends, apply a clean theme. ggplot(gop) + geom_bar(aes(x=reorder(growth, -Sepal.Length.mean), y=Sepal.Length.mean, fill=growth), stat=&#39;identity&#39;) + coord_flip() + xlab(&quot;Growth categories&quot;) + guides(fill=F) + theme_minimal() ## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be ## `FALSE`. Use &quot;none&quot; instead as of ggplot2 3.3.4. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see ## where this warning was generated. set theme Themes can be standardized across reports. ggthemes offers a set of recognizable styles. library(ggthemes) ## Warning: package &#39;ggthemes&#39; was built under R version 4.4.3 ggplot(data = iris) + geom_bar(aes(x = growth)) + theme_economist() 4.3.2 Grouped bar chart -bar chart with different panels Faceting is a quick way to compare the same plot structure across groups. ggplot(mpg, aes(x = class)) + geom_bar() + facet_wrap( ~ year) actual number (groups are stacked by default) Stacked bars are useful for composition, but can be harder to compare across groups. ggplot(gop) + geom_bar(aes(x=growth, y=Sepal.Length.mean, fill=Species) , stat=&#39;identity&#39; ) This shows a stacked count plot by default. ggplot(mpg, aes(x = class )) + geom_bar(aes(group = year, fill = year), position = &quot;stack&quot;) percentage Position “fill” converts to proportions (each bar sums to 1). This is ideal for comparing composition across categories. ggplot(gop) + geom_bar(aes(x=growth, y=Sepal.Length.mean, fill=Species), stat=&#39;identity&#39;, position=&#39;fill&#39;) groups are dodge with actual number Dodged bars are easier for comparing group values directly. ggplot(gop) + geom_bar(aes(x=growth, y=Sepal.Length.mean, fill=Species), stat=&#39;identity&#39;, position=&#39;dodge&#39;) groups are dodge with percentage Here we compute within-growth proportions manually, then plot as dodged bars. This is a useful pattern when you want side-by-side proportions rather than stacked proportions. gop2 &lt;- gop %&gt;% group_by(growth ) %&gt;% mutate(Sepal.Length.prop=Sepal.Length.mean/sum(Sepal.Length.mean)) ggplot(gop2) + geom_bar(aes(x=growth, y=Sepal.Length.prop, fill=Species), stat=&#39;identity&#39;, position=&#39;dodge&#39;) + ylab(&quot;Votes (%)&quot;) 4.4 Line charts Line charts are best when x represents an ordered scale (time, dose, sequence). If x is not truly ordered, lines can mislead, so use carefully. This simple line plot demonstrates the syntax, though in real applications you usually group lines by an ID or category. ggplot(iris) + geom_line(aes(x=Sepal.Length , y=Petal.Length)) 4.4.1 Grouped by colour variable Mapping color to Species creates separate lines for each group. This is a common pattern for showing group trends. ggplot(iris) + geom_line(aes(x=Sepal.Length , y=Petal.Length, colour = Species)) grouped by state then set how many rows or columns Faceting creates separate small panels for each group. This can improve readability when multiple lines overlap in one plot. ggplot(iris) + geom_line(aes(x=Sepal.Length , y=Petal.Length) ) + facet_wrap(~Species, nrow = 1) + #set how many rows coord_flip() 4.4.2 Multiple aesthetics Here we show multiple mappings and transformations: - a thicker line, - log scaling on x, - point size and color mapped to variables, - and faceting by Species. This is a realistic “final figure” style example—dense but informative. iris &lt;- iris %&gt;% mutate(growth = ifelse(Petal.Width &gt; 1.5, &quot;Wide&quot;, &quot;Normal&quot;)) ggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) + geom_line(size=2,color=&quot;purple&quot;)+ # number format scale_x_log10(labels = scales::label_number())+ geom_point( aes(size = Sepal.Length,colour = as.factor(growth)),show.legend = F)+ facet_wrap(~ Species) 4.5 ggplot2 parameters To go further, it helps to understand the “grammar of graphics” idea behind ggplot2: - data provides the rows, - aes maps variables to visual properties, - geoms draw shapes, - scales control transformations and legends, - guides refine legends, - themes control appearance. For detail, please read this article and this one. The following chunk lists a large set of packages used for advanced visualization workflows. In practice, you do not need all of them for basic plots, but it is useful to know what exists: - ggrepel for non-overlapping labels, - patchwork for combining plots, - gganimate for animation, - sf for spatial data, - ggthemes for themes, etc. This example is a classic demonstration that summary statistics can be misleading. Different datasets can share the same mean, variance, and correlation—but look completely different when plotted. Always plot your data. library(datasauRus) ## Warning: package &#39;datasauRus&#39; was built under R version 4.4.3 ggplot( )+ geom_point(data=datasaurus_dozen[datasaurus_dozen$dataset==&quot;dino&quot;,], aes(x = x, y = y),color= &quot;#7CAE00&quot; ) + theme_void()+ theme(legend.position = &quot;none&quot;) 4.5.1 Components of plot A practical way to think about building plots: Components of plot Data: is a data frame Aesthetics: is used to indicate x and y variables and to control the color, the size or the shape … Geometry: the type of graphics (bar plot, line plot, scatter plot…) adjust parameters adjust legend using guide_ adjust color, size, and shape using scale_, guide_ can further adjust scale_ adjust panel, background, axis (font, color, size, angle), title, legend (position), caption using theme types of plots geom_boxplot(): Box plot geom_violin(): Violin plot geom_dotplot(): Dot plot geom_jitter(): Jitter charts geom_line(): Line plot geom_bar(): Bar plot geom_errorbar(): Error bars geom_point(): Scatter plot geom_smooth(): Add smoothed curve geom_quantile(): Add quantile lines geom_text(): Textual annotations geom_density(): Create a smooth density curve geom_histogram(): Histogram example This example uses the Old Faithful geyser dataset and overlays a 2D density estimate. This is a useful technique when points overlap heavily and you want to show concentration. data(&quot;faithful&quot;) # Basic scatterplot ggplot(data = faithful, mapping = aes(x = eruptions, y = waiting)) + geom_point()+ stat_density_2d(aes(fill = ..level..), geom=&quot;polygon&quot;) # Data and mapping can be given both as global (in ggplot()) or per layer # ggplot() + # geom_point(mapping = aes(x = eruptions, y = waiting), # data = faithful) 4.5.2 Create main title, axis labels, caption Titles and captions turn plots into “report-ready” figures. Axis labels should make units explicit, and captions should record the data source if applicable. pay attention whether argument is factor or continuous. ggplot(data = faithful, mapping = aes(x = eruptions, y = waiting)) + geom_point()+ labs(title = &quot;Number of xxx&quot;, caption = &quot;source: http://xxx&quot;, x = &quot;Eruptions&quot; , y = &quot;Waiting time&quot; ) + # customize title, axis, caption theme( plot.title = element_text(color=&quot;red&quot;, size=14, face=&quot;bold.italic&quot;), plot.caption = element_text(color=&quot;red&quot;, size=10, face=&quot;italic&quot;), axis.title.x = element_text(color=&quot;blue&quot;, size=14, face=&quot;bold&quot;), axis.title.y = element_text(color=&quot;#993333&quot;, size=14, face=&quot;bold&quot;) )+ # hide main title theme(plot.title = element_blank() ) 4.5.3 Create legend title, position Legends should explain the mapping clearly (what does color mean? what does size mean?). You can also move the legend depending on layout constraints (e.g., left for tall plots, bottom for wide panels). p &lt;- ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting, colour = eruptions &lt; 3))+ labs(color=&quot;Numbers of eruptions&quot;) + theme(legend.position = &quot;left&quot;)+ # Change the appearance of legend title and labels theme(legend.title = element_text(colour=&quot;blue&quot;), legend.text = element_text(colour=&quot;red&quot;))+ # Change legend box background color theme(legend.background = element_rect(fill=NULL)) print(p) customize legends using scale functions Scales can rename and reorder legend entries. This is often necessary when boolean variables show up as “FALSE/TRUE” but you want more meaningful labels. # how to change order of legend? # Set legend title and labels p+ scale_color_discrete(name = &quot;Numbers of eruptions change&quot;, labels = c(&quot;F&quot;, &quot;T&quot; )) customize legend guide_colorbar() is typically used for continuous variables; guide_legend() is used for discrete variables (colors, shapes, linetypes). This block shows how to control legend title and label appearance in detail. ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting, size = eruptions &lt; 3))+ guides(size = guide_legend( # legend title title = &quot;title is too low&quot;, title.position = &quot;bottom&quot;, title.vjust = -5, # legend label label.position = &quot;left&quot;, label.hjust = 1, label.theme = element_text(size = 15, face = &quot;italic&quot;, colour = &quot;red&quot;, angle = 0), # label reverse reverse = TRUE, # width of bin keywidth = 1, ncol = 4 ) ) ## Warning: Using size for a discrete variable is not advised. delete a legend A common practical cleanup is to remove redundant legends when multiple aesthetics are shown. Here we keep the color legend but remove size legend. ggplot(mpg, aes(x = displ, y = hwy, color = class, size = cyl)) + geom_point() + guides( color = guide_legend(&quot;type&quot;), # keep size = &quot;none&quot; # remove ) combine two legends when they use the same variable (mapping) Mapping both color and size to the same variable can create duplicate legends. Here we unify them by explicitly setting guides. ggplot(mpg, aes(x = displ, y = hwy, color = cyl, size = cyl)) + geom_point() + scale_color_viridis_c() + guides( color = guide_legend(&quot;title&quot;), size = guide_legend(&quot;title&quot;) ) # guide = &quot;legend&quot; 4.5.4 Change plot colors This section demonstrates the key concept: set color into aes() means the color varies by a variable. set color outside of aes() means a constant color. set color into aes() ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting, colour = eruptions &lt; 3)) set color outside of aes() ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting), colour = &#39;steelblue&#39;) Colour the histogram with color and fill This maps outline color and fill color to logical conditions. In practice, you might map fill to a categorical group or a binned continuous variable. ggplot(faithful) + geom_histogram(aes(x = eruptions,color=eruptions &lt; 3, fill=eruptions &lt; 4)) ## `stat_bin()` using `bins = 30`. Pick better value ## `binwidth`. Colour the histogram by waiting and changing position Position controls how groups are displayed (stacked, dodged, or overlaid). ggplot(faithful) + geom_histogram(aes(x = eruptions,color=waiting&gt;60), position = &#39;dodge&#39;) ## `stat_bin()` using `bins = 30`. Pick better value ## `binwidth`. Overlaying histograms can be informative but can also become messy; using transparency or density curves can help. ggplot(faithful) + geom_histogram(aes(x = eruptions,color=waiting&gt;60), position = &#39;identity&#39;) ## `stat_bin()` using `bins = 30`. Pick better value ## `binwidth`. For fill and stack position, please see position section. change colors manually using scale Manual scales are helpful for consistency across plots in a report. This example shows both fill and outline color manual control. ggplot(faithful) + geom_histogram(aes(x = eruptions,color=eruptions &lt; 3, fill=eruptions &lt; 4))+ # Box plot scale_fill_manual(values=c(&quot;#999999&quot;, &quot;#E69F00&quot;)) # Scatter plot scale_color_manual(values=c( &quot;#E69F00&quot;, &quot;#56B4E9&quot;)) ## &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt; ## aesthetics: colour ## axis_order: function ## break_info: function ## break_positions: function ## breaks: waiver ## call: call ## clone: function ## dimension: function ## drop: TRUE ## expand: waiver ## fallback_palette: function ## get_breaks: function ## get_breaks_minor: function ## get_labels: function ## get_limits: function ## get_transformation: function ## guide: legend ## is_discrete: function ## is_empty: function ## labels: waiver ## limits: NULL ## make_sec_title: function ## make_title: function ## map: function ## map_df: function ## minor_breaks: waiver ## n.breaks.cache: NULL ## na.translate: TRUE ## na.value: grey50 ## name: waiver ## palette: function ## palette.cache: NULL ## position: left ## range: environment ## rescale: function ## reset: function ## train: function ## train_df: function ## transform: function ## transform_df: function ## super: &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt; using scale brewer automatically Brewer palettes are widely used, especially for categorical palettes that are print-friendly. ggplot(faithful) + geom_histogram(aes(x = eruptions,color=eruptions &lt; 3, fill=eruptions &lt; 4))+ # Box plot scale_fill_brewer(palette=&quot;Dark2&quot;) ## `stat_bin()` using `bins = 30`. Pick better value ## `binwidth`. # Scatter plot scale_color_brewer(palette=&quot;Set1&quot;) ## &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt; ## aesthetics: colour ## axis_order: function ## break_info: function ## break_positions: function ## breaks: waiver ## call: call ## clone: function ## dimension: function ## drop: TRUE ## expand: waiver ## fallback_palette: function ## get_breaks: function ## get_breaks_minor: function ## get_labels: function ## get_limits: function ## get_transformation: function ## guide: legend ## is_discrete: function ## is_empty: function ## labels: waiver ## limits: NULL ## make_sec_title: function ## make_title: function ## map: function ## map_df: function ## minor_breaks: waiver ## n.breaks.cache: NULL ## na.translate: TRUE ## na.value: NA ## name: waiver ## palette: function ## palette.cache: NULL ## position: left ## range: environment ## rescale: function ## reset: function ## train: function ## train_df: function ## transform: function ## transform_df: function ## super: &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt; # using guide to change the color of legend key using gray colors using scale Grayscale is useful for black-and-white printing or when you want to avoid color emphasis. # p + scale_fill_grey() #no fill element # p + scale_color_grey() Gradient or continuous colors (can set the middle point aswhite) Continuous gradients are typically used for continuous variables. A diverging gradient is helpful when there is a meaningful midpoint. # Color by cty values sp2&lt;-ggplot(mpg, aes(x = displ, y = hwy)) + geom_point(aes(color = cty)) sp2 # Change the low and high colors # Sequential color scheme sp2+scale_color_gradient(low=&quot;blue&quot;, high=&quot;red&quot;) # Diverging color scheme mid&lt;-mean(mpg$cty) sp2+scale_color_gradient2(midpoint=mid, low=&quot;blue&quot;, mid=&quot;white&quot;, high=&quot;red&quot; ) 4.5.5 Change points shapes, transparent and size Transparency (alpha) helps when points overlap. Shape and size can carry additional information, but too many aesthetics can overwhelm the reader, so use thoughtfully. make the points larger and slightly transparent. ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting, shape= eruptions &lt; 3, size=eruptions), color=&quot;steelblue&quot;, alpha=0.5) Reversing legend order can improve interpretability when sizes represent magnitude. # hwo to reverse order of legend size ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting, shape= eruptions &lt; 3, size=eruptions), color=&quot;steelblue&quot;, alpha=0.5)+ scale_shape_manual(values=c(10, 23 ))+ theme(legend.position=&quot;top&quot;) 4.5.6 Change bars position Position is one of the most practical controls for bar charts: - default is stacked, - dodge is side-by-side, - fill is normalized to proportions. This block shows all four layouts in a panel. p &lt;- ggplot(mpg, aes(fl, fill = drv)) p1 &lt;- p + geom_bar () p2 &lt;- p + geom_bar(position = &quot;dodge&quot;) p3 &lt;-p + geom_bar(position = &quot;fill&quot;) p4 &lt;-p + geom_bar(position = &quot;stack&quot;) library(gridExtra) ## Warning: package &#39;gridExtra&#39; was built under R version 4.4.3 ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine grid.arrange(p1, p2, p3,p4, ncol=2) 4.5.7 Add text annotations Text annotations can highlight key points or provide labels. Be cautious with large datasets; consider ggrepel for label repulsion when needed. ggplot(data=mpg[(1:100), ], aes(x = displ, y = hwy)) + geom_point(aes(color = cty))+ geom_text(aes(label = manufacturer ), size = 2, vjust = -1) #vjust is site not direction 4.5.8 Add a line that (separates points) Reference lines guide interpretation: - geom_abline() for a line defined by slope/intercept, - geom_hline() for a horizontal reference, - geom_vline() for a vertical reference. These are often used to mark thresholds or decision boundaries. ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting))+ geom_abline(slope=-13,intercept = 100,color=&quot;red&quot;, linetype = &quot;dashed&quot;)+ # Add horizontal line at y = 2O; change line type and color geom_hline(yintercept=20, linetype=&quot;dotted&quot;, color = &quot;red&quot;)+ # Add vertical line at x = 3; change line type, color and size geom_vline(xintercept = 3, color = &quot;blue&quot;, size=1.5) # Add regression line add segment and arrow Segments and arrows are useful for pointing out specific regions of the plot. This is common in presentations and teaching. ggplot(mpg, aes(x = displ, y = hwy )) + geom_point() + # Add horizontal line segment geom_segment(aes(x = 2, y = 15, xend = 3, yend = 15, size=3, color=&quot;red&quot;)) + geom_segment(aes(x = 3, y = 33, xend = 2.5 , yend = 30), arrow = arrow(length = unit(0.5, &quot;cm&quot;))) ## Warning in geom_segment(aes(x = 2, y = 15, xend = 3, yend = 15, size = 3, : All aesthetics have length 1, but the data has 234 ## rows. ## ℹ Please consider using `annotate()` or provide this ## layer with data containing a single row. ## Warning in geom_segment(aes(x = 3, y = 33, xend = 2.5, yend = 30), arrow = arrow(length = unit(0.5, : All aesthetics have length 1, but the data has 234 ## rows. ## ℹ Please consider using `annotate()` or provide this ## layer with data containing a single row. fitted curve A smooth curve shows the overall pattern. You can also subset data to remove categories that behave differently, as shown here. ggplot(data=mpg[mpg$fl!=&quot;c&quot;,], aes(x = displ, y = hwy)) + geom_point( ) + geom_smooth(color=&quot;red&quot;) #fitted curve ## `geom_smooth()` using method = &#39;loess&#39; and formula = ## &#39;y ~ x&#39; Quantile regression (geom_quantile()) can show how relationships differ across conditional quantiles. This is a useful exploratory tool when variance is not constant. ggplot(data=mpg[mpg$fl!=&quot;c&quot;,], aes(x = displ, y = hwy)) + geom_point( ) + geom_quantile() + geom_rug()+ theme_minimal() ## Smoothing formula not specified. Using: y ~ x customize line This example shows how to control multiple aesthetics and manually customize scales. It also demonstrates a very important rule: if the plot is too complicated, consider simplifying—readability matters more than showing every possible mapping. ggplot(mpg, aes(x = displ, y = hwy, color = fl, linetype = fl, size=fl)) + geom_point() + geom_line(aes( ) )+ labs(color=&quot;What is fl&quot;)+ # customize linetype, color, size scale_linetype_manual(values=c(&quot;twodash&quot;, &quot;dotted&quot;,&quot;twodash&quot;, &quot;dotted&quot;,&quot;twodash&quot;))+ scale_color_manual(name = &quot;continents&quot;,breaks = c(&quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;Europe&quot;, &quot;Oceania&quot;),labels = c(&quot;africa&quot;, &quot;americas&quot;, &quot;asia&quot;, &quot;europe&quot;, &quot;oceania&quot;), values=c(&#39;#999999&#39;,&#39;#E69F00&#39;,&#39;#999999&#39;,&#39;#E69F00&#39;,&#39;#999999&#39;) )+ #using breaks define three labels scale_size_manual(values=seq(1,4, 0.2))+ theme(legend.position=&quot;top&quot;) + guides(color=&quot;legend&quot;) 4.5.9 Using scale_ function Every aesthetic mapping has an associated scale, even if you do not specify it. Learning to use scale_*() functions is one of the fastest ways to level up plot quality. uisng scale_colour_brewer. ggplot(mpg) + geom_point(aes(x = displ, y = hwy, colour = class)) + scale_colour_brewer(type = &#39;qual&#39;) RColorBrewer This displays available palettes—useful when choosing a consistent palette for a report. RColorBrewer::display.brewer.all() using different palettes ggplot(mpg) + geom_point(aes(x = displ, y = hwy, colour = class)) + scale_colour_brewer (palette = &#39;Paired&#39;) showing cyl with size Combining color and size is common, but be mindful of legend clutter. Use breaks to simplify size legend. ggplot(mpg) + geom_point(aes(x = displ, y = hwy, colour = class, size=cyl)) + scale_colour_brewer(palette = &#39;Set1&#39; ) + scale_size (breaks = c(4,6)) using guides to modify the scale_ Guides can override legend aesthetics. This is useful when you want legend points to be clearly visible and consistent. ggplot(mpg) + geom_point(aes(x = displ, y = hwy, colour = cyl, size=cyl)) + # scale_colour_brewer(palette = &#39;Set1&#39;) + #can not continuous scale_size (breaks = c(4,5,6)) + guides( size = guide_legend( override.aes = list(color = c(&#39;red&#39;, &#39;blue&#39;, &#39;black&#39;)))) unite legends when multiple aesthetics are mapped to the same variable. This can reduce legend duplication and improve readability. ggplot(mpg) + geom_point(aes(x = displ, y = hwy, colour = cyl, size=cyl)) + guides(colour=&quot;legend&quot;) category is also ok Discrete mapping works similarly, but some aesthetics (like size) may behave differently and can create awkward legends. ggplot(mpg) + geom_point(aes(x = displ, y = hwy, colour = fl, size=fl)) + guides(colour=&quot;legend&quot;) #size is not ok ## Warning: Using size for a discrete variable is not advised. x and y also have associated scales Scales can transform axes and control tick marks. This example demonstrates manual x breaks and log2 transformation for y. ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + scale_x_continuous(breaks = c(3.5, 5, 6)) + scale_y_continuous(trans = &#39;log2&#39;) 4.5.10 Change coordinates Coordinate transformations can change the story of a plot. Use them intentionally—especially for polar charts, which can look impressive but may distort comparisons. coord_polar This converts bar charts into a circular layout. Use with caution: it is harder to compare lengths in polar coordinates. ggplot(mpg) + geom_bar(aes(x = class)) + coord_polar() This version changes the polar mapping. expand_limits() is used to create extra space. ggplot(mpg) + geom_bar(aes(x = class)) + coord_polar(theta = &#39;y&#39;) + expand_limits(y = 70) specify the scale of coordinate You can set y limits and breaks. In practice, avoid truncating axes unless you have a strong reason and clearly indicate it. require(scales) ## Loading required package: scales ## Warning: package &#39;scales&#39; was built under R version 4.4.3 ## ## Attaching package: &#39;scales&#39; ## The following objects are masked from &#39;package:psych&#39;: ## ## alpha, rescale ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor ggplot(mpg) + geom_bar(aes(x = class)) + scale_y_continuous(limits = c(0, 50), breaks = seq(0, 50, 01)) ## Warning: Removed 1 row containing missing values or values ## outside the scale range (`geom_bar()`). # scale_y_continuous(labels = percent) # labels as percents # + # scale_x_discrete(labels=c(1:7) ) using coord_cartesian zoom in coord_cartesian() zooms without dropping data outside the viewing range. This is usually preferable to setting scale limits, which can remove data. # have been deleted ggplot(mpg) + geom_bar(aes(x = class)) + scale_y_continuous(limits = c(0, 30))+ scale_x_discrete( limit=c(&quot;midsize&quot;,&quot;compact&quot;)) ## Warning: Removed 146 rows containing non-finite outside the ## scale range (`stat_count()`). ## Warning: Removed 2 rows containing missing values or values ## outside the scale range (`geom_bar()`). This version zooms into y limits while keeping the underlying data. ggplot(mpg) + geom_bar(aes(x = class)) + coord_cartesian( ylim = c(0, 30))+ scale_x_discrete( limit=c(&quot;midsize&quot;,&quot;compact&quot;)) ## Warning: Removed 146 rows containing non-finite outside the ## scale range (`stat_count()`). reverse direction of axes Axis reversal is useful for certain interpretive effects (e.g., ranking charts). Be careful combining multiple transformations in one plot—clarity first. ggplot(mpg) + geom_point(aes(x = hwy, y = displ))+ scale_x_continuous(breaks = c(20, 30, 35,40)) + scale_y_reverse()+ scale_y_continuous(trans=&quot;log2&quot;) ## Scale for y is already present. ## Adding another scale for y, which will replace the ## existing scale. # log10, sqrt, reverse, scale_y_continuous(trans=&quot;log2&quot;) 4.5.11 Customize axis ticks Small axis choices (angle, size, color) can significantly improve readability. This is especially important when plots are used in slides or printed reports. change axis text font, color, size, angle using theme # when use theme, scale, guide? ggplot(mpg) + geom_point(aes(x = hwy, y = displ))+ theme(axis.text.x = element_text(face=&quot;bold&quot;, color=&quot;#993333&quot;, size=14, angle=45), axis.text.y = element_text(face=&quot;bold&quot;, color=&quot;blue&quot;, size=7, angle=90)) remove aixs ticks and tick labels Sometimes minimal plots are more effective—especially when the audience already understands the scale or when the plot is purely illustrative. ggplot(mpg) + geom_point(aes(x = hwy, y = displ))+ theme( axis.text.x = element_blank(), # Remove x axis tick labels axis.text.y = element_blank(), # Remove y axis tick labels axis.ticks = element_blank()) # Remove ticks 4.5.12 Flip and reverse plot Flipping and reversing can help match how people naturally read categories (top-to-bottom ranking charts) or fit long labels. boxplot and violin Violin plots show distributions; boxplots show summary statistics. Overlaying both gives a compact distribution + summary view. ggplot(mpg ) + geom_violin( aes(x = as.factor(cyl), y=hwy ,color=as.factor(cyl) ) ,trim = FALSE,width = 4 ) + geom_boxplot( aes(x = as.factor(cyl), y=hwy ), notch = F , width = 0.1) ## Warning: `position_dodge()` requires non-overlapping x ## intervals. dotplot Dotplots can show distributions with less emphasis on smooth density. Adding a mean and SD summary helps anchor interpretation. ggplot(mpg ,aes(x = as.factor(cyl), y=hwy ) )+ geom_dotplot(aes(color =as.factor(cyl), fill = as.factor(cyl)), binaxis = &quot;y&quot;, stackdir = &quot;center&quot;) + stat_summary(fun.data=&quot;mean_sdl&quot; ) ## Bin width defaults to 1/30 of the range of the data. ## Pick better value with `binwidth`. errorbar This section builds a grouped summary dataset (df3) and then plots error bars. Error bars are useful for showing variability around means, but always clarify whether it is SD, SE, or CI. df3 &lt;- data_summary(mpg, varname=&quot;hwy&quot;, grps= c(&quot;cyl&quot; )) ## Loading required package: plyr ## ------------------------------------------------------------------------------ ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------------ ## ## Attaching package: &#39;plyr&#39; ## The following object is masked from &#39;package:ggpubr&#39;: ## ## mutate ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ## The following object is masked from &#39;package:purrr&#39;: ## ## compact head(df3) cyl hwy sd 4 28.80247 4.515030 5 28.75000 0.500000 6 22.82278 3.685590 8 17.62857 3.262307 This plot uses line + error bars. Grouping is set via aes(group = 0) as a simple way to connect points in one line. ggplot( df3, aes(as.factor(cyl) , (hwy), ymin = hwy-sd, ymax = hwy+sd) ) + geom_line(aes(group = 0 )) + geom_errorbar(aes(color = as.factor(cyl) ),width = 0.2) This version uses bars plus error bars, then flips coordinates for readability. ggplot( df3, aes(as.factor(cyl) , (hwy), ymin = hwy-sd, ymax = hwy+sd) ) + geom_bar(aes(fill = as.factor(cyl)), stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar(aes( ),width = 0.2) + coord_flip() using original data Instead of precomputing summaries, you can also rely on stat_summary() directly on raw data. This is convenient and reduces manual preprocessing. ggplot(mpg, aes(cyl, hwy)) + stat_summary(geom = &quot;bar&quot;) + stat_summary(geom = &quot;errorbar&quot;) ## No summary function supplied, defaulting to ## `mean_se()` ## No summary function supplied, defaulting to ## `mean_se()` flip ggplot(mpg) + geom_bar(aes(x = class)) + coord_flip() reverse Be cautious: reversing y for bar charts can confuse readers unless you have a strong reason. ggplot(mpg) + geom_bar(aes(x = class)) + scale_y_reverse() 4.5.13 Create stats Every geom has a stat. The stat can be overwritten if we use any additional computations. Understanding stats is a major ggplot skill: - bar charts default to counts (stat_count), - histograms default to bin counts, - smoothing defaults to fitting a curve, - and stat_summary() computes summary measures. Using original data (counts for cyl). ggplot(mpg ) + geom_bar (aes(x = cyl ),position = &#39;identity&#39; ) #using original data using transformed variables This section sets up data transformation patterns (commented out). The main idea: if you compute summary statistics yourself, you must use stat='identity' to plot them. library(dplyr) library(ggplot2) # mpg_counted &lt;- # count(mpg, cyl ) # head(mpg_counted) # ggplot(mpg_counted) + # geom_smooth(aes(x = cyl , y = n)) + # geom_bar (aes(x = cyl , y = n), stat = &#39;identity&#39;) #using summary data using the after_stat() function inside aes(). This computes proportions directly inside the plot call, which is very convenient. Here we plot class proportions instead of raw counts. require(scales) ggplot(mpg) + geom_bar(aes(x = class, y = after_stat( count / sum(count))))+ scale_y_continuous(labels = percent) # labels decimals as percents using density geometric in histogram Overlaying histogram density and a smoothed density curve is a classic approach to show distribution shape. ggplot(mpg,aes(x = hwy)) + geom_histogram(aes(y=..density..))+ geom_density( ) ## `stat_bin()` using `bins = 30`. Pick better value ## `binwidth`. Use stat_summary() to add the mean of hwy for each group STAT vs. GEOM is a practical concept: geom_* draws shapes, stat_* computes something first (means, SE, CI, etc.) and then draws. This panel shows different summary layers and a 95% interval by setting mult = 1.96. p1 &lt;- ggplot(mpg,aes(x = class, y = hwy) ) + stat_summary( geom = &quot;pointrange&quot;, fun.data = mean_se ) p2 &lt;- ggplot(mpg,aes(x = class, y = hwy) ) + stat_summary( ) p3 &lt;- ggplot(mpg,aes(x = class, y = hwy) ) + stat_summary( )+ stat_summary( fun.data = ~mean_se(., mult = 1.96), # Increase `mult` value for bigger interval! geom = &quot;errorbar&quot;, ) library(patchwork) ## Warning: package &#39;patchwork&#39; was built under R version 4.4.3 p1+p2+p3 ## No summary function supplied, defaulting to ## `mean_se()` ## No summary function supplied, defaulting to ## `mean_se()` Overlaying the group mean on top of raw points is a great way to show both individual variability and group central tendency. ggplot(mpg) + geom_point(aes(x = class, y = hwy), width = 0.2)+ stat_summary(aes( x = class,y = hwy), geom=&quot;point&quot;,color=&quot;red&quot;,size=4) ## Warning in geom_point(aes(x = class, y = hwy), width = 0.2): Ignoring unknown ## parameters: `width` ## No summary function supplied, defaulting to ## `mean_se()` jitter points Jittering helps reduce overplotting when x is discrete. This is one of the most common practical improvements for categorical scatter plots. ggplot(mpg) + geom_jitter(aes(x = class, y = hwy), width = 0.2)+ stat_summary(aes( x = class,y = hwy), geom=&quot;point&quot;,color=&quot;red&quot;,size=4) ## No summary function supplied, defaulting to ## `mean_se()` 4.5.14 Facets Facets are one of ggplot2’s strongest features. They let you compare the same relationship across groups. facet_wrap() allows you to place facet side by side into a rectangular layout. facet_grid() allows you to specify different directions and works on two variables. share the axes between the different panels Shared axes are helpful for direct comparison across panels. ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_wrap(~ class) Facet grid with rows: drv by row. ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_grid(drv~ . ) Facet grid with columns: drv by column. ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_grid(~ drv ) Facet by two variables at once. This is powerful but can create many small panels; use only when needed. ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_grid(year ~ drv) do not share the axes between the different panels Free scales can make each panel readable but can harm comparability across panels. Use free scales when ranges differ drastically across groups. ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_wrap(~ drv , scales = &quot;free&quot;) only free y axes This is a compromise: preserve x comparability but allow y to adjust per group. ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_wrap(~ drv , scales = &quot;free_y&quot;) adjust y scale (space) between the panels For bar charts with many categories, allowing free space can prevent labels from being squeezed. ggplot(mpg) + geom_bar(aes(y = manufacturer)) + facet_grid(class ~ .) This version frees y spacing and y scale to improve panel readability. ggplot(mpg) + geom_bar(aes(y = manufacturer)) + facet_grid(class ~ ., space = &quot;free_y&quot;, scales = &quot;free_y&quot;) display by adding multiple variables together Facetting by both year and drive type in one call. This is useful when you want a grid of comparisons. ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_wrap(year ~ drv) 4.5.15 Theme Themes control the overall appearance: background, grid lines, fonts, and spacing. theme_minimal Minimal themes are clean for slides and modern reports. ggplot(mpg) + geom_bar(aes(y = class)) + facet_wrap(~year) + theme_minimal() Further adjustments theme_bw is a common default for academic-style plots. This block demonstrates how to customize facet strip text, captions, and grid lines. ggplot(mpg) + geom_bar(aes(y = class)) + facet_wrap(~year) + labs(title = &quot;Number of car models per class&quot;, caption = &quot;source: http://fueleconomy.gov&quot;, x = NULL, y = NULL) + theme_bw() + theme( strip.text = element_text(face = &#39;bold&#39;, hjust = 0), plot.caption = element_text(face = &#39;italic&#39;), panel.grid.major = element_line(&#39;white&#39;, size = 0.5), panel.grid.minor = element_blank(), panel.grid.major.y = element_blank() # , # panel.ontop = TRUE ) theme_classic Classic themes remove background grids and look clean for publication. This example also shows manual fill scales with custom legend labels. ggplot(mpg) + geom_bar(aes(y = class, fill = drv) ,position = &quot;dodge&quot;) + facet_wrap(~year) + labs(title = &quot;Number of car models per class&quot;, caption = &quot;source: http://fueleconomy.gov&quot;, x = &#39;Number of cars&#39;, y = NULL)+ scale_fill_manual(name = &quot;Drive Models&quot;,values=c(&quot;black&quot;, &quot;grey50&quot;, &quot;grey80&quot;), labels = c(&quot;4w&quot;,&quot;Fw&quot;,&quot;Rw&quot; )) + # scale_x_continuous(expand = c(0, NA)) + theme_classic() + theme( # text = element_text(&#39;Avenir Next Condensed&#39;), # strip.text = element_text(face = &#39;bold&#39;, hjust = 0), plot.caption = element_text(face = &#39;italic&#39;), panel.grid.major = element_line(&#39;white&#39;, size = 0.5), panel.grid.minor = element_blank(), panel.grid.major.y = element_blank() # panel.ontop = TRUE) ) using ggthemes Themes from ggthemes provide recognizable styles. Use them when consistent branding is desired. library(ggthemes) ggplot(mpg) + geom_bar(aes(y = class, fill = drv) ,position = &quot;dodge&quot;) + facet_wrap(~year) + labs(title = &quot;Number of car models per class&quot;, caption = &quot;source: http://fueleconomy.gov&quot;, x = &#39;Number of cars&#39;, y = NULL)+ ggthemes::theme_economist() customized theme This block demonstrates a wide range of theme elements you can control: legend background, key, position, panel background and border, plot background, axis line, ticks, and text rotation, facet strip appearance, panel spacing. In practice, you typically standardize a simpler theme, but it is valuable to see what is possible. ggplot(mpg) + geom_bar(aes(y = class, fill = drv) ,position = &quot;dodge&quot;) + facet_wrap(~year) + labs(title = &quot;Number of car models per class&quot;, caption = &quot;source: http://fueleconomy.gov&quot;, x = &#39;Number of cars&#39;, y = NULL)+ theme( # 1 change legend legend.background = element_rect( fill = &quot;#fff6c2&quot;, color = &quot;black&quot;, linetype = &quot;dashed&quot; ), legend.key = element_rect(fill = &quot;grey&quot;, color = &quot;brown&quot;), legend.position = &quot;bottom&quot;, # 2 change panel (middle erea) background panel.background = element_rect( fill = &quot;#005F59&quot;, color = &quot;red&quot;, size = 3 ), panel.border = element_rect( color = &quot;black&quot;, fill = &quot;transparent&quot;, linetype = &quot;dashed&quot;, size = 3 ), # 3 change plot background plot.background = element_rect( fill = &quot;#a1dce9&quot;, color = &quot;black&quot;, size = 1.3 ), # 4 change axis elements axis.line = element_line(color = &quot;orange&quot;, size = 2), axis.title = element_text(color = &quot;red&quot;, face = &quot;italic&quot;), axis.ticks = element_line(color = &quot;purple&quot;, size = 3), axis.text = element_text(color = &quot;blue&quot;), axis.text.x = element_text(angle = 45, hjust = 1), # 5 change facet panel strip.background = element_rect(fill = &quot;orange&quot;), strip.text = element_text(color = &quot;red&quot;), panel.spacing = unit(0.3, &quot;inch&quot;) ) 4.5.16 How to setup subscripts or superscripts Scientific writing often needs subscripts/superscripts (e.g., CO_2, R²). This link provides a focused guide for that formatting in R Markdown and ggplot text elements. see here 4.6 How to create advanced plots Once you master the basics (data → aes → geoms → scales → themes), advanced plots become much easier. This link collects more advanced patterns and examples. see here "],["basic-statistics.html", "5 Basic statistics 5.1 The essentials of R 5.2 Central Limit Theorem 5.3 Common statistical distribution", " 5 Basic statistics This chapter introduces a practical “starter kit” for working in R from a statistician’s perspective. Before we discuss formal statistical concepts, we need a stable workflow: how to create objects, inspect and clean vectors, manipulate data frames, summarize data, and fit a basic model. These fundamentals are what you will repeatedly use in real projects—whether you are cleaning EHR data, summarizing trial endpoints, or building an analysis dataset for modeling. The goal here is not to memorize functions, but to understand what each operation is doing and why it matters. Most errors in applied statistics are not due to a complex model; they come from small issues early in the pipeline: missing values, incorrect variable types, accidental coercion to character, or merges that silently duplicate rows. This chapter makes those risks explicit and gives you a set of reliable patterns. 5.1 The essentials of R R is built around objects. You create an object (vector, matrix, list, data frame), inspect it, transform it, and then use it as input to another function. When you become comfortable with object types and common manipulations, statistical workflows become much faster and safer. 5.1.1 Manipulation of vector A vector is the simplest data structure in R: an ordered collection of values. However, vectors can hide common pitfalls—especially when they contain mixed types (numbers + characters + missing values). In practice, mixed-type vectors appear when importing data (e.g., a numeric column contains \"O\" due to a data entry issue). The code below demonstrates several key diagnostics: unique() and length() are used to quickly inspect distinct values and count how many unique entries exist—useful when checking a categorical variable, or spotting unexpected values. as.numeric() converts the vector to numeric, but any non-numeric values become NA. This is one of the most common sources of “silent data loss” in analyses. log() illustrates that once coercion introduces NA, downstream transformations may produce missing results. sum(..., na.rm=TRUE) shows a safe pattern for aggregation in the presence of missing values. sort(decreasing=TRUE) is a quick way to inspect extremes and potential outliers. is.na() and indexing (x[!is.na(x)]) demonstrate a standard workflow for filtering out missing values. %in% tests membership (very useful for validation checks). grepl() performs pattern matching and is helpful for detecting problematic strings during cleaning. library(tidyverse) library(dplyr) vec &lt;- c(3,5,2,1,5,&quot;O&quot;,NA) length(unique(vec)) ## [1] 6 num_vec &lt;- as.numeric(vec) log(num_vec) ## [1] 1.0986123 1.6094379 0.6931472 0.0000000 1.6094379 NA NA sum(c(num_vec, NA), na.rm=T) ## [1] 16 sort(num_vec, decreasing = T) ## [1] 5 5 3 2 1 is.na(num_vec) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE num_vec[!is.na(num_vec)] ## [1] 3 5 2 1 5 c(5,6) %in% vec ## [1] TRUE FALSE grepl(&quot;5&quot;, vec) ## [1] FALSE TRUE FALSE FALSE TRUE FALSE FALSE A practical habit: when you coerce types (e.g., as.numeric()), always check how many NAs were created and why. If a numeric variable suddenly has many NAs, the root cause is usually dirty input values (spaces, commas, symbols, or typos like \"O\" instead of 0). 5.1.2 Generate sequence or repeted sequece Simulating data, creating index variables, and generating repeated patterns are extremely common tasks in statistics. Two workhorses are: seq() to generate sequences (e.g., time points, dose levels, grid search values). rep() to repeat values by cycles (times) or in blocks (each), often used to build study designs or longitudinal datasets. seq(from = 0, to = 10, by = 0.5) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 ## [16] 7.5 8.0 8.5 9.0 9.5 10.0 rep(x = 1:3, times = 4) ## [1] 1 2 3 1 2 3 1 2 3 1 2 3 rep(x = 1:3, each = 4) ## [1] 1 1 1 1 2 2 2 2 3 3 3 3 Conceptually: - times repeats the whole vector multiple times. - each repeats each element multiple times before moving to the next. 5.1.3 Get directory and write data out and in A reproducible workflow needs a stable approach to file paths. getwd() tells you the working directory, and setwd() sets it. Writing and reading data are also routine steps when sharing outputs, debugging, or building analysis datasets. Important practice notes: - If your project grows, prefer project-based workflows (e.g., RStudio projects) rather than repeatedly calling setwd(). - When exporting, keep track of whether row names are included; they can accidentally become a new column on import. getwd() ## [1] &quot;C:/Users/hed2/Downloads/others/mybook2/mybook2&quot; setwd(getwd()) write.csv(cars, &quot;cars.csv&quot;, row.names=F) dataframe &lt;- read.csv(&quot;cars.csv&quot;) 5.1.4 Function Functions let you encapsulate repeated logic and ensure consistency. In applied statistics, functions are often used to: - standardize transformations, - compute derived variables, - generate reports, - run simulation loops. The function below transforms x into a modified value. This is intentionally simple, but the pattern is the same for more complex analysis utilities. my_func &lt;- function(x){ x_mod &lt;- (x + 7) * 4 return(x_mod) } my_func(num_vec) ## [1] 40 48 36 32 48 NA NA Practical note: a function is safest when it handles missing values and validates input types. Even when you don’t add validation now, it helps to remember that your “future self” (or collaborator) will appreciate defensive checks. 5.1.5 Plot Exploratory plots help you understand distributions, detect outliers, and identify relationships before modeling. Base R plotting is fast and lightweight, which is why it remains common in statistical practice. A scatterplot (plot(y ~ x, data=...)) is the basic tool for relationships. A histogram (hist()) checks distribution shape, skewness, and potential anomalies. plot(dist ~ speed, data=cars) hist(cars$dist ) 5.1.6 Build model and plot A linear model (lm) is often the first modeling step: it provides a baseline, helps you understand effect size and direction, and reveals whether a relationship is approximately linear. This section fits a simple model and overlays the fitted regression line on the scatterplot. The additional vertical and horizontal lines serve as reference thresholds (e.g., a clinically meaningful cutoff, or a design constraint). model &lt;- lm(dist ~ speed, data=cars) plot(dist ~ speed, data=cars) abline(model) abline(v = 25) abline(h = 15) In practice, it is common to annotate plots with reference lines—especially when discussing thresholds, eligibility criteria, or operational boundaries. 5.1.7 Rename names of columns Clean variable names are more than aesthetics: they affect model formulas, joining keys, and the readability of analysis code. The code below inspects column names and then renames them. A caution: introducing spaces (e.g., \"speed per hour\") makes later coding more cumbersome because you must use backticks in formulas and selection. In many applied projects, analysts prefer names like speed_per_hour for reliability. names(cars) ## [1] &quot;speed&quot; &quot;dist&quot; names(cars) &lt;- c(&quot;speed per hour&quot;, &quot;total dist&quot;) 5.1.8 Class of dataframe Understanding classes is crucial because many R functions behave differently depending on the object type. matrix and data.frame look similar but differ in important ways: A matrix is homogeneous (all values must be the same type). A data frame can store different types across columns (numeric, factor, character). The code below converts cars to a matrix and back to a data frame, then checks classes. It also demonstrates transposition (t()), which is defined for matrices. matrix &lt;- as.matrix(cars) df &lt;- as.data.frame(matrix) class(matrix) ## [1] &quot;matrix&quot; &quot;array&quot; class(df) ## [1] &quot;data.frame&quot; # tranform t(matrix) speed per hour 4 4 7 7 8 9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25 total dist 2 10 4 22 16 10 18 26 34 17 28 14 20 24 28 26 34 34 46 26 36 60 80 20 26 54 32 40 32 40 50 42 56 76 84 36 46 68 32 48 52 56 64 66 54 70 92 93 120 85 Practical warning: converting a data frame with mixed types to a matrix often forces everything to character. That can break models and summaries if you do not convert back carefully. 5.1.9 Generate new variable for dataframe (character) Identifiers and grouping variables are often created using string concatenation. paste0() is a clean way to build IDs without spaces. The examples below create patterned labels like \"raster_1\", then attach them to the data frame. These patterns are useful when simulating repeated measures or defining cluster membership. paste0(&quot;raster_&quot;, 1:10) ## [1] &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_6&quot; ## [7] &quot;raster_7&quot; &quot;raster_8&quot; &quot;raster_9&quot; &quot;raster_10&quot; paste0(&quot;raster_&quot;, rep(x = 1:5, times = 10)) ## [1] &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; ## [7] &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; ## [13] &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; ## [19] &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; ## [25] &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; ## [31] &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; ## [37] &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; ## [43] &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; ## [49] &quot;raster_4&quot; &quot;raster_5&quot; df$group &lt;- paste0(&quot;raster_&quot;, rep(x = 1:5, times = 10)) df$id &lt;- paste0(&quot;raster_&quot;, 1:50) 5.1.10 Create a new dataframe using ‘rnorm’ - random number from distribution Simulation is a core skill in modern statistical practice. Here we generate: - a numeric variable (sample) from a normal distribution, - a grouping variable, - an ID variable to support merging. The function rnorm(n, mean, sd) generates normal random variables. Rounding is used for readability. sample &lt;- round((rnorm(50,0, 1)),2) group &lt;- paste0(&quot;raster_&quot;, rep(x = 1:5, times = 10)) df_join &lt;- data.frame(sample, group) df_join$id &lt;- paste0(&quot;raster_&quot;, 1:50) 5.1.11 Left join two dataframes Merging tables is one of the most error-prone steps in applied analysis. left_join() keeps all rows from the left table and adds matching columns from the right table. Key practice: - Always confirm uniqueness of the key (id) in each table before joining. - After joining, check row counts and inspect for accidental duplication. library(dplyr) data_all &lt;- left_join(df, df_join, by=&quot;id&quot;) head(data_all) speed per hour total dist group.x id sample group.y 4 2 raster_1 raster_1 0.84 raster_1 4 10 raster_2 raster_2 0.15 raster_2 7 4 raster_3 raster_3 -1.14 raster_3 7 22 raster_4 raster_4 1.25 raster_4 8 16 raster_5 raster_5 0.43 raster_5 9 10 raster_1 raster_6 -0.30 raster_1 5.1.12 Select variables Selecting columns is a common step for building analysis-ready datasets. This also helps reduce clutter when checking intermediate results. select(data_all, group.x, id ) group.x id raster_1 raster_1 raster_2 raster_2 raster_3 raster_3 raster_4 raster_4 raster_5 raster_5 raster_1 raster_6 raster_2 raster_7 raster_3 raster_8 raster_4 raster_9 raster_5 raster_10 raster_1 raster_11 raster_2 raster_12 raster_3 raster_13 raster_4 raster_14 raster_5 raster_15 raster_1 raster_16 raster_2 raster_17 raster_3 raster_18 raster_4 raster_19 raster_5 raster_20 raster_1 raster_21 raster_2 raster_22 raster_3 raster_23 raster_4 raster_24 raster_5 raster_25 raster_1 raster_26 raster_2 raster_27 raster_3 raster_28 raster_4 raster_29 raster_5 raster_30 raster_1 raster_31 raster_2 raster_32 raster_3 raster_33 raster_4 raster_34 raster_5 raster_35 raster_1 raster_36 raster_2 raster_37 raster_3 raster_38 raster_4 raster_39 raster_5 raster_40 raster_1 raster_41 raster_2 raster_42 raster_3 raster_43 raster_4 raster_44 raster_5 raster_45 raster_1 raster_46 raster_2 raster_47 raster_3 raster_48 raster_4 raster_49 raster_5 raster_50 5.1.13 Filter observations Filtering creates analytic subsets, such as: - a treatment arm, - a subgroup, - an eligibility population, - a set of observations meeting a condition. This section shows filtering by a grouping string, and filtering by numeric conditions (with a variable name that contains spaces, requiring backticks). raster_1 &lt;- filter(data_all, group.x == &quot;raster_1&quot;) raster_1 speed per hour total dist group.x id sample group.y 4 2 raster_1 raster_1 0.84 raster_1 9 10 raster_1 raster_6 -0.30 raster_1 11 28 raster_1 raster_11 0.55 raster_1 13 26 raster_1 raster_16 -0.21 raster_1 14 36 raster_1 raster_21 -0.40 raster_1 15 54 raster_1 raster_26 -0.03 raster_1 17 50 raster_1 raster_31 -1.55 raster_1 19 36 raster_1 raster_36 -0.50 raster_1 20 52 raster_1 raster_41 0.45 raster_1 24 70 raster_1 raster_46 -2.31 raster_1 speed_dist &lt;- filter(data_all, data_all$`speed per hour` &lt; 11 &amp; data_all$`total dist` &gt;= 10) speed_dist speed per hour total dist group.x id sample group.y 4 10 raster_2 raster_2 0.15 raster_2 7 22 raster_4 raster_4 1.25 raster_4 8 16 raster_5 raster_5 0.43 raster_5 9 10 raster_1 raster_6 -0.30 raster_1 10 18 raster_2 raster_7 0.90 raster_2 10 26 raster_3 raster_8 0.88 raster_3 10 34 raster_4 raster_9 0.82 raster_4 5.1.14 Append rows Row-binding is used when you want to stack two datasets with the same structure. This is common when combining: - multiple batches, - subsets, - cohorts. rbind() requires matching columns (names and order). In tidyverse workflows, bind_rows() is often more forgiving, but rbind() is fine when structures match exactly. rbind(raster_1,speed_dist) speed per hour total dist group.x id sample group.y 4 2 raster_1 raster_1 0.84 raster_1 9 10 raster_1 raster_6 -0.30 raster_1 11 28 raster_1 raster_11 0.55 raster_1 13 26 raster_1 raster_16 -0.21 raster_1 14 36 raster_1 raster_21 -0.40 raster_1 15 54 raster_1 raster_26 -0.03 raster_1 17 50 raster_1 raster_31 -1.55 raster_1 19 36 raster_1 raster_36 -0.50 raster_1 20 52 raster_1 raster_41 0.45 raster_1 24 70 raster_1 raster_46 -2.31 raster_1 4 10 raster_2 raster_2 0.15 raster_2 7 22 raster_4 raster_4 1.25 raster_4 8 16 raster_5 raster_5 0.43 raster_5 9 10 raster_1 raster_6 -0.30 raster_1 10 18 raster_2 raster_7 0.90 raster_2 10 26 raster_3 raster_8 0.88 raster_3 10 34 raster_4 raster_9 0.82 raster_4 5.1.15 Create new variables instead of old variables Data cleaning often involves transforming a variable into a more usable form. Here we round sample to one decimal place. Note that mutate() returns a modified data frame; you typically assign it back if you want to keep the change. mutate(data_all, sample = round(sample,1)) speed per hour total dist group.x id sample group.y 4 2 raster_1 raster_1 0.8 raster_1 4 10 raster_2 raster_2 0.1 raster_2 7 4 raster_3 raster_3 -1.1 raster_3 7 22 raster_4 raster_4 1.2 raster_4 8 16 raster_5 raster_5 0.4 raster_5 9 10 raster_1 raster_6 -0.3 raster_1 10 18 raster_2 raster_7 0.9 raster_2 10 26 raster_3 raster_8 0.9 raster_3 10 34 raster_4 raster_9 0.8 raster_4 11 17 raster_5 raster_10 0.7 raster_5 11 28 raster_1 raster_11 0.6 raster_1 12 14 raster_2 raster_12 -0.1 raster_2 12 20 raster_3 raster_13 -0.3 raster_3 12 24 raster_4 raster_14 -0.4 raster_4 12 28 raster_5 raster_15 -0.7 raster_5 13 26 raster_1 raster_16 -0.2 raster_1 13 34 raster_2 raster_17 -1.3 raster_2 13 34 raster_3 raster_18 2.2 raster_3 13 46 raster_4 raster_19 1.2 raster_4 14 26 raster_5 raster_20 -1.1 raster_5 14 36 raster_1 raster_21 -0.4 raster_1 14 60 raster_2 raster_22 -0.5 raster_2 14 80 raster_3 raster_23 0.8 raster_3 15 20 raster_4 raster_24 -0.1 raster_4 15 26 raster_5 raster_25 0.2 raster_5 15 54 raster_1 raster_26 0.0 raster_1 16 32 raster_2 raster_27 0.0 raster_2 16 40 raster_3 raster_28 1.4 raster_3 17 32 raster_4 raster_29 -0.2 raster_4 17 40 raster_5 raster_30 1.5 raster_5 17 50 raster_1 raster_31 -1.6 raster_1 18 42 raster_2 raster_32 0.6 raster_2 18 56 raster_3 raster_33 0.1 raster_3 18 76 raster_4 raster_34 0.2 raster_4 18 84 raster_5 raster_35 0.4 raster_5 19 36 raster_1 raster_36 -0.5 raster_1 19 46 raster_2 raster_37 -0.3 raster_2 19 68 raster_3 raster_38 -1.0 raster_3 20 32 raster_4 raster_39 -1.1 raster_4 20 48 raster_5 raster_40 0.3 raster_5 20 52 raster_1 raster_41 0.4 raster_1 20 56 raster_2 raster_42 0.0 raster_2 20 64 raster_3 raster_43 0.9 raster_3 22 66 raster_4 raster_44 2.0 raster_4 23 54 raster_5 raster_45 -0.5 raster_5 24 70 raster_1 raster_46 -2.3 raster_1 24 92 raster_2 raster_47 1.0 raster_2 24 93 raster_3 raster_48 -0.7 raster_3 24 120 raster_4 raster_49 -0.7 raster_4 25 85 raster_5 raster_50 1.0 raster_5 5.1.16 summarise statistics Summarization produces descriptive statistics and quick QA checks. In practice, it is a good idea to confirm that you are summarizing the intended variables and that the variable types are correct. A practical note for this code chunk: max(\"total dist\") will not compute the maximum of the column; it is taking a character string. In real analyses, always verify that your summary outputs look plausible. summarise(data_all, mean_speed = mean(sample), max_dist = max( &quot;total dist&quot; )) mean_speed max_dist 0.1104 total dist 5.1.17 Group dataframe then summarise statistics Grouping is essential for stratified summaries (by arm, site, subgroup, visit). The typical pattern is: group_by() summarise() This yields one row per group. data_all_group &lt;- group_by(data_all, group.x) summarise(data_all_group, mean_speed = mean(sample), max_dist = max( &quot;total dist&quot; )) mean_speed max_dist 0.1104 total dist 5.1.18 Ungroup then summarise statistics After group operations, the data may remain grouped. ungroup() removes grouping, which prevents unexpected behavior in later steps. This is a common best practice: ungroup after grouped summaries unless you intentionally want grouping to persist. ungroup_data &lt;- ungroup( data_all_group) summarise( ungroup_data , mean_speed = mean(sample), max_dist = max( &quot;total dist&quot; )) mean_speed max_dist 0.1104 total dist 5.1.19 Summary linear regression model This section fits a linear regression using the renamed columns. The summary() output provides: - coefficient estimates, - standard errors, - t-tests and p-values (under standard assumptions), - R-squared and residual standard error. Even when you plan to use more advanced models, a simple linear regression is a valuable baseline for interpretation and for detecting obvious data issues. mod1 &lt;- lm(cars$`total dist` ~ cars$`speed per hour` ) summary(mod1) ## ## Call: ## lm(formula = cars$`total dist` ~ cars$`speed per hour`) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## cars$`speed per hour` 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 5.1.20 Create frequency table Frequency tables help you check distributions across groups, detect empty cells, and validate merges. Two-way tables are also a quick way to identify whether a categorical variable is unevenly distributed across groups. table(data_all_group$`speed per hour`,data_all_group$group.x ) / raster_1 raster_2 raster_3 raster_4 raster_5 4 1 1 0 0 0 7 0 0 1 1 0 8 0 0 0 0 1 9 1 0 0 0 0 10 0 1 1 1 0 11 1 0 0 0 1 12 0 1 1 1 1 13 1 1 1 1 0 14 1 1 1 0 1 15 1 0 0 1 1 16 0 1 1 0 0 17 1 0 0 1 1 18 0 1 1 1 1 19 1 1 1 0 0 20 1 1 1 1 1 22 0 0 0 1 0 23 0 0 0 0 1 24 1 1 1 1 0 25 0 0 0 0 1 5.1.21 Value and variable label Labels are especially useful for reporting, tables, and clinical datasets where you want human-readable metadata. This section shows: inspecting levels of a factor, relabeling factor levels, adding a variable label using Hmisc::label(). table(iris$Species) setosa versicolor virginica 50 50 50 iris$Species &lt;- factor(iris$Species,labels = c( &quot;setosanew&quot;,&quot;versicolornew&quot;,&quot;virginianew&quot;)) table(iris$Species) setosanew versicolornew virginianew 50 50 50 library(Hmisc) label(iris$Species) &lt;- &quot;Species types&quot; table(iris$Species) setosanew versicolornew virginianew 50 50 50 In applied work, consistent labeling helps downstream reporting tools and reduces ambiguity when sharing datasets with collaborators. 5.1.22 Recode a variable Recoding is frequently used to: - create categorical versions of continuous variables, - define risk groups, - implement analysis definitions (e.g., responder/non-responder). This chunk uses nested ifelse() to create a derived variable based on Sepal.Length. While nested ifelse() works, in complex real projects, case_when() is often clearer and less error-prone. The key concept remains: define rules explicitly and validate results with a frequency table. irisifelse &lt;- iris%&gt;% mutate(Sepal.Length2 = ifelse(Sepal.Length &lt; 6 , &quot;level1&quot;, ifelse(Sepal.Length &lt; 7 , &quot;level2&quot;, Sepal.Length))) table(irisifelse$Sepal.Length2) 7 7.1 7.2 7.3 7.4 7.6 7.7 7.9 level1 level2 1 1 3 1 1 1 4 1 83 54 5.2 Central Limit Theorem The Central Limit Theorem (CLT) is one of the most important ideas in statistics: it justifies why normal-based inference often works even when the underlying data are not normal, as long as sample sizes are reasonably large and observations are independent. In practice, the CLT supports: - approximate confidence intervals for means, - normal approximations for many estimators, - reasoning about sampling variability. see here 5.3 Common statistical distribution Statistical distributions are the language of uncertainty. In applied work, you encounter them in: - modeling outcomes (normal, binomial, Poisson), - generating simulations, - defining priors and likelihoods, - interpreting p-values and confidence intervals. see here Chapter takeaways By the end of this chapter, you should be comfortable with: Inspecting vectors, handling missing values, and diagnosing coercion issues Generating sequences and repeated patterns for indexing and simulation Reading/writing data and understanding the working directory Writing simple functions to standardize repeated steps Making quick exploratory plots Fitting and interpreting a basic linear regression Managing variable names, classes, and joins Building group-wise summaries and validating derived variables These are not “intro programming trivia”—they are the daily tools of statistical practice. Once these fundamentals are stable, you can scale up to robust workflows: reproducible reporting, simulation-based power analysis, and model-based inference. "],["statistical-models.html", "6 Statistical models 6.1 Simple linear regression 6.2 Multiple linear regression 6.3 Multiple linear regression practice 6.4 Variable selection 6.5 Linear mixed model theory 6.6 Linear mixed model practice 6.7 Linear mixed model covariance decomposition with random intercept — lme4 6.8 Linear Mixed Model Covariance Decomposition with Random Slopes (lme4) 6.9 Linear Mixed Model Covariance Decomposition (nlme) 6.10 Manual simulating data for linear mix model 6.11 How to calculate the prediction interval for LMM 6.12 Least-squares means with interaction effect 6.13 Spline regression", " 6 Statistical models 6.1 Simple linear regression Simple linear regression is the most common “first model” in applied statistics. It answers a basic question: How does the expected value of an outcome \\(Y\\) change as a function of a single predictor \\(X\\)? Even if your final analysis uses more sophisticated methods (mixed models, GLMs, survival models), simple linear regression remains a key building block. It teaches you how estimation works, why assumptions matter, and how uncertainty is quantified through standard errors, confidence intervals, and hypothesis tests. In practice, regression has two equally important goals: 1) Explanation (estimating association or effect size), and 2) Prediction (forecasting outcomes at new values of \\(X\\)). 6.1.1 Linear regeression assumptions A regression model is only as reliable as its assumptions. In real-world analysis, the assumptions are rarely perfect, but they guide diagnostics and help you understand when inference may break down. There are four principal assumptions: Linearity of the relationship between dependent and independent variables. This means the conditional mean \\(E(Y|X)\\) is well-approximated by a straight line. If the true relationship is curved, the linear model may still be useful as a local approximation, but interpretation can become misleading. Statistical independence of the errors with \\(y\\) variable. Independence is often violated in longitudinal data, clustered data (e.g., patients within sites), or time series. When independence fails, standard errors are typically wrong—often too small, leading to overly optimistic p-values and confidence intervals that are too narrow. Homoscedasticity (constant variance) of the errors for all \\(x\\). If variability increases with \\(X\\) (a “fanning out” pattern), the model may still estimate the mean trend reasonably, but standard errors and tests can be distorted unless you use robust methods or transform variables. Normality of the error distribution. Normality matters mainly for small-sample inference. Large samples rely less on strict normality due to asymptotic approximations. Also note: normality is assumed for the errors, not necessarily for \\(X\\) or \\(Y\\) marginally. if independent assumption violated, the estimated standard errors tend to underestimate the true standard error. P value associated thus is lower. only the prediction errors need to be normally distributed. but with extremely asymmetric or long-tailed, it may be hard to fit them (x and y) into a linear model whose errors will be normally distributed. 6.1.2 Population regression function The population regression function is the ideal target we would like to know: the true conditional mean of \\(Y\\) given \\(X\\). Regression is fundamentally about modeling and estimating this conditional expectation. Regression is to estimate and/or predict the population mean (expectation) of dependent variable (yi) by a known or a set value of explanatory variables (xi). Population regression line (PRL) is the trajectory of the conditional expectation value given Xi. \\[ E(Y|X_i)=f(X_i)=\\beta_1+\\beta_2X_i \\] This is an unknown but fixed value (can be estimated). A key interpretation: - \\(\\beta_1\\) is the expected value of \\(Y\\) when \\(X=0\\) (sometimes meaningful, sometimes not). - \\(\\beta_2\\) is the expected change in \\(Y\\) for a one-unit increase in \\(X\\). 6.1.3 Population regression model In the population, actual observations deviate from the regression function due to randomness and unmeasured factors. We represent this deviation as an error term \\(u_i\\). \\[ Y_i=\\beta_1+\\beta_2X_i+u_i \\] the errors \\(u_i=y_i-\\hat{y}_i\\) have equal variance In applied interpretation, \\(u_i\\) captures everything not explained by \\(X\\): measurement noise, omitted variables, and inherent randomness. 6.1.4 Sample regression model In practice we observe data and estimate coefficients. The sample regression model replaces unknown population parameters with estimates (hats), and uses residuals \\(e_i\\) as estimated errors. (using hat to indicate sample) \\[ Y_i=\\hat{\\beta}_1+\\hat{\\beta}_2X_i+e_i \\] since \\[ u_i \\sim N(0,\\sigma^2) \\] or \\[ e_i \\sim N(0,\\hat{\\sigma} ^2) \\] and i.i.d., independent identically distribution, the probability distributions are all the same and variables are independent of each other. \\[ \\begin{align} u_i \\sim i.i.d \\ N(0,\\sigma^2) \\end{align} \\] then \\[ \\begin{align} Y_i- \\beta_1+\\beta_2X_i (\\hat{Y_i}) &amp;\\sim i.i.d \\ N(0,\\sigma^2)\\\\ \\end{align} \\] This i.i.d. assumption is what allows us to derive standard errors and perform t-tests and F-tests in the classical linear regression framework. 6.1.5 Least squares: minimize \\(Q=\\sum (Y_i-\\hat{Y}_i)^2\\) The ordinary least squares (OLS) estimator chooses coefficients that minimize the total squared residual error. Squaring penalizes large deviations and gives a unique and mathematically convenient solution. thence, to minimize Q \\(\\sum{(Y_i-\\hat{Y}_i)^2}\\) to solve b0 and b1. \\[ \\begin{align} Min(Q) &amp;=\\sum{(Y_i-\\hat{Y}_i)^2}\\\\ &amp;=\\sum{\\left ( Y_i-(\\hat{\\beta}_1+\\hat{\\beta}_2X_i) \\right )^2}\\\\ &amp;=f(\\hat{\\beta}_1,\\hat{\\beta}_2) \\end{align} \\] 6.1.6 Solve \\(\\hat{\\beta}_1,\\hat{\\beta}_2\\) and variance Once the least squares problem is solved, you get closed-form estimators for the slope and intercept. The sampling variability of these estimators depends on: - the error variance \\(\\sigma^2\\), - the spread of \\(X\\) values (more spread → more information → smaller variance). \\[ \\begin{align} \\begin{split} \\hat{\\beta}_2 &amp;=\\frac{\\sum{x_iy_i}}{\\sum{x_i^2}}\\\\ \\hat{\\beta_1} &amp;=\\bar{Y}_i-\\hat{\\beta}_2\\bar{X}_i \\end{split} \\\\ var(\\hat{\\beta}_2) =\\sigma_{\\hat{\\beta}_2}^2&amp;=\\frac{1}{\\sum{x_i^2}}\\cdot\\sigma^2&amp;&amp;\\text{} \\\\ var(\\hat{\\beta}_1) =\\sigma_{\\hat{\\beta}_1}^2 &amp;=\\frac{\\sum{X_i^2}}{n\\sum{x_i^2}}\\cdot\\sigma^2 \\end{align} \\] A practical takeaway: if your \\(X\\) values are tightly clustered, \\(\\sum x_i^2\\) is small and the slope becomes hard to estimate precisely. 6.1.7 Calculate the variance \\(\\hat{\\sigma}^2\\) of error \\(e_i\\) The residual variance is estimated from the residual sum of squares. Conceptually, it measures how much unexplained variability remains after fitting the regression line. (for sample) \\[ \\begin{align} \\hat{Y}_i &amp;=\\hat{\\beta}_1+\\hat{\\beta}_2X_i \\\\ e_i &amp;=Y_i-\\hat{Y}_i \\\\ \\hat{\\sigma}^2 &amp;=\\frac{\\sum{e_i^2}}{n-1}=\\frac{\\sum{(Y_i-\\hat{Y}_i)^2}}{n-1} \\end{align} \\] (Practical note: in classical regression, the unbiased estimator typically uses \\(n-2\\) in the denominator for simple linear regression because two parameters were estimated. Your expression shows the core idea—estimating variance from squared residuals.) 6.1.8 Sum of squares decomposition A central identity in regression is that total variability can be decomposed into explained and unexplained components. This is the basis of \\(R^2\\) and the ANOVA-style F test. \\[ \\begin{align} (Y_i-\\bar{Y_i}) &amp;= (\\hat{Y_i}-\\bar{Y_i}) +(Y_i-\\hat{Y_i}) \\\\ \\sum{y_i^2} &amp;= \\sum{\\hat{y_i}^2} +\\sum{e_i^2} \\\\ TSS&amp;=ESS+RSS \\end{align} \\] Interpretation: - TSS: total sum of squares (overall variability around the mean) - ESS: explained sum of squares (variability explained by the regression) - RSS: residual sum of squares (unexplained variability) 6.1.9 Coefficient of determination \\(R^2\\) and goodness of fit \\(R^2\\) is the proportion of variability explained by the model. It is descriptive: a higher \\(R^2\\) means the fitted line tracks the data more closely, but it does not guarantee causality or correctness of assumptions. \\[ \\begin{align} r^2 &amp;=\\frac{ESS}{TSS}=\\frac{\\sum{(\\hat{Y_i}-\\bar{Y})^2}}{\\sum{(Y_i-\\bar{Y})^2}}\\\\ &amp;=1-\\frac{RSS}{TSS}=1-\\frac{\\sum{(Y_i-\\hat{Y_i})^2}}{\\sum{(Y_i-\\bar{Y})^2}} \\end{align} \\] In practice, always interpret \\(R^2\\) alongside residual diagnostics. A model can have a decent \\(R^2\\) but still violate key assumptions (e.g., heteroscedasticity). 6.1.10 Test of regression coefficients Hypothesis testing in regression typically focuses on whether coefficients differ from zero (or another clinically meaningful value). Under classical assumptions, coefficients are normally distributed around their true values. since \\[ \\begin{align} \\hat{\\beta_2} &amp;\\sim N(\\beta_2,\\sigma^2_{\\hat{\\beta_2}}) \\\\ \\hat{\\beta_1} &amp;\\sim N(\\beta_1,\\sigma^2_{\\hat{\\beta_1}}) \\end{align} \\] and \\[ \\begin{align} S_{\\hat{\\beta}_2} &amp;=\\sqrt{\\frac{1}{\\sum{x_i^2}}}\\cdot\\hat{\\sigma} \\\\ S_{\\hat{\\beta}_1} &amp;=\\sqrt{\\frac{\\sum{X_i^2}}{n\\sum{x_i^2}}}\\cdot\\hat{\\sigma} \\end{align} \\] therefore \\[ \\begin{align} t_{\\hat{\\beta_2}}^{\\ast}&amp;=\\frac{\\hat{\\beta_2}-\\beta_2}{S_{\\hat{\\beta_2}}} =\\frac{\\hat{\\beta_2}}{S_{\\hat{\\beta_2}}} =\\frac{\\hat{\\beta_2}}{\\sqrt{\\frac{1}{\\sum{x_i^2}}}\\cdot\\hat{\\sigma}} \\sim t(n-2) \\\\ t_{\\hat{\\beta_1}}^{\\ast}&amp;=\\frac{\\hat{\\beta_1}-\\beta_1}{S_{\\hat{\\beta_1}}} =\\frac{\\hat{\\beta_1}}{S_{\\hat{\\beta_1}}} =\\frac{\\hat{\\beta_1}}{\\sqrt{\\frac{\\sum{X_i^2}}{n\\sum{x_i^2}}}\\cdot\\hat{\\sigma}} \\sim t(n-2) \\end{align} \\] In applied reporting, the slope test is often the primary focus, because it corresponds to whether \\(X\\) is associated with \\(Y\\) in a linear trend. 6.1.11 Statistical test of model Beyond individual coefficients, we may want to test whether the model as a whole explains a statistically significant amount of variability compared with a null model. since \\[ \\begin{align} Y_i&amp;\\sim i.i.d \\ N(\\beta_1+\\beta_2X_i,\\sigma^2)\\\\ \\end{align} \\] and \\[ \\begin{align} ESS&amp;=\\sum{(\\hat{Y_i}-\\bar{Y})^2} \\sim \\chi^2(df_{ESS}) \\\\ RSS&amp;=\\sum{(Y_i-\\hat{Y_i})^2} \\sim \\chi^2(df_{RSS}) \\end{align} \\] therefore \\[ \\begin{align} F^{\\ast}&amp;=\\frac{ESS/df_{ESS}}{RSS/df_{RSS}}=\\frac{MSS_{ESS}}{MSS_{RSS}}\\\\ &amp;=\\frac{\\sum{(\\hat{Y_i}-\\bar{Y})^2}/df_{ESS}}{\\sum{(Y_i-\\hat{Y_i})^2}/df_{RSS}} \\\\ &amp;=\\frac{\\hat{\\beta_2}^2\\sum{x_i^2}}{\\sum{e_i^2}/{(n-2)}}\\\\ &amp;=\\frac{\\hat{\\beta_2}^2\\sum{x_i^2}}{\\hat{\\sigma}^2} \\end{align} \\] The F test evaluates whether the explained variation (ESS) is large relative to residual variation (RSS), after accounting for degrees of freedom. 6.1.12 Mean prediction Prediction in regression has two common targets: Mean prediction: the expected outcome for individuals with a given \\(X_0\\). Individual prediction: the outcome for a new single individual with \\(X_0\\). Mean prediction is more precise because it estimates an average, not a single future value. since \\[ \\begin{align} \\mu_{\\hat{Y}_0}&amp;=E(\\hat{Y}_0)\\\\ &amp;=E(\\hat{\\beta}_1+\\hat{\\beta}_2X_0)\\\\ &amp;=\\beta_1+\\beta_2X_0\\\\ &amp;=E(Y|X_0) \\end{align} \\] and \\[ \\begin{align} var(\\hat{Y}_0)&amp;=\\sigma^2_{\\hat{Y}_0}\\\\ &amp;=E(\\hat{\\beta}_1+\\hat{\\beta}_2X_0)\\\\ &amp;=\\sigma^2 \\left( \\frac{1}{n}+ \\frac{(X_0-\\bar{X})^2}{\\sum{x_i^2}} \\right) \\end{align} \\] therefore \\[ \\begin{align} \\hat{Y}_0&amp; \\sim N(\\mu_{\\hat{Y}_0},\\sigma^2_{\\hat{Y}_0})\\\\ \\hat{Y}_0&amp; \\sim N \\left(E(Y|X_0), \\sigma^2 \\left( \\frac{1}{n}+ \\frac{(X_0-\\bar{X})^2}{\\sum{x_i^2}} \\right) \\right) \\end{align} \\] then construct t statistic to estimate CI \\[ \\begin{align} t_{\\hat{Y}_0}&amp; =\\frac{\\hat{Y}_0-E(Y|X_0)}{S_{\\hat{Y}_0}} \\sim t(n-2) \\end{align} \\] \\[ \\begin{align} \\hat{Y}_0-t_{1-\\alpha/2}(n-2) \\cdot S_{\\hat{Y}_0} \\leq E(Y|X_0) \\leq \\hat{Y}_0+t_{1-\\alpha/2}(n-2) \\cdot S_{\\hat{Y}_0} \\end{align} \\] Interpretation: the CI here is for the mean response at \\(X_0\\). It is narrow when: - \\(n\\) is large, and - \\(X_0\\) is close to \\(\\bar{X}\\) (more information near the center of the data). 6.1.13 Individual prediction Individual prediction intervals are wider because they must account for: - uncertainty in estimating the mean trend, and - the irreducible random error for a new observation. since \\[ \\begin{align} (Y_0-\\hat{Y}_0)&amp; \\sim N \\left(\\mu_{(Y_0-\\hat{Y}_0)},\\sigma^2_{(Y_0-\\hat{Y}_0)} \\right)\\\\ (Y_0-\\hat{Y}_0)&amp; \\sim N \\left(0, \\sigma^2 \\left(1+ \\frac{1}{n}+ \\frac{(X_0-\\bar{X})^2}{\\sum{x_i^2}} \\right) \\right) \\end{align} \\] and Construct t statistic \\[ \\begin{align} t_{\\hat{Y}_0}&amp; =\\frac{\\hat{Y}_0-E(Y|X_0)}{S_{\\hat{Y}_0}} \\sim t(n-2) \\end{align} \\] and \\[ \\begin{align} S_{\\hat{Y}_0}&amp; = \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n}+ \\frac{(X_0-\\bar{X})^2}{\\sum{x_i^2}} \\right)} \\end{align} \\] therefore \\[ \\begin{align} \\hat{Y}_0-t_{1-\\alpha/2}(n-2) \\cdot S_{\\hat{Y}_0} \\leq E(Y|X_0) \\leq \\hat{Y}_0+t_{1-\\alpha/2}(n-2) \\cdot S_{\\hat{Y}_0} \\end{align} \\] it is harder to predict your weight based on your age than to predict the mean weight of people who are your age. so, the interval of individual prediction is wider than those of mean prediction. A practical mental model: - Mean CI answers: “What is the expected mean outcome at \\(X_0\\)?” - Prediction interval answers: “Where might a new individual outcome fall at \\(X_0\\)?” 6.2 Multiple linear regression Multiple linear regression generalizes the simple model by allowing multiple predictors. The key shift is that each coefficient is interpreted as an effect holding other variables constant. 6.2.1 Matrix format \\[ \\begin{align} Y_i&amp;=\\beta_1+\\beta_2X_{2i}+\\beta_3X_{3i}+\\cdots+\\beta_kX_{ki}+u_i &amp;&amp; \\ \\end{align} \\] \\[ \\begin{equation} \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\cdots \\\\ Y_n \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; X_{21} &amp; X_{31} &amp; \\cdots &amp; X_{k1} \\\\ 1 &amp; X_{22} &amp; X_{32} &amp; \\cdots &amp; X_{k2} \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ 1 &amp; X_{2n} &amp; X_{3n} &amp; \\cdots &amp; X_{kn} \\end{bmatrix} \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\\\ \\end{bmatrix}+ \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\\\ \\end{bmatrix} \\end{equation} \\] \\[ \\begin{alignat}{4} \\mathbf{y} &amp;= &amp;\\mathbf{X}&amp;\\mathbf{\\beta}&amp;+&amp;\\mathbf{u} \\\\ (n \\times 1) &amp; &amp;{(n \\times k)} &amp;{(k \\times 1)}&amp;+&amp;{(n \\times 1)} \\end{alignat} \\] The matrix form is not just notation—it simplifies derivations and makes the estimator compact and general. 6.2.2 Variance covariance matrix of random errors The classical assumption is that errors are independent, have equal variance, and have zero covariance. This leads to a diagonal variance-covariance structure proportional to the identity matrix. because \\[ \\mathbf{u} \\sim N(\\mathbf{0},\\sigma^2\\mathbf{I})\\text{ population}\\\\ \\mathbf{e} \\sim N(\\mathbf{0},\\sigma^2\\mathbf{I})\\text{ sample}\\ \\] therefore \\[ \\begin{align} var-cov(\\mathbf{u})&amp;=E(\\mathbf{uu&#39;})\\\\ &amp;= \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12}^2 &amp;\\cdots &amp;\\sigma_{1n}^2\\\\ \\sigma_{21}^2 &amp; \\sigma_2^2 &amp;\\cdots &amp;\\sigma_{2n}^2\\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp;\\vdots \\\\ \\sigma_{n1}^2 &amp; \\sigma_{n2}^2 &amp;\\cdots &amp;\\sigma_n^2\\\\ \\end{bmatrix} &amp;&amp; \\leftarrow (E{(u_i)}=0)\\\\ &amp;= \\begin{bmatrix} \\sigma^2 &amp; \\sigma_{12}^2 &amp;\\cdots &amp;\\sigma_{1n}^2\\\\ \\sigma_{21}^2 &amp; \\sigma^2 &amp;\\cdots &amp;\\sigma_{2n}^2\\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp;\\vdots \\\\ \\sigma_{n1}^2 &amp; \\sigma_{n2}^2 &amp;\\cdots &amp;\\sigma^2\\\\ \\end{bmatrix} &amp;&amp; \\leftarrow (var{(u_i)}=\\sigma^2)\\\\ &amp;= \\begin{bmatrix} \\sigma^2 &amp; 0 &amp;\\cdots &amp;0\\\\ 0 &amp; \\sigma^2 &amp;\\cdots &amp;0\\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp;\\vdots \\\\ 0 &amp; 0 &amp;\\cdots &amp;\\sigma^2\\\\ \\end{bmatrix} &amp;&amp; \\leftarrow (cov{(u_i,u_j)}=0,i \\neq j)\\\\ &amp;=\\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp;\\cdots &amp;0\\\\ 0 &amp; 1 &amp;\\cdots &amp;0\\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp;\\vdots \\\\ 0 &amp; 0 &amp;\\cdots &amp;1\\\\ \\end{bmatrix}\\\\ &amp;=\\sigma^2\\mathbf{I} \\end{align} \\] In applied work, this assumption is often challenged by clustering and repeated measures. When violated, analysts may move to robust standard errors, GLS, or mixed effects models. 6.2.3 Minimize \\(Q=\\sum (y-\\hat{y})^2\\) In matrix form, OLS still minimizes the squared residuals, but the algebra becomes compact and scalable. \\[ \\begin{align} Q&amp;=\\sum{e_i^2}\\\\ &amp;=\\mathbf{e&#39;e}\\\\ &amp;=\\mathbf{(y-X\\hat{\\beta})&#39;(y-X\\hat{\\beta})}\\\\ &amp;=\\mathbf{y&#39;y-2\\hat{\\beta}&#39;X&#39;y+\\hat{\\beta}&#39;X&#39;X\\hat{\\beta}} \\end{align} \\] 6.2.4 Solve \\(\\hat{\\beta}\\) by derivation Setting the derivative with respect to \\(\\hat{\\beta}\\) equal to zero yields the normal equations. Solving them gives the closed-form OLS estimator. (population=sample) \\[ \\begin{align} \\frac{\\partial Q}{\\partial \\mathbf{\\hat{\\beta}}}&amp;=0\\\\ \\frac{\\partial(\\mathbf{y&#39;y-2\\hat{\\beta}&#39;X&#39;y+\\hat{\\beta}&#39;X&#39;X\\hat{\\beta}})}{\\partial \\mathbf{\\hat{\\beta}}}&amp;=0\\\\ -2\\mathbf{X&#39;y}+2\\mathbf{X&#39;X\\hat{\\beta}}&amp;=0\\\\ -\\mathbf{X&#39;y}+\\mathbf{X&#39;X\\hat{\\beta}}&amp;=0\\\\ \\mathbf{X&#39;X\\hat{\\beta}} &amp;=\\mathbf{X&#39;y} \\end{align} \\] \\[ \\begin{align} \\mathbf{\\hat{\\beta}} &amp;=\\mathbf{(X&#39;X)^{-1}X&#39;y} \\end{align} \\] Interpretation: \\((X&#39;X)^{-1}\\) reflects the information in the design matrix. When predictors are highly correlated (multicollinearity), \\(X&#39;X\\) becomes nearly singular and coefficient estimates become unstable. 6.2.5 Solve \\(var\\text{-}cov(\\mathbf{\\hat{\\beta}})\\) The variance-covariance matrix of \\(\\hat{\\beta}\\) is the core object for inference in multiple regression. It contains: - variances of each coefficient (diagonal), - covariances between coefficients (off-diagonal). \\[ \\begin{align} var-cov(\\mathbf{\\hat{\\beta}}) &amp;=\\mathbf{E\\left( \\left(\\hat{\\beta}-E(\\hat{\\beta}) \\right) \\left( \\hat{\\beta}-E(\\hat{\\beta}) \\right )&#39; \\right)}\\\\ &amp;=\\mathbf{E\\left( \\left(\\hat{\\beta}-{\\beta} \\right) \\left( \\hat{\\beta}-\\beta \\right )&#39; \\right)} \\\\ &amp;=\\mathbf{E\\left( \\left((X&#39;X)^{-1}X&#39;u \\right) \\left( (X&#39;X)^{-1}X&#39;u \\right )&#39; \\right)} \\\\ &amp;=\\mathbf{E\\left( (X&#39;X)^{-1}X&#39;uu&#39;X(X&#39;X)^{-1} \\right)} \\\\ &amp;= \\mathbf{(X&#39;X)^{-1}X&#39;E(uu&#39;)X(X&#39;X)^{-1}} \\\\ &amp;= \\mathbf{(X&#39;X)^{-1}X&#39;}\\sigma^2\\mathbf{IX(X&#39;X)^{-1}} \\\\ &amp;= \\sigma^2\\mathbf{(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1}} \\\\ &amp;= \\sigma^2\\mathbf{(X&#39;X)^{-1}} \\\\ \\end{align} \\] 6.2.6 Solve \\(S^2(\\mathbf{\\hat{\\beta}})\\) (sample) In practice, \\(\\sigma^2\\) is unknown. We estimate it from residuals and then plug it into the variance-covariance formula. where \\[ \\begin{align} \\hat{\\sigma}^2&amp;=\\frac{\\sum{e_i^2}}{n-k}=\\frac{\\mathbf{e&#39;e}}{n-k} \\\\ E(\\hat{\\sigma}^2)&amp;=\\sigma^2 \\end{align} \\] therefore \\[ \\begin{align} S^2_{ij}(\\mathbf{\\hat{\\beta}}) &amp;= \\hat{\\sigma}^2\\mathbf{(X&#39;X)^{-1}} \\\\ &amp;= \\frac{\\mathbf{e&#39;e}}{n-k}\\mathbf{(X&#39;X)^{-1}} \\\\ \\end{align} \\] which is variance-covariance of coefficients 6.2.7 Sum of squares decomposition (matrix format) The same TSS/ESS/RSS decomposition generalizes to multiple regression, forming the basis of \\(R^2\\) and the overall F test. \\[ \\begin{align} TSS&amp;=\\mathbf{y&#39;y}-n\\bar{Y}^2 \\\\ RSS&amp;=\\mathbf{ee&#39;}=\\mathbf{yy&#39;-\\hat{\\beta}&#39;X&#39;y} \\\\ ESS&amp;=\\mathbf{\\hat{\\beta}&#39;X&#39;y}-n\\bar{Y}^2 \\end{align} \\] 6.2.8 Determination coefficient \\(R^2\\) and goodness of fit \\[ \\begin{align} R^2&amp;=\\frac{ESS}{TSS}\\\\ &amp;=\\frac{\\mathbf{\\hat{\\beta}&#39;X&#39;y}-n\\bar{Y}^2}{\\mathbf{y&#39;y}-n\\bar{Y}^2} \\end{align} \\] Interpretation remains the same: \\(R^2\\) is descriptive goodness-of-fit. In multiple regression it almost always increases as you add predictors, which is why adjusted \\(R^2\\) or out-of-sample validation is often preferred for model selection. 6.2.9 Test of regression coefficients Multiple regression inference typically includes: - individual coefficient tests (is \\(\\beta_j=0\\)?), - joint tests (are multiple coefficients simultaneously zero?). because \\[ \\begin{align} \\mathbf{u}&amp;\\sim N(\\mathbf{0},\\sigma^2\\mathbf{I}) \\\\ \\mathbf{\\hat{\\beta}} &amp;\\sim N\\left(\\mathbf{\\beta},\\sigma^2\\mathbf{X&#39;X}^{-1} \\right) \\\\ \\end{align} \\] therefore (for all coefficients test, vector, see above \\(S_{\\hat{\\beta}}^2\\) ) \\[ \\begin{align} \\mathbf{t_{\\hat{\\beta}}}&amp;=\\mathbf{\\frac{\\hat{\\beta}-\\beta}{S_{\\hat{\\beta}}}} \\sim \\mathbf{t(n-k)} \\end{align} \\] (for individual coefficient test) \\[ \\begin{align} \\mathbf{t_{\\hat{\\beta}}^{\\ast}}&amp;=\\frac{\\mathbf{\\hat{\\beta}}}{\\mathbf{\\sqrt{S^2_{ij}(\\hat{\\beta}_{kk})}}} \\end{align} \\] where \\[ S^2_{ij}(\\hat{\\beta}_{kk})=[s^2_{\\hat{\\beta}_1},s^2_{\\hat{\\beta}_2},\\cdots,s^2_{\\hat{\\beta}_k}]&#39; \\] they are on diagonal line of the matrix of \\(S^2(\\mathbf{\\hat{\\beta}})\\) 6.2.10 Test of model The overall model test compares: - an unrestricted model with predictors, versus - a restricted model (often intercept-only). unrestricted model \\[ \\begin{align} u_i &amp;\\sim i.i.d \\ N(0,\\sigma^2)\\\\ Y_i&amp;\\sim i.i.d \\ N(\\beta_1+\\beta_2X_i+\\cdots+\\beta_kX_i,\\sigma^2)\\\\ RSS_U&amp;=\\sum{(Y_i-\\hat{Y_i})^2} \\sim \\chi^2(n-k) \\\\ \\end{align} \\] restricted model \\[ \\begin{align} u_i &amp;\\sim i.i.d \\ N(0,\\sigma^2)\\\\ Y_i&amp;\\sim i.i.d \\ N(\\beta_1,\\sigma^2)\\\\ RSS_R&amp;=\\sum{(Y_i-\\hat{Y_i})^2} \\sim \\chi^2(n-1) \\\\ \\end{align} \\] F test \\[ \\begin{align} F^{\\ast}&amp;=\\frac{(RSS_R-RSS_U)/(k-1)}{RSS_U/(n-k)} \\\\ &amp;=\\frac{ESS_U/df_{ESS_U}}{RSS_U/df_{RSS_U}} \\\\ &amp;\\sim F(df_{ESS_U},df_{RSS_U}) \\end{align} \\] \\[ \\begin{align} F^{\\ast}&amp;=\\frac{ESS_U/df_{ESS_U}}{RSS_U/df_{RSS_U}} =\\frac{\\left(\\mathbf{\\hat{\\beta}&#39;X&#39;y}-n\\bar{Y}^2 \\right)/{(k-1)}}{\\left(\\mathbf{yy&#39;-\\hat{\\beta}&#39;X&#39;y}\\right)/{(n-k)}} \\end{align} \\] 6.2.11 Mean prediction (multiple regression) The mean prediction generalizes naturally: you plug in a covariate vector \\(X_0\\). The uncertainty depends on the leverage of \\(X_0\\) through \\(X_0(X&#39;X)^{-1}X_0&#39;\\). since \\[ \\begin{align} E(\\hat{Y}_0)&amp;=E\\mathbf{(X_0\\hat{\\beta})}=\\mathbf{X_0\\beta}=E\\mathbf{(Y_0)}\\\\ var(\\hat{Y}_0)&amp;=E\\mathbf{(X_0\\hat{\\beta}-X_0\\beta)}^2\\\\ &amp;=E\\mathbf{\\left( X_0(\\hat{\\beta}-\\beta)(\\hat{\\beta}-\\beta)&#39;X_0&#39; \\right)}\\\\ &amp;=E\\mathbf{X_0\\left( (\\hat{\\beta}-\\beta)(\\hat{\\beta}-\\beta)&#39; \\right)X_0&#39;}\\\\ &amp;=\\sigma^2\\mathbf{X_0\\left( X&#39;X \\right)^{-1}X_0&#39;}\\\\ \\end{align} \\] and \\[ \\begin{align} \\hat{Y}_0&amp; \\sim N(\\mu_{\\hat{Y}_0},\\sigma^2_{\\hat{Y}_0})\\\\ \\hat{Y}_0&amp; \\sim N\\left(E(Y_0|X_0), \\sigma^2\\mathbf{X_0(X&#39;X)^{-1}X_0&#39;}\\right) \\end{align} \\] construct t statistic \\[ \\begin{align} t_{\\hat{Y}_0}&amp; =\\frac{\\hat{Y}_0-E(Y|X_0)}{S_{\\hat{Y}_0}} &amp;\\sim t(n-k) \\end{align} \\] therefore \\[ \\begin{align} \\hat{Y}_0-t_{1-\\alpha/2}(n-2) \\cdot S_{\\hat{Y}_0} \\leq E(Y|X_0) \\leq \\hat{Y}_0+t_{1-\\alpha/2}(n-2) \\cdot S_{\\hat{Y}_0} \\end{align} \\] where \\[ \\begin{align} \\mathbf{S_{\\hat{Y}_0}} &amp;=\\sqrt{\\hat{\\sigma}^2X_0(X&#39;X)^{-1}X_0&#39;} \\\\ \\hat{\\sigma}^2&amp;=\\frac{\\mathbf{ee&#39;}}{(n-k)} \\end{align} \\] 6.2.12 Individual prediction (multiple regression) Individual prediction adds the irreducible error term for a new observation, making the interval wider than the mean-response interval. since \\[ \\begin{align} e_0&amp;=Y_0-\\hat{Y}_0 \\end{align} \\] and \\[ \\begin{align} E(e_0)&amp;=E(Y_0-\\hat{Y}_0)\\\\ &amp;=E(\\mathbf{X_0\\beta}+u_0-\\mathbf{X_0\\hat{\\beta}})\\\\ &amp;=E\\left(u_0-\\mathbf{X_0 (\\hat{\\beta}- \\beta)} \\right)\\\\ &amp;=E\\left(u_0-\\mathbf{X_0 (X&#39;X)^{-1}X&#39;u} \\right)\\\\ &amp;=0 \\end{align} \\] \\[ \\begin{align} var(e_0)&amp;=E(Y_0-\\hat{Y}_0)^2\\\\ &amp;=E(e_0^2)\\\\ &amp;=E\\left(u_0-\\mathbf{X_0 (X&#39;X)^{-1}X&#39;u} \\right)^2\\\\ &amp;=\\sigma^2\\left( 1+ \\mathbf{X_0(X&#39;X)^{-1}X_0&#39;}\\right) \\end{align} \\] and \\[ \\begin{align} e_0&amp; \\sim N(\\mu_{e_0},\\sigma^2_{e_0})\\\\ e_0&amp; \\sim N\\left(0, \\sigma^2\\left(1+\\mathbf{X_0(X&#39;X)^{-1}X_0&#39;}\\right)\\right) \\end{align} \\] construct a t statistic \\[ \\begin{align} t_{e_0}&amp; =\\frac{\\hat{Y}_0-Y_0}{S_{e_0}} \\sim t(n-k) \\end{align} \\] therefore \\[ \\begin{align} \\hat{Y}_0-t_{1-\\alpha/2}(n-2) \\cdot S_{Y_0-\\hat{Y}_0} \\leq (Y_0|X_0) \\leq \\hat{Y}_0+t_{1-\\alpha/2}(n-2) \\cdot S_{Y_0-\\hat{Y}_0} \\end{align} \\] where \\[ \\begin{align} S_{Y_0-\\hat{Y}_0}=S_{e_0} &amp;=\\sqrt{\\hat{\\sigma}^2 \\left( 1+X_0(X&#39;X)^{-1}X_0&#39; \\right) } \\\\ \\hat{\\sigma}^2&amp;=\\frac{\\mathbf{ee&#39;}}{(n-k)} \\end{align} \\] A final practical takeaway: - If your goal is decision-making about the average response at a covariate profile, use mean-response inference. - If your goal is forecasting an individual outcome, expect much wider uncertainty bands, even with a well-fitted model. 6.3 Multiple linear regression practice This section is a hands-on workflow for multiple linear regression using the classic Boston housing dataset. The goal is not only to fit a model, but to practice the full sequence that a statistician typically follows in real projects: understand the dataset and variable types, check missingness and data quality, explore distributions and correlations, consider transformations for modeling stability, build training/test splits for honest evaluation, fit models (baseline, stepwise, polynomial, interaction, robust), check assumptions and diagnostics, compare models using information criteria and cross-validation, interpret coefficients and relative importance, perform prediction and validate performance. Even though this example is not a clinical dataset, the workflow is transferable to many applied settings. 6.3.1 Load required packages We start by loading packages that support regression modeling, data exploration, and diagnostics: MASS: contains the Boston dataset and robust regression (rlm) psych: descriptive summaries and EDA panels car: regression utilities like VIF library(car) library(MASS) library(psych) 6.3.2 Loading and describing data We load the dataset and create a working copy. In a real analysis, it is good practice to keep a pristine “original” object and perform transformations on copies. describe() provides a compact summary: N, mean, SD, min/max, and distribution hints for each variable. This is often more informative than simply printing the first few rows. data(Boston) data_ori &lt;- Boston describe(data_ori) ## data_ori ## ## 14 Variables 506 Observations ## -------------------------------------------------------------------------------- ## crim ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 504 1 3.614 5.794 0.02791 0.03819 ## .25 .50 .75 .90 .95 ## 0.08204 0.25651 3.67708 10.75300 15.78915 ## ## lowest : 0.00632 0.00906 0.01096 0.01301 0.01311 ## highest: 45.7461 51.1358 67.9208 73.5341 88.9762 ## -------------------------------------------------------------------------------- ## zn ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 26 0.603 11.36 18.77 0.0 0.0 ## .25 .50 .75 .90 .95 ## 0.0 0.0 12.5 42.5 80.0 ## ## lowest : 0 12.5 17.5 18 20 , highest: 82.5 85 90 95 100 ## -------------------------------------------------------------------------------- ## indus ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 76 0.982 11.14 7.705 2.18 2.91 ## .25 .50 .75 .90 .95 ## 5.19 9.69 18.10 19.58 21.89 ## ## lowest : 0.46 0.74 1.21 1.22 1.25 , highest: 18.1 19.58 21.89 25.65 27.74 ## -------------------------------------------------------------------------------- ## chas ## n missing distinct Info Sum Mean Gmd ## 506 0 2 0.193 35 0.06917 0.129 ## ## -------------------------------------------------------------------------------- ## nox ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 81 1 0.5547 0.1295 0.4092 0.4270 ## .25 .50 .75 .90 .95 ## 0.4490 0.5380 0.6240 0.7130 0.7400 ## ## lowest : 0.385 0.389 0.392 0.394 0.398, highest: 0.713 0.718 0.74 0.77 0.871 ## -------------------------------------------------------------------------------- ## rm ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 446 1 6.285 0.7515 5.314 5.594 ## .25 .50 .75 .90 .95 ## 5.886 6.208 6.623 7.152 7.588 ## ## lowest : 3.561 3.863 4.138 4.368 4.519, highest: 8.375 8.398 8.704 8.725 8.78 ## -------------------------------------------------------------------------------- ## age ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 356 0.999 68.57 31.52 17.72 26.95 ## .25 .50 .75 .90 .95 ## 45.02 77.50 94.07 98.80 100.00 ## ## lowest : 2.9 6 6.2 6.5 6.6 , highest: 98.8 98.9 99.1 99.3 100 ## -------------------------------------------------------------------------------- ## dis ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 412 1 3.795 2.298 1.462 1.628 ## .25 .50 .75 .90 .95 ## 2.100 3.207 5.188 6.817 7.828 ## ## lowest : 1.1296 1.137 1.1691 1.1742 1.1781 ## highest: 9.2203 9.2229 10.5857 10.7103 12.1265 ## -------------------------------------------------------------------------------- ## rad ## n missing distinct Info Mean Gmd ## 506 0 9 0.959 9.549 8.518 ## ## Value 1 2 3 4 5 6 7 8 24 ## Frequency 20 24 38 110 115 26 17 24 132 ## Proportion 0.040 0.047 0.075 0.217 0.227 0.051 0.034 0.047 0.261 ## ## For the frequency table, variable is rounded to the nearest 0 ## -------------------------------------------------------------------------------- ## tax ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 66 0.981 408.2 181.7 222 233 ## .25 .50 .75 .90 .95 ## 279 330 666 666 666 ## ## lowest : 187 188 193 198 216, highest: 432 437 469 666 711 ## -------------------------------------------------------------------------------- ## ptratio ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 46 0.978 18.46 2.383 14.70 14.75 ## .25 .50 .75 .90 .95 ## 17.40 19.05 20.20 20.90 21.00 ## ## lowest : 12.6 13 13.6 14.4 14.7, highest: 20.9 21 21.1 21.2 22 ## -------------------------------------------------------------------------------- ## black ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 357 0.986 356.7 65.5 84.59 290.27 ## .25 .50 .75 .90 .95 ## 375.38 391.44 396.23 396.90 396.90 ## ## lowest : 0.32 2.52 2.6 3.5 3.65 , highest: 396.28 396.3 396.33 396.42 396.9 ## -------------------------------------------------------------------------------- ## lstat ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 455 1 12.65 7.881 3.708 4.680 ## .25 .50 .75 .90 .95 ## 6.950 11.360 16.955 23.035 26.808 ## ## lowest : 1.73 1.92 1.98 2.47 2.87 , highest: 34.37 34.41 34.77 36.98 37.97 ## -------------------------------------------------------------------------------- ## medv ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 229 1 22.53 9.778 10.20 12.75 ## .25 .50 .75 .90 .95 ## 17.02 21.20 25.00 34.80 43.40 ## ## lowest : 5 5.6 6.3 7 7.2 , highest: 46.7 48.3 48.5 48.8 50 ## -------------------------------------------------------------------------------- summary() is a base R quick scan: it gives min/median/mean/max for numeric variables and counts for factors. This is a standard first step to detect strange ranges or unrealistic values. summary(data_ori) crim zn indus chas nox rm age dis rad tax ptratio black lstat medv Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 Min. : 1.73 Min. : 5.00 1st Qu.: 0.08205 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 1st Qu.: 6.95 1st Qu.:17.02 Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 Median : 5.000 Median :330.0 Median :19.05 Median :391.44 Median :11.36 Median :21.20 Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 Mean :12.65 Mean :22.53 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 3rd Qu.:16.95 3rd Qu.:25.00 Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 Max. :37.97 Max. :50.00 6.3.3 Create table 1 A “Table 1” is a standard descriptive table for reporting baseline characteristics (clinical trials, observational studies, epidemiology, etc.). Here we create a Table 1 across all variables without grouping, mainly to practice the tool and check distributions. library(boot) library(table1) ## Warning: package &#39;table1&#39; was built under R version 4.4.3 table1(~ . , data=data_ori) Overall(N=506) crim Mean (SD) 3.61 (8.60) Median [Min, Max] 0.257 [0.00632, 89.0] zn Mean (SD) 11.4 (23.3) Median [Min, Max] 0 [0, 100] indus Mean (SD) 11.1 (6.86) Median [Min, Max] 9.69 [0.460, 27.7] chas Mean (SD) 0.0692 (0.254) Median [Min, Max] 0 [0, 1.00] nox Mean (SD) 0.555 (0.116) Median [Min, Max] 0.538 [0.385, 0.871] rm Mean (SD) 6.28 (0.703) Median [Min, Max] 6.21 [3.56, 8.78] age Mean (SD) 68.6 (28.1) Median [Min, Max] 77.5 [2.90, 100] dis Mean (SD) 3.80 (2.11) Median [Min, Max] 3.21 [1.13, 12.1] rad Mean (SD) 9.55 (8.71) Median [Min, Max] 5.00 [1.00, 24.0] tax Mean (SD) 408 (169) Median [Min, Max] 330 [187, 711] ptratio Mean (SD) 18.5 (2.16) Median [Min, Max] 19.1 [12.6, 22.0] black Mean (SD) 357 (91.3) Median [Min, Max] 391 [0.320, 397] lstat Mean (SD) 12.7 (7.14) Median [Min, Max] 11.4 [1.73, 38.0] medv Mean (SD) 22.5 (9.20) Median [Min, Max] 21.2 [5.00, 50.0] Practical note: in real reporting, Table 1 is usually stratified by a group (e.g., treatment arm), but the unstratified version is still useful as a data audit. 6.3.4 Missingness checking Before modeling, verify whether any variables have missing values and whether patterns exist. The Boston dataset is typically complete, but this step is included because real datasets almost never are. md.pattern() (from mice) shows missingness patterns and counts by variable. library(mice) ## Warning: package &#39;mice&#39; was built under R version 4.4.3 md.pattern(data_ori) ## /\\ /\\ ## { `---&#39; } ## { O O } ## ==&gt; V &lt;== No need for mice. This data set is completely observed. ## \\ \\|/ / ## `-----&#39; crim zn indus chas nox rm age dis rad tax ptratio black lstat medv 506 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 If missingness exists, you should decide whether it is: - MCAR (completely at random), - MAR (at random conditional on observed data), - MNAR (not at random). That decision influences the imputation strategy and the validity of downstream inference. 6.3.5 Exploratory data analysis Exploratory data analysis (EDA) is where you learn the “shape” of the data: correlations, nonlinear relationships, skewness, and potential outliers. 6.3.5.1 Correlation matrix and bivariate panels pairs.panels() is a powerful all-in-one diagnostic: - scatterplots for each pair, - correlations, - histograms (or density plots) on the diagonal. This often reveals multicollinearity and potential transformations needed. library(psych) pairs.panels(data_ori) 6.3.5.2 Histograms Histograms quickly show skewness, long tails, spikes, and potential coding problems (e.g., a continuous variable with only a few discrete values). library(DataExplorer) plot_histogram(data_ori) 6.3.6 Transformations Many predictors in real-world socioeconomic and biomedical data are skewed. Transformations can: - improve linearity, - stabilize variance, - reduce influence of extreme values, - make residuals closer to normal. Here you apply a set of transformations guided by a common heuristic: - log transforms for strictly positive skewed variables, - square root for moderately skewed, - reflections when needed to “flip” direction (as shown with age and black). The goal is not “making everything normal,” but making linear modeling assumptions more reasonable. library(tidyverse) data_trans = data_ori %&gt;% mutate(age= sqrt(max(age)+1-age), black= log10(max(black)+1-black), crim= log10(crim), dis= sqrt(dis) ) plot_histogram(data_trans) # pairs.panels(data2) ! How to transform data for normality. A practical note: transformations should be applied thoughtfully and documented clearly, especially if you need interpretability. For example, a log-transformed predictor means coefficients represent changes per multiplicative change in the original scale. 6.3.7 Check linearity between \\(y\\) and \\(x\\) Before fitting a multivariable model, it helps to check whether key predictors have roughly linear relationships with the outcome. Scatterplots can immediately show: - curvature (suggesting polynomial terms), - clusters (suggesting interactions or stratification), - heteroscedasticity (spread changes with \\(X\\)). attach(data_trans) plot(medv, rm) plot(medv,lstat) plot(medv,age) plot(medv, black) plot(medv,crim) In practice, the Boston housing dataset is known for strong relationships between medv and both lstat and rm, and these often show nonlinearity—motivating quadratic terms and interactions later. 6.3.8 Data imputation and normalization This workflow demonstrates KNN imputation using caret::preProcess(method = \"knnImpute\"). Even if the dataset has no missing values, this section is included because missingness is common in applied work. 6.3.8.1 For original “data” We store medv separately before preprocessing, then restore it afterward. This preserves the target variable while applying preprocessing to predictors. library(caret) # Create the knn imputation model on the training data y=data_ori$medv preProcess_missingdata_model &lt;- preProcess(data_ori , method=&#39;knnImpute&#39;) preProcess_missingdata_model ## Created from 506 samples and 14 variables ## ## Pre-processing: ## - centered (14) ## - ignored (0) ## - 5 nearest neighbor imputation (14) ## - scaled (14) Then we apply the model and verify missingness is resolved. anyNA() is a quick binary check; in larger projects you may also compute missing rates by column. # Use the imputation model to predict the values of missing data points library(RANN) # required for knnInpute ## Warning: package &#39;RANN&#39; was built under R version 4.4.3 data_ori &lt;- predict(preProcess_missingdata_model, newdata = data_ori ) anyNA(data_ori ) ## [1] FALSE data_ori$medv &lt;- y 6.3.8.2 For transformed “data2” We repeat the same workflow for the transformed dataset. This creates a fair comparison between “original scale” and “transformed scale” modeling pipelines. library(caret) y2=data_trans$medv # Create the knn imputation model on the training data preProcess_missingdata_model2 &lt;- preProcess(data_trans , method=&#39;knnImpute&#39;) preProcess_missingdata_model2 ## Created from 506 samples and 14 variables ## ## Pre-processing: ## - centered (14) ## - ignored (0) ## - 5 nearest neighbor imputation (14) ## - scaled (14) # Use the imputation model to predict the values of missing data points library(RANN) # required for knnInpute data_trans &lt;- predict(preProcess_missingdata_model2, newdata = data_trans ) anyNA(data_trans ) ## [1] FALSE data_trans$medv &lt;- y2 6.3.9 Generate dummy variables Categorical predictors must be encoded properly before modeling. In many workflows, converting variables to factor is enough because lm() automatically handles factors using dummy coding (with a reference level). also can do using as.factor function for predictors x In real projects, pay attention to: - reference group selection, - whether categories are sparse, - and whether encoding must be consistent across train/test. 6.3.10 Splitting data into training and test data A training/test split provides external-style evaluation: the model is fit on training data and assessed on held-out test data. This helps detect overfitting, especially when you: - try many model variants, - add polynomial terms, - include interactions, - or apply variable selection procedures. # Create the training and test datasets set.seed(123) # for original data # Step 1: Get row numbers for the training data trainRowNumbers &lt;- createDataPartition(data_ori$medv, p=0.8, list=FALSE) # Step 2: Create the training dataset data &lt;- data_ori[trainRowNumbers,] # Step 3: Create the test dataset testdata &lt;- data_ori[-trainRowNumbers,] # for transformed data # Step 1: Get row numbers for the training data trainRowNumbers2 &lt;- createDataPartition(data_trans$medv, p=0.8, list=FALSE) # Step 2: Create the training dataset data2 &lt;- data_trans[trainRowNumbers2,] # Step 3: Create the test dataset testdata2 &lt;- data_trans[-trainRowNumbers2,] Practical note: createDataPartition() creates balanced partitions with respect to the outcome distribution, which can be helpful when the outcome is skewed. 6.3.11 Step regression Stepwise regression is a common teaching tool and sometimes used in quick exploratory modeling. It iteratively adds/removes variables to optimize a criterion (typically AIC by default). However, in serious applied work, stepwise selection can be unstable and can inflate type I error if you treat the final p-values as if selection never happened. Treat it as a screening tool, and validate with cross-validation. model_o = lm( medv ~. , data=data2) step(model_o,direction = &quot;both&quot;) ## Start: AIC=1281.15 ## medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + ## tax + ptratio + black + lstat ## ## Df Sum of Sq RSS AIC ## - black 1 0.17 8847.0 1279.2 ## - age 1 7.07 8853.9 1279.5 ## - crim 1 14.36 8861.2 1279.8 ## - indus 1 24.08 8870.9 1280.3 ## &lt;none&gt; 8846.8 1281.2 ## - rad 1 103.22 8950.0 1283.9 ## - tax 1 156.33 9003.1 1286.3 ## - zn 1 198.34 9045.2 1288.2 ## - chas 1 251.31 9098.1 1290.5 ## - nox 1 692.00 9538.8 1309.8 ## - ptratio 1 840.04 9686.9 1316.1 ## - rm 1 965.90 9812.7 1321.3 ## - dis 1 1349.41 10196.2 1336.9 ## - lstat 1 2766.14 11613.0 1389.9 ## ## Step: AIC=1279.16 ## medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + ## tax + ptratio + lstat ## ## Df Sum of Sq RSS AIC ## - age 1 7.15 8854.1 1277.5 ## - crim 1 14.32 8861.3 1277.8 ## - indus 1 24.52 8871.5 1278.3 ## &lt;none&gt; 8847.0 1279.2 ## + black 1 0.17 8846.8 1281.2 ## - rad 1 103.72 8950.7 1281.9 ## - tax 1 157.40 9004.4 1284.3 ## - zn 1 198.20 9045.2 1286.2 ## - chas 1 251.25 9098.2 1288.6 ## - nox 1 695.37 9542.4 1308.0 ## - ptratio 1 850.76 9697.7 1314.5 ## - rm 1 966.99 9814.0 1319.4 ## - dis 1 1375.04 10222.0 1336.0 ## - lstat 1 2770.28 11617.3 1388.0 ## ## Step: AIC=1277.49 ## medv ~ crim + zn + indus + chas + nox + rm + dis + rad + tax + ## ptratio + lstat ## ## Df Sum of Sq RSS AIC ## - crim 1 18.36 8872.5 1276.3 ## - indus 1 25.56 8879.7 1276.7 ## &lt;none&gt; 8854.1 1277.5 ## + age 1 7.15 8847.0 1279.2 ## + black 1 0.26 8853.9 1279.5 ## - rad 1 97.20 8951.3 1279.9 ## - tax 1 152.93 9007.1 1282.5 ## - zn 1 196.76 9050.9 1284.4 ## - chas 1 255.17 9109.3 1287.0 ## - nox 1 694.68 9548.8 1306.2 ## - ptratio 1 843.66 9697.8 1312.5 ## - rm 1 1023.40 9877.5 1320.0 ## - dis 1 1633.60 10487.7 1344.4 ## - lstat 1 2978.57 11832.7 1393.5 ## ## Step: AIC=1276.33 ## medv ~ zn + indus + chas + nox + rm + dis + rad + tax + ptratio + ## lstat ## ## Df Sum of Sq RSS AIC ## - indus 1 21.92 8894.4 1275.3 ## &lt;none&gt; 8872.5 1276.3 ## + crim 1 18.36 8854.1 1277.5 ## + age 1 11.19 8861.3 1277.8 ## + black 1 0.13 8872.4 1278.3 ## - tax 1 158.39 9030.9 1281.5 ## - zn 1 180.19 9052.7 1282.5 ## - rad 1 212.19 9084.7 1284.0 ## - chas 1 249.50 9122.0 1285.6 ## - nox 1 689.33 9561.8 1304.8 ## - ptratio 1 873.78 9746.3 1312.6 ## - rm 1 1025.43 9897.9 1318.8 ## - dis 1 1701.37 10573.9 1345.7 ## - lstat 1 2996.77 11869.3 1392.8 ## ## Step: AIC=1275.34 ## medv ~ zn + chas + nox + rm + dis + rad + tax + ptratio + lstat ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 8894.4 1275.3 ## + indus 1 21.92 8872.5 1276.3 ## + crim 1 14.72 8879.7 1276.7 ## + age 1 11.89 8882.5 1276.8 ## + black 1 0.00 8894.4 1277.3 ## - zn 1 206.52 9100.9 1282.7 ## - chas 1 237.50 9131.9 1284.1 ## - tax 1 281.42 9175.8 1286.0 ## - rad 1 293.27 9187.7 1286.5 ## - nox 1 800.54 9695.0 1308.4 ## - ptratio 1 929.05 9823.5 1313.8 ## - rm 1 1083.34 9977.8 1320.1 ## - dis 1 1706.94 10601.4 1344.8 ## - lstat 1 3024.07 11918.5 1392.5 ## ## Call: ## lm(formula = medv ~ zn + chas + nox + rm + dis + rad + tax + ## ptratio + lstat, data = data2) ## ## Coefficients: ## (Intercept) zn chas nox rm dis ## 22.509 1.016 0.759 -2.764 2.176 -3.970 ## rad tax ptratio lstat ## 2.119 -2.317 -1.991 -4.224 # summary(step(model_o,direction = &quot;both&quot;)) 6.3.12 Create a model after selecting variables After variable selection (or based on domain knowledge), we fit a parsimonious model and inspect its statistical summary. The summary() output gives: - coefficient estimates, - standard errors and t-tests, - residual standard error, - \\(R^2\\) and adjusted \\(R^2\\), - overall F-statistic. model_trasf &lt;- lm(formula = medv ~ zn + chas + nox + rm + dis + rad + tax + ptratio + lstat, data = data2) summary(model_trasf) ## ## Call: ## lm(formula = medv ~ zn + chas + nox + rm + dis + rad + tax + ## ptratio + lstat, data = data2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.2923 -2.4690 -0.5086 1.6269 24.5813 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.5090 0.2352 95.718 &lt; 2e-16 *** ## zn 1.0161 0.3347 3.036 0.002554 ** ## chas 0.7590 0.2331 3.256 0.001227 ** ## nox -2.7640 0.4624 -5.978 5.05e-09 *** ## rm 2.1760 0.3129 6.954 1.47e-11 *** ## dis -3.9697 0.4548 -8.729 &lt; 2e-16 *** ## rad 2.1194 0.5858 3.618 0.000335 *** ## tax -2.3171 0.6538 -3.544 0.000441 *** ## ptratio -1.9909 0.3092 -6.440 3.47e-10 *** ## lstat -4.2244 0.3636 -11.618 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.733 on 397 degrees of freedom ## Multiple R-squared: 0.7187, Adjusted R-squared: 0.7123 ## F-statistic: 112.7 on 9 and 397 DF, p-value: &lt; 2.2e-16 At this stage, interpretability matters: each coefficient reflects the effect of that predictor holding others constant, assuming linearity and correct specification. 6.3.13 Multicollinearity checking Multicollinearity inflates standard errors and makes coefficient estimates unstable. Variance inflation factor (VIF) is a standard diagnostic: - VIF ≈ 1: no collinearity - VIF moderately large: correlation among predictors - Very large VIF: serious instability vif(model_trasf) ## zn chas nox rm dis rad tax ptratio ## 2.024269 1.044796 4.074241 1.721080 3.759267 6.008751 7.469414 1.743359 ## lstat ## 2.372125 In practice, multicollinearity is common in socioeconomic variables and in biomedical lab panels. If VIF is high, consider: - removing redundant variables, - combining variables (indexes), - penalized regression methods (ridge/lasso). 6.3.14 Plot model to check assumptions The default plot(lm_object) produces key diagnostic plots: - residuals vs fitted (linearity, heteroscedasticity), - normal Q-Q (normality of residuals), - scale-location (variance stability), - residuals vs leverage (influential points). plot(model_trasf) 6.3.14.1 Histogram of residuals A quick distribution check; it complements the Q-Q plot. resid&lt;- model_trasf$residuals hist(resid) 6.3.14.2 F test of model ANOVA for the fitted model provides model-level significance and decomposition of sums of squares. anova(model_trasf) Df Sum Sq Mean Sq F value Pr(&gt;F) zn 1 4019.0308 4019.03076 179.38831 0e+00 chas 1 1011.5766 1011.57663 45.15144 0e+00 nox 1 2785.8086 2785.80857 124.34378 0e+00 rm 1 8487.0727 8487.07268 378.81810 0e+00 dis 1 951.7774 951.77743 42.48232 0e+00 rad 1 558.8550 558.85503 24.94434 9e-07 tax 1 767.8718 767.87176 34.27374 0e+00 ptratio 1 1119.3945 1119.39453 49.96386 0e+00 lstat 1 3024.0665 3024.06652 134.97836 0e+00 Residuals 397 8894.4215 22.40408 NA NA 6.3.14.3 Coefficients table This is the standard regression table with estimates, SEs, t-statistics, and p-values. coef(summary(model_trasf)) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 22.5089571 0.2351584 95.718273 0.0000000 zn 1.0160975 0.3346703 3.036115 0.0025545 chas 0.7589909 0.2331142 3.255876 0.0012275 nox -2.7639762 0.4623875 -5.977619 0.0000000 rm 2.1760130 0.3129267 6.953747 0.0000000 dis -3.9697228 0.4547944 -8.728609 0.0000000 rad 2.1193795 0.5857829 3.618029 0.0003352 tax -2.3171470 0.6537962 -3.544143 0.0004408 ptratio -1.9908948 0.3091666 -6.439552 0.0000000 lstat -4.2244121 0.3636086 -11.618019 0.0000000 6.3.14.4 Confidence intervals Confidence intervals help quantify uncertainty around effect sizes and are generally more informative than p-values alone. confint(model_trasf) 2.5 % 97.5 % (Intercept) 22.0466457 22.971269 zn 0.3581500 1.674045 chas 0.3006983 1.217283 nox -3.6730103 -1.854942 rm 1.5608125 2.791214 dis -4.8638292 -3.075616 rad 0.9677553 3.271004 tax -3.6024824 -1.031812 ptratio -2.5987032 -1.383086 lstat -4.9392512 -3.509573 6.3.15 Add polynomial (quadratic) terms When scatterplots suggest curvature, a quadratic term can capture nonlinearity without fully abandoning linear regression. Here we add \\(rm^2\\) and \\(lstat^2\\). This often improves fit when relationships are curved. model_trasf_poly &lt;- lm(formula = medv ~ zn + chas + nox + I(rm^2) + dis + rad + tax + ptratio + I(lstat^2), data = data2) summary(model_trasf_poly) ## ## Call: ## lm(formula = medv ~ zn + chas + nox + I(rm^2) + dis + rad + tax + ## ptratio + I(lstat^2), data = data2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.0736 -3.4029 -0.6212 2.8340 29.4942 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.2859 0.3526 63.212 &lt; 2e-16 *** ## zn 1.5524 0.4096 3.790 0.000174 *** ## chas 0.9570 0.2834 3.377 0.000805 *** ## nox -4.4962 0.5455 -8.242 2.50e-15 *** ## I(rm^2) 1.5313 0.1589 9.637 &lt; 2e-16 *** ## dis -3.3186 0.5644 -5.880 8.69e-09 *** ## rad 3.0529 0.7024 4.346 1.76e-05 *** ## tax -3.6647 0.7861 -4.662 4.28e-06 *** ## ptratio -2.6800 0.3706 -7.232 2.47e-12 *** ## I(lstat^2) -1.3170 0.1918 -6.865 2.56e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.763 on 397 degrees of freedom ## Multiple R-squared: 0.583, Adjusted R-squared: 0.5735 ## F-statistic: 61.67 on 9 and 397 DF, p-value: &lt; 2.2e-16 Practical note: polynomial terms can improve fit but may complicate interpretation. Always validate that the improvement generalizes (e.g., CV). 6.3.16 Add interaction terms Interactions allow the effect of one predictor to depend on another. Conceptually, they represent effect modification. Here we model the interaction \\(rm \\times lstat\\). This is a meaningful interaction in Boston housing: the benefit of more rooms may differ across neighborhood socioeconomic status proxies. rm and lstat R2 &gt;0.7 indicates a good fit of the model model_trasf_term &lt;- lm(formula = medv ~ zn + chas + nox + (rm* lstat) + dis + rad + tax + ptratio , data = data2) summary(model_trasf_term) ## ## Call: ## lm(formula = medv ~ zn + chas + nox + (rm * lstat) + dis + rad + ## tax + ptratio, data = data2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.4595 -2.3458 -0.2389 1.7950 26.6992 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.4297 0.2379 90.060 &lt; 2e-16 *** ## zn 0.5221 0.3046 1.714 0.087296 . ## chas 0.6538 0.2095 3.120 0.001941 ** ## nox -2.0295 0.4218 -4.812 2.13e-06 *** ## rm 1.6459 0.2860 5.754 1.75e-08 *** ## lstat -5.8715 0.3669 -16.002 &lt; 2e-16 *** ## dis -3.1810 0.4161 -7.645 1.60e-13 *** ## rad 1.9251 0.5262 3.658 0.000288 *** ## tax -1.9187 0.5883 -3.261 0.001205 ** ## ptratio -1.5554 0.2811 -5.534 5.70e-08 *** ## rm:lstat -1.8202 0.1852 -9.830 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.249 on 396 degrees of freedom ## Multiple R-squared: 0.7739, Adjusted R-squared: 0.7682 ## F-statistic: 135.5 on 10 and 396 DF, p-value: &lt; 2.2e-16 Diagnostic plots remain essential because adding interactions can create leverage points and change residual structure. plot(model_trasf_term) 6.3.17 Robust regression Outliers and influential points can dominate OLS. Robust regression (rlm) downweights extreme residuals and can produce more stable estimates. This is especially relevant when: - the dataset contains measurement errors, - there are heavy tails, - influential observations distort inference. robust_model_term &lt;- rlm(medv ~ zn + chas + nox + (rm* lstat) + dis + rad + tax + ptratio , data = data2) summary(robust_model_term) ## ## Call: rlm(formula = medv ~ zn + chas + nox + (rm * lstat) + dis + rad + ## tax + ptratio, data = data2) ## Residuals: ## Min 1Q Median 3Q Max ## -20.90999 -1.74873 -0.09845 1.92931 33.87244 ## ## Coefficients: ## Value Std. Error t value ## (Intercept) 20.9624 0.1656 126.5763 ## zn 0.1998 0.2120 0.9424 ## chas 0.5198 0.1458 3.5643 ## nox -1.3458 0.2935 -4.5846 ## rm 2.8773 0.1991 14.4524 ## lstat -4.5706 0.2554 -17.8979 ## dis -1.9208 0.2896 -6.6326 ## rad 0.9005 0.3663 2.4586 ## tax -1.7199 0.4095 -4.2004 ## ptratio -1.2451 0.1956 -6.3652 ## rm:lstat -1.9899 0.1289 -15.4398 ## ## Residual standard error: 2.698 on 396 degrees of freedom Practical note: robust regression changes the objective function and standard inference is different. It’s often used for sensitivity analysis rather than as the sole primary model. 6.3.18 Create a model before transforming data To understand the impact of transformation, we fit the analogous model on the original (non-transformed) training dataset. This helps assess whether transformation improves: - fit, - residual behavior, - predictive performance. model_trasf_orig &lt;- lm(formula = medv ~ zn + chas + nox + rm + dis + rad + tax + ptratio + lstat, data = data) summary(model_trasf_orig) ## ## Call: ## lm(formula = medv ~ zn + chas + nox + rm + dis + rad + tax + ## ptratio + lstat, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.219 -2.729 -0.463 1.920 25.992 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.5094 0.2430 92.619 &lt; 2e-16 *** ## zn 0.8232 0.3754 2.193 0.028904 * ## chas 0.6582 0.2412 2.728 0.006652 ** ## nox -1.9351 0.4727 -4.093 5.15e-05 *** ## rm 2.3985 0.3128 7.668 1.36e-13 *** ## dis -2.9289 0.4565 -6.416 3.99e-10 *** ## rad 2.2109 0.6348 3.483 0.000551 *** ## tax -2.1880 0.6896 -3.173 0.001627 ** ## ptratio -2.0274 0.3254 -6.230 1.19e-09 *** ## lstat -4.3534 0.3844 -11.325 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.895 on 397 degrees of freedom ## Multiple R-squared: 0.7212, Adjusted R-squared: 0.7149 ## F-statistic: 114.1 on 9 and 397 DF, p-value: &lt; 2.2e-16 6.3.18.1 Non-nested model comparisons (AIC) AIC compares models by balancing fit and complexity. Lower AIC indicates better tradeoff (within the candidate set). AIC(model_trasf,model_trasf_orig) df AIC model_trasf 11 2432.353 model_trasf_orig 11 2459.780 6.3.19 K-fold cross validation Cross-validation provides a more direct estimate of predictive performance. It is especially important after model selection or when comparing multiple model families. Here we use 10-fold CV via DAAG::cv.glm(). # install.packages(&quot;DAAG&quot;) library(DAAG) ## Warning: package &#39;DAAG&#39; was built under R version 4.4.3 set.seed(123) model_trasf_term_cv &lt;- glm( medv ~ zn + chas + nox + (rm* lstat) + dis + rad + tax + ptratio , data = data2) cv.err &lt;- cv.glm(data2, model_trasf_term_cv, K = 10)$delta cv.err ## [1] 19.24588 19.15400 Interpretation: - The delta output typically includes raw and adjusted CV error estimates. - Compare CV errors across competing models; smaller values indicate better predictive performance. 6.3.20 Nonnest models comparisons Once you have multiple candidate models (original scale, transformed, polynomial, interaction), compare them side-by-side. AIC is one way; CV error is another. In applied practice, you often consider both: - AIC for model parsimony, - CV for predictive robustness. AIC(model_trasf_term,model_trasf,model_trasf_orig,model_trasf_poly) df AIC model_trasf_term 12 2345.492 model_trasf 11 2432.353 model_trasf_orig 11 2459.780 model_trasf_poly 11 2592.584 # interaction, transformation, original, polynomial by order (`data` has been normalized but not `data2`) 6.3.21 Posterior predictive / diagnostic checks Even after selecting a “best” model, the most important step is to verify assumptions and identify influential observations. performance::check_model() provides a comprehensive set of diagnostics in one call: - linearity, - homoscedasticity, - influential points, - collinearity, - normality of residuals. # install.packages(&quot;performance&quot;) library(performance) ## Warning: package &#39;performance&#39; was built under R version 4.4.3 check_model(model_trasf_term) In practice, if diagnostics are poor, consider: - transformations, - adding nonlinear terms, - robust SEs, - or moving to a more appropriate model class. 6.3.22 Forest plot for coefficients Coefficient plots help communicate results clearly. They emphasize effect size and uncertainty, not only p-values. library(sjPlot) plot_model(model_trasf_term, show.values = TRUE, value.offset = 0.4) 6.3.23 Relative Importance When predictors are correlated, raw coefficients are not always a good measure of “importance.” Relative importance methods attempt to quantify each predictor’s contribution to explained variance. Here, relaimpo is used with bootstrap resampling to assess stability. library(relaimpo) ## Warning: package &#39;relaimpo&#39; was built under R version 4.4.3 ## Warning: package &#39;survey&#39; was built under R version 4.4.3 ## Warning: package &#39;mitools&#39; was built under R version 4.4.3 # calc.relimp(fit,type=c(&quot;lmg&quot;,&quot;last&quot;,&quot;first&quot;,&quot;pratt&quot;), # rela=TRUE) # Bootstrap Measures of Relative Importance (1000 samples) boot &lt;- boot.relimp(model_trasf_term, b = 10, type =c(&quot;lmg&quot; ), rank = TRUE, # type =c(&quot;lmg&quot;,&quot;last&quot;,&quot;first&quot;,&quot;pratt&quot;) diff = TRUE, rela = TRUE) booteval.relimp(boot) # print result ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Response variable: medv ## Total response variance: 77.88147 ## Analysis based on 407 observations ## ## 10 Regressors: ## zn chas nox rm lstat dis rad tax ptratio rm:lstat ## Proportion of variance explained by model: 77.39% ## Metrics are normalized to sum to 100% (rela=TRUE). ## ## Relative importance metrics: ## ## lmg ## zn 0.03220843 ## chas 0.01925573 ## nox 0.05753222 ## rm 0.24841214 ## lstat 0.32582422 ## dis 0.04647715 ## rad 0.03216196 ## tax 0.05940953 ## ptratio 0.09003437 ## rm:lstat 0.08868423 ## ## Average coefficients for different model sizes: ## ## 1X 2Xs 3Xs 4Xs 5Xs 6Xs ## zn 3.1505046 2.1309586 1.5419108 1.1885764 0.9622432 0.8066737 ## chas 1.4317561 1.3828980 1.2922217 1.1827563 1.0710746 0.9666952 ## nox -3.6683140 -2.8590532 -2.3987148 -2.1635246 -2.0628637 -2.0336275 ## rm 5.8715576 5.1812470 4.6686609 4.2214027 3.7902531 3.3584729 ## lstat -6.3961998 -6.2413211 -6.1543204 -6.0962610 -6.0505503 -6.0111059 ## dis 2.3495853 0.6106749 -0.5986185 -1.4554886 -2.0655023 -2.4944977 ## rad -3.2090101 -1.5469225 -0.4505596 0.2850626 0.7884232 1.1408255 ## tax -4.1727995 -3.6061521 -3.1776467 -2.8413999 -2.5683300 -2.3434564 ## ptratio -4.2048171 -3.4233285 -2.9476391 -2.6199136 -2.3633992 -2.1469790 ## rm:lstat -0.7386853 -1.0072327 -1.1975350 -1.3430505 -1.4610987 -1.5602846 ## 7Xs 8Xs 9Xs 10Xs ## zn 0.6942682 0.6122971 0.5558327 0.5221292 ## chas 0.8734919 0.7916003 0.7191770 0.6537613 ## nox -2.0351844 -2.0434312 -2.0444974 -2.0294702 ## rm 2.9235837 2.4886854 2.0599352 1.6458985 ## lstat -5.9756419 -5.9424093 -5.9088868 -5.8714971 ## dis -2.7882468 -2.9825750 -3.1064663 -3.1809633 ## rad 1.3969994 1.5963587 1.7666711 1.9250667 ## tax -2.1625306 -2.0281734 -1.9455332 -1.9186629 ## ptratio -1.9610124 -1.8027959 -1.6694147 -1.5553973 ## rm:lstat -1.6447082 -1.7160362 -1.7745576 -1.8202278 ## ## ## Confidence interval information ( 10 bootstrap replicates, bty= perc ): ## Relative Contributions with confidence intervals: ## ## Lower Upper ## percentage 0.95 0.95 0.95 ## zn.lmg 0.0322 _______HIJ 0.0204 0.0357 ## chas.lmg 0.0193 ______GHIJ 0.0016 0.0501 ## nox.lmg 0.0575 ____EFG___ 0.0430 0.0708 ## rm.lmg 0.2484 AB________ 0.1916 0.3284 ## lstat.lmg 0.3258 AB________ 0.2617 0.3836 ## dis.lmg 0.0465 _____FGHIJ 0.0286 0.0548 ## rad.lmg 0.0322 ______GHIJ 0.0215 0.0465 ## tax.lmg 0.0594 ___DEF____ 0.0479 0.0796 ## ptratio.lmg 0.0900 __CD______ 0.0711 0.1202 ## rm:lstat.lmg 0.0887 __CDE_____ 0.0606 0.1037 ## ## Letters indicate the ranks covered by bootstrap CIs. ## (Rank bootstrap confidence intervals always obtained by percentile method) ## CAUTION: Bootstrap confidence intervals can be somewhat liberal. ## ## ## Differences between Relative Contributions: ## ## Lower Upper ## difference 0.95 0.95 0.95 ## zn-chas.lmg 0.0130 -0.0192 0.0276 ## zn-nox.lmg -0.0253 * -0.0504 -0.0137 ## zn-rm.lmg -0.2162 * -0.3028 -0.1559 ## zn-lstat.lmg -0.2936 * -0.3478 -0.2274 ## zn-dis.lmg -0.0143 -0.0235 0.0019 ## zn-rad.lmg 0.0000 -0.0221 0.0084 ## zn-tax.lmg -0.0272 * -0.0532 -0.0166 ## zn-ptratio.lmg -0.0578 * -0.0911 -0.0377 ## zn-rm:lstat.lmg -0.0565 * -0.0746 -0.0305 ## chas-nox.lmg -0.0383 * -0.0584 -0.0033 ## chas-rm.lmg -0.2292 * -0.3144 -0.1753 ## chas-lstat.lmg -0.3066 * -0.3672 -0.2117 ## chas-dis.lmg -0.0272 -0.0385 0.0137 ## chas-rad.lmg -0.0129 -0.0449 0.0201 ## chas-tax.lmg -0.0402 * -0.0781 -0.0008 ## chas-ptratio.lmg -0.0708 * -0.1187 -0.0260 ## chas-rm:lstat.lmg -0.0694 * -0.1021 -0.0338 ## nox-rm.lmg -0.1909 * -0.2781 -0.1378 ## nox-lstat.lmg -0.2683 * -0.3298 -0.2047 ## nox-dis.lmg 0.0111 -0.0010 0.0275 ## nox-rad.lmg 0.0254 * 0.0048 0.0288 ## nox-tax.lmg -0.0019 -0.0283 0.0061 ## nox-ptratio.lmg -0.0325 * -0.0689 -0.0130 ## nox-rm:lstat.lmg -0.0312 * -0.0525 -0.0008 ## rm-lstat.lmg -0.0774 -0.1920 0.0488 ## rm-dis.lmg 0.2019 * 0.1368 0.2922 ## rm-rad.lmg 0.2163 * 0.1599 0.3069 ## rm-tax.lmg 0.1890 * 0.1310 0.2806 ## rm-ptratio.lmg 0.1584 * 0.1065 0.2525 ## rm-rm:lstat.lmg 0.1597 * 0.1047 0.2605 ## lstat-dis.lmg 0.2793 * 0.2253 0.3288 ## lstat-rad.lmg 0.2937 * 0.2231 0.3519 ## lstat-tax.lmg 0.2664 * 0.1899 0.3229 ## lstat-ptratio.lmg 0.2358 * 0.1493 0.2985 ## lstat-rm:lstat.lmg 0.2371 * 0.1658 0.3018 ## dis-rad.lmg 0.0143 -0.0088 0.0231 ## dis-tax.lmg -0.0129 * -0.0419 -0.0043 ## dis-ptratio.lmg -0.0436 * -0.0825 -0.0303 ## dis-rm:lstat.lmg -0.0422 * -0.0660 -0.0071 ## rad-tax.lmg -0.0272 * -0.0331 -0.0205 ## rad-ptratio.lmg -0.0579 * -0.0737 -0.0367 ## rad-rm:lstat.lmg -0.0565 * -0.0714 -0.0240 ## tax-ptratio.lmg -0.0306 * -0.0462 -0.0123 ## tax-rm:lstat.lmg -0.0293 -0.0453 0.0050 ## ptratio-rm:lstat.lmg 0.0014 -0.0253 0.0346 ## ## * indicates that CI for difference does not include 0. ## CAUTION: Bootstrap confidence intervals can be somewhat liberal. plot(booteval.relimp(boot,sort=TRUE)) # plot result ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints ## Warning in norm.inter(t, alpha): extreme order statistics used as endpoints Practical note: bootstrap sample size b should be much larger (e.g., 500–2000) for stable inference; here it is small for demonstration. 6.3.24 Model prediction Prediction is where the model becomes operational. We generate: - prediction intervals (interval=\"predict\") for individual outcomes, - confidence intervals (interval=\"confidence\") for mean responses. First, create a predictor dataset. library(dplyr) data_pred &lt;- dplyr::select(data2 , zn , chas , nox , rm , dis , rad , tax , ptratio , lstat) data_pred[1:10,] zn chas nox rm dis rad tax ptratio lstat 1 0.2845483 -0.2723291 -0.1440749 0.4132629 0.2785465 -0.9818712 -0.6659492 -1.4575580 -1.0744990 2 -0.4872402 -0.2723291 -0.7395304 0.1940824 0.6786919 -0.8670245 -0.9863534 -0.3027945 -0.4919525 4 -0.4872402 -0.2723291 -0.8344581 1.0152978 1.1314532 -0.7521778 -1.1050216 0.1129203 -1.3601708 5 -0.4872402 -0.2723291 -0.8344581 1.2273620 1.1314532 -0.7521778 -1.1050216 0.1129203 -1.0254866 6 -0.4872402 -0.2723291 -0.8344581 0.2068916 1.1314532 -0.7521778 -1.1050216 0.1129203 -1.0422909 7 0.0487240 -0.2723291 -0.2648919 -0.3880270 0.9295961 -0.5224844 -0.5769480 -1.5037485 -0.0312367 8 0.0487240 -0.2723291 -0.2648919 -0.1603069 1.0872565 -0.5224844 -0.5769480 -1.5037485 0.9097999 9 0.0487240 -0.2723291 -0.2648919 -0.9302853 1.1392843 -0.5224844 -0.5769480 -1.5037485 2.4193794 10 0.0487240 -0.2723291 -0.2648919 -0.3994130 1.3357787 -0.5224844 -0.5769480 -1.5037485 0.6227277 11 0.0487240 -0.2723291 -0.2648919 0.1314594 1.2422167 -0.5224844 -0.5769480 -1.5037485 1.0918456 6.3.24.1 Prediction interval (individual prediction) Prediction intervals are wider because they include residual variability. predy &lt;- predict(model_trasf_term, data_pred[1:10,], interval=&quot;predict&quot;) predy fit lwr upr 1 30.258620 21.8534712 38.66377 2 24.415330 16.0263022 32.80436 4 31.759215 23.3073401 40.21109 5 29.920476 21.4789966 38.36196 6 26.441050 18.0254500 34.85665 7 20.820462 12.3963513 29.24457 8 15.455998 6.9807773 23.93122 9 8.991037 0.4048231 17.57725 10 16.144719 7.6745761 24.61486 11 13.847673 5.3305253 22.36482 6.3.24.2 Confidence interval (mean prediction) Confidence intervals for the mean response are narrower than prediction intervals. confy &lt;- predict(model_trasf_term, data_pred[1:10,], interval=&quot;confidence&quot;) confy fit lwr upr 1 30.258620 29.329961 31.18728 2 24.415330 23.646134 25.18452 4 31.759215 30.474666 33.04376 5 29.920476 28.706202 31.13475 6 26.441050 25.422125 27.45998 7 20.820462 19.733484 21.90744 8 15.455998 14.025877 16.88612 9 8.991037 7.006358 10.97572 10 16.144719 14.745003 17.54443 11 13.847673 12.187044 15.50830 6.3.24.3 CI width A quick way to compute interval width is upper minus lower bound. confy %*% c(0, -1, 1) 1 1.857318 2 1.538391 4 2.569097 5 2.428550 6 2.037850 7 2.173956 8 2.860241 9 3.969358 10 2.799431 11 3.321259 predy %*% c(0, -1, 1) 1 16.81030 2 16.77805 4 16.90375 5 16.88296 6 16.83120 7 16.84822 8 16.95044 9 17.17243 10 16.94029 11 17.03430 6.3.25 Compare predictions vs actual values A scatterplot of observed vs predicted values provides a quick sense of calibration. If the model is well-calibrated, points tend to align along the diagonal; the fitted line provides a rough check. plot(data2$medv,predict(model_trasf_term) ) fit &lt;- lm(predict(model_trasf_term)~data2$medv) abline(fit) 6.3.26 Manual computation: \\(\\hat{y}\\) and confidence interval This section demonstrates how the regression prediction and CI formulas relate to matrix algebra. It is useful for understanding what predict() does internally. We fit a smaller model and compute \\(\\hat{y}\\) using \\(X\\hat{\\beta}\\). data_ci &lt;- dplyr::select(data2, zn ,chas ,tax , medv) model_ci &lt;- lm(formula = medv ~ zn + chas +tax , data = data_ci) summary(model_ci) ## ## Call: ## lm(formula = medv ~ zn + chas + tax, data = data_ci) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.182 -4.715 -1.305 2.345 34.377 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.3073 0.3699 60.307 &lt; 2e-16 *** ## zn 2.1080 0.3919 5.379 1.27e-07 *** ## chas 1.4174 0.3599 3.939 9.66e-05 *** ## tax -3.4463 0.3985 -8.649 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.46 on 403 degrees of freedom ## Multiple R-squared: 0.2907, Adjusted R-squared: 0.2855 ## F-statistic: 55.07 on 3 and 403 DF, p-value: &lt; 2.2e-16 compute y hat and compare with y predict and actual y XCI &lt;- data.frame(intercept=1, data_ci[,1:3]) comp_y &lt;- as.matrix(XCI)%*%as.numeric(model_ci$coefficients) head(cbind(comp_y,predict(model_ci, XCI), data_ci[,4])) 1 24.81617 24.81617 24.0 2 24.29345 24.29345 21.6 4 24.70242 24.70242 33.4 5 24.70242 24.70242 36.2 6 24.70242 24.70242 28.7 7 24.01233 24.01233 22.9 compute ci library(matlib) ## Warning: package &#39;matlib&#39; was built under R version 4.4.3 ## ## Attaching package: &#39;matlib&#39; ## The following object is masked from &#39;package:psych&#39;: ## ## tr var.yhat &lt;- sigma(model_ci)**2* as.matrix(XCI[1 ,]) %*% inv(t(as.matrix(XCI)) %*% as.matrix (XCI))%*%t(as.matrix(XCI[1 ,])) # var.yhat cbind( (predict(model_ci, XCI[1 ,])-1.96 * sqrt(var.yhat)), (predict(model_ci, XCI[1 ,]) ), (predict(model_ci, XCI[1 ,])+1.96 * sqrt(var.yhat)) ) 1 1 23.91998 24.81617 25.71237 predict(model_ci, XCI[1 ,], interval=&quot;confidence&quot;) fit lwr upr 24.81617 23.9173 25.71505 \\[ E(\\hat{Y}_0)=\\sigma^2\\mathbf{X_0\\left( X&#39;X\\right)^{-1}X_0&#39;} \\] This matrix form highlights the concept of leverage: when \\(X_0\\) is far from the center of the predictor space, uncertainty increases. 6.3.27 External data validation This is a simple demonstration of external-style validation: evaluate performance on the held-out test set created earlier. We compute common predictive metrics: - \\(R^2\\) (proportion of variance explained), - RMSE (penalizes larger errors), - MAE (more robust to outliers). library(caret) testdata2_pred &lt;- dplyr::select(testdata2, zn , chas , nox , rm , dis , rad , tax , ptratio , lstat) R_sq &lt;- R2(testdata2$medv,predict(model_trasf_term,testdata2_pred)) RMSE &lt;- RMSE(testdata2$medv,predict(model_trasf_term,testdata2_pred)) MAE &lt;- MAE(testdata2$medv,predict(model_trasf_term,testdata2_pred)) print(c(R_sq, RMSE, MAE)) ## [1] 0.8465718 4.1960036 3.4352611 In applied practice, the held-out evaluation is often the most trusted summary of model usefulness, especially when many model variants were tried. 6.4 Variable selection Variable selection is a large topic. In practice, you should align the selection method with your goal: - inference (interpretability, prespecified covariates, stability), - prediction (regularization, cross-validation, performance focus). see here 6.4.1 Chapter takeaways This multiple regression practice section demonstrates a complete applied modeling workflow: - data understanding (describe/summary/Table 1), - quality checks (missingness), - EDA (histograms, correlations, scatterplots), - transformations (stabilize modeling assumptions), - model building (baseline, stepwise, polynomial, interaction, robust), - diagnostics (assumption checks), - comparison (AIC, CV), - interpretation (coefficients, CI, relative importance), - prediction and validation (intervals, test-set metrics). The same structure generalizes naturally to clinical and biomedical regression problems—only the outcome type and modeling family may change. 6.5 Linear mixed model theory Linear mixed models (LMMs) extend ordinary linear regression to handle correlated data. In practice, correlation arises when observations are clustered or repeated, for example: patients nested within hospitals, students nested within schools, repeated lab measurements within a subject, longitudinal follow-up over time. If we ignore this correlation and use ordinary regression, standard errors are often too small, confidence intervals become too narrow, and p-values may look more “significant” than they should. LMMs address this by modeling two sources of variation: fixed effects: population-average effects you want to estimate (e.g., treatment, age, sex) random effects: cluster- or subject-specific deviations that induce correlation (e.g., hospital-specific baseline stress) 6.5.1 Matrix format The mixed model is commonly written as: \\[ \\mathbf{y} = \\boldsymbol{X\\beta} + \\boldsymbol{Zu} + \\boldsymbol{\\varepsilon} \\] Interpretation of each component: \\(\\mathbf{y}\\): \\(N \\times 1\\) outcome vector \\(\\mathbf{X}\\): \\(N \\times r\\) design matrix for fixed effects \\(\\boldsymbol{\\beta}\\): \\(r \\times 1\\) coefficient vector for fixed effects \\(\\mathbf{Z}\\): \\(N \\times m\\) design matrix for random effects \\(\\boldsymbol{u}\\): \\(m \\times 1\\) random effect vector \\(\\boldsymbol{\\varepsilon}\\): \\(N \\times 1\\) residual vector So, \\(\\mathbf{X\\beta}\\) describes the systematic (population-average) part of the mean, while \\(\\mathbf{Zu}\\) describes cluster-specific departures from that mean. The dimension bookkeeping is worth emphasizing because it prevents many implementation errors: \\[ \\overbrace{\\mathbf{y}}^{N \\times 1}= \\overbrace{\\underbrace{\\mathbf{X}}_{N \\times r}\\underbrace{\\boldsymbol{\\beta}}_{r \\times 1}}^{N \\times 1} + \\overbrace{\\underbrace{\\mathbf{Z}}_{N \\times m}\\underbrace{\\boldsymbol{u}}_{m \\times 1}}^{N \\times 1} + \\overbrace{\\boldsymbol{\\varepsilon}}^{N \\times 1}. \\] 6.5.2 Why random effects create correlation A key conceptual point: observations within the same cluster share the same random effect(s). That shared term makes them correlated. For example, with a random intercept by hospital, \\[ Y_{ij} = \\beta_0 + u_{0j} + \\beta_1 X_{ij} + \\varepsilon_{ij}, \\] two nurses \\(i\\) and \\(i&#39;\\) in the same hospital \\(j\\) both include \\(u_{0j}\\), so they are correlated even if their residual errors \\(\\varepsilon\\) are independent. 6.5.3 Example: reducing the work stress of nurses Suppose you measure work stress from \\(N=1000\\) nurses across 25 hospitals. Let: outcome \\(y\\): stress score fixed effects \\(X\\): age, gender, experience, ward type, intervention random effect: hospital-level random intercept only The model is: \\[ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{u} + \\boldsymbol{\\varepsilon}, \\] with dimensions: \\[ \\overbrace{\\mathbf{y}}^{1000 \\times 1} = \\overbrace{\\underbrace{\\mathbf{X}}_{1000 \\times r}\\underbrace{\\boldsymbol{\\beta}}_{r \\times 1}}^{1000 \\times 1} + \\overbrace{\\underbrace{\\mathbf{Z}}_{1000 \\times 25}\\underbrace{\\boldsymbol{u}}_{25 \\times 1}}^{1000 \\times 1} + \\overbrace{\\boldsymbol{\\varepsilon}}^{1000 \\times 1}. \\] A practical interpretation of the random intercept: - each hospital has its own baseline stress level, - fixed effects estimate population-average effects after accounting for hospital differences. 6.5.4 The dependent variable \\(\\mathbf{y}\\) Your vector representation emphasizes that outcomes are stacked: \\[ \\mathbf{y} = \\left[ \\begin{array}{c} y_1\\\\ y_2\\\\ \\vdots\\\\ y_{1000} \\end{array} \\right]. \\] In practice, the ordering can be any consistent ordering (by hospital then nurse, or by nurse id then time), as long as \\(\\mathbf{X}\\) and \\(\\mathbf{Z}\\) align row-by-row. 6.5.5 Fixed effects design matrix \\(\\mathbf{X}\\) \\(\\mathbf{X}\\) contains covariates used to estimate the population-average mean function. An intercept column is usually included. Your example matrix is exactly what \\(\\mathbf{X}\\) is meant to represent: a structured layout of predictors aligned with each observation. Two practical reminders: Categorical variables (gender, ward type, intervention) are represented through indicator columns (dummy coding) once you fit the model in software. Centering/scaling continuous variables (age, experience) is often helpful, especially when random slopes or interactions are present. 6.5.6 Fixed effect coefficients \\(\\boldsymbol{\\hat{\\beta}}\\) \\(\\boldsymbol{\\beta}\\) answers questions like: “What is the average difference in stress between intervention and control, after adjusting for covariates and hospital clustering?” “How does stress change with age, on average, after controlling for hospital differences?” In LMMs, fixed effect interpretation is typically conditional on the random effects structure being correct, but the coefficients represent population-average contrasts in the linear predictor. 6.5.6.1 How \\(\\hat{\\beta}\\) is estimated (ML and REML) A clean way to explain estimation is to separate: - the mean model: \\(\\mathbf{X}\\beta\\) - the covariance model: \\(\\mathbf{V} = \\mathbf{ZGZ&#39;} + \\mathbf{R}\\) Once \\(\\mathbf{V}\\) is known, the generalized least squares estimator for \\(\\beta\\) is: \\[ \\hat{\\beta}(\\gamma)=\\left(\\mathbf{X}&#39;\\mathbf{V}(\\gamma)^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}&#39;\\mathbf{V}(\\gamma)^{-1}\\mathbf{y}, \\] where \\(\\gamma\\) represents covariance parameters (e.g., \\(\\sigma^2\\), \\(\\sigma_u^2\\), correlations). This highlights the logic: - ordinary least squares weights by \\((X&#39;X)^{-1}\\), - mixed models weight by \\((X&#39;V^{-1}X)^{-1}\\) because observations are correlated. ML vs REML (intuition): - ML estimates \\(\\beta\\) and variance parameters together by maximizing the likelihood of \\(\\mathbf{y}\\). - REML maximizes a likelihood of linear combinations of \\(\\mathbf{y}\\) that remove fixed effects, helping reduce small-sample bias in variance components. Your “three-step” profile approach is the standard computational idea: 1) write \\(\\hat{\\beta}\\) as a function of variance parameters, 2) optimize the likelihood over variance parameters, 3) plug back to obtain \\(\\hat{\\beta}\\). 6.5.7 Variance–covariance matrix of \\(\\hat{\\beta}\\) Once variance parameters are estimated, the approximate sampling distribution is: \\[ \\hat{\\beta} \\sim \\mathcal{N}\\left(\\beta,\\; \\mathrm{Var}(\\hat{\\beta})\\right), \\quad \\mathrm{Var}(\\hat{\\beta})=\\left(\\mathbf{X}&#39;\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}. \\] This matrix is the engine behind: - standard errors, - Wald tests, - confidence intervals for fixed effects. A practical note: in many software outputs, the reported degrees of freedom for fixed effects tests are not simply \\(N-r\\), because correlation and random effects reduce effective information. Approximations like Satterthwaite/Kenward–Roger are widely used. 6.5.8 Random effects design matrix \\(\\mathbf{Z}\\) Your \\(\\mathbf{Z}\\) matrix is a clean illustration of a random intercept by hospital: each row has a “1” in the column corresponding to that nurse’s hospital, zeros elsewhere. This structure ensures every observation in hospital \\(j\\) shares the same \\(u_j\\). If you add random slopes (say, random slope of time within hospital), \\(\\mathbf{Z}\\) gets additional columns corresponding to those slope terms, and \\(\\mathbf{G}\\) becomes a block structure rather than purely diagonal. 6.5.9 Random effects \\(\\boldsymbol{u}\\) and BLUP intuition We usually assume: \\[ \\boldsymbol{u}\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{G}),\\qquad \\boldsymbol{\\varepsilon}\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{R}), \\] and independence between \\(\\boldsymbol{u}\\) and \\(\\boldsymbol{\\varepsilon}\\). The conditional mean of \\(u\\) given the data leads to the familiar “BLUP” form: \\[ \\hat{\\mathbf{u}}=\\mathbf{G}\\mathbf{Z}&#39;\\mathbf{V}^{-1}(\\mathbf{y}-\\mathbf{X}\\hat{\\beta}). \\] Interpretation: - if a hospital’s observed mean stress is above the overall mean, \\(\\hat{u}_j\\) tends to be positive, - the estimate is shrunk toward zero depending on \\(\\sigma^2\\) and \\(\\sigma_u^2\\), - shrinkage is stronger when within-hospital noise is large or hospital sample size is small. This “partial pooling” is a major advantage of mixed models. 6.5.10 Covariance structures: \\(\\mathbf{G}\\), \\(\\mathbf{R}\\), and \\(\\mathbf{V}\\) The total covariance of \\(\\mathbf{y}\\) is: \\[ \\mathbf{V}=\\mathrm{Var}(\\mathbf{y})=\\mathbf{Z}\\mathbf{G}\\mathbf{Z}&#39;+\\mathbf{R}. \\] Random intercept only: - \\(\\mathbf{G}\\) is often \\(\\sigma_u^2\\mathbf{I}\\) (simple diagonal). - \\(\\mathbf{R}\\) is often \\(\\sigma^2\\mathbf{I}\\) (i.i.d. residuals). But LMMs become most powerful when you allow more realistic \\(\\mathbf{R}\\) structures for repeated measures: compound symmetry (equal correlations), AR(1) (correlation decays with lag), unstructured (max flexibility, parameter-heavy), spatial correlation (depends on distance/time). A useful applied rule is: start simple, then add complexity only when diagnostics or design justify it. 6.5.11 Estimating variance parameters In practice, \\(\\sigma^2\\) and variance components like \\(\\sigma_u^2\\) are estimated by optimizing ML or REML criteria. Your derivation illustrates the idea that in special balanced cases, closed-form relationships connect variance parameters to mean squares; generally software uses numerical optimization. A simple intuition: - \\(\\sigma_u^2\\) measures between-cluster variability, - \\(\\sigma^2\\) measures within-cluster variability. Their ratio is tightly related to the intraclass correlation (ICC) in random-intercept models: \\[ \\mathrm{ICC}=\\frac{\\sigma_u^2}{\\sigma_u^2+\\sigma^2}. \\] (Useful for interpreting how strongly clustered the data are.) 6.5.12 Model statement and interpretation Your final distribution statement is an important “model contract”: \\[ (\\mathbf{y}\\mid \\beta,\\; \\boldsymbol{u}=u)\\sim \\mathcal{N}(\\mathbf{X}\\beta+\\mathbf{Z}u,\\mathbf{R}). \\] Meaning: - conditional on random effects, observations follow a linear model with residual covariance \\(\\mathbf{R}\\); - marginally (integrating out \\(u\\)), \\(\\mathbf{y}\\sim \\mathcal{N}(\\mathbf{X}\\beta,\\mathbf{V})\\). This distinction matters because: - fixed effects often describe marginal mean trends, - random effects describe cluster-specific deviations and induce correlation. 6.5.13 Testing and model comparison There are two common testing layers: Fixed effects tests: test linear hypotheses \\(L&#39;\\beta=c\\). Wald-type tests are common, but degrees-of-freedom approximations (Satterthwaite/KR) often improve accuracy in finite samples. Random effects / variance components tests: compare nested models via likelihood ratio tests. Boundary issues (variance ≥ 0) lead to mixture \\(\\chi^2\\) null distributions in simple cases, which is why “half p-values” sometimes appear in random-effect tests. For non-nested models, AIC/BIC provide pragmatic selection criteria: \\[ \\mathrm{AIC}=-2\\log L+2p,\\qquad \\mathrm{BIC}=-2\\log L+p\\log(n). \\] Applied note: - AIC tends to favor predictive performance (less penalty), - BIC tends to favor simpler models (more penalty as \\(n\\) grows). 6.5.14 Diagnostics (what to actually check) Diagnostics in mixed models should be done with the data structure in mind: Residual vs fitted: check nonlinearity and heteroscedasticity Normal Q-Q: for residuals and sometimes for random effects Influence: clusters can be influential (a single hospital) Random effects: do they look approximately normal and centered? Correlation structure: does the chosen \\(\\mathbf{R}\\) fit the repeated-measures pattern? Standardized or studentized residual ideas carry over, but “leverage” and “influence” are more complex because of correlation and random effects. 6.5.15 Practical takeaway for readers If you remember only one framework, remember this: Choose a mean model: \\(\\mathbf{X}\\beta\\) (what predictors affect the outcome?) Choose a correlation model: \\(\\mathbf{V}=\\mathbf{ZGZ&#39;}+\\mathbf{R}\\) (why are observations correlated?) Fit with ML/REML, then evaluate: fixed effect interpretability, variance components (how much clustering/repeated correlation exists), diagnostics, and model comparison (AIC/BIC/LRT/CV). That workflow is what makes LMMs a practical tool for longitudinal and multilevel data. 6.6 Linear mixed model practice This section walks through a practical progression from ordinary regression to mixed-effects modeling, using two datasets: lmm.data: a continuous outcome example (extro) with predictors (open, agree, social) and grouping variables (school, class). nurse: a multilevel stress dataset with hospital/ward clustering. The overall learning objectives are: Understand why we move from OLS/GLM to LMM (correlated observations within clusters). Learn the difference between treating a grouping variable as a fixed effect vs a random effect. Practice specifying nested random effects, random slopes, and comparing models. Extract model components (fixed effects, random effects, fitted values, residuals). Apply common mixed-model diagnostics and post-hoc summaries. Use ICC to quantify clustering strength. Throughout, we keep the R code intact and focus on interpretation, reasoning, and “what to look for”. 6.6.1 Loading data and library We begin with lme4 for fitting mixed models using lmer(), and arm for convenient display output. The dataset is read from a URL and inspected. Conceptually, when you see school and class in the same dataset, you should immediately ask: Are observations within the same school/class likely correlated? Is class nested within school, or can the same class label appear in multiple schools? Those questions guide whether you use (1|school), (1|class), or nested terms like (1|school/class). # --- Linear Mixed Model (LMM) Data Simulation --- set.seed(42) # 1. Define Data Structure n_schools &lt;- 6 n_classes_per_school &lt;- 4 n_students_per_class &lt;- 50 total_n &lt;- n_schools * n_classes_per_school * n_students_per_class # Total: 1200 observations # 2. Generate Hierarchical IDs # School level (Level 3) school_ids &lt;- rep(paste0(&quot;School_&quot;, 1:n_schools), each = n_classes_per_school * n_students_per_class) # Class level (Level 2) - Nested within schools class_ids &lt;- rep(rep(letters[1:n_classes_per_school], each = n_students_per_class), times = n_schools) # Individual student level (Level 1) student_ids &lt;- 1:total_n # 3. Generate Predictors (Fixed Effects) # Simulating psychological assessment scores using Normal Distribution open &lt;- rnorm(total_n, mean = 50, sd = 10) # Openness agree &lt;- rnorm(total_n, mean = 50, sd = 10) # Agreeableness social &lt;- rnorm(total_n, mean = 50, sd = 10) # Social Engagement # 4. Generate Random Effects # Intercept deviations for schools and classes (Random Intercepts) school_effects &lt;- rep(rnorm(n_schools, mean = 0, sd = 5), each = n_classes_per_school * n_students_per_class) class_effects &lt;- rep(rnorm(n_schools * n_classes_per_school, mean = 0, sd = 3), each = n_students_per_class) # 5. Generate Dependent Variable (Outcome: Extroversion) # Equation: extro = intercept + coefficients * predictors + random_effects + error error &lt;- rnorm(total_n, mean = 0, sd = 2) # Residual error extro &lt;- 30 + (0.5 * open) + (0.3 * agree) + (0.2 * social) + school_effects + class_effects + error # 6. Combine into Data Frame lmm_data_sim &lt;- data.frame( id = student_ids, extro = extro, open = open, agree = agree, social = social, class = class_ids, school = school_ids ) # Preview the first few rows head(lmm_data_sim) id extro open agree social class school 1 92.96908 63.70958 42.53484 63.27505 a School_1 2 81.63828 44.35302 50.36606 43.99164 a School_1 3 87.33240 53.63128 53.23310 50.56507 a School_1 4 84.91356 56.32863 53.79676 44.68924 a School_1 5 89.25167 54.04268 58.76557 49.19101 a School_1 6 90.21962 48.93875 59.33388 51.60756 a School_1 library(lme4) # load library library(arm) # convenience functions for regression in R ## Warning: package &#39;arm&#39; was built under R version 4.4.3 ## ## arm (Version 1.14-4, built: 2024-4-1) ## Working directory is C:/Users/hed2/Downloads/others/mybook2/mybook2 ## ## Attaching package: &#39;arm&#39; ## The following object is masked from &#39;package:performance&#39;: ## ## display ## The following object is masked from &#39;package:boot&#39;: ## ## logit ## The following object is masked from &#39;package:car&#39;: ## ## logit ## The following object is masked from &#39;package:scales&#39;: ## ## rescale ## The following objects are masked from &#39;package:psych&#39;: ## ## logit, rescale, sim lmm.data &lt;- lmm_data_sim # read.table(&quot;http://jaredknowles.com/s/lmm.data.txt&quot;, # header=TRUE, sep=&quot;,&quot;, na.strings=&quot;NA&quot;, dec=&quot;.&quot;, strip.white=TRUE) #summary(lmm.data) head(lmm.data) id extro open agree social class school 1 92.96908 63.70958 42.53484 63.27505 a School_1 2 81.63828 44.35302 50.36606 43.99164 a School_1 3 87.33240 53.63128 53.23310 50.56507 a School_1 4 84.91356 56.32863 53.79676 44.68924 a School_1 5 89.25167 54.04268 58.76557 49.19101 a School_1 6 90.21962 48.93875 59.33388 51.60756 a School_1 6.6.2 General linear regression (OLS) This is the baseline model that assumes: all observations are independent, a single residual variance applies to all observations, no clustering correlation exists. Even if this model fits numerically, it can give misleading standard errors when data are clustered. OLSexamp &lt;- lm(extro ~ open + agree + social, data = lmm.data) display(OLSexamp) ## lm(formula = extro ~ open + agree + social, data = lmm.data) ## coef.est coef.se ## (Intercept) 30.80 1.07 ## open 0.51 0.01 ## agree 0.30 0.01 ## social 0.19 0.01 ## --- ## n = 1200, k = 4 ## residual sd = 4.29, R-Squared = 0.68 What to look at here - Which predictors appear associated with extro? - Do the effect sizes seem plausible? - This model provides a baseline for later comparison. 6.6.3 Generalized linear regression (GLM) Here you fit glm() with the same formula. Since no family= argument is specified, it defaults to Gaussian, meaning it is essentially equivalent to lm() for continuous outcomes (up to minor implementation differences). This step is mainly a bridge to show “we can also use the GLM framework,” but the independence assumption is still the same. MLexamp &lt;- glm(extro ~ open + agree + social, data=lmm.data) display(MLexamp) ## glm(formula = extro ~ open + agree + social, data = lmm.data) ## coef.est coef.se ## (Intercept) 30.80 1.07 ## open 0.51 0.01 ## agree 0.30 0.01 ## social 0.19 0.01 ## --- ## n = 1200, k = 4 ## residual deviance = 22048.4, null deviance = 67937.3 (difference = 45888.9) ## overdispersion parameter = 18.4 ## residual sd is sqrt(overdispersion) = 4.29 6.6.4 Varying intercept by adding a stratum variable as fixed effect (GLM) Now we include class as a fixed effect. This means: each class gets its own intercept shift, we estimate a separate parameter for each class (minus a reference level). This approach can work well when the number of classes is small and you only care about those specific classes. But when there are many classes, fixed effects can become parameter-heavy and reduce generalizability. MLexamp.2 &lt;- glm(extro ~ open + agree + social + class, data=lmm.data ) display(MLexamp.2) ## glm(formula = extro ~ open + agree + social + class, data = lmm.data) ## coef.est coef.se ## (Intercept) 31.03 1.06 ## open 0.51 0.01 ## agree 0.29 0.01 ## social 0.19 0.01 ## classb 0.51 0.34 ## classc 0.38 0.34 ## classd -2.13 0.34 ## --- ## n = 1200, k = 7 ## residual deviance = 20687.5, null deviance = 67937.3 (difference = 47249.8) ## overdispersion parameter = 17.3 ## residual sd is sqrt(overdispersion) = 4.16 6.6.5 Comparisons of models (fixed effect approach) Comparing AIC between the model without and with class fixed effects gives a first signal: does adding class improve fit enough to justify the extra parameters? The anova(..., test=\"F\") here compares nested Gaussian models. AIC(MLexamp) ## [1] 6908.556 AIC(MLexamp.2) ## [1] 6838.103 anova(MLexamp, MLexamp.2, test=&quot;F&quot;) Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) 1196 22048.42 NA NA NA NA 1193 20687.51 3 1360.911 26.16018 0 Interpretation guide - If adding class meaningfully improves fit, it suggests there is between-class variability (i.e., intercepts differ by class). - But fixed effects do not explicitly model correlation; they just soak up mean differences. 6.6.6 Adding class and school as fixed effects (GLM) This expands the fixed-effect stratification by adding the interaction school:class, which is effectively a separate intercept for each school-by-class combination. This can become even more parameter-heavy, but it mimics what a nested random intercept structure is trying to capture—except without shrinkage and without a variance-component interpretation. MLexamp.4 &lt;- glm(extro ~ open + agree + social + school:class, data=lmm.data ) display(MLexamp.4) ## glm(formula = extro ~ open + agree + social + school:class, data = lmm.data) ## coef.est coef.se ## (Intercept) 34.93 0.58 ## open 0.50 0.01 ## agree 0.29 0.01 ## social 0.20 0.01 ## schoolSchool_1:classa 0.40 0.40 ## schoolSchool_2:classa -3.03 0.40 ## schoolSchool_3:classa -3.59 0.40 ## schoolSchool_4:classa -12.58 0.40 ## schoolSchool_5:classa -7.94 0.40 ## schoolSchool_6:classa 5.06 0.40 ## schoolSchool_1:classb -1.89 0.40 ## schoolSchool_2:classb -1.05 0.40 ## schoolSchool_3:classb -6.62 0.40 ## schoolSchool_4:classb -4.28 0.40 ## schoolSchool_5:classb -2.07 0.40 ## schoolSchool_6:classb -2.63 0.40 ## schoolSchool_1:classc -4.95 0.40 ## schoolSchool_2:classc -6.09 0.40 ## schoolSchool_3:classc -0.06 0.40 ## schoolSchool_4:classc -6.35 0.40 ## schoolSchool_5:classc -3.27 0.40 ## schoolSchool_6:classc 1.40 0.40 ## schoolSchool_1:classd -8.65 0.40 ## schoolSchool_2:classd -6.48 0.40 ## schoolSchool_3:classd -7.07 0.40 ## schoolSchool_4:classd -8.39 0.40 ## schoolSchool_5:classd -3.75 0.40 ## --- ## n = 1200, k = 27 ## residual deviance = 4777.8, null deviance = 67937.3 (difference = 63159.6) ## overdispersion parameter = 4.1 ## residual sd is sqrt(overdispersion) = 2.02 AIC(MLexamp) ## [1] 6908.556 AIC(MLexamp.4) ## [1] 5119.435 anova(MLexamp, MLexamp.4, test=&quot;F&quot;) Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) 1196 22048.422 NA NA NA NA 1173 4777.768 23 17270.65 184.3546 0 Practical warning - Fixed-effect stratification can overfit when many groups exist, especially with small group sizes. - Mixed models typically handle this more gracefully via partial pooling. 6.6.7 Considering different slopes by stratum (GLM) Here you fit separate regression models within each (school, class) stratum. This is a “fully stratified” approach: each group gets its own intercept and its own slopes for open, agree, social. This is useful for exploration: it shows whether slopes vary meaningfully across groups. But it creates many separate models and makes it hard to summarize results. require(plyr) modellist &lt;- dlply(lmm.data, .(school, class), function(x) glm(extro~ open + agree + social, data=x)) strat1 &lt;- display(modellist[[1]]) ## glm(formula = extro ~ open + agree + social, data = x) ## coef.est coef.se ## (Intercept) 35.42 2.73 ## open 0.48 0.02 ## agree 0.29 0.03 ## social 0.21 0.04 ## --- ## n = 50, k = 4 ## residual deviance = 174.7, null deviance = 2113.8 (difference = 1939.2) ## overdispersion parameter = 3.8 ## residual sd is sqrt(overdispersion) = 1.95 strat2 &lt;- display(modellist[[2]]) ## glm(formula = extro ~ open + agree + social, data = x) ## coef.est coef.se ## (Intercept) 33.40 2.32 ## open 0.46 0.03 ## agree 0.32 0.03 ## social 0.20 0.03 ## --- ## n = 50, k = 4 ## residual deviance = 198.8, null deviance = 2211.3 (difference = 2012.6) ## overdispersion parameter = 4.3 ## residual sd is sqrt(overdispersion) = 2.08 strat24 &lt;- display(modellist[[24]]) ## glm(formula = extro ~ open + agree + social, data = x) ## coef.est coef.se ## (Intercept) 34.32 3.17 ## open 0.54 0.03 ## agree 0.30 0.04 ## social 0.15 0.03 ## --- ## n = 50, k = 4 ## residual deviance = 220.2, null deviance = 1843.6 (difference = 1623.5) ## overdispersion parameter = 4.8 ## residual sd is sqrt(overdispersion) = 2.19 AIC(MLexamp) ## [1] 6908.556 # AIC(strat1) # AIC(strat2) # AIC(strat24) how to combine these models? From a modeling standpoint, this question motivates mixed models: If intercepts vary across groups → random intercept. If slopes vary across groups → random slope(s). Mixed models combine evidence across groups through shrinkage, instead of fitting each group completely separately. So “combining these models” typically means moving to a hierarchical model where group-to-group differences are random effects rather than separate fixed models. 6.6.8 Varying intercept with LMM Now we switch to a true mixed-effects model. Adding (1|class) says: “each class has its own intercept deviation.” This is a direct way to model within-class correlation: - observations within the same class share the same random intercept. MLexamp.6 &lt;- lmer(extro ~ open + agree + social + (1|class), data=lmm.data) display(MLexamp.6) ## lmer(formula = extro ~ open + agree + social + (1 | class), data = lmm.data) ## coef.est coef.se ## (Intercept) 30.72 1.20 ## open 0.51 0.01 ## agree 0.29 0.01 ## social 0.19 0.01 ## ## Error terms: ## Groups Name Std.Dev. ## class (Intercept) 1.21 ## Residual 4.16 ## --- ## number of obs: 1200, groups: class, 4 ## AIC = 6870.4, DIC = 6817.7 ## deviance = 6838.0 How to interpret - Fixed effects (open, agree, social) are population-average. - The random intercept variance tells you how much extroversion differs across classes after adjusting for covariates. 6.6.8.1 Compare fixed-effect class vs random-effect class You compare AIC of: - MLexamp.2 (class fixed effects) - MLexamp.6 (class random intercept) Lower AIC suggests better tradeoff of fit vs complexity. AIC(MLexamp.2) ## [1] 6838.103 AIC(MLexamp.6) ## [1] 6870.354 6.6.9 Add class and school as random effects This specifies two crossed random intercepts: (1|school) allows schools to differ in baseline extro (1|class) allows classes to differ as well This is appropriate if class is not strictly nested in school in coding, or if there are shared class labels across schools. MLexamp.7 &lt;- lmer(extro ~ open + agree + social + (1|school) + (1|class), data=lmm.data) display(MLexamp.7) ## lmer(formula = extro ~ open + agree + social + (1 | school) + ## (1 | class), data = lmm.data) ## coef.est coef.se ## (Intercept) 31.24 1.54 ## open 0.50 0.01 ## agree 0.29 0.01 ## social 0.19 0.01 ## ## Error terms: ## Groups Name Std.Dev. ## school (Intercept) 2.82 ## class (Intercept) 1.21 ## Residual 3.27 ## --- ## number of obs: 1200, groups: school, 6; class, 4 ## AIC = 6320.3, DIC = 6266 ## deviance = 6286.2 6.6.10 Nested terms: school/class This explicitly encodes nesting: (1|school/class) expands to (1|school) + (1|school:class) Meaning: - school random intercept, plus - class-within-school random intercept. This is usually the right structure when each class belongs to only one school. MLexamp.8 &lt;- lmer(extro ~ open + agree + social + (1|school/class), data=lmm.data) display(MLexamp.8) ## lmer(formula = extro ~ open + agree + social + (1 | school/class), ## data = lmm.data) ## coef.est coef.se ## (Intercept) 31.02 1.26 ## open 0.50 0.01 ## agree 0.29 0.01 ## social 0.20 0.01 ## ## Error terms: ## Groups Name Std.Dev. ## class:school (Intercept) 3.20 ## school (Intercept) 2.33 ## Residual 2.02 ## --- ## number of obs: 1200, groups: class:school, 24; school, 6 ## AIC = 5247, DIC = 5186.3 ## deviance = 5209.6 6.6.11 Varying slope with LMM Random slopes allow the effect of a predictor (here open) to vary across clusters. (1+open|school/class) means: - random intercept and random slope of open at the school:class level, - plus the corresponding variance and covariance between intercept and slope. This answers a deeper scientific question: - does “openness” relate to extroversion differently across different classes/schools? MLexamp.9 &lt;- lmer(extro ~ open + agree + social + (1+open|school/class), data=lmm.data) ## boundary (singular) fit: see help(&#39;isSingular&#39;) display(MLexamp.9) ## lmer(formula = extro ~ open + agree + social + (1 + open | school/class), ## data = lmm.data) ## coef.est coef.se ## (Intercept) 30.96 1.02 ## open 0.50 0.01 ## agree 0.29 0.01 ## social 0.20 0.01 ## ## Error terms: ## Groups Name Std.Dev. Corr ## class:school (Intercept) 3.31 ## open 0.01 -1.00 ## school (Intercept) 1.42 ## open 0.02 1.00 ## Residual 2.01 ## --- ## number of obs: 1200, groups: class:school, 24; school, 6 ## AIC = 5246, DIC = 5178.7 ## deviance = 5201.3 6.6.11.1 Compare AIC across candidate structures This is a practical model selection exercise: MLexamp.4 uses fixed school:class intercepts MLexamp.7 uses random school and class intercepts (separate) MLexamp.8 uses nested random intercepts MLexamp.9 adds random slope of open AIC(MLexamp.4) #using the interaction of class and school as fixed effect ## [1] 5119.435 AIC(MLexamp.7) #using class and school as random intercepts ## [1] 6320.338 AIC(MLexamp.8) #using nested term of class and school as random intercepts ## [1] 5246.96 AIC(MLexamp.9) #adding open as random slope based on MLexamp.8 ## [1] 5245.997 Interpretation guide - If random effects variances are near zero and AIC does not improve, random effects may not be needed. - If AIC improves substantially when adding nesting or random slopes, clustering and/or slope heterogeneity is likely important. 6.6.12 Summarize model 8 summary() provides: fixed effects estimates and standard errors, random effects variance components (school and class-within-school), residual variance, fit criteria. summary(MLexamp.8) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: extro ~ open + agree + social + (1 | school/class) ## Data: lmm.data ## ## REML criterion at convergence: 5233 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.12970 -0.68636 -0.00039 0.67466 2.95243 ## ## Random effects: ## Groups Name Variance Std.Dev. ## class:school (Intercept) 10.247 3.201 ## school (Intercept) 5.413 2.327 ## Residual 4.073 2.018 ## Number of obs: 1200, groups: class:school, 24; school, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 31.016666 1.259145 24.63 ## open 0.495711 0.005878 84.33 ## agree 0.293323 0.005936 49.41 ## social 0.195343 0.005725 34.12 ## ## Correlation of Fixed Effects: ## (Intr) open agree ## open -0.233 ## agree -0.228 -0.022 ## social -0.229 0.026 -0.015 Your note is exactly the applied takeaway: If the variance components for random effects are extremely small relative to the total variance, then there is little clustering signal; LMM may not be necessary. But be careful: “small” depends on outcome scale and study design—often it is better to quantify via ICC (later section). 6.6.13 Testing random effects (LRT logic) Testing random effects is subtle because variance parameters lie on the boundary (they cannot be negative). This breaks standard chi-square asymptotics. Your code demonstrates a conservative mixture-chi-square approach by hand. # W2 will have to build the test statistic by hand because we cannot rely on asymptotics L_Model= logLik(MLexamp.8)[1] L_Model1=logLik(MLexamp.9)[1] LRT_statistic=-2*(L_Model1-L_Model) # Model has 3 parameters associated with the Random Effects # Model1 has 1 parameter associated with the Random Effects # DF=2 # We will look at a mixture of chi-squared distributions with 1 and 2 degrees of freedom 0.5*pchisq(LRT_statistic,df=2,lower.tail = F)+0.5*pchisq(LRT_statistic,df=1,lower.tail = F) ## [1] 1 Practical guidance: - Use LRT for nested random effects cautiously. - In real applied work, AIC/BIC + stability + diagnostics are often used together rather than relying only on a p-value. 6.6.14 Visualizing grouping structure A grouped line plot helps you “see” whether the relationship between open and extro differs by school: parallel-ish lines suggest varying intercepts, different slopes suggest random slopes may be needed. library(ggplot2) ggplot(lmm.data,aes(x= open,y=extro ,group=school,color= school))+geom_line() 6.6.15 Residual checks by grouping variables These plots help detect whether certain schools or classes have systematically different residual behavior (e.g., variance differences, outliers, patterning). plot(MLexamp.8, school~resid(.)) plot(MLexamp.8, class~resid(.)) 6.6.16 Residuals vs explanatory variables A common check for: - nonlinearity, - heteroscedasticity (variance changes with predictor level), - outliers. plot(MLexamp.8, resid(.)~open) # plot(MLexamp.8,resid(.,type=&quot;p&quot;)~fitted(.) | BMIGRP_fm002) 6.6.17 Normality checks 6.6.17.1 QQ plot of residuals Residual normality is not always critical for point estimation, but it matters more for small-sample inference and extreme prediction intervals. qqnorm(resid(MLexamp.8) ) 6.6.17.2 Random effect coefficients normality Random effects are assumed normal in standard LMMs. A QQ plot of estimated random intercepts (or slopes) checks whether this seems reasonable. test= ((ranef(MLexamp.8)[1])[[&quot;class:school&quot;]]) qqnorm ((test$`(Intercept)`) ) 6.6.18 Extracting elements (parameters) These are the standard extraction tools: fixef(): fixed effects \\(\\hat{\\beta}\\) ranef(): random effects \\(\\hat{u}\\) coef(): conditional coefficients (fixed + random), i.e., group-specific coefficients fixef(MLexamp.8) ## (Intercept) open agree social ## 31.0166664 0.4957109 0.2933230 0.1953426 ranef(MLexamp.8) ## $`class:school` ## (Intercept) ## a:School_1 4.1886142 ## a:School_2 1.0466568 ## a:School_3 0.6059617 ## a:School_4 -5.9229747 ## a:School_5 -3.7634378 ## a:School_6 5.6331936 ## b:School_1 1.9100724 ## b:School_2 3.0071916 ## b:School_3 -2.4043278 ## b:School_4 2.3149458 ## b:School_5 2.0630662 ## b:School_6 -2.0026469 ## c:School_1 -1.1286244 ## c:School_2 -1.9962792 ## c:School_3 4.1081884 ## c:School_4 0.2610122 ## c:School_5 0.8674500 ## c:School_6 1.9964598 ## d:School_1 -4.7916596 ## d:School_2 -2.3777254 ## d:School_3 -2.8508902 ## d:School_4 -1.7647766 ## d:School_5 0.3896333 ## d:School_6 0.6108966 ## ## $school ## (Intercept) ## School_1 0.09424432 ## School_2 -0.16912820 ## School_3 -0.28582870 ## School_4 -2.70039511 ## School_5 -0.23417485 ## School_6 3.29528254 ## ## with conditional variances for &quot;class:school&quot; &quot;school&quot; coef(MLexamp.8) ## $`class:school` ## (Intercept) open agree social ## a:School_1 35.20528 0.4957109 0.293323 0.1953426 ## a:School_2 32.06332 0.4957109 0.293323 0.1953426 ## a:School_3 31.62263 0.4957109 0.293323 0.1953426 ## a:School_4 25.09369 0.4957109 0.293323 0.1953426 ## a:School_5 27.25323 0.4957109 0.293323 0.1953426 ## a:School_6 36.64986 0.4957109 0.293323 0.1953426 ## b:School_1 32.92674 0.4957109 0.293323 0.1953426 ## b:School_2 34.02386 0.4957109 0.293323 0.1953426 ## b:School_3 28.61234 0.4957109 0.293323 0.1953426 ## b:School_4 33.33161 0.4957109 0.293323 0.1953426 ## b:School_5 33.07973 0.4957109 0.293323 0.1953426 ## b:School_6 29.01402 0.4957109 0.293323 0.1953426 ## c:School_1 29.88804 0.4957109 0.293323 0.1953426 ## c:School_2 29.02039 0.4957109 0.293323 0.1953426 ## c:School_3 35.12485 0.4957109 0.293323 0.1953426 ## c:School_4 31.27768 0.4957109 0.293323 0.1953426 ## c:School_5 31.88412 0.4957109 0.293323 0.1953426 ## c:School_6 33.01313 0.4957109 0.293323 0.1953426 ## d:School_1 26.22501 0.4957109 0.293323 0.1953426 ## d:School_2 28.63894 0.4957109 0.293323 0.1953426 ## d:School_3 28.16578 0.4957109 0.293323 0.1953426 ## d:School_4 29.25189 0.4957109 0.293323 0.1953426 ## d:School_5 31.40630 0.4957109 0.293323 0.1953426 ## d:School_6 31.62756 0.4957109 0.293323 0.1953426 ## ## $school ## (Intercept) open agree social ## School_1 31.11091 0.4957109 0.293323 0.1953426 ## School_2 30.84754 0.4957109 0.293323 0.1953426 ## School_3 30.73084 0.4957109 0.293323 0.1953426 ## School_4 28.31627 0.4957109 0.293323 0.1953426 ## School_5 30.78249 0.4957109 0.293323 0.1953426 ## School_6 34.31195 0.4957109 0.293323 0.1953426 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; 6.6.19 Fitted values and residuals Fitted values (fitted) represent the model’s conditional mean, which includes group-level random effects. Residuals represent the within-group deviations not explained by the model. yhat &lt;- fitted(MLexamp.8) head(yhat) ## 1 2 3 4 5 6 ## 91.71782 80.65276 87.37714 87.73178 88.93546 87.04414 summary(yhat) Min. 1st Qu. Median Mean 3rd Qu. Max. 59.11443 75.30169 80.02914 80.06942 84.89566 102.5105 residuals &lt;- resid(MLexamp.8) head(residuals) ## 1 2 3 4 5 6 ## 1.25126333 0.98551428 -0.04473698 -2.81822020 0.31620284 3.17547575 summary(residuals) Min. 1st Qu. Median Mean 3rd Qu. Max. -6.316344 -1.385218 -0.0007856 0 1.3616 5.958581 hist(residuals,50) 6.6.20 Fitted lines by group This plot visualizes model-implied trajectories by school. Because fitted values in a random-intercept model contain partial pooling, you often see less variation than raw group means—this is expected and is one of the main benefits of mixed models. lmm.data$Fitted=predict(MLexamp.8) # subjects=unique(lmm.data$school) #Taking the identifiers and saving it # sam=&quot;V&quot; # Taking a random sample of 10 Subjects ggplot(lmm.data ,aes(x=open ,y=Fitted,group=school,color=school ) )+geom_line(se=F) ## Warning in geom_line(se = F): Ignoring unknown parameters: `se` 6.6.21 Model diagnostics (second dataset: Nurses) This section shows how the same mixed model ideas apply in a more realistic hierarchical structure: hospital/ward. 6.6.21.1 Loading nurse data You read an SPSS dataset from GitHub and fit a nested random intercept model. Important conceptual point: if the outcome is truly ordinal, a Gaussian LMM is an approximation. For ordinal outcomes, you would typically consider an ordinal mixed model (e.g., cumulative logit mixed model). But as you note, this is for demonstration of diagnostics. # --- Data Simulation --- library(dplyr) set.seed(2026) # --- 1. Define Hierarchical Structure --- n_hospitals &lt;- 5 # Number of hospitals (Level 3) n_wards_per_hosp &lt;- 10 # Wards per hospital (Total 50 wards) n_nurses_per_ward &lt;- 15 # Nurses per ward (Total 750 nurses) total_n &lt;- n_hospitals * n_wards_per_hosp * n_nurses_per_ward # --- 2. Generate Hierarchical IDs --- hospital &lt;- rep(1:n_hospitals, each = n_wards_per_hosp * n_nurses_per_ward) ward &lt;- rep(1:(n_hospitals * n_wards_per_hosp), each = n_nurses_per_ward) nurse_id &lt;- 1:total_n # --- 3. Generate Predictors (Fixed Effects) --- # experien: Nurse work experience in years (ranging 1-25) experien &lt;- runif(total_n, 1, 25) # wardtype: Type of ward (0 = General, 1 = Special/ICU) - Level 2 variable # First generate types for each ward, then broadcast to individual nurses unique_ward_types &lt;- rbinom(n_hospitals * n_wards_per_hosp, 1, 0.4) wardtype &lt;- rep(unique_ward_types, each = n_nurses_per_ward) # expcon: Experience control/interaction term (binary experimental condition) expcon &lt;- rbinom(total_n, 1, 0.5) # --- 4. Define Random Effects (Intercept Deviations) --- # Hospital-level random intercepts (u_h) - Variance at Level 3 u_h &lt;- rep(rnorm(n_hospitals, 0, 0.8), each = n_wards_per_hosp * n_nurses_per_ward) # Ward-level random intercepts (u_w) - Variance at Level 2 u_w &lt;- rep(rnorm(n_hospitals * n_wards_per_hosp, 0, 1.2), each = n_nurses_per_ward) # --- 5. Generate Dependent Variable: stress --- # Model Equation: # stress = intercept + 0.5*wardtype - 0.1*experien + 0.3*expcon + Random Effects + Residual Error intercept &lt;- 5.0 beta_wardtype &lt;- 0.5 beta_experien &lt;- -0.1 beta_expcon &lt;- 0.3 error &lt;- rnorm(total_n, 0, 1.0) # Residual error (Level 1 variance) stress &lt;- intercept + (beta_wardtype * wardtype) + (beta_experien * experien) + (beta_expcon * expcon) + u_h + u_w + error # --- 6. Assemble into Data Frame --- nurses_sim_data &lt;- data.frame( nurse_id = nurse_id, hospital = as.factor(hospital), ward = as.factor(ward), wardtype = as.factor(wardtype), experien = experien, expcon = expcon, stress = stress ) # Display the first few rows of the simulated dataset head(nurses_sim_data) nurse_id hospital ward wardtype experien expcon stress 1 1 1 0 17.768163 1 5.001246 2 1 1 0 14.356732 0 7.377821 3 1 1 0 4.363359 1 6.298894 4 1 1 0 7.857359 1 5.904764 5 1 1 0 14.328856 0 4.593614 6 1 1 0 1.603148 1 3.892058 library(haven) nurse &lt;- nurses_sim_data # read_sav(file=&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%202/Nurses/SPSS/Nurses.sav?raw=true&quot;) MLexamp.18 &lt;- lmer(stress ~ experien+ wardtype+ expcon + (1|hospital/ward), data=nurse) summary(MLexamp.18) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: stress ~ experien + wardtype + expcon + (1 | hospital/ward) ## Data: nurse ## ## REML criterion at convergence: 2293.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.08705 -0.64245 0.01861 0.61302 2.58650 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ward:hospital (Intercept) 2.1225 1.4569 ## hospital (Intercept) 0.4072 0.6382 ## Residual 0.9680 0.9839 ## Number of obs: 750, groups: ward:hospital, 50; hospital, 5 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 5.035277 0.396294 12.706 ## experien -0.111048 0.005457 -20.350 ## wardtype1 0.426834 0.442263 0.965 ## expcon 0.481870 0.074216 6.493 ## ## Correlation of Fixed Effects: ## (Intr) expern wrdty1 ## experien -0.179 ## wardtype1 -0.402 0.000 ## expcon -0.101 0.016 0.004 outcome is ordinal variable 6.6.21.2 Residual normality: residual vs fitted, QQ plot You compute residuals and inspect: - scale-location style behavior via \\(\\sqrt{|residual|}\\) vs fitted, - normality via QQ plot. lmerresid &lt;- resid(MLexamp.18) par(mfrow = c(1,2)) plot(sqrt(abs(lmerresid)) ~ predict(MLexamp.18)) qqnorm(lmerresid) qqline(lmerresid) Interpretation guide: - A strong trend in residual spread vs fitted suggests heteroscedasticity. - Strong deviation from the QQ line suggests heavy tails or skewness. 6.6.21.3 Checking variance homogeneity Residuals plotted against predictors can reveal: - differing variance across categories (wardtype, expcon), - nonlinear patterns (experien). Your plotting uses color palettes for readability. par(mfrow = c(2,2)) plot(lmerresid ~ fitted(MLexamp.18)) with(nurse, plot(lmerresid ~ experien , col = heat.colors(20))) with(nurse, plot(lmerresid ~ wardtype , col = rainbow(3))) with(nurse, plot(lmerresid ~ expcon , col = rainbow(5))) # with(lmm.data, plot(lmerresid ~ class, col = rainbow(5))) 6.6.21.4 Influence points (Cook’s distance) Influence in mixed models can be assessed using tools like influence.ME. Cook’s distance highlights observations that disproportionately change fixed effects. In clustered data, influential points can be: - a single extreme observation, - or an entire cluster with unusual behavior. library(influence.ME) ## ## Attaching package: &#39;influence.ME&#39; ## The following object is masked from &#39;package:arm&#39;: ## ## se.fixef ## The following object is masked from &#39;package:stats&#39;: ## ## influence lmer3.infl &lt;- influence(MLexamp.18, obs = TRUE) par(mfrow = c(1,1)) plot(cooks.distance(lmer3.infl), type = &quot;p&quot;, pch = 20) 6.6.21.5 Random effect normality A QQ plot of the estimated random intercepts checks whether the random effects distribution is approximately normal. par(mfrow = c(1,3)) qqnorm(ranef(MLexamp.18)$`ward:hospital`$`(Intercept)`) (If the QQ plot shows strong asymmetry or heavy tails, consider whether the random effects distribution assumption is reasonable, or whether model specification is missing covariates.) 6.6.21.6 Post hoc analysis confint() can provide confidence intervals for variance parameters (and fixed effects depending on settings). You request parameters 1:4, which typically corresponds to variance components and fixed effects depending on the model and method. confint(MLexamp.18, parm = 1:4, oldNames = FALSE) ## Computing profile confidence intervals ... 2.5 % 97.5 % sd_(Intercept)|ward:hospital 1.1784347 1.808224 sd_(Intercept)|hospital 0.0000000 1.467998 sigma 0.9331689 1.036270 (Intercept) 4.2180132 5.848215 6.6.21.6.1 Estimated marginal means (lsmeans / emmeans) emmeans produces adjusted means (marginal means) for categorical predictors, averaged over other covariates. The interaction plot helps interpret whether the effect of expcon differs by wardtype (effect modification). mylsmeans &lt;- emmeans::emmeans(MLexamp.18, c(&quot;wardtype&quot;, &quot;expcon&quot; )) sum_mylsmeans &lt;- summary(mylsmeans) with(sum_mylsmeans, interaction.plot(wardtype , expcon ,emmean, col = 2:4)) 6.6.21.6.2 Pairwise comparisons Pairwise differences provide estimated contrasts; plotting SE and estimates is a quick way to see magnitude and uncertainty. sum_difflsmeans &lt;- summary(pairs(mylsmeans)) head(sum_difflsmeans) contrast estimate SE df t.ratio p.value wardtype0 expcon0 - wardtype1 expcon0 -0.4268335 0.4461313 45.32772 -0.9567443 0.7743988 wardtype0 expcon0 - wardtype0 expcon1 -0.4818697 0.0742242 700.55632 -6.4920809 0.0000000 wardtype0 expcon0 - wardtype1 expcon1 -0.9087032 0.4525239 48.02449 -2.0080779 0.1993094 wardtype1 expcon0 - wardtype0 expcon1 -0.0550361 0.4520031 47.79373 -0.1217605 0.9993454 wardtype1 expcon0 - wardtype1 expcon1 -0.4818697 0.0742242 700.55632 -6.4920809 0.0000000 wardtype0 expcon1 - wardtype1 expcon1 -0.4268335 0.4461313 45.32772 -0.9567443 0.7743988 barplot(sum_difflsmeans$SE) barplot(sum_difflsmeans$estimate) 6.6.21.6.3 One-stop diagnostic plot performance::check_model() is a convenient diagnostic suite for mixed models (residuals, QQ, collinearity, etc.). performance::check_model(MLexamp.18) 6.6.22 Intra class correlation (ICC) ICC quantifies the proportion of outcome variance attributable to clustering. It is often the fastest way to decide whether mixed modeling is warranted. Your code fits the null model and prints the summary, from which you can compute ICC. lmm.null &lt;- lmer(extro ~ 1 + (1|school), data = lmm.data) summary(lmm.null) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: extro ~ 1 + (1 | school) ## Data: lmm.data ## ## REML criterion at convergence: 8100.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.75973 -0.65491 -0.00704 0.64908 2.99247 ## ## Random effects: ## Groups Name Variance Std.Dev. ## school (Intercept) 8.855 2.976 ## Residual 49.276 7.020 ## Number of obs: 1200, groups: school, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 80.069 1.232 65.01 Your interpretation statement is exactly the right reading: a large ICC means that group membership (school) accounts for a large portion of variance in extro, strongly supporting the need for random effects. 6.6.22.1 Notice / practical reminders Back-transforming expected values: if you transform the outcome, marginal means should be interpreted on the transformed scale unless explicitly back-transformed. Box–Cox transformations can help stabilize variance and improve normality when residual diagnostics show strong deviations. Mixed model choice should balance: scientific interpretability, diagnostics, parsimony (AIC/BIC), and validation where possible. This section now gives a complete “from OLS to LMM” practical pipeline: build, compare, diagnose, interpret, and quantify clustering (ICC). 6.7 Linear mixed model covariance decomposition with random intercept — lme4 This section walks through the same workflow you used above, but with a stronger emphasis on (1) what each matrix represents, (2) where the covariance decomposition shows up, and (3) how the “manual” computations match lme4. We focus on the random-intercept LMM: \\[ \\mathbf{y} = \\mathbf{X}\\beta + \\mathbf{Z}u + \\varepsilon, \\qquad u \\sim N(0,\\mathbf{G}), \\qquad \\varepsilon \\sim N(0,\\mathbf{R}) \\] with the implied marginal model \\[ \\mathbf{y} \\sim N(\\mathbf{X}\\beta,\\ \\mathbf{V}), \\qquad \\mathbf{V}=\\mathbf{Z}\\mathbf{G}\\mathbf{Z}&#39;+\\mathbf{R}. \\] In lme4 (without nlme residual correlation structures), residuals are independent: \\[ \\mathbf{R}=\\sigma^2\\mathbf{I}. \\] For a random intercept only model with grouping variable Subject, the random-effect covariance is \\[ \\mathbf{G}=\\sigma_u^2\\mathbf{I}. \\] This is the classic compound symmetry pattern: within a subject, all repeated observations share covariance \\(\\sigma_u^2\\), and the diagonal includes \\(\\sigma_u^2+\\sigma^2\\). 6.7.1 Load data We use the Orthodont dataset and store it as Data. data(Orthodont, package=&quot;nlme&quot;) Data &lt;- Orthodont 6.7.2 Plot means and variances by higher level variable (grouping) Before fitting models, we quickly examine whether subjects differ in their outcome level and variability. If subject means vary a lot, random intercepts are plausible. If within-subject variances vary wildly, a more complex residual structure might be needed (though lme4 cannot directly specify correlated residuals). barplot(with(Data, tapply(distance, list(subject = Subject), mean))) barplot(with(Data, tapply(distance, list(subject = Subject), var))) 6.7.3 Using glm-style fixed effects (baseline reference) 6.7.3.1 Using glm (subject as fixed effect) This model allows a separate intercept for each subject as a fixed parameter: \\[ \\text{distance}_{ij}=\\beta_0+\\beta_1\\text{age}_{ij}+\\beta_2\\text{Sex}_{ij}+\\alpha_j+\\varepsilon_{ij}. \\] It often fits well, but it treats subject effects as fixed and does not provide a variance component interpretation \\(\\sigma_u^2\\). fit.lm &lt;- lm(distance ~ age+ Sex + Subject, data = Data) anova(fit.lm) Df Sum Sq Mean Sq F value Pr(&gt;F) age 1 235.3560 235.356019 114.838287 0 Sex 1 140.4649 140.464857 68.537629 0 Subject 25 377.9148 15.116591 7.375904 0 Residuals 80 163.9565 2.049456 NA NA 6.7.3.2 Using glm with random slopes (aov Error term) This approach is the classical ANOVA / variance-component view. For balanced repeated measures, it connects to mean squares. fit.aov &lt;- aov(distance ~ age+ Sex + Error(Subject), data = Data) summary(fit.aov) ## ## Error: Subject ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Sex 1 140.5 140.46 9.292 0.00538 ** ## Residuals 25 377.9 15.12 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 235.4 235.36 114.8 &lt;2e-16 *** ## Residuals 80 164.0 2.05 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.7.3.2.1 How can we compute \\(\\sigma_a^2\\) from this output? Your note is the key identity: \\(\\sigma^2 = \\mathrm{MSE}\\) \\(E(\\mathrm{MSA}) = \\sigma_a^2 + n\\sigma^2\\) so \\[ \\sigma_a^2=\\frac{\\mathrm{MSA}-\\mathrm{MSE}}{n}. \\] Practical caution: Orthodont is not perfectly balanced in the same way as textbook ANOVA examples, so this computation is best viewed as an approximation. Likelihood-based LMM estimation (REML/ML) is typically preferred for unbalanced data. 6.7.4 Using LMM (random intercept) in lme4 6.7.4.1 Fit the random intercept model We fit \\[ \\text{distance}_{ij}=\\beta_0+\\beta_1\\text{age}_{ij}+\\beta_2\\text{Sex}_{ij}+u_{0j}+\\varepsilon_{ij}, \\] with \\(u_{0j}\\sim N(0,\\sigma_u^2)\\), \\(\\varepsilon_{ij}\\sim N(0,\\sigma^2)\\). library(lme4) fit.lmer &lt;- lmer(distance ~ age + Sex +(1 | Subject), data = Data) summary(fit.lmer) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: distance ~ age + Sex + (1 | Subject) ## Data: Data ## ## REML criterion at convergence: 437.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7489 -0.5503 -0.0252 0.4534 3.6575 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Subject (Intercept) 3.267 1.807 ## Residual 2.049 1.432 ## Number of obs: 108, groups: Subject, 27 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 17.70671 0.83392 21.233 ## age 0.66019 0.06161 10.716 ## SexFemale -2.32102 0.76142 -3.048 ## ## Correlation of Fixed Effects: ## (Intr) age ## age -0.813 ## SexFemale -0.372 0.000 default is compound symmetry (induced by the random intercept). More precisely: the random intercept implies constant covariance \\(\\sigma_u^2\\) for any two observations from the same subject, residual errors are independent (identity \\(\\mathbf{R}=\\sigma^2\\mathbf{I}\\)). 6.7.5 Get \\(\\mathbf{X}\\), \\(\\mathbf{y}\\), \\(\\mathbf{Z}\\) These are the core objects behind the matrix formulas. \\(\\mathbf{X}\\): fixed-effects design matrix (Intercept, age, Sex…) \\(\\mathbf{y}\\): response vector \\(\\mathbf{Z}\\): random-effects design matrix (subject membership for random intercept) X=(getME(fit.lmer, &quot;X&quot;)) head(X) (Intercept) age SexFemale 1 8 0 1 10 0 1 12 0 1 14 0 1 8 0 1 10 0 y=getME(fit.lmer, &quot;y&quot;) head(y) ## [1] 26.0 25.0 29.0 31.0 21.5 22.5 Z &lt;- getME(fit.lmer, &quot;Z&quot;) head(Z) ## 6 x 27 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 27 column names &#39;M16&#39;, &#39;M05&#39;, &#39;M02&#39; ... ]] ## ## 1 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . ## 5 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . 6.7.6 Get fixed and random effect coefficients \\(\\hat{\\beta}\\) are population-average (fixed) coefficients. \\(\\hat{u}\\) are BLUPs (random intercepts per subject). coef() combines them into subject-specific coefficients. bhat &lt;- getME(fit.lmer, &quot;fixef&quot;) #getME(fit.lmer, &quot;beta&quot;) # fixef(fit.lmer) bhat ## (Intercept) age SexFemale ## 17.7067130 0.6601852 -2.3210227 uhat &lt;- ranef(fit.lmer) uhat ## $Subject ## (Intercept) ## M16 -1.70183357 ## M05 -1.70183357 ## M02 -1.37767479 ## M11 -1.16156894 ## M07 -1.05351602 ## M08 -0.94546309 ## M03 -0.62130432 ## M12 -0.62130432 ## M13 -0.62130432 ## M14 -0.08103969 ## M09 0.13506616 ## M15 0.78338371 ## M06 1.21559540 ## M04 1.43170125 ## M01 2.40417758 ## M10 3.91691853 ## F10 -3.58539251 ## F09 -1.31628108 ## F06 -1.31628108 ## F01 -1.10017524 ## F05 -0.01964599 ## F07 0.30451279 ## F02 0.30451279 ## F08 0.62867156 ## F03 0.95283034 ## F04 1.92530666 ## F11 3.22194176 ## ## with conditional variances for &quot;Subject&quot; coef(fit.lmer) ## $Subject ## (Intercept) age SexFemale ## M16 16.00488 0.6601852 -2.321023 ## M05 16.00488 0.6601852 -2.321023 ## M02 16.32904 0.6601852 -2.321023 ## M11 16.54514 0.6601852 -2.321023 ## M07 16.65320 0.6601852 -2.321023 ## M08 16.76125 0.6601852 -2.321023 ## M03 17.08541 0.6601852 -2.321023 ## M12 17.08541 0.6601852 -2.321023 ## M13 17.08541 0.6601852 -2.321023 ## M14 17.62567 0.6601852 -2.321023 ## M09 17.84178 0.6601852 -2.321023 ## M15 18.49010 0.6601852 -2.321023 ## M06 18.92231 0.6601852 -2.321023 ## M04 19.13841 0.6601852 -2.321023 ## M01 20.11089 0.6601852 -2.321023 ## M10 21.62363 0.6601852 -2.321023 ## F10 14.12132 0.6601852 -2.321023 ## F09 16.39043 0.6601852 -2.321023 ## F06 16.39043 0.6601852 -2.321023 ## F01 16.60654 0.6601852 -2.321023 ## F05 17.68707 0.6601852 -2.321023 ## F07 18.01123 0.6601852 -2.321023 ## F02 18.01123 0.6601852 -2.321023 ## F08 18.33538 0.6601852 -2.321023 ## F03 18.65954 0.6601852 -2.321023 ## F04 19.63202 0.6601852 -2.321023 ## F11 20.92865 0.6601852 -2.321023 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; Interpretation: \\(\\hat{\\beta}_0\\) is the overall intercept. \\(\\hat{u}_j\\) is subject \\(j\\)’s deviation from the overall intercept. Subject \\(j\\)’s conditional intercept is \\(\\hat{\\beta}_0+\\hat{u}_j\\). 6.7.7 Random effect covariance structure and the role of \\(\\theta\\) In lme4, random-effects parameters are represented via a Cholesky-factor parameterization. Conceptually: \\(\\theta\\) parameterizes the relative Cholesky factors, from \\(\\theta\\) we can reconstruct the random-effect covariance \\(\\Sigma\\) (often denoted \\(\\mathbf{G}\\) for a random-effect term). Your 2×2 example (intercept + slope) illustrates: \\[ \\Sigma = \\left( \\begin{array}{cc} \\theta_1 &amp; 0 \\\\ \\theta_2 &amp; \\theta_3 \\end{array} \\right) \\left( \\begin{array}{cc} \\theta_1 &amp; \\theta_2 \\\\ 0 &amp; \\theta_3 \\end{array} \\right) = \\left( \\begin{array}{cc} \\theta_1^2 &amp; \\theta_1 \\theta_2 \\\\ \\theta_1 \\theta_2 &amp; \\theta_2^2 + \\theta_3^2 \\end{array} \\right) = \\left( \\begin{array}{cc} \\sigma_1^2 &amp; \\sigma_{12} \\\\ \\sigma_{12} &amp; \\sigma_2^2 \\end{array} \\right). \\] For a random intercept only model, there is only one standard deviation parameter, so \\(\\theta\\) reduces to \\(\\sigma_u\\) (up to the scaling conventions used internally). 6.7.7.1 Build a “random intercept covariance object” for Subject VarCorr(fit.lmer) reports the random intercept SD for Subject. You build a diagonal matrix of that SD across subjects: vc &lt;- VarCorr(fit.lmer) Lambda_new &lt;-vc[[&quot;Subject&quot;]][1]*diag(length(levels(Data$Subject))) head(Lambda_new) 3.266784 0.000000 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 3.266784 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 3.266784 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.266784 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 0.000000 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(Lambda_new) ## [1] 27 27 Notes for interpretation: If you want \\(\\mathbf{G}\\) on the variance scale, it is \\(\\sigma_u^2\\mathbf{I}\\). Many internal pieces in lme4 are stored in factorized form; the important point is that the random intercept induces a block-constant covariance when mapped through \\(\\mathbf{Z}\\). 6.7.8 Residual variance and \\(\\mathbf{R}=\\sigma^2\\mathbf{I}\\) Extract residual SD from the fitted model and square it: \\[ \\mathbf{R}=\\sigma^2\\mathbf{I}_N. \\] sigma &lt;- getME(fit.lmer, &quot;sigma&quot;)**2 sigma ## [1] 2.049456 head(sigma*diag(nrow(Data))) 2.049456 0.000000 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 2.049456 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 2.049456 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 2.049456 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 2.049456 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 0.000000 2.049456 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.7.9 Marginal covariance matrix \\(\\mathbf{V}\\) of \\(\\mathbf{y}\\) The key decomposition is: \\[ \\mathbf{V}=\\mathbf{Z}\\mathbf{G}\\mathbf{Z}&#39;+\\sigma^2\\mathbf{I}. \\] In your implementation: \\(\\mathbf{Z}\\Lambda_{\\text{new}}\\mathbf{Z}&#39;\\) creates within-subject covariance, \\(\\sigma^2\\mathbf{I}\\) adds residual variance. VM &lt;- Z%*%Lambda_new%*%t(Z)+sigma*diag(nrow(Data)) head(VM) ## 6 x 108 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 108 column names &#39;1&#39;, &#39;2&#39;, &#39;3&#39; ... ]] ## ## 1 5.316240 3.266784 3.266784 3.266784 . . . . . . . ## 2 3.266784 5.316240 3.266784 3.266784 . . . . . . . ## 3 3.266784 3.266784 5.316240 3.266784 . . . . . . . ## 4 3.266784 3.266784 3.266784 5.316240 . . . . . . . ## 5 . . . . 5.316240 3.266784 3.266784 3.266784 . . . ## 6 . . . . 3.266784 5.316240 3.266784 3.266784 . . . ## ## 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## 1 . . . . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . dim(VM) ## [1] 108 108 This \\(\\mathbf{V}\\) is the object that controls: GLS estimation of \\(\\beta\\), shrinkage and BLUP estimation of \\(u\\), standard errors of \\(\\hat{\\beta}\\). 6.7.10 Fixed effect coefficient covariance matrix From theory: \\[ \\mathrm{Var}(\\hat{\\beta})= (\\mathbf{X}&#39;\\mathbf{V}^{-1}\\mathbf{X})^{-1}. \\] vcov(fit.lmer) returns the estimated \\(\\mathrm{Var}(\\hat{\\beta})\\). vcov &lt;- vcov(fit.lmer) #fixed cov vcov ## 3 x 3 Matrix of class &quot;dpoMatrix&quot; ## (Intercept) age SexFemale ## (Intercept) 0.69542669 -4.174818e-02 -2.361967e-01 ## age -0.04174818 3.795289e-03 -3.051416e-17 ## SexFemale -0.23619673 -3.051416e-17 5.797556e-01 # computer correlation coefficients vcov@x[2]/prod(sqrt( diag(vcov(fit.lmer))[-3] )) ## [1] -0.8126236 Interpretation: - diagonal elements are variances of coefficients, - square roots are standard errors, - off-diagonal elements encode correlation among coefficient estimates. 6.7.11 Random effect covariance matrix (standard deviation scale) VarCorr() prints SDs (and correlations if applicable). Matrix::bdiag(vc) produces a block-diagonal covariance structure across random-effect terms. vc &lt;- VarCorr(fit.lmer) vc ### default print method: standard dev and corr ## Groups Name Std.Dev. ## Subject (Intercept) 1.8074 ## Residual 1.4316 as.matrix(Matrix::bdiag(vc)) #random effect covariance matrix (Intercept) (Intercept) 3.266784 # https://stackoverflow.com/questions/47307340/extracting-the-i-estimated-variance-covariance-matrix-of-random-effects-and-or # https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect022.htm And your empirical SD check based on BLUPs: uintercept &lt;- (uhat[[&quot;Subject&quot;]]) sd(uintercept[,1]) ## [1] 1.647809 Important: the SD of the BLUPs is typically smaller than \\(\\hat{\\sigma}_u\\) due to shrinkage. 6.7.12 Compute fixed effect coefficients manually (GLS) Given \\(\\mathbf{V}\\), the GLS estimator is: \\[ \\hat{\\beta}= (\\mathbf{X}&#39;\\mathbf{V}^{-1}\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{V}^{-1}\\mathbf{y}. \\] You compute it directly and compare with bhat. library(matlib) inv(t(as.matrix(X))%*%inv(as.matrix(VM))%*%as.matrix(X))%*%t(as.matrix(X))%*%inv(as.matrix(VM))%*%y 17.7067076 0.6601871 -2.3210230 bhat ## (Intercept) age SexFemale ## 17.7067130 0.6601852 -2.3210227 6.7.13 Compute covariance of fixed effect coefficients manually Theory: \\[ \\mathrm{Var}(\\hat{\\beta})=(\\mathbf{X}&#39;\\mathbf{V}^{-1}\\mathbf{X})^{-1}. \\] Then standard errors are the square roots of the diagonal. inv(t(as.matrix(X))%*%inv(as.matrix(VM))%*%as.matrix(X)) 0.6954267 -0.0417482 -0.2361967 -0.0417482 0.0037953 0.0000000 -0.2361967 0.0000000 0.5797556 # standard error sqrt(inv(t(as.matrix(X))%*%inv(as.matrix(VM))%*%as.matrix(X))) ## Warning in sqrt(inv(t(as.matrix(X)) %*% inv(as.matrix(VM)) %*% as.matrix(X))): ## NaNs produced 0.8339225 NaN NaN NaN 0.0616059 0.0000000 NaN 0.0000000 0.7614169 # the following equals lmm summary vcov(fit.lmer) ## 3 x 3 Matrix of class &quot;dpoMatrix&quot; ## (Intercept) age SexFemale ## (Intercept) 0.69542669 -4.174818e-02 -2.361967e-01 ## age -0.04174818 3.795289e-03 -3.051416e-17 ## SexFemale -0.23619673 -3.051416e-17 5.797556e-01 sqrt(vcov(fit.lmer)) ## Warning in g({: NaNs produced ## 3 x 3 Matrix of class &quot;dsyMatrix&quot; ## (Intercept) age SexFemale ## (Intercept) 0.8339225 NaN NaN ## age NaN 0.06160592 NaN ## SexFemale NaN NaN 0.7614168 6.7.14 Compute random effect coefficients manually (BLUPs) The BLUP formula is: \\[ \\hat{u}=\\mathbf{G}\\mathbf{Z}&#39;\\mathbf{V}^{-1}(\\mathbf{y}-\\mathbf{X}\\hat{\\beta}). \\] Your code implements the same structure and compares to ranef(). comput_uhat &lt;- (as.matrix(Lambda_new))%*%t(Z)%*%inv(as.matrix(VM))%*%(y-as.matrix(X)%*%(bhat)) cbind((comput_uhat@x),(uhat[[&quot;Subject&quot;]])) (comput_uhat@x) (Intercept) M16 -1.7018335 -1.7018336 M05 -1.7018335 -1.7018336 M02 -1.3776748 -1.3776748 M11 -1.1615689 -1.1615689 M07 -1.0535160 -1.0535160 M08 -0.9454631 -0.9454631 M03 -0.6213043 -0.6213043 M12 -0.6213043 -0.6213043 M13 -0.6213043 -0.6213043 M14 -0.0810397 -0.0810397 M09 0.1350662 0.1350662 M15 0.7833837 0.7833837 M06 1.2155954 1.2155954 M04 1.4317012 1.4317013 M01 2.4041775 2.4041776 M10 3.9169184 3.9169185 F10 -3.5853924 -3.5853925 F09 -1.3162811 -1.3162811 F06 -1.3162811 -1.3162811 F01 -1.1001752 -1.1001752 F05 -0.0196460 -0.0196460 F07 0.3045128 0.3045128 F02 0.3045128 0.3045128 F08 0.6286716 0.6286716 F03 0.9528303 0.9528303 F04 1.9253066 1.9253067 F11 3.2219417 3.2219418 6.7.14.1 Covariance object used for random effects You print the diagonal matrix you constructed: head(Lambda_new) 3.266784 0.000000 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 3.266784 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 3.266784 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.266784 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 0.000000 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 In a teaching narrative, this is where you can emphasize: in random intercept models, \\(\\mathbf{G}\\) is extremely simple, all complexity in \\(\\mathbf{V}\\) comes from how \\(\\mathbf{Z}\\) maps subject membership into the observation-level covariance. 6.7.15 Compute predicted values Conditional fitted values are: \\[ \\hat{y}=\\mathbf{X}\\hat{\\beta}+\\mathbf{Z}\\hat{u}. \\] You compute and compare with fitted(). yhat &lt;- X%*%(bhat)+Z%*%(uhat[[&quot;Subject&quot;]][[&quot;(Intercept)&quot;]]) head(yhat) ## 6 x 1 Matrix of class &quot;dgeMatrix&quot; ## [,1] ## 1 25.39237 ## 2 26.71274 ## 3 28.03311 ## 4 29.35348 ## 5 21.61052 ## 6 22.93089 head(fitted(fit.lmer)) ## 1 2 3 4 5 6 ## 25.39237 26.71274 28.03311 29.35348 21.61052 22.93089 Interpretation: \\(\\mathbf{X}\\hat{\\beta}\\) is the population-average prediction, \\(\\mathbf{Z}\\hat{u}\\) adds subject-specific intercept shifts, fitted(fit.lmer) corresponds to the conditional fitted values under the LMM. 6.7.15.1 Key takeaway This random-intercept example is the cleanest case where the entire mixed model machinery reduces to: specify \\(\\mathbf{V}=\\mathbf{Z}\\mathbf{G}\\mathbf{Z}&#39;+\\sigma^2\\mathbf{I}\\), compute GLS \\(\\hat{\\beta}\\), compute BLUP \\(\\hat{u}\\), obtain fitted values \\(\\hat{y}=\\mathbf{X}\\hat{\\beta}+\\mathbf{Z}\\hat{u}\\). 6.8 Linear Mixed Model Covariance Decomposition with Random Slopes (lme4) In this section, we extend the covariance decomposition of linear mixed models (LMMs) from random intercept models to random slope models. Allowing random slopes means that, in addition to subject-specific baselines (intercepts), the effect of a covariate (here, age) is also allowed to vary across subjects. This is particularly important in longitudinal and growth-curve settings, where individuals may follow systematically different trajectories over time. We continue to use the Orthodont dataset, a classic longitudinal dataset that records the distance of orthodontic measurements over age, stratified by subject and sex. 6.8.1 Load data We begin by loading the Orthodont dataset from the nlme package. Each subject has repeated measurements over age, making this dataset well suited for mixed-effects modeling. data(Orthodont, package=&quot;nlme&quot;) Data &lt;- Orthodont 6.8.2 Using a linear mixed model with random slopes We now fit a linear mixed model with: - fixed effects for age and Sex, - a random intercept for each Subject, - and a random slope for age within each Subject. Mathematically, this model can be written as \\[ y_{ij} = (\\beta_0 + u_{0j}) + (\\beta_1 + u_{1j}) \\, \\text{age}_{ij} + \\beta_2 \\, \\text{Sex}_{ij} + \\varepsilon_{ij}, \\] where: - \\((u_{0j}, u_{1j})\\) are subject-specific random effects, - \\(\\varepsilon_{ij} \\sim N(0, \\sigma^2)\\) are residual errors. library(lme4) fit.lmer.slope &lt;- lmer(distance ~ age + Sex +(1+ age | Subject), data = Data) summary(fit.lmer.slope) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: distance ~ age + Sex + (1 + age | Subject) ## Data: Data ## ## REML criterion at convergence: 435.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.0815 -0.4568 0.0155 0.4471 3.8944 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 7.82249 2.7969 ## age 0.05126 0.2264 -0.77 ## Residual 1.71621 1.3100 ## Number of obs: 108, groups: Subject, 27 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 17.63518 0.88623 19.899 ## age 0.66019 0.07125 9.265 ## SexFemale -2.14544 0.75746 -2.832 ## ## Correlation of Fixed Effects: ## (Intr) age ## age -0.838 ## SexFemale -0.348 0.000 Although the residual covariance is independent (diagonal), the random-effects covariance structure is no longer scalar. Instead, each subject has a 2×2 covariance matrix corresponding to the random intercept and random slope. 6.8.3 Extracting the design matrices X, y, and Z To understand the internal structure of the fitted model, we explicitly extract: - X: the fixed-effects design matrix, - y: the response vector, - Z: the random-effects design matrix. The Z matrix is particularly important here because it now contains two columns per subject: one for the random intercept and one for the random slope. X=(getME(fit.lmer.slope, &quot;X&quot;)) head(X) (Intercept) age SexFemale 1 8 0 1 10 0 1 12 0 1 14 0 1 8 0 1 10 0 y=getME(fit.lmer.slope, &quot;y&quot;) head(y) ## [1] 26.0 25.0 29.0 31.0 21.5 22.5 Z &lt;- getME(fit.lmer.slope, &quot;Z&quot;) head(Z) ## 6 x 54 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 54 column names &#39;M16&#39;, &#39;M16&#39;, &#39;M05&#39; ... ]] ## ## 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 8 . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 10 . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 12 . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 14 . . . . . . . . ## 5 . . . . 1 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . 1 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## 1 . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . dim(Z) ## [1] 108 54 Each block of columns in Z corresponds to one subject, and within each block, the columns represent the intercept and slope contributions. 6.8.4 Fixed and random effect coefficients We next extract: - \\(\\hat{\\beta}\\), the estimated fixed-effect coefficients, - \\(\\hat{u}\\), the estimated random effects (BLUPs). The random effects are returned as a list indexed by grouping factor. bhat &lt;- getME(fit.lmer.slope, &quot;fixef&quot;) # fixed effects bhat ## (Intercept) age SexFemale ## 17.6351805 0.6601852 -2.1454431 uhat &lt;- ranef(fit.lmer.slope) uhat ## $Subject ## (Intercept) age ## M16 -0.96698456 -0.06544230 ## M05 -2.15621081 0.04447130 ## M02 -1.58143839 0.02194874 ## M11 0.38737823 -0.13961777 ## M07 -1.40307472 0.03606404 ## M08 0.37415109 -0.11799495 ## M03 -0.83491587 0.02435289 ## M12 -1.82593774 0.11594755 ## M13 -5.59182087 0.46400729 ## M14 0.51944692 -0.04982259 ## M09 -1.07941522 0.11835170 ## M15 -1.11909663 0.18322017 ## M06 1.03469722 0.02495756 ## M04 3.20171821 -0.15492789 ## M01 0.96194797 0.14388309 ## M10 3.04960613 0.09373458 ## F10 -2.31273350 -0.13319695 ## F09 0.32324282 -0.16262237 ## F06 -0.07316594 -0.12598450 ## F01 0.11181130 -0.12268061 ## F05 1.43310624 -0.14279903 ## F07 0.62044804 -0.03708906 ## F02 -0.37057384 0.05450561 ## F08 2.38444671 -0.16952522 ## F03 -0.01384650 0.08273621 ## F04 2.30508388 -0.03978828 ## F11 2.62212981 0.05331079 ## ## with conditional variances for &quot;Subject&quot; Each subject now has two random-effect coefficients: one intercept deviation and one slope deviation. 6.8.5 Random-effect covariance structure (matrix expansion) In a random slope model, the random-effects covariance matrix for a single subject is: \\[ \\mathbf{G}_{\\text{subject}} = \\begin{pmatrix} \\sigma^2_{\\text{int}} &amp; \\sigma_{\\text{int,slope}} \\\\ \\sigma_{\\text{int,slope}} &amp; \\sigma^2_{\\text{slope}} \\end{pmatrix}. \\] The full covariance matrix \\(\\mathbf{G}\\) is block-diagonal, with one such matrix per subject. We explicitly construct this block-diagonal matrix using bdiag(). vc &lt;- VarCorr(fit.lmer.slope) Lambda_new &lt;- bdiag(replicate(length(levels(Data$Subject)), vc[[&quot;Subject&quot;]], simplify = FALSE)) head(Lambda_new) ## 6 x 54 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] 7.822487 -0.48495997 . . . . . . . . ## [2,] -0.484960 0.05126495 . . . . . . . . ## [3,] . . 7.822487 -0.48495997 . . . . . . ## [4,] . . -0.484960 0.05126495 . . . . . . ## [5,] . . . . 7.822487 -0.48495997 . . . . ## [6,] . . . . -0.484960 0.05126495 . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . ## [2,] . . . . . . . ## [3,] . . . . . . . ## [4,] . . . . . . . ## [5,] . . . . . . . ## [6,] . . . . . . . dim(Lambda_new) ## [1] 54 54 Here: - VarCorr() provides the estimated standard deviations and correlations, - bdiag() assembles the subject-specific covariance matrices into a global random-effects covariance matrix. Note In lme4, the internal parameters theta correspond to the Cholesky factors of these covariance matrices. Given theta, the full covariance matrix is reconstructed implicitly. 6.8.6 Residual variance and identity matrix The residual errors are assumed independent with variance \\(\\sigma^2\\). We extract \\(\\sigma^2\\) and explicitly form \\(\\sigma^2 I\\). sigma &lt;- getME(fit.lmer.slope, &quot;sigma&quot;)**2 sigma ## [1] 1.71621 head(sigma*diag(nrow(Data))) 1.71621 0.00000 0.00000 0.00000 0.00000 0.00000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.00000 1.71621 0.00000 0.00000 0.00000 0.00000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.00000 0.00000 1.71621 0.00000 0.00000 0.00000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.00000 0.00000 0.00000 1.71621 0.00000 0.00000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.00000 0.00000 0.00000 0.00000 1.71621 0.00000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.00000 0.00000 0.00000 0.00000 0.00000 1.71621 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.8.7 Covariance matrix of the response vector y The marginal covariance of \\(\\mathbf{y}\\) is: \\[ \\mathbf{V} = \\mathbf{Z G Z}^\\top + \\sigma^2 \\mathbf{I}. \\] We compute this matrix explicitly. VM &lt;- Z%*%Lambda_new%*%t(Z)+sigma*diag(nrow(Data)) head(VM) ## 6 x 108 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 108 column names &#39;1&#39;, &#39;2&#39;, &#39;3&#39; ... ]] ## ## 1 5.060294 3.194403 3.044722 2.895042 . . . . . . . ## 2 3.194403 4.965992 3.305161 3.360540 . . . . . . . ## 3 3.044722 3.305161 5.281810 3.826039 . . . . . . . ## 4 2.895042 3.360540 3.826039 6.007747 . . . . . . . ## 5 . . . . 5.060294 3.194403 3.044722 2.895042 . . . ## 6 . . . . 3.194403 4.965992 3.305161 3.360540 . . . ## ## 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## 1 . . . . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . dim(VM) ## [1] 108 108 This matrix fully characterizes the dependence structure induced by the random intercepts and random slopes. 6.8.8 Covariance matrix of fixed-effect coefficients The covariance matrix of \\(\\hat{\\beta}\\) is given by: \\[ \\operatorname{Var}(\\hat{\\beta}) = (\\mathbf{X}^\\top \\mathbf{V}^{-1} \\mathbf{X})^{-1}. \\] We confirm that this matches the output from vcov(). vcov &lt;- vcov(fit.lmer.slope) # fixed-effect covariance vcov ## 3 x 3 Matrix of class &quot;dpoMatrix&quot; ## (Intercept) age SexFemale ## (Intercept) 0.78540227 -5.292131e-02 -2.337501e-01 ## age -0.05292131 5.076869e-03 -5.668086e-16 ## SexFemale -0.23375009 -5.668086e-16 5.737502e-01 # compute correlation coefficient vcov@x[2]/prod(sqrt( diag(vcov(fit.lmer.slope))[-3] )) ## [1] -0.8380822 6.8.9 Random-effect covariance matrix The block-diagonal random-effects covariance matrix can also be extracted directly. vc &lt;- VarCorr(fit.lmer.slope) vc ## standard deviations and correlations ## Groups Name Std.Dev. Corr ## Subject (Intercept) 2.79687 ## age 0.22642 -0.766 ## Residual 1.31004 as.matrix(Matrix::bdiag(vc)) # random-effect covariance matrix (Intercept) age (Intercept) 7.822487 -0.4849600 age -0.484960 0.0512649 6.8.10 Computing fixed-effect coefficients manually Using the generalized least squares (GLS) formula, \\[ \\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{V}^{-1} \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{V}^{-1}\\mathbf{y}, \\] we compute \\(\\hat{\\beta}\\) explicitly and verify it matches the model output. library(matlib) inv(t(as.matrix(X))%*%inv(as.matrix(VM))%*%as.matrix(X))%*%t(as.matrix(X))%*%inv(as.matrix(VM))%*%y 17.635190 0.660189 -2.145444 bhat ## (Intercept) age SexFemale ## 17.6351805 0.6601852 -2.1454431 6.8.11 Covariance of fixed-effect coefficients We also compute the covariance matrix and standard errors manually. inv(t(as.matrix(X))%*%inv(as.matrix(VM))%*%as.matrix(X)) 0.7854023 -0.0529213 -0.2337501 -0.0529213 0.0050769 0.0000000 -0.2337501 0.0000000 0.5737502 # standard errors sqrt(inv(t(as.matrix(X))%*%inv(as.matrix(VM))%*%as.matrix(X))) ## Warning in sqrt(inv(t(as.matrix(X)) %*% inv(as.matrix(VM)) %*% as.matrix(X))): ## NaNs produced 0.8862292 NaN NaN NaN 0.0712522 0.000000 NaN 0.0000000 0.757463 # matches lmer output vcov(fit.lmer.slope) ## 3 x 3 Matrix of class &quot;dpoMatrix&quot; ## (Intercept) age SexFemale ## (Intercept) 0.78540227 -5.292131e-02 -2.337501e-01 ## age -0.05292131 5.076869e-03 -5.668086e-16 ## SexFemale -0.23375009 -5.668086e-16 5.737502e-01 sqrt(vcov(fit.lmer.slope)) ## Warning in g({: NaNs produced ## 3 x 3 Matrix of class &quot;dsyMatrix&quot; ## (Intercept) age SexFemale ## (Intercept) 0.8862292 NaN NaN ## age NaN 0.07125215 NaN ## SexFemale NaN NaN 0.757463 6.8.12 Computing random-effect coefficients The BLUPs of the random effects are given by: \\[ \\hat{\\mathbf{u}} = \\mathbf{GZ}^\\top \\mathbf{V}^{-1} (\\mathbf{y} - \\mathbf{X}\\hat{\\beta}). \\] We compute these manually and compare them with ranef(). comput_uhat &lt;- (as.matrix(Lambda_new))%*%t(Z)%*%inv(as.matrix(VM))%*%(y-as.matrix(X)%*%(bhat)) cbind((comput_uhat@x),(uhat[[&quot;Subject&quot;]])) ## Warning in data.frame(..., check.names = FALSE): row names were found from a ## short variable and have been discarded (comput_uhat@x) (Intercept) age -0.9669845 -0.9669846 -0.0654423 -0.0654423 -2.1562108 0.0444713 -2.1562108 -1.5814384 0.0219487 0.0444713 0.3873782 -0.1396178 -1.5814383 -1.4030747 0.0360640 0.0219487 0.3741511 -0.1179949 0.3873782 -0.8349159 0.0243529 -0.1396178 -1.8259377 0.1159476 -1.4030747 -5.5918209 0.4640073 0.0360640 0.5194469 -0.0498226 0.3741511 -1.0794152 0.1183517 -0.1179950 -1.1190966 0.1832202 -0.8349158 1.0346972 0.0249576 0.0243529 3.2017182 -0.1549279 -1.8259377 0.9619480 0.1438831 0.1159476 3.0496061 0.0937346 -5.5918208 -2.3127335 -0.1331970 0.4640073 0.3232428 -0.1626224 0.5194469 -0.0731659 -0.1259845 -0.0498226 0.1118113 -0.1226806 -1.0794152 1.4331062 -0.1427990 0.1183517 0.6204480 -0.0370891 -1.1190966 -0.3705738 0.0545056 0.1832202 2.3844467 -0.1695252 1.0346972 -0.0138465 0.0827362 0.0249576 2.3050839 -0.0397883 3.2017181 2.6221298 0.0533108 -0.1549279 -0.9669846 -0.0654423 0.9619480 -2.1562108 0.0444713 0.1438831 -1.5814384 0.0219487 3.0496061 0.3873782 -0.1396178 0.0937346 -1.4030747 0.0360640 -2.3127334 0.3741511 -0.1179949 -0.1331970 -0.8349159 0.0243529 0.3232428 -1.8259377 0.1159476 -0.1626224 -5.5918209 0.4640073 -0.0731659 0.5194469 -0.0498226 -0.1259845 -1.0794152 0.1183517 0.1118113 -1.1190966 0.1832202 -0.1226806 1.0346972 0.0249576 1.4331062 3.2017182 -0.1549279 -0.1427990 0.9619480 0.1438831 0.6204480 3.0496061 0.0937346 -0.0370891 -2.3127335 -0.1331970 -0.3705738 0.3232428 -0.1626224 0.0545056 -0.0731659 -0.1259845 2.3844467 0.1118113 -0.1226806 -0.1695252 1.4331062 -0.1427990 -0.0138465 0.6204480 -0.0370891 0.0827362 -0.3705738 0.0545056 2.3050838 2.3844467 -0.1695252 -0.0397883 -0.0138465 0.0827362 2.6221297 2.3050839 -0.0397883 0.0533108 2.6221298 0.0533108 6.8.13 Predicted values Predicted values are obtained by combining fixed and random components: \\[ \\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\beta} + \\mathbf{Z}\\hat{\\mathbf{u}}. \\] yhat &lt;- X%*%(bhat)+Z%*%comput_uhat head(yhat) ## 6 x 1 Matrix of class &quot;dgeMatrix&quot; ## [,1] ## 1 25.02967 ## 2 26.63781 ## 3 28.24595 ## 4 29.85408 ## 5 21.51081 ## 6 22.87508 head(fitted(fit.lmer.slope)) ## 1 2 3 4 5 6 ## 25.02967 26.63781 28.24595 29.85408 21.51081 22.87508 6.8.14 Model with two grouping factors We now illustrate a model with two random-effect grouping factors, adding a random intercept for age as well. library(lme4) fit.lmer.slope2 &lt;- lmer(distance ~ age + Sex +(1+ age | Subject)+ (1 | age), data = Data) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(fit.lmer.slope2) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: distance ~ age + Sex + (1 + age | Subject) + (1 | age) ## Data: Data ## ## REML criterion at convergence: 435.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.0821 -0.4568 0.0154 0.4471 3.8942 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 7.81589 2.7957 ## age 0.05122 0.2263 -0.77 ## age (Intercept) 0.00000 0.0000 ## Residual 1.71643 1.3101 ## Number of obs: 108, groups: Subject, 27; age, 4 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 17.63511 0.88612 19.902 ## age 0.66019 0.07124 9.267 ## SexFemale -2.14527 0.75745 -2.832 ## ## Correlation of Fixed Effects: ## (Intr) age ## age -0.838 ## SexFemale -0.348 0.000 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) 6.8.15 Extracting Z for the two-factor model Here, the Z matrix expands accordingly. The dimension reflects the combined random effects. Z2 &lt;- getME(fit.lmer.slope2, &quot;Z&quot;) head(Z2,10) ## 10 x 58 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 58 column names &#39;M16&#39;, &#39;M16&#39;, &#39;M05&#39; ... ]] ## ## 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 8 . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 10 . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 12 . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 14 . . . . . . . ## 5 . . . . 1 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . 1 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 7 . . . . 1 12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 8 . . . . 1 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 9 . . . . . . . . . . . . 1 8 . . . . . . . . . . . . . . . . . . . . . . . ## 10 . . . . . . . . . . . . 1 10 . . . . . . . . . . . . . . . . . . . . . . . ## ## 1 . . . . . . . . . . . . . . . . . 1 . . . ## 2 . . . . . . . . . . . . . . . . . . 1 . . ## 3 . . . . . . . . . . . . . . . . . . . 1 . ## 4 . . . . . . . . . . . . . . . . . . . . 1 ## 5 . . . . . . . . . . . . . . . . . 1 . . . ## 6 . . . . . . . . . . . . . . . . . . 1 . . ## 7 . . . . . . . . . . . . . . . . . . . 1 . ## 8 . . . . . . . . . . . . . . . . . . . . 1 ## 9 . . . . . . . . . . . . . . . . . 1 . . . ## 10 . . . . . . . . . . . . . . . . . . 1 . . dim(Z2) ## [1] 108 58 6.8.16 Nested versus crossed random effects Finally, we demonstrate a nested random-effects structure, where subjects are nested within sex. library(lme4) fit.lmer.slope3 &lt;- lmer(distance ~ age + Sex +(1 | Sex/Subject), data = Data) ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: large eigenvalue ratio ## - Rescale variables? summary(fit.lmer.slope3) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: distance ~ age + Sex + (1 | Sex/Subject) ## Data: Data ## ## REML criterion at convergence: 437.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7489 -0.5503 -0.0252 0.4534 3.6575 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Subject:Sex (Intercept) 3.2668 1.8074 ## Sex (Intercept) 0.9392 0.9691 ## Residual 2.0495 1.4316 ## Number of obs: 108, groups: Subject:Sex, 27; Sex, 2 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 17.70671 1.27852 13.85 ## age 0.66019 0.06161 10.72 ## SexFemale -2.32102 1.56785 -1.48 ## ## Correlation of Fixed Effects: ## (Intr) age ## age -0.530 ## SexFemale -0.586 0.000 ## optimizer (nloptwrap) convergence code: 0 (OK) ## Model is nearly unidentifiable: large eigenvalue ratio ## - Rescale variables? The corresponding Z matrix reflects this nesting. Z3 &lt;- getME(fit.lmer.slope3, &quot;Z&quot;) head(Z3,10) ## 10 x 29 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 29 column names &#39;M16:Male&#39;, &#39;M05:Male&#39;, &#39;M02:Male&#39; ... ]] ## ## 1 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . 1 . ## 2 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . 1 . ## 3 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . 1 . ## 4 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . 1 . ## 5 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . 1 . ## 6 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . 1 . ## 7 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . 1 . ## 8 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . 1 . ## 9 . . . . . . 1 . . . . . . . . . . . . . . . . . . . . 1 . ## 10 . . . . . . 1 . . . . . . . . . . . . . . . . . . . . 1 . dim(Z3) ## [1] 108 29 6.8.17 Remark: Nested vs. crossed random effects Nested random effects occur when one grouping factor appears only within levels of another: (1 | group1/group2) Crossed random effects occur when observations are independently grouped by multiple factors: (1 | group1) + (1 | group2) Understanding how these structures affect the Z matrix and covariance decomposition is essential for correctly specifying and interpreting linear mixed models in practice. 6.9 Linear Mixed Model Covariance Decomposition (nlme) This chapter is a practical, “under-the-hood” guide to linear mixed models (LMMs) using nlme, with a strong focus on covariance decomposition and how it connects to estimation, inference, and prediction. We will fit models in R, extract the key design matrices, and then reproduce (parts of) the computations by hand to make the machinery transparent. Throughout, we work with the Orthodont dataset, a classic longitudinal dataset with repeated measurements per subject. The key statistical idea is simple: repeated observations within the same subject are correlated, that correlation can be explained by random effects, correlated residual errors, or both, and once we specify the covariance structure correctly, estimation becomes a generalized least squares (GLS) problem. 6.9.1 Load data data(Orthodont, package=&quot;nlme&quot;) Data &lt;- Orthodont The Orthodont dataset contains repeated measurements of a distance outcome over time (age). Each subject is measured multiple times, so the data are clustered by Subject. This repeated-measures structure is exactly where mixed models become useful. A quick reminder of notation we will use in this chapter: \\(i\\) indexes subjects, \\(j\\) indexes repeated observations within subject \\(i\\). \\(y_{ij}\\) is the response (distance). \\(\\mathbf{X}\\) is the fixed-effect design matrix. \\(\\mathbf{Z}\\) is the random-effect design matrix. \\(\\boldsymbol\\beta\\) are fixed-effect parameters. \\(\\mathbf{u}\\) are random effects. \\(\\boldsymbol\\varepsilon\\) are residual errors. 6.9.2 Exploratory visualization Before fitting a model, we want to visually confirm two things: the overall mean trend of distance over age, and whether subjects appear to have systematic deviations from that trend. The first plot stratifies by sex and overlays a linear trend line. library(ggplot2) ggplot(Data, aes(y = distance, x = age, color = Sex)) + geom_smooth(method = &quot;lm&quot;,se=F) + geom_point() + theme_classic() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Interpretation: - If the slopes differ between sexes, that suggests an interaction (not modeled here). - If points form vertical “bands” by subject (hidden here), that suggests subject-to-subject heterogeneity. The second plot includes Subject in the aesthetics. Even if the visual is busy, it helps us see whether each subject follows a roughly parallel trajectory shifted up or down. That pattern is the classic signal for a random intercept. library(ggplot2) ggplot(Data, aes(y = distance, x = age, fill = Subject, color = Sex)) + geom_smooth(method = &quot;lm&quot;,se=F) + geom_point() + theme_classic() ## `geom_smooth()` using formula = &#39;y ~ x&#39; If subjects show consistent vertical separation across age, a random intercept is a natural first model. If subjects also differ in slope, we would consider random slopes (covered in a separate chapter / section). 6.9.3 Using lme (nlme) We first fit a random intercept model using nlme::lme. The model is: \\[ y_{ij} = \\beta_0 + \\beta_1 \\, \\text{age}_{ij} + \\beta_2 \\, \\text{Sex}_i + b_i + \\varepsilon_{ij}, \\] with assumptions \\[ b_i \\sim N(0,\\sigma_b^2), \\quad \\varepsilon_{ij} \\sim N(0,\\sigma^2), \\quad b_i \\perp \\varepsilon_{ij}. \\] This implies a within-subject covariance of compound symmetry form (equal correlation between any two observations from the same subject), because the shared random intercept induces correlation. We fit the model in nlme and also fit the “equivalent” random-intercept model in lme4 (we will use lme4 later to extract \\(\\mathbf{X}\\), \\(\\mathbf{Z}\\), and \\(\\mathbf{y}\\)). library(nlme) ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmList ## The following object is masked from &#39;package:matlib&#39;: ## ## Dim ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse fit.lmer2 &lt;- lme(distance ~ age + Sex , random = ~1 | Subject, data = Data) summary(fit.lmer2) ## Linear mixed-effects model fit by REML ## Data: Data ## AIC BIC logLik ## 447.5125 460.7823 -218.7563 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.807425 1.431592 ## ## Fixed effects: distance ~ age + Sex ## Value Std.Error DF t-value p-value ## (Intercept) 17.706713 0.8339225 80 21.233044 0.0000 ## age 0.660185 0.0616059 80 10.716263 0.0000 ## SexFemale -2.321023 0.7614168 25 -3.048294 0.0054 ## Correlation: ## (Intr) age ## age -0.813 ## SexFemale -0.372 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.74889609 -0.55034466 -0.02516628 0.45341781 3.65746539 ## ## Number of Observations: 108 ## Number of Groups: 27 library(lme4) # lme4 fit.lmer &lt;- lmer(distance ~ age + Sex +(1 | Subject), data = Data) # compound symmetry Practical note: - In this simplest random-intercept setting, lme and lmer should agree closely on fixed-effect estimates and variance components (allowing for minor numerical differences). - The big advantage of nlme is the ability to specify residual correlation structures (AR(1), exponential, Gaussian, unstructured, etc.), which we explore later. 6.9.4 Inference for fixed effects After fitting the model, we usually want to test whether covariates such as age and Sex are associated with the outcome. Mixed models complicate testing because correlation changes both the standard errors and the effective degrees of freedom. 6.9.4.1 t test (Wald-type test) Here we compute a t-statistic “by hand” from the fixed-effect estimates and their estimated covariance matrix: \\[ t_k = \\frac{\\hat\\beta_k}{\\sqrt{\\widehat{\\mathrm{Var}}(\\hat\\beta_k)}}. \\] We then compute two-sided p-values using the degrees of freedom stored by nlme. tstat &lt;- fixef(fit.lmer2)/sqrt(diag(vcov(fit.lmer2))) pval &lt;- 2*pt(-abs(tstat), df = fit.lmer2$fixDF$X) Tresult &lt;- data.frame(t = tstat, p = round(pval, 4)) print(Tresult) ## t p ## (Intercept) 21.233044 0.0000 ## age 10.716263 0.0000 ## SexFemale -3.048294 0.0054 Why this matters: - vcov(fit.lmer2) is not the OLS covariance; it is the GLS-based covariance that accounts for \\(\\mathbf{V}\\), the marginal covariance of \\(\\mathbf{y}\\). - The degrees of freedom in nlme reflect an approximate finite-sample adjustment for mixed models, not a simple \\(n-p\\). 6.9.4.2 F test (ANOVA table) The anova() method provides an ANOVA-style summary of fixed effects. anova(fit.lmer2 ) numDF denDF F-value p-value (Intercept) 1 80 4123.155691 0.0000000 age 1 80 114.838287 0.0000000 Sex 1 25 9.292099 0.0053751 Interpretation: - This is typically a Wald-type test presented as an F statistic. - It answers whether each term contributes after accounting for the covariance structure. 6.9.4.3 Restricted model test (linear contrast) Sometimes we want to test a specific linear hypothesis about fixed effects: \\[ H_0: \\mathbf{L}\\boldsymbol\\beta = 0. \\] The L vector specifies a linear combination of coefficients. In this example, L = c(0,1,0) targets the second fixed-effect coefficient (often the age effect, depending on parameter ordering). anova(fit.lmer2, L = c(0, 1, 0)) numDF denDF F-value p-value 1 80 114.8383 0 This approach generalizes cleanly to: - testing multiple coefficients jointly, - testing contrasts among factor levels, - testing spline basis groups, etc. 6.9.4.4 Likelihood ratio test (nested models) We compare: full model: distance ~ age + Sex reduced model: distance ~ Sex Both have the same random intercept structure, so the models are nested in their fixed effects. fit.lmer2.2 &lt;- lme(distance ~ Sex , random = ~1 | Subject, data = Data) anova(fit.lmer2, fit.lmer2.2) ## Warning in anova.lme(fit.lmer2, fit.lmer2.2): fitted objects with different ## fixed effects. REML comparisons are not meaningful. call Model df AIC BIC logLik Test L.Ratio p-value fit.lmer2 lme.formula(fixed = distance ~ age + Sex, data = Data, random = ~1 | Subject) 1 5 447.5125 460.7823 -218.7563 NA NA fit.lmer2.2 lme.formula(fixed = distance ~ Sex, data = Data, random = ~1 | Subject) 2 4 513.8718 524.5255 -252.9359 1 vs 2 68.35926 0 A likelihood ratio test uses: \\[ \\mathrm{LR} = 2(\\ell_1 - \\ell_0), \\] and compares LR to a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in number of parameters (here, the difference in fixed effects). And now by hand logLik &lt;- logLik(fit.lmer2) logLik0 &lt;- logLik(fit.lmer2.2) LR &lt;- 2 * (logLik - logLik0) pval &lt;- pchisq(LR, df = 2, lower.tail = FALSE) LRresult &lt;- data.frame(LR = LR, p = round(pval, 4), row.names = &quot;age&quot;) print(LRresult) ## LR p ## age 68.35926 0 Practical note: - For testing variance components (e.g., whether a random effect variance is zero), LR tests can be nonstandard due to boundary issues. - Here we are testing a fixed effect, so the usual large-sample chi-square reference is commonly used. 6.9.5 Model diagnosing Even if inference looks “significant,” model assumptions can be wrong. Diagnostic plots help assess: residual patterns (nonlinearity, heteroscedasticity), normality of residuals, influential clusters, random-effects distribution. library(nlme) plot.lme(fit.lmer2) In longitudinal data, a frequent warning sign is residual correlation that remains after adding random effects. If residuals within each subject still show time dependence, we may need an explicit residual correlation structure (AR(1), exponential, etc.), which we will fit later in this chapter. 6.9.6 Get X, y, Z matrices (using lme4) To connect the model to matrix formulas, we extract the core objects: \\(\\mathbf{X}\\): fixed-effect design matrix \\(\\mathbf{Z}\\): random-effect design matrix \\(\\mathbf{y}\\): response vector We use lme4 because getME() provides these matrices directly in a standard form. X=(getME(fit.lmer, &quot;X&quot;)) y=getME(fit.lmer, &quot;y&quot;) Z &lt;- getME(fit.lmer, &quot;Z&quot;) Z2 &lt;- model.matrix(~Subject-1,data=Data) Interpretation: - For a random intercept model, \\(\\mathbf{Z}\\) is essentially a subject indicator matrix (one column per subject). - Each row has a single 1 indicating the subject membership. 6.9.6.1 Check Z matrix We confirm that Z matches a manually created subject dummy matrix. dummyz&lt;- as.matrix (Z)==Z2 table(dummyz) TRUE 2916 dim(Z) ## [1] 108 27 This is a useful sanity check: it ensures that when we later compute \\(\\mathbf{Z}\\mathbf{D}\\mathbf{Z}^\\top\\), we are using the same \\(\\mathbf{Z}\\) that the model implies. 6.9.6.2 Check y and X matrices We also check that columns in \\(\\mathbf{X}\\) match the original variables. dummyx&lt;- Data[,c(2)]==X[,2] table(dummyx) TRUE 108 6.9.7 Fixed effect coefficient The fixed effects \\(\\hat{\\boldsymbol\\beta}\\) represent the population-average association between distance and the covariates, after accounting for within-subject correlation. bhat &lt;- fit.lmer2$coef$fixed bhat ## (Intercept) age SexFemale ## 17.7067130 0.6601852 -2.3210227 # fixef() 6.9.7.1 Fixed effect confidence intervals nlme::intervals() provides approximate confidence intervals for fixed effects. intervals(fit.lmer2) ## Approximate 95% confidence intervals ## ## Fixed effects: ## lower est. upper ## (Intercept) 16.0471544 17.7067130 19.3662716 ## age 0.5375855 0.6601852 0.7827849 ## SexFemale -3.8891901 -2.3210227 -0.7528554 ## ## Random Effects: ## Level: Subject ## lower est. upper ## sd((Intercept)) 1.310357 1.807425 2.493049 ## ## Within-group standard error: ## lower est. upper ## 1.226095 1.431592 1.671531 These intervals are typically Wald-type: \\[ \\hat\\beta_k \\pm t_{0.975,\\;df}\\sqrt{\\widehat{\\mathrm{Var}}(\\hat\\beta_k)}. \\] 6.9.7.2 Plot fixed effect coefficients and their CI Visualizing coefficients often communicates results faster than tables, especially in teaching materials. tab &lt;- cbind(Est = intervals(fit.lmer2)[[&quot;fixed&quot;]][,2], LL = intervals(fit.lmer2)[[&quot;fixed&quot;]][,1], UL= intervals(fit.lmer2)[[&quot;fixed&quot;]][,3]) #Extracting the fixed effect estimates and manually calculating the 95% confidence limits #qnorm extracts the standard normal critical value for 1-alpha/2= 1-0.05/2=0.975 Odds= (tab) round(Odds,4) Est LL UL (Intercept) 17.7067 16.0472 19.3663 age 0.6602 0.5376 0.7828 SexFemale -2.3210 -3.8892 -0.7529 Odds=as.data.frame(Odds) Odds$Label=rownames(Odds) ggplot(Odds[-1,],aes(x=Est,y=Label))+geom_point()+ geom_errorbarh(aes(xmax = UL, xmin = LL))+ theme_bw()+geom_vline(xintercept=0,col=&quot;darkgray&quot;,size=1.2,linetype=2)+ theme(axis.title.y = element_blank(),axis.text = element_text(size=10), axis.title.x = element_text(size=12))+labs(x=&quot;Beta coefficients&quot;) ## Warning: `geom_errorbarh()` was deprecated in ggplot2 4.0.0. ## ℹ Please use the `orientation` argument of ## `geom_errorbar()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see ## where this warning was generated. Interpretation tips: - Intervals crossing 0 suggest the effect may be weak relative to uncertainty. - The vertical line at 0 is a convenient “no effect” reference. And now by hand We replicate the fixed-effect CI using the stored degrees of freedom and the covariance matrix. print(tquants &lt;- qt(0.975, df = fit.lmer2$fixDF$X)) ## (Intercept) age SexFemale ## 1.990063 1.990063 2.059539 Low &lt;- fixef(fit.lmer2) - tquants * sqrt(diag(vcov(fit.lmer2))) Upp &lt;- fixef(fit.lmer2) + tquants * sqrt(diag(vcov(fit.lmer2))) EstInt &lt;- data.frame(Lower = Low, Estimate = fixef(fit.lmer2), Upper = Upp) print(EstInt) ## Lower Estimate Upper ## (Intercept) 16.0471544 17.7067130 19.3662716 ## age 0.5375855 0.6601852 0.7827849 ## SexFemale -3.8891901 -2.3210227 -0.7528554 This is an important conceptual checkpoint: it shows that once we know \\(\\widehat{\\mathrm{Var}}(\\hat{\\boldsymbol\\beta})\\), confidence intervals reduce to standard linear model logic—the “mixed model” complexity is contained in estimating \\(\\mathbf{V}\\) and thus the correct covariance of \\(\\hat{\\boldsymbol\\beta}\\). 6.9.8 Random effect coefficient The random effects are subject-specific intercept deviations (BLUPs / empirical Bayes estimates). These quantify how each subject’s average distance differs from the population mean after adjusting for covariates. fit.lmer2$coef$random ## $Subject ## (Intercept) ## M16 -1.70183357 ## M05 -1.70183357 ## M02 -1.37767479 ## M11 -1.16156894 ## M07 -1.05351602 ## M08 -0.94546309 ## M03 -0.62130432 ## M12 -0.62130432 ## M13 -0.62130432 ## M14 -0.08103969 ## M09 0.13506616 ## M15 0.78338371 ## M06 1.21559540 ## M04 1.43170125 ## M01 2.40417758 ## M10 3.91691853 ## F10 -3.58539251 ## F09 -1.31628108 ## F06 -1.31628108 ## F01 -1.10017523 ## F05 -0.01964599 ## F07 0.30451279 ## F02 0.30451279 ## F08 0.62867156 ## F03 0.95283034 ## F04 1.92530666 ## F11 3.22194176 Interpretation: - Large positive random intercept: subject tends to have larger distance than average. - Large negative random intercept: subject tends to have smaller distance than average. - Shrinkage: subjects with fewer observations are pulled more strongly toward 0. 6.9.9 Fixed effect coefficients covariance The covariance matrix of fixed effect estimates is central for inference: \\[ \\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol\\beta}) = (\\mathbf{X}^\\top \\hat{\\mathbf{V}}^{-1}\\mathbf{X})^{-1}. \\] We inspect both the matrix and the standard errors. fit.lmer2$varFix (Intercept) age SexFemale (Intercept) 0.6954267 -0.0417482 -0.2361967 age -0.0417482 0.0037953 0.0000000 SexFemale -0.2361967 0.0000000 0.5797556 # vcov(fit.lmer2) sqrt(diag(vcov(fit.lmer2))) ## (Intercept) age SexFemale ## 0.83392247 0.06160592 0.76141685 6.9.9.1 Fixed effect coefficients correlation The next line manually computes a correlation between two fixed-effect estimates by dividing an off-diagonal covariance term by the product of standard deviations. (In practice, you would usually compute the full correlation matrix, but this single calculation emphasizes the concept.) -0.04174818/prod(sqrt( diag(fit.lmer2$varFix)[-3] )) ## [1] -0.8126236 Why care about correlation among estimates? - Strong correlation can make interpretation unstable (e.g., intercept and slope correlation in centered vs uncentered age). - It affects joint inference and multicollinearity diagnostics. 6.9.10 Random effect covariance and correlation For a random intercept model, the random-effect covariance \\(\\mathbf{D}\\) is a \\(1\\times1\\) matrix (a single variance). In more complex models (random slopes), \\(\\mathbf{D}\\) becomes a full covariance matrix. Here we extract the random-effect covariance from the internal structures. pdMatrix(fit.lmer2$modelStruct$reStruct[[1]])*fit.lmer2$sigma**2 (Intercept) (Intercept) 3.266784 getVarCov(fit.lmer2) #using default ## Random effects variance covariance matrix ## (Intercept) ## (Intercept) 3.2668 ## Standard Deviations: 1.8074 getVarCov(fit.lmer2, individual = &quot;F01&quot;, type = &quot;marginal&quot;) ## Subject F01 ## Marginal variance covariance matrix ## 1 2 3 4 ## 1 5.3162 3.2668 3.2668 3.2668 ## 2 3.2668 5.3162 3.2668 3.2668 ## 3 3.2668 3.2668 5.3162 3.2668 ## 4 3.2668 3.2668 3.2668 5.3162 ## Standard Deviations: 2.3057 2.3057 2.3057 2.3057 getVarCov(fit.lmer2, type = &quot;conditional&quot;) ## Subject M01 ## Conditional variance covariance matrix ## 1 2 3 4 ## 1 2.0495 0.0000 0.0000 0.0000 ## 2 0.0000 2.0495 0.0000 0.0000 ## 3 0.0000 0.0000 2.0495 0.0000 ## 4 0.0000 0.0000 0.0000 2.0495 ## Standard Deviations: 1.4316 1.4316 1.4316 1.4316 Interpretation: - type=\"marginal\" returns the covariance of \\(\\mathbf{y}\\) within a subject after integrating over random effects. - type=\"conditional\" returns the covariance of \\(\\mathbf{y}\\) given the random effects (often closer to \\(\\boldsymbol\\Sigma\\)). We also look at variance components and compute the proportion of each component. vc &lt;- as.numeric(as.matrix(VarCorr(fit.lmer2))[,1]) vc/sum(vc) ## [1] 0.6144914 0.3855086 VarCorr(fit.lmer2) ## Subject = pdLogChol(1) ## Variance StdDev ## (Intercept) 3.266784 1.807425 ## Residual 2.049456 1.431592 This proportion is a useful descriptive measure: - If random intercept variance dominates, subject-to-subject differences explain most variability. - If residual variance dominates, within-subject noise is large relative to between-subject differences. 6.9.11 Get y covariance directly A core identity in LMMs is the marginal covariance decomposition: \\[ \\mathbf{V} = \\mathbf{Z}\\mathbf{D}\\mathbf{Z}^\\top + \\boldsymbol\\Sigma. \\] In the simplest random intercept model with independent residuals, \\(\\boldsymbol\\Sigma = \\sigma^2 \\mathbf{I}\\), and \\(\\mathbf{Z}\\mathbf{D}\\mathbf{Z}^\\top\\) creates equal covariance between any two observations in the same subject. We extract \\(\\mathbf{V}\\) directly. = Z REcov Zt + sigma**2 require(mgcv) ## Loading required package: mgcv ## This is mgcv 1.9-1. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. library(nlme) cov &lt;- extract.lme.cov(fit.lmer2,Data) head(cov) 5.316240 3.266784 3.266784 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 5.316240 3.266784 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 3.266784 5.316240 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 3.266784 3.266784 5.316240 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 5.316240 3.266784 3.266784 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.266784 5.316240 3.266784 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(cov) ## [1] 108 108 Practical reason to do this: - Once we have \\(\\mathbf{V}\\), we can compute GLS estimates and prediction variances directly using matrix formulas. - This also makes it easy to compare alternative covariance models: only \\(\\mathbf{V}\\) changes. 6.9.12 Residual variance For the baseline random intercept model, the residual variance is \\(\\sigma^2\\). fit.lmer2$sigma**2 ## [1] 2.049456 6.9.13 Compute fixed effect coefficients (by GLS) The GLS estimator is: \\[ \\hat{\\boldsymbol\\beta} = (\\mathbf{X}^\\top \\mathbf{V}^{-1}\\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{V}^{-1}\\mathbf{y}. \\] We compute it directly using the extracted covariance matrix. library(matlib) inv(t(as.matrix(X))%*%inv(as.matrix(cov))%*%as.matrix(X))%*%t(as.matrix(X))%*%inv(as.matrix(cov))%*%y 17.7067076 0.6601871 -2.3210230 This should match fit.lmer2$coef$fixed (up to numerical rounding). This is the central “bridge” between mixed models and GLS: mixed models reduce to GLS once \\(\\mathbf{V}\\) is known. 6.9.14 Compute covariance of fixed effect coefficients As noted earlier: \\[ \\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol\\beta}) = (\\mathbf{X}^\\top \\mathbf{V}^{-1}\\mathbf{X})^{-1}. \\] inv(t(as.matrix(X))%*%inv(as.matrix(cov))%*%as.matrix(X)) 0.6954267 -0.0417482 -0.2361967 -0.0417482 0.0037953 0.0000000 -0.2361967 0.0000000 0.5797556 6.9.15 Compute random effect coefficients (BLUP) The BLUP / empirical Bayes estimate satisfies: \\[ \\hat{\\mathbf{u}} = \\mathbf{D}\\mathbf{Z}^\\top \\mathbf{V}^{-1}(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol\\beta}). \\] For a random intercept model, \\(\\mathbf{D}\\) is just the scalar \\(\\sigma_b^2\\). In code, Lambda_new constructs a diagonal matrix with that variance repeated for each subject (one random intercept per subject). Lambda_new &lt;-as.numeric(VarCorr(fit.lmer2)[1])*diag(length(levels(Data$Subject))) # head(Lambda_new) Now we compute \\(\\hat{\\mathbf{u}}\\) and compare with the model’s own random-effect estimates. uhat &lt;- fit.lmer2$coef$random comput_uhat &lt;- (as.matrix(Lambda_new))%*%t(Z)%*%inv(as.matrix(cov))%*%(y-as.matrix(X)%*%(bhat)) cbind((comput_uhat@x),(uhat[[&quot;Subject&quot;]])) (Intercept) M16 -1.7018337 -1.7018336 M05 -1.7018337 -1.7018336 M02 -1.3776749 -1.3776748 M11 -1.1615690 -1.1615689 M07 -1.0535161 -1.0535160 M08 -0.9454632 -0.9454631 M03 -0.6213044 -0.6213043 M12 -0.6213044 -0.6213043 M13 -0.6213044 -0.6213043 M14 -0.0810397 -0.0810397 M09 0.1350662 0.1350662 M15 0.7833838 0.7833837 M06 1.2155955 1.2155954 M04 1.4317013 1.4317013 M01 2.4041777 2.4041776 M10 3.9169188 3.9169185 F10 -3.5853927 -3.5853925 F09 -1.3162812 -1.3162811 F06 -1.3162812 -1.3162811 F01 -1.1001753 -1.1001752 F05 -0.0196460 -0.0196460 F07 0.3045128 0.3045128 F02 0.3045128 0.3045128 F08 0.6286716 0.6286716 F03 0.9528304 0.9528303 F04 1.9253068 1.9253067 F11 3.2219420 3.2219418 Why this is important: - It demonstrates that random effects are not “free parameters” estimated independently. - They are predictions, borrowing strength from the full dataset via \\(\\mathbf{V}^{-1}\\). - Shrinkage happens naturally through this formula. 6.9.16 Compute predicted values In mixed models, the word “prediction” can mean different things: conditional (subject-specific) prediction: includes random effects marginal (population-average) prediction: excludes random effects Both are valid, but they answer different scientific questions. In most applied analyses: - “What is the expected trajectory for this subject?” → conditional - “What is the average trajectory in the population?” → marginal We now formalize both views. 6.9.16.1 The conditional distribution Conditioning on random effects: \\[ \\mathbf{y} \\mid \\mathbf{b} \\sim \\mathbf{N}\\left(\\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{Z}\\mathbf{b}, \\boldsymbol{\\Sigma}\\right). \\] This implies that the conditional mean depends on both fixed and random components. 6.9.16.1.1 Conditional mean prediction For a new observation with design rows \\(\\mathbf{X}_0\\) and \\(\\mathbf{Z}_0\\): \\[ E(\\hat{Y}_0) = \\mathbf{X}_0\\hat{\\boldsymbol\\beta} + \\mathbf{Z}_0\\hat{\\mathbf{u}}. \\] The uncertainty in the conditional mean prediction comes from uncertainty in estimating \\(\\boldsymbol\\beta\\) and \\(\\mathbf{u}\\) (and possibly from uncertainty in variance components, which we typically ignore in simple plug-in formulas). Your derivation expands: \\[ \\mathrm{Var}(X+Y)=\\mathrm{Var}(X)+\\mathrm{Var}(Y)+2\\mathrm{Cov}(X,Y), \\] so if we treat the fixed and random predictors as independent (a simplifying approximation), the covariance term is ignored and the prediction variance becomes “fixed-part variance + random-part variance.” That leads to the practical decomposition: \\[ \\mathrm{Var}(\\hat{Y}_0) \\approx \\mathbf{X}_0 \\,\\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol\\beta})\\, \\mathbf{X}_0^\\top + \\mathbf{Z}_0 \\,\\widehat{\\mathrm{Cov}}(\\hat{\\mathbf{u}})\\, \\mathbf{Z}_0^\\top. \\] 6.9.16.1.2 Conditional individual prediction If we want a prediction interval for an actual future \\(Y_0\\) (not just its mean), we must add residual variance: \\[ \\mathrm{Var}(Y_0 - \\hat{Y}_0)\\approx \\mathrm{Var}(\\hat{Y}_0) + \\sigma^2. \\] This is exactly the distinction in your code between SE_rand (mean) and SE_rand2 (individual). 6.9.16.2 Lower level (conditional prediction) in R Here we compute subject-specific fitted values explicitly: \\[ \\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol\\beta} + \\mathbf{Z}\\hat{\\mathbf{u}}. \\] # how to calculate predicted values yhat &lt;- X%*%(fit.lmer2$coef$fixed)+Z%*% as.numeric ( uhat[[&quot;Subject&quot;]]) head(cbind (yhat,predict(fit.lmer2),y)) #create individual trajectory curve ## 6 x 3 Matrix of class &quot;dgeMatrix&quot; ## y ## 1 25.39237 25.39237 26.0 ## 2 26.71274 26.71274 25.0 ## 3 28.03311 28.03311 29.0 ## 4 29.35348 29.35348 31.0 ## 5 21.61052 21.61052 21.5 ## 6 22.93089 22.93089 22.5 The comparison shows that: - your manual computation equals predict(fit.lmer2) at the default prediction level (subject-specific). Next we compute an approximate standard error of conditional predictions by adding two components: - fixed-effect contribution: \\(\\mathbf{X}\\,\\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol\\beta})\\,\\mathbf{X}^\\top\\) - random-effect contribution: \\(\\mathbf{Z}\\,\\widehat{\\mathrm{Cov}}(\\hat{\\mathbf{u}})\\,\\mathbf{Z}^\\top\\) (approximated here using the random intercept variance) #compute standard error for marginal predictions predvar_rand &lt;- diag(X %*% fit.lmer2$varFix %*% t(X)) + diag(Z %*% diag(getVarCov(fit.lmer2)[1] ,27) %*% t(Z)) SE_rand &lt;- sqrt (predvar_rand) #mean prediction SE_rand2 &lt;- sqrt(predvar_rand+fit.lmer2$sigma^2) #individual prediction head(SE_rand,20) ## 1 2 3 4 5 6 7 8 ## 1.880728 1.872639 1.872639 1.880728 1.880728 1.872639 1.872639 1.880728 ## 9 10 11 12 13 14 15 16 ## 1.880728 1.872639 1.872639 1.880728 1.880728 1.872639 1.872639 1.880728 ## 17 18 19 20 ## 1.880728 1.872639 1.872639 1.880728 head(SE_rand2,20) ## 1 2 3 4 5 6 7 8 ## 2.363598 2.357166 2.357166 2.363598 2.363598 2.357166 2.357166 2.363598 ## 9 10 11 12 13 14 15 16 ## 2.363598 2.357166 2.357166 2.363598 2.363598 2.357166 2.357166 2.363598 ## 17 18 19 20 ## 2.363598 2.357166 2.357166 2.363598 A conceptual caution: - In full generality, \\(\\widehat{\\mathrm{Cov}}(\\hat{\\mathbf{u}})\\) is not simply \\(\\sigma_b^2 I\\); it depends on \\(\\mathbf{D}\\) and \\(\\mathbf{V}^{-1}\\). - Your expression is a reasonable pedagogical approximation that highlights the idea of adding fixed and random contributions. 6.9.17 The marginal distribution Integrating out random effects yields: \\[ \\mathbf{y} \\sim \\mathrm{N}\\left(\\mathbf{X}\\boldsymbol{\\beta}, \\mathbf{V}\\right), \\] where \\[ \\mathrm{Cov}(\\mathbf{y})=\\mathbf{V}=\\mathbf{Z}\\mathbf{D}\\mathbf{Z}^\\top+\\boldsymbol{\\Sigma}. \\] Here \\(\\mathbf{Z}\\mathbf{D}\\mathbf{Z}^\\top\\) is the covariance induced by random effects, and \\(\\boldsymbol{\\Sigma}\\) is the residual covariance (often \\(\\sigma^2 I\\) unless we specify a correlation structure). 6.9.17.1 Marginal mean prediction The marginal mean depends only on fixed effects: \\[ E(\\hat{Y}_0)=\\mathbf{X}_0\\hat{\\boldsymbol\\beta}. \\] The variance of the marginal mean prediction uses only \\(\\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol\\beta})\\): \\[ \\mathrm{Var}(\\hat{Y}_0)=\\mathbf{X}_0 \\,\\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol\\beta})\\, \\mathbf{X}_0^\\top. \\] 6.9.17.2 Marginal individual prediction For predicting a new observation at the population level, we add residual variance: \\[ \\mathrm{Var}(Y_0)\\approx \\mathrm{Var}(\\hat{Y}_0)+\\sigma^2. \\] 6.9.17.3 Higher level (marginal) in R Your code computes the marginal prediction SE using only the fixed-effect variance component: #compute standard error for marginal predictions predvar &lt;- diag(X %*% fit.lmer2$varFix %*% t(X)) SE &lt;- sqrt (predvar) #mean prediction SE2 &lt;- sqrt(predvar+fit.lmer2$sigma^2) #individual prediction head(SE,10) ## 1 2 3 4 5 6 7 8 ## 0.5199561 0.4898898 0.4898898 0.5199561 0.5199561 0.4898898 0.4898898 0.5199561 ## 9 10 ## 0.5199561 0.4898898 head(SE2,10) ## 1 2 3 4 5 6 7 8 ## 1.523092 1.513092 1.513092 1.523092 1.523092 1.513092 1.513092 1.523092 ## 9 10 ## 1.523092 1.513092 Then it forms approximate 95% bounds: mean prediction band: \\(\\hat{\\mu} \\pm 1.96\\,SE\\) individual prediction band: \\(\\hat{\\mu} \\pm 1.96\\,SE2\\) up=predict(fit.lmer2, newdata=Data, level=0) +1.96 *SE #mean prediction up2=predict(fit.lmer2, newdata=Data, level=0) +1.96 *SE2 #individual prediction head(up) ## 1 2 3 4 5 6 ## 24.00731 25.26875 26.58912 27.96842 24.00731 25.26875 head(up2) ## 1 2 3 4 5 6 ## 25.97346 27.27423 28.59460 29.93457 25.97346 27.27423 Note the crucial use of level=0: - level=0 gives marginal predictions (fixed effects only) - level=1 (or default) typically includes random effects for subject-specific predictions Finally, ggeffects::ggpredict() provides a convenient interface for marginal effects plots (your code uses %&gt;%, but we keep it unchanged as requested). library(tidyverse) library(ggeffects) ## Warning: package &#39;ggeffects&#39; was built under R version 4.4.3 ggpredict(fit.lmer2,terms=c(&quot;age&quot; )) x predicted std.error conf.low conf.high group 8 22.98819 0.5199561 21.96910 24.00729 1 10 24.30856 0.4898898 23.34840 25.26873 1 12 25.62894 0.4898898 24.66877 26.58910 1 14 26.94931 0.5199561 25.93021 27.96840 1 # ggpredict(fit.lmer2,terms=c(&quot;age&quot; )) %&gt;% plot(rawdata = T, dot.alpha = 0.2) # ggpredict(fit.lmer2,&quot;age&quot;, type = &quot;re&quot; ) %&gt;% plot(rawdata = T, dot.alpha = 0.2) library(sjPlot) plot_model(fit.lmer2, type = &quot;pred&quot;, terms = &quot;age&quot;, show.data = TRUE, dot.alpha = 0.2) Your “Compute SE by hand” numeric values are a useful checkpoint: they suggest that the analytic SE derived from \\(\\mathbf{X}\\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol\\beta})\\mathbf{X}^\\top\\) matches a simpler calculation for specific design points. 6.9.18 Extending the covariance structure: correlated residual errors So far, our baseline model assumed independent residuals: \\[ \\boldsymbol{\\Sigma}=\\sigma^2\\mathbf{I}. \\] But longitudinal data often have residual correlations over time even after accounting for random effects. nlme allows us to specify parametric correlation structures for residuals within subjects. A key conceptual point: random effects model between-subject heterogeneity (e.g., different baseline levels), residual correlation structures model within-subject temporal dependence beyond random effects. 6.9.19 Using lme with Gaussian correlation A Gaussian correlation structure implies residual correlation decays with distance in time (here, age). With a nugget effect, we allow a discontinuity at distance 0 (measurement error). library(nlme) fit.lmer.gaus &lt;- lme(distance ~ age + Sex , random = ~1 | Subject, correlation=corGaus(form= ~ age|Subject, nugget=TRUE), data = Data) summary(fit.lmer.gaus) ## Linear mixed-effects model fit by REML ## Data: Data ## AIC BIC logLik ## 451.4311 470.0089 -218.7156 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.795504 1.446192 ## ## Correlation Structure: Gaussian spatial correlation ## Formula: ~age | Subject ## Parameter estimate(s): ## range nugget ## 1.1324270 0.1039536 ## Fixed effects: distance ~ age + Sex ## Value Std.Error DF t-value p-value ## (Intercept) 17.717033 0.8448813 80 20.969848 0.0000 ## age 0.659620 0.0628344 80 10.497756 0.0000 ## SexFemale -2.325355 0.7612563 25 -3.054629 0.0053 ## Correlation: ## (Intr) age ## age -0.818 ## SexFemale -0.367 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.70618679 -0.54458678 -0.01435195 0.45606804 3.62915585 ## ## Number of Observations: 108 ## Number of Groups: 27 Variogram plots visualize how empirical semi-variance changes with time separation. In longitudinal settings, they act like a diagnostic for residual correlation: if semi-variance increases with lag, that indicates correlation decays as time separation grows. print(plot(Variogram(fit.lmer.gaus, form =~as.numeric(age)|Subject, data = Data))) We can again extract \\(\\mathbf{V}\\) (now incorporating the Gaussian residual correlation) and verify that GLS formulas still apply. The only difference is that \\(\\mathbf{V}\\) is no longer compound symmetric. require(mgcv) library(nlme) cov.gaus &lt;- extract.lme.cov(fit.lmer.gaus,Data) head(cov.gaus) 5.315307 3.306657 3.223843 3.223836 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.306657 5.315307 3.306657 3.223843 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.223843 3.306657 5.315307 3.306657 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.223836 3.223843 3.306657 5.315307 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 5.315307 3.306657 3.223843 3.223836 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.306657 5.315307 3.306657 3.223843 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(cov.gaus) ## [1] 108 108 Compute fixed effects using the Gaussian covariance: \\[ \\hat{\\boldsymbol\\beta}_{GLS} = (\\mathbf{X}^\\top \\mathbf{V}^{-1}\\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{V}^{-1}\\mathbf{y}. \\] library(matlib) inv(t(as.matrix(X))%*%inv(as.matrix(cov.gaus))%*%as.matrix(X))%*%t(as.matrix(X))%*%inv(as.matrix(cov.gaus))%*%y 17.7170272 0.6596201 -2.3253552 fit.lmer.gaus$coef$fixed ## (Intercept) age SexFemale ## 17.717033 0.659620 -2.325355 This equivalence is a powerful principle: &gt; Mixed models are GLS with a structured covariance matrix. 6.9.20 Using lme with autoregressive (AR1) An AR(1) residual structure assumes correlation decays geometrically with time lag: \\[ \\mathrm{Cor}(\\varepsilon_{ij},\\varepsilon_{ik})=\\rho^{|t_{ij}-t_{ik}|}, \\] under equally spaced times; with irregular spacing, nlme uses an appropriate generalization based on the numeric time covariate. fit.lmer.AR1 &lt;- lme(distance ~ age + Sex , random = ~1 | Subject, correlation=corAR1(form= ~ as.numeric(age)|Subject ), data = Data) summary(fit.lmer.AR1) ## Linear mixed-effects model fit by REML ## Data: Data ## AIC BIC logLik ## 449.5125 465.4363 -218.7563 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.807425 1.431592 ## ## Correlation Structure: ARMA(1,0) ## Formula: ~as.numeric(age) | Subject ## Parameter estimate(s): ## Phi1 ## 0 ## Fixed effects: distance ~ age + Sex ## Value Std.Error DF t-value p-value ## (Intercept) 17.706713 0.8339225 80 21.233044 0.0000 ## age 0.660185 0.0616059 80 10.716263 0.0000 ## SexFemale -2.321023 0.7614168 25 -3.048294 0.0054 ## Correlation: ## (Intr) age ## age -0.813 ## SexFemale -0.372 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.74889609 -0.55034466 -0.02516628 0.45341781 3.65746539 ## ## Number of Observations: 108 ## Number of Groups: 27 Variogram diagnostic: print(plot(Variogram(fit.lmer.AR1, form =~as.numeric(age)|Subject, data = Data))) Extract covariance: require(mgcv) library(nlme) cov.AR1 &lt;- extract.lme.cov(fit.lmer.AR1,Data) head(cov.AR1) 5.316240 3.266784 3.266784 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 5.316240 3.266784 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 3.266784 5.316240 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 3.266784 3.266784 5.316240 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 5.316240 3.266784 3.266784 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.266784 5.316240 3.266784 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(cov.AR1) ## [1] 108 108 6.9.21 Using lme with exponential correlation Exponential correlation decays continuously with lag: \\[ \\mathrm{Cor}(\\varepsilon_{ij},\\varepsilon_{ik})=\\exp(-\\phi |t_{ij}-t_{ik}|). \\] This is often used in spatial and longitudinal settings. fit.lmer.Exp &lt;- lme(distance ~ age + Sex , random = ~1 | Subject, correlation=corExp(form= ~ as.numeric(age)|Subject ), data = Data) summary(fit.lmer.Exp) ## Linear mixed-effects model fit by REML ## Data: Data ## AIC BIC logLik ## 449.3968 465.3206 -218.6984 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.788899 1.454494 ## ## Correlation Structure: Exponential spatial correlation ## Formula: ~as.numeric(age) | Subject ## Parameter estimate(s): ## range ## 0.7045117 ## Fixed effects: distance ~ age + Sex ## Value Std.Error DF t-value p-value ## (Intercept) 17.721416 0.8500193 80 20.848250 0.0000 ## age 0.659405 0.0634074 80 10.399499 0.0000 ## SexFemale -2.327485 0.7611852 25 -3.057711 0.0053 ## Correlation: ## (Intr) age ## age -0.821 ## SexFemale -0.365 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.683026666 -0.540915318 -0.008097445 0.461167542 3.612579065 ## ## Number of Observations: 108 ## Number of Groups: 27 Variogram: print(plot(Variogram(fit.lmer.Exp, form =~as.numeric(age)|Subject, data = Data))) Extract covariance: require(mgcv) library(nlme) cov.Exp &lt;- extract.lme.cov(fit.lmer.Exp,Data) head(cov.Exp) 5.315710 3.323904 3.207397 3.200582 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.323904 5.315710 3.323904 3.207397 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.207397 3.323904 5.315710 3.323904 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.200582 3.207397 3.323904 5.315710 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 5.315710 3.323904 3.207397 3.200582 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.323904 5.315710 3.323904 3.207397 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(cov.Exp) ## [1] 108 108 6.9.22 Using lme with unstructured residual correlation (corSymm) An unstructured residual correlation places minimal constraints on within-subject correlations, allowing each pairwise correlation to be estimated (subject to positive definiteness). Because this can be unstable, nlme often pairs it with heteroscedasticity modeling such as varIdent. Conceptually, this approach says: - “I do not want to impose a parametric decay pattern.” - “Let the data estimate the correlation pattern.” fit.lmer.corSymm &lt;- lme(distance ~ age + Sex , random = ~1 | (Subject), correlation=corSymm(form= ~ 1| (Subject) ), weights = varIdent(form=~1|age), data = Data) summary(fit.lmer.corSymm) ## Linear mixed-effects model fit by REML ## Data: Data ## AIC BIC logLik ## 456.6945 493.85 -214.3473 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.783141 1.481552 ## ## Correlation Structure: General ## Formula: ~1 | (Subject) ## Parameter estimate(s): ## Correlation: ## 1 2 3 ## 2 -0.260 ## 3 0.238 -0.149 ## 4 -0.251 -0.007 0.426 ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | age ## Parameter estimates: ## 8 10 12 14 ## 1.000000 0.686841 1.199078 1.000414 ## Fixed effects: distance ~ age + Sex ## Value Std.Error DF t-value p-value ## (Intercept) 17.417591 0.8657147 80 20.119319 0.0000 ## age 0.674651 0.0702289 80 9.606462 0.0000 ## SexFemale -2.045167 0.7361373 25 -2.778241 0.0102 ## Correlation: ## (Intr) age ## age -0.840 ## SexFemale -0.346 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.33886326 -0.56222386 0.01374189 0.52235946 3.99060056 ## ## Number of Observations: 108 ## Number of Groups: 27 Variogram: print(plot(Variogram(fit.lmer.corSymm, form =~as.numeric(age)|Subject, data = Data))) Extract covariance: require(mgcv) library(nlme) cov.corSymm &lt;- extract.lme.cov(fit.lmer.corSymm,Data) head(cov.corSymm) 5.374589 2.786932 3.807032 2.628359 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.786932 4.215083 2.909655 3.168382 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.807032 2.909655 6.335531 4.301450 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.628359 3.168382 4.301450 5.376406 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 5.374589 2.786932 3.807032 2.628359 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 2.786932 4.215083 2.909655 3.168382 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(cov.corSymm) ## [1] 108 108 6.9.23 Using lme with compound symmetry residual correlation (corCompSymm) Compound symmetry as a residual correlation structure assumes constant correlation within a subject: \\[ \\mathrm{Cor}(\\varepsilon_{ij},\\varepsilon_{ik})=\\rho \\quad (j\\neq k). \\] This resembles what a random intercept induces, but note the difference: Random intercept creates compound symmetry through \\(\\mathbf{Z}\\mathbf{D}\\mathbf{Z}^\\top\\) corCompSymm creates compound symmetry through \\(\\boldsymbol\\Sigma\\) In practice, both can produce similar marginal covariance patterns, but they represent different modeling stories. fit.lmer.Symm &lt;- lme(distance ~ age + Sex , random = ~1 | (Subject), correlation=corCompSymm(form= ~ as.numeric(age)| (Subject) ), data = Data) summary(fit.lmer.Symm) ## Linear mixed-effects model fit by REML ## Data: Data ## AIC BIC logLik ## 449.5125 465.4363 -218.7563 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.807425 1.431592 ## ## Correlation Structure: Compound symmetry ## Formula: ~as.numeric(age) | (Subject) ## Parameter estimate(s): ## Rho ## 0 ## Fixed effects: distance ~ age + Sex ## Value Std.Error DF t-value p-value ## (Intercept) 17.706713 0.8339225 80 21.233044 0.0000 ## age 0.660185 0.0616059 80 10.716263 0.0000 ## SexFemale -2.321023 0.7614168 25 -3.048294 0.0054 ## Correlation: ## (Intr) age ## age -0.813 ## SexFemale -0.372 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.74889609 -0.55034466 -0.02516628 0.45341781 3.65746539 ## ## Number of Observations: 108 ## Number of Groups: 27 Variogram: print(plot(Variogram(fit.lmer.Symm, form =~as.numeric(age)|Subject, data = Data))) Extract covariance: require(mgcv) library(nlme) cov.Symm &lt;- extract.lme.cov(fit.lmer.Symm,Data) head(cov.Symm) 5.316240 3.266784 3.266784 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 5.316240 3.266784 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 3.266784 5.316240 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 3.266784 3.266784 5.316240 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 5.316240 3.266784 3.266784 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.266784 5.316240 3.266784 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(cov.Symm) ## [1] 108 108 6.9.24 GLS models: correlation in residuals without random effects The gls() function fits models where all correlation is placed in the residuals. This can be useful when: you do not need subject-level random effects, or you want a pure covariance-modeling approach. 6.9.25 GLS with unstructured correlation gls.corsymm &lt;- gls(distance ~ Sex +age, Orthodont, correlation = corSymm(form = ~ 1 | Subject), weights = varIdent(form=~1|age)) summary(gls.corsymm) ## Generalized least squares fit by REML ## Model: distance ~ Sex + age ## Data: Orthodont ## AIC BIC logLik ## 454.6945 489.196 -214.3473 ## ## Correlation Structure: General ## Formula: ~1 | Subject ## Parameter estimate(s): ## Correlation: ## 1 2 3 ## 2 0.586 ## 3 0.652 0.563 ## 4 0.489 0.666 0.737 ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | age ## Parameter estimates: ## 8 10 12 14 ## 1.0000000 0.8855866 1.0857309 1.0001682 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 17.417595 0.8657121 20.119383 0.0000 ## SexFemale -2.045167 0.7361425 -2.778222 0.0065 ## age 0.674651 0.0702284 9.606521 0.0000 ## ## Correlation: ## (Intr) SexFml ## SexFemale -0.346 ## age -0.840 0.000 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.508191537 -0.584408334 0.003646016 0.531073432 2.179748120 ## ## Residual standard error: 2.318325 ## Degrees of freedom: 108 total; 105 residual 6.9.26 GLS with compound symmetry gls.comsys &lt;- gls(distance ~ Sex +age, Orthodont, correlation = corCompSymm(form = ~ 1 | Subject)) summary(gls.comsys) ## Generalized least squares fit by REML ## Model: distance ~ Sex + age ## Data: Orthodont ## AIC BIC logLik ## 447.5125 460.7823 -218.7563 ## ## Correlation Structure: Compound symmetry ## Formula: ~1 | Subject ## Parameter estimate(s): ## Rho ## 0.6144914 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 17.706713 0.8339225 21.233044 0.0000 ## SexFemale -2.321023 0.7614169 -3.048294 0.0029 ## age 0.660185 0.0616059 10.716263 0.0000 ## ## Correlation: ## (Intr) SexFml ## SexFemale -0.372 ## age -0.813 0.000 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.59712955 -0.64544226 -0.02540005 0.51680604 2.32947531 ## ## Residual standard error: 2.305697 ## Degrees of freedom: 108 total; 105 residual Interpretation guidance: - gls() is often a good baseline when you want to emphasize covariance structure. - lme() is preferred when subject-specific effects are scientifically meaningful (e.g., patient-specific baselines). 6.9.27 Practical limitation of lme4 lme4 is extremely powerful for random effects, but it does not natively support rich residual correlation structures. In lme4, residuals are assumed independent conditional on random effects (i.e., \\(\\boldsymbol\\Sigma=\\sigma^2\\mathbf{I}\\)). You can fit complex random-effects covariance structures, but not AR(1)/Gaussian/exponential residual correlations in the same direct way as nlme. This is why, in longitudinal data analysis, a common workflow is: use lme4 for flexible random-effects modeling and efficient fitting, use nlme when you need explicit residual correlation structures. 6.10 Manual simulating data for linear mix model see here 6.11 How to calculate the prediction interval for LMM see here 6.12 Least-squares means with interaction effect see here 6.13 Spline regression see here "],["probability.html", "7 Probability 7.1 Probability basics 7.2 Probability R practice", " 7 Probability Probability is the mathematical language for uncertainty. In statistics, we use probability to formalize how likely events are, how random quantities behave, and how sampling variability arises. This chapter is organized in two parts: Probability basics: the conceptual and mathematical foundation (events, rules, distributions, expectation). Probability R practice: simulation and computation in R, which often provides intuition even when the exact analytic solution is difficult. A key theme is that probability is not only “abstract theory”—it directly supports the logic of estimation, confidence intervals, and hypothesis testing. 7.1 Probability basics 7.1.1 Events An outcome is a single possible result of a random experiment. An event is a set (collection) of outcomes. The sample space (outcome space) is the set of all possible outcomes. Example: two dice. Outcome: \\((1,6)\\) means die 1 shows 1 and die 2 shows 6. Sample space: all 36 ordered pairs \\((i,j)\\) where \\(i,j\\in\\{1,2,3,4,5,6\\}\\). Event example: “both dice show the same face” is \\[E=\\{(1,1),(2,2),(3,3),(4,4),(5,5),(6,6)\\}.\\] In practice: - We use events when we talk about “probability that something happens.” - We use random variables when we want a numeric summary of outcomes, such as “sum of two dice.” A common confusion worth separating early (even if we do not fully formalize it here): Independence of events: statements about sets, e.g., \\(A\\) independent of \\(B\\). Independence of random variables: statements about distributions, e.g., \\(X\\) independent of \\(Y\\). Independence of observations: a modeling assumption in data analysis, often violated in clustered/longitudinal data (which motivates mixed models, GEE, etc.). 7.1.2 Probability formulas 7.1.2.1 Discrete case For equally likely outcomes, the probability of an event is: \\[ P(E) = \\frac{\\text{number of outcomes in }E}{\\text{number of possible outcomes}}. \\] This definition is intuitive but has two important limitations: It requires that outcomes be equally likely. It is mainly useful when the sample space is countable and not too large. When outcomes are not equally likely, we assign each outcome \\(\\omega\\) a probability \\(P(\\omega)\\), and then: \\[ P(E)=\\sum_{\\omega\\in E}P(\\omega). \\] 7.1.2.2 Continuous case For a continuous random variable \\(X\\) with density \\(f(x)\\): \\[ P(a \\le X \\le b)=\\int_a^b f(x)\\,dx. \\] This expresses the fundamental idea: Probabilities of continuous random variables are areas under a curve. A crucial consequence: \\[ P(X=k)=0 \\] for any fixed value \\(k\\) when \\(X\\) is continuous. So the statement “\\(X=3\\)” has probability zero, but intervals like “\\(2.9 \\le X \\le 3.1\\)” have positive probability. 7.1.3 Calculation of probability (operations) Most probability calculations are built from three core operations: Union (“A or B”) Intersection (“A and B”) Conditioning (“A given B”) 7.1.3.1 Union probability (addition rule) \\[ \\begin{aligned} P(A \\cup B) &amp;= P(A)+P(B)-P(A \\cap B),\\\\ P(A \\cup B) &amp;= P(A)+P(B)\\quad \\text{if }A\\text{ and }B\\text{ are mutually exclusive.} \\end{aligned} \\] Interpretation: - We subtract \\(P(A\\cap B)\\) because it is counted twice when adding \\(P(A)+P(B)\\). - “Mutually exclusive” means \\(A\\cap B=\\emptyset\\) (they cannot happen together). 7.1.3.2 Joint probability (multiplication rule) \\[ \\begin{aligned} P(A \\cap B) &amp;= P(A\\mid B)P(B)=P(B\\mid A)P(A),\\\\ P(A \\cap B) &amp;= P(A)P(B)\\quad \\text{if }A\\text{ and }B\\text{ are independent.} \\end{aligned} \\] Interpretation: - Independence is a strong statement: knowing \\(B\\) does not change the probability of \\(A\\). - Many real-world problems are not independent, which is why conditional probability is central. 7.1.3.3 Marginal probability A marginal probability is “standalone,” without conditioning: \\[ P(A)\\quad \\text{or}\\quad P(B). \\] In multivariable settings, “marginal” also means summing/integrating out other variables. 7.1.3.4 Conditional probability \\[ P(A\\mid B)=\\dfrac{P(A \\cap B)}{P(B)}. \\] Interpretation: - Restrict attention to the world where \\(B\\) happened, then measure how often \\(A\\) occurs inside that restricted world. A short caution on wording: People sometimes say “p-values are conditional probabilities.” This is only partly true. A p-value is a probability computed under the null model and conditional on the testing procedure (and sometimes on nuisance estimates). It is not the probability that the null hypothesis is true. 7.1.4 Bayes’s theorem Bayes’s theorem is essentially a systematic way to reverse conditioning. 7.1.4.1 Multiplication law (re-stated) \\[ P(A \\cap B)=P(A\\mid B)P(B)=P(B\\mid A)P(A). \\] 7.1.4.2 Bayes’s formula For mutually exclusive and exhaustive events \\(B_1,\\ldots,B_n\\): \\[ P(B_j\\mid A)=\\frac{P(A\\mid B_j)P(B_j)}{P(A)}. \\] This shows that the posterior probability of a cause \\(B_j\\) after observing evidence \\(A\\) is proportional to: how likely the evidence is under the cause (\\(P(A\\mid B_j)\\)), and how likely the cause was before seeing the evidence (\\(P(B_j)\\)). 7.1.4.3 Law of total probability \\[ P(A)=P(A\\mid B_1)P(B_1)+\\cdots+P(A\\mid B_n)P(B_n). \\] This is the denominator in Bayes’s theorem and is often the step where many probability problems become manageable: break a difficult event into simpler conditional pieces. 7.1.5 Random variables and distribution functions A random variable is a numerical function of the outcome of a random experiment. We use random variables because they let us summarize events numerically and apply algebra/calculus. There are three major distribution functions: PMF (probability mass function) for discrete variables, e.g., binomial, Poisson. PDF (probability density function) for continuous variables, e.g., normal, exponential. CDF (cumulative distribution function) for both, defined by: \\[ F(x)=P(X\\le x). \\] The CDF is universal: even when a PDF does not exist, the CDF still defines the distribution. 7.1.6 Probability distributions (joint, marginal, conditional) For two discrete random variables \\((X,Y)\\), the joint distribution is: \\[ P(X=x_i, Y=y_j)=p_{ij},\\quad i,j=1,2,\\ldots \\] The marginal distribution of \\(X\\) is obtained by summing out \\(Y\\): \\[ P(X=x_i)=\\sum_{j=1}^{\\infty} p_{ij}=p_i. \\] The conditional distribution of \\(Y\\) given \\(X=x_i\\) is: \\[ P(Y=y_j\\mid X=x_i)=\\frac{p_{ij}}{p_i}. \\] This joint → marginal/conditional workflow is a general template used throughout statistics, including regression, Bayesian inference, and graphical models. 7.1.7 Conditional expectation Conditional expectation is the expected value of a random variable under a conditional distribution. Intuitively, it is the “average of \\(Y\\) when we hold \\(X\\) fixed.” Discrete form (conceptual structure): \\[ E(Y\\mid X)=\\sum_{y} y\\cdot P(Y=y\\mid X). \\] Continuous form: \\[ E(Y\\mid X)=\\int y\\cdot f_{Y\\mid X}(y\\mid X)\\,dy. \\] (Your notes use a sample-style indexing; the key conceptual point is: expectation is always “value times probability,” summed or integrated over the support.) Unconditional expectation (discrete): \\[ \\mu=E(X)=\\sum x_i f(x_i). \\] Practical connection: - In regression, \\(E(Y\\mid X=x)\\) is modeled as a function of \\(x\\). - Thus conditional expectation becomes the theoretical basis for regression modeling. 7.1.8 Conditional variance Variance measures dispersion around the mean: \\[ \\sigma^2=\\mathrm{Var}(X)=\\sum (x_i-\\mu)^2 f(x_i). \\] Conditional variance is defined similarly but under the conditional distribution: \\[ \\mathrm{Var}(Y\\mid X)=E\\left((Y-E(Y\\mid X))^2\\mid X\\right). \\] Practical connection: - Many models (e.g., linear regression, GLMs, mixed models) can be viewed as specifying a mean structure \\(E(Y\\mid X)\\) and a variance/covariance structure \\(\\mathrm{Var}(Y\\mid X)\\). 7.1.9 Sampling In statistics, we use a sample to learn about a population. A sample statistic (e.g., \\(\\bar X\\)) varies from sample to sample, and that variability is the foundation of uncertainty quantification. The error from using statistics to estimate parameters is summarized by the standard error. Standard error of the sample mean under i.i.d. sampling: \\[ SD(\\bar{X})=SE(\\bar{X})=\\dfrac{\\sigma}{\\sqrt{n}}. \\] This formula is simple, but it encodes important assumptions: - independent observations, - common variance \\(\\sigma^2\\), - finite variance. When these assumptions fail (e.g., correlation), we must modify the standard error (again connecting back to covariance modeling). 7.1.10 Central limit theorem and law of large numbers Law of large numbers (LLN): sample averages converge to the population mean as \\(n\\) grows. Central limit theorem (CLT): for large \\(n\\), the sampling distribution of \\(\\bar X\\) is approximately normal: \\[ \\bar X \\approx N\\left(\\mu,\\frac{\\sigma^2}{n}\\right), \\] even if the population is not normal (under mild conditions). This approximation is the backbone of classical confidence intervals and many hypothesis tests. 7.1.11 Confidence interval A confidence interval has the general form: \\[ \\text{point estimate} \\pm M\\times \\widehat{SE}(\\text{estimate}). \\] The multiplier \\(M\\) depends on the desired confidence level and the sampling distribution (z, t, etc.). The tradeoff is fundamental: Higher confidence → larger \\(M\\) → wider interval → lower precision. For a proportion, the common large-sample margin of error is: \\[ E=z_{\\alpha/2}\\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}}. \\] 7.1.12 Introduction to hypothesis testing Hypothesis testing is a structured decision framework: Set up hypotheses and choose significance level \\(\\alpha\\). Construct and calculate a test statistic. Compute a p-value under the null model. Make a decision and state a conclusion in context. In many common tests, the p-value is computed using a known reference distribution (normal, t, chi-square, F). In more complex problems, simulation becomes a practical alternative. 7.2 Probability R practice R lets us compute probabilities, simulate random experiments, and visualize distributions. Simulation is especially valuable when: exact probability calculations are complicated, or the analytic form is unknown. Pros: - minimal probability theory required, - often intuitive and flexible. Cons: - simulation gives approximate answers, - accuracy depends on the number of simulations and computational cost. 7.2.1 Integrate Integration is how we compute probabilities for continuous random variables and expectations of functions. integrate(function(x){x^2},0,2)$value ## [1] 2.666667 Interpretation: - This returns \\(\\int_0^2 x^2\\,dx\\), illustrating numerical integration in R. - For probability, we typically integrate a density over an interval. 7.2.2 Derivation (symbolic differentiation) deriv() can generate derivatives of expressions, useful for sensitivity checks or simple symbolic gradients. fxy = expression(2*x^2+y+3*x*y^2) dxy = deriv(fxy, c(&quot;x&quot;, &quot;y&quot;), func = TRUE) dxy(1,2) ## [1] 16 ## attr(,&quot;gradient&quot;) ## x y ## [1,] 16 13 Interpretation: - This computes partial derivatives with respect to \\(x\\) and \\(y\\) at \\((1,2)\\). - In statistics, derivatives appear in likelihood maximization, gradient-based optimization, and delta-method approximations. 7.2.3 Create random variables with specific distributions R provides standard “distribution functions,” typically in four families: d* density/pmf: \\(f(x)\\) p* CDF: \\(P(X\\le x)\\) q* quantile: inverse CDF r* random generation dnorm(0)# density at a number ## [1] 0.3989423 pnorm(1.28)# cumulative possibility ## [1] 0.8997274 qnorm(0.95)# quantile ## [1] 1.644854 rnorm(10)# random numbers ## [1] -0.455364836 -0.521148357 -0.494445354 0.749817888 0.141561597 ## [6] 0.121387509 -2.177944382 0.757740713 0.375375011 -0.006467571 The naming convention is consistent across many distributions (normal, t, chi-square, Poisson, binomial, exponential, etc.), which makes R especially convenient for probability practice. 7.2.3.1 Using covariance matrix to generate multivariate normal variables The multivariate normal is a cornerstone in statistics because linear combinations remain normal and because many estimators have asymptotic normality. library(MASS) Sigma &lt;- matrix(c(10,3,3,2),2,2) mvrnorm(n=20, rep(0, 2), Sigma) -0.2665594 -0.0774868 2.7866828 0.7426437 1.2282546 1.3251634 -0.8505176 0.5162904 -0.0126082 0.7838208 -0.7243446 0.0724909 -8.3928470 -1.6886123 -1.1046392 0.5370612 1.3658650 1.1767135 -1.9054060 -1.2319559 -0.8789834 0.3941995 -6.0085862 -2.1536238 0.8189866 1.3600976 4.7487759 0.7383102 0.0129275 -0.0814486 -0.4798287 0.0700314 1.8806579 0.6530998 4.6464324 0.3021236 7.9990662 1.1896375 -2.8460051 0.1155733 Interpretation: - Sigma encodes variances (diagonal) and covariance (off-diagonal). - Covariance controls correlation and joint behavior, which becomes central in multivariate inference and mixed models. 7.2.4 Probability function examples Here we compute common tail probabilities and quantiles used in hypothesis testing. pnorm(1.96, 0,1) ## [1] 0.9750021 qnorm(0.025, 0,1) ## [1] -1.959964 pchisq(3.84,1,lower.tail=F) ## [1] 0.05004352 mean(rchisq(10000,1)&gt;3.84) #simulation ## [1] 0.0501 Interpretation: - pnorm(1.96) is the classic 97.5th percentile check for \\(N(0,1)\\). - qnorm(0.025) gives the 2.5th percentile. - pchisq(3.84,1,lower.tail=F) computes the upper-tail probability beyond the chi-square cutoff 3.84 (often used in 1-df likelihood ratio tests). - The final line approximates the same tail probability via simulation, reinforcing that probability can be estimated empirically. 7.2.5 Vector and operations Probability simulation relies heavily on vectorization. seq(1,10, 2) ## [1] 1 3 5 7 9 x=rep(1:3,6) x ## [1] 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 y=rep(1:3, each = 6) y ## [1] 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 3 x+y ## [1] 2 3 4 2 3 4 3 4 5 3 4 5 4 5 6 4 5 6 x-y ## [1] 0 1 2 0 1 2 -1 0 1 -1 0 1 -2 -1 0 -2 -1 0 x*y ## [1] 1 2 3 1 2 3 2 4 6 2 4 6 3 6 9 3 6 9 x/y ## [1] 1.0000000 2.0000000 3.0000000 1.0000000 2.0000000 3.0000000 0.5000000 ## [8] 1.0000000 1.5000000 0.5000000 1.0000000 1.5000000 0.3333333 0.6666667 ## [15] 1.0000000 0.3333333 0.6666667 1.0000000 x%*%y 72 Notes: - Elementwise operations (+,-,*,/) are the default. - %*% is matrix multiplication; for vectors it yields an inner product when dimensions align. 7.2.6 Select and substitute elements of vector Subsetting is essential for event counting in simulations. x[c(2,3)] ## [1] 2 3 x[-1] ## [1] 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 x[x&gt;2] ## [1] 3 3 3 3 3 3 x[x==2] ## [1] 2 2 2 2 2 2 # substitute x[x == 2] &lt;- 0.5 x ## [1] 1.0 0.5 3.0 1.0 0.5 3.0 1.0 0.5 3.0 1.0 0.5 3.0 1.0 0.5 3.0 1.0 0.5 3.0 Interpretation: - Logical conditions create event indicators. - Assignment replaces selected outcomes, which is useful for recoding or constructing piecewise variables. 7.2.7 Matrix and operations Matrices are used for multivariate distributions and linear algebra operations in estimation. matrix(1:10,2,5) 1 3 5 7 9 2 4 6 8 10 matrix(1:10,5,2) 1 6 2 7 3 8 4 9 5 10 a &lt;- matrix(12:20,3,3) a[2,] ## [1] 13 16 19 a[,2] ## [1] 15 16 17 a[-2,] 12 15 18 14 17 20 a[,-2] 12 18 13 19 14 20 a[2,1]=21 a 12 15 18 21 16 19 14 17 20 7.2.8 Compute inverse, determinant and eigenvalues These operations appear in multivariate normal densities, GLS estimators, and covariance decompositions. a&lt;-matrix(c(11,21,31,21,32,43,12,32,54),3,3) solve(a) -1.9775281 3.471910 -1.6179775 0.7977528 -1.247191 0.5617978 0.5000000 -1.000000 0.5000000 det(a) ## [1] -178 solve(a)*det(a) 352 -618 288 -142 222 -100 -89 178 -89 t(a) 11 21 31 21 32 43 12 32 54 eigen(a) ## eigen() decomposition ## $values ## [1] 91.6892193 5.6541299 -0.3433491 ## ## $vectors ## [,1] [,2] [,3] ## [1,] -0.2573423 -0.7530908 -0.9049786 ## [2,] -0.5253459 -0.1712782 0.3538153 ## [3,] -0.8110405 0.6352306 0.2362806 Interpretation: - solve(a) computes \\(a^{-1}\\) (if invertible). - det(a) is used in densities and volume scaling. - eigen(a) decomposes into eigenvalues/eigenvectors (useful for checking positive definiteness of covariance matrices). 7.2.9 Dataframe Data frames are the standard structure for statistical data in R. name&lt;-c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;) chinese&lt;-c(92,96,95) math&lt;-c(86, 85, 92) score&lt;-data.frame(name, chinese, math) score name chinese math A 92 86 B 96 85 C 95 92 score[2] chinese 92 96 95 score$math ## [1] 86 85 92 7.2.10 Solve problems using simulation Simulation approximates probabilities by repeated random experiments, using: \\[ P(E)\\approx \\frac{\\#\\{\\text{simulations where }E\\text{ occurs}\\}}{\\text{number of simulations}}. \\] 7.2.10.1 for loop sim&lt;-10000 p&lt;-numeric(sim) # numeric=NULL for (i in 1:sim){ p[i]&lt;- abs(mean(rnorm(10,20,sqrt(3)))-mean(rnorm(15,20,sqrt(3))))&lt;0.1 } mean(p) ## [1] 0.1114 Interpretation: - In each simulation, we generate two samples from the same normal distribution and compare their sample means. - The event is “absolute difference in means &lt; 0.1.” - mean(p) estimates the probability of that event. 7.2.10.2 using replicate mean(replicate(10000,abs(mean(rnorm(10,20,sqrt(3)))-mean(rnorm(15,20,sqrt(3))))&lt;0.1)) ## [1] 0.109 replicate() is cleaner and less error-prone for repeated simulations. It is often a good default unless performance constraints require custom vectorization. 7.2.10.3 using apply function (vectorized simulation) A&lt;-matrix(rnorm(250000, 20,sqrt(3)),10000,25) head(A) 18.70574 20.88733 17.52812 19.56535 17.73622 21.98701 20.00801 21.14956 22.81945 18.47995 21.35431 20.90751 21.31785 23.92822 20.58894 18.71456 19.69867 21.61542 19.10213 18.89764 18.36881 20.89675 23.82558 21.33796 20.13462 20.11965 19.79799 23.63198 19.88155 18.89218 22.20987 18.73646 20.44873 22.57918 18.58609 20.66191 18.30840 20.18984 21.29121 18.29128 19.06406 23.94727 19.63847 17.29944 18.97062 22.34136 21.76254 21.32133 19.77645 19.46760 22.15332 19.14180 19.91843 19.67503 19.03136 19.83104 20.90498 18.66252 22.32577 20.74567 19.39415 21.46328 22.40127 20.40425 16.83219 20.48671 18.26579 19.27627 19.42452 19.40516 20.41231 19.07215 17.95481 20.50409 21.01829 21.26067 19.10844 20.10449 20.61276 21.30556 18.91464 16.12576 20.33453 18.42996 19.09259 23.13648 20.61350 18.65005 18.43049 18.29093 20.94691 18.36321 22.23422 20.81836 18.44632 21.25736 19.29372 18.62801 21.28730 21.01738 20.05354 19.70608 20.30916 19.74466 22.21281 18.40272 19.52576 22.23956 19.52812 21.71667 19.63384 22.08427 19.09510 18.11768 22.50618 19.55186 18.74422 21.38810 19.84704 18.29330 21.27049 20.16627 16.88002 23.07446 19.72053 20.49934 18.78863 16.07264 20.20901 19.21209 20.36903 21.74771 17.90861 21.47081 19.73442 21.15830 20.55732 17.24140 19.50684 20.92777 19.30014 22.93655 22.65659 19.34171 21.30277 20.29218 20.28845 19.63053 20.84432 18.22206 f&lt;-function(x) {abs(mean(x[1:10])-mean(x[11:25]))} # solve the mean by apply mean(apply(A,1,f)&gt;0.1) ## [1] 0.8878 Interpretation: - This creates 10,000 simulated experiments at once. - Each row is one experiment with 25 observations; the first 10 and last 15 form two groups. - Vectorization often improves speed and clarity when structured carefully. 7.2.10.4 using probability method pnorm(0.1,0,sqrt(0.5))-pnorm(-0.1,0,sqrt(0.5)) ## [1] 0.1124629 This line attempts an analytic shortcut: if the difference of two independent sample means is normal with known variance, we can compute the probability directly using the normal CDF. This is a good example of how simulation and theory complement each other: simulation suggests the answer, and probability theory can sometimes confirm it exactly. 7.2.11 Permutations and combinations Combinatorics supports counting arguments for discrete probability. choose(10,2) ## [1] 45 # combn(10,2) factorial(10) ## [1] 3628800 prod(1:10) ## [1] 3628800 Interpretation: - choose(n,k) computes \\(\\binom{n}{k}\\). - factorial(10) equals \\(10!\\). - prod(1:10) is another way to compute \\(10!\\) numerically. 7.2.12 Search value position in vector These are practical utilities for data cleaning and simulation summaries. a&lt;-c(1,2,3,5,0,9) which(a==min(a)) ## [1] 5 sum(a) ## [1] 20 unique(a) ## [1] 1 2 3 5 0 9 length(a) ## [1] 6 min(a) ## [1] 0 max(a) ## [1] 9 all(c(3,4) %in% a) ## [1] FALSE 7.2.13 Solve directly and optimize Finding roots and optimizing functions are common in probability (e.g., solving for quantiles, MLE equations). f&lt;-function(x){x^2-exp(x)} uniroot(f,c(-0.8,-0.6)) $root ## [1] -0.7034781 f2&lt;-function(x){abs(x^2-exp(x))} optimize (f2,lower=-0.8,upper=-0.6)$minimum ## [1] -0.703479 Interpretation: - uniroot() solves \\(f(x)=0\\) in a range. - optimize() finds the minimum of a function in a range, which can be used when the root is hard to bracket. 7.2.14 Calculate probability using simulation method 7.2.14.1 Example: select 3 numbers out of 1:10, sum equals 9 badge&lt;-1:10 sim&lt;-10000 p&lt;-numeric(sim) for (i in 1:sim){ a&lt;-sample(badge,3,replace=F) p[i]&lt;-sum(a)==9 } mean(p) ## [1] 0.0247 Interpretation: - The probability is estimated by the fraction of times the sampled triple sums to 9. - Because sampling is without replacement, the outcome space is combinations (unordered) but simulation avoids manual counting. 7.2.14.2 Example: tangyuan flavors This simulation checks whether each block of 6 draws contains all three flavors. The logic uses unique() counts to represent the event “all flavors present.” Tangyuan&lt;-c(rep(&#39;A&#39;,8),rep(&#39;B&#39;,8),rep(&#39;C&#39;,8)) sim&lt;-10000 p&lt;-numeric(sim) # how to do it according to the condition for (i in 1:sim){ a&lt;-sample(Tangyuan,24,replace=F) p[i]&lt;-(length(unique(a[1:6]))==3)&amp;(length(unique(a[7:12]))==3)&amp;(length(unique(a[13:18]))==3)&amp;(length(unique(a[19:24]))==3) } mean(p) ## [1] 0.4966 Interpretation: - Each chunk represents a “serving” or “round.” - The event is that each serving includes all three flavors at least once. 7.2.14.3 Example: two boxes, same color box1&lt;-c(rep(&#39;white&#39;,5), rep(&quot;black&quot;,11), rep(&#39;red&#39;,8)) box2&lt;-c(rep(&#39;white&#39;,10), rep(&quot;black&quot;,8), rep(&#39;red&#39;,6)) sim&lt;-10000 p&lt;-numeric(sim) for (i in 1:sim){ a&lt;-sample(box1, 1) b&lt;-sample(box2, 1) p[i]&lt;- a==b } mean(p) ## [1] 0.3258 Interpretation: - This approximates \\(P(\\text{color}_1=\\text{color}_2)\\). - A quick analytic approach would sum products of marginal probabilities by color; simulation provides a fast check. 7.2.14.4 Example: sampling with replacement, both white box&lt;-c(rep(&quot;white&quot;,4),rep(&quot;red&quot;,2)) sim&lt;-10000 t&lt;-numeric(sim) for (i in 1:sim){ a&lt;-sample(box, 2 ,replace=T) # there are two white balls t[i]&lt;-length(a[a==&quot;white&quot;])==2 } mean(t) ## [1] 0.4501 Interpretation: - With replacement, the draws are independent. - This event probability is approximately \\((4/6)^2\\); simulation verifies it. 7.2.14.5 Birthday problem: at least two share a birthday among 30 n&lt;-30 sim&lt;-10000 t&lt;-numeric(sim) for (i in 1:sim){ a&lt;-sample(1:365, n, replace=T) t[i]&lt;-n-length(unique(a)) } 1-mean(t==0) ## [1] 0.7111 # probability 1-prod(365:(365-30+1))/365^30 ## [1] 0.7063162 Interpretation: - Simulation estimates \\(P(\\text{at least one match})\\). - The exact probability uses the complement event “all birthdays distinct.” 7.2.15 Discrete random variable 7.2.15.1 Example: binomial (guessing answers) If each question has 4 options and you guess randomly, the number correct out of 5 follows \\(X\\sim\\mathrm{Bin}(5,1/4)\\). x&lt;-0:5 y&lt;-dbinom(x,5,1/4) plot(x,y,col=2,type=&#39;h&#39;) Interpretation: - The plot is a PMF: it shows \\(P(X=k)\\) as vertical bars. 7.2.15.2 Example: most likely number of hits If hit probability is 0.02 per trial and there are 400 trials, the number of hits is \\(X\\sim\\mathrm{Bin}(400,0.02)\\), and the “most likely” value is the mode (the \\(k\\) with maximum PMF). k&lt;-0:400 p&lt;-dbinom(k,400,0.02) plot(k,p,type=&#39;h&#39;,col=2) plot(k,p,type=&#39;h&#39;,col=2,xlim=c(0,20)) dbinom(7,400,0.02) ## [1] 0.1406443 dbinom(8,400,0.02) ## [1] 0.1410031 Interpretation: - The zoomed-in plot helps locate the mass near the mean \\(np=8\\). - Comparing dbinom(7,...) and dbinom(8,...) checks which value is more likely. 7.2.16 Exponent distribution If lifetime is exponential with rate \\(\\lambda=1/2000\\), then: \\[ P(X&gt;1000)=\\int_{1000}^{\\infty}\\lambda e^{-\\lambda x}\\,dx = e^{-\\lambda\\cdot 1000}. \\] Your code computes this probability using integration, the CDF, and simulation. integrate(dexp,rate=1/2000,1000,Inf)$value ## [1] 0.6065307 f&lt;-function(x){dexp(x,rate=1/2000)} integrate(f,1000,Inf) $value ## [1] 0.6065307 1-pexp(1000,rate=1/2000) ## [1] 0.6065307 mean(rexp(10000,rate=1/2000)&gt;1000) ## [1] 0.6035 Interpretation: - 1-pexp(...) is usually the cleanest way. - Simulation provides a numerical check and builds intuition about tail probabilities. 7.2.17 Normal distribution plot Normal density plots are helpful for understanding how mean and standard deviation control shape. 7.2.17.1 Changing the standard deviation x&lt;-seq(-3,3,0.01) plot(x, dnorm(x,mean=0, sd=2),type=&quot;l&quot;,xlab=&quot;x&quot;,ylab = &quot;f(x)&quot;, col=1,lwd=2,ylim=c(0,1)) #density function lines(x, dnorm(x,mean=0, sd=1),lty=2, col=2,lwd=2) lines(x, dnorm(x,mean=0, sd=0.5), lty=3,col=3,lwd=2) exbeta&lt;-c(expression(paste(mu,&quot;=0,&quot;, sigma,&quot;=2&quot;)), expression(paste(mu,&quot;=0,&quot;,sigma,&quot;=1&quot;)), expression(paste(mu,&quot;=0,&quot;, sigma,&quot;=0.5&quot;))) legend(&quot;topright&quot;, exbeta, lty = c(1, 2,3),col=c(1,2,3),lwd=2) Interpretation: - Larger \\(\\sigma\\) spreads the curve and lowers the peak. - Smaller \\(\\sigma\\) concentrates mass near the mean and increases the peak. 7.2.17.2 Shifting the mean x&lt;-seq(-3,3,0.01) plot(x, dnorm(x,mean=-1, sd=1),type=&quot;l&quot;,xlab=&quot;x&quot;,ylab = &quot;f(x)&quot;, col=1,lwd=2,ylim=c(0,0.6)) lines(x, dnorm(x,mean=0, sd=1),lty=2, col=2,lwd=2) lines(x, dnorm(x,mean=1, sd=1), lty=3,col=3,lwd=2) exbeta&lt;-c(expression(paste(mu,&quot;=-1,&quot;, sigma,&quot;=1&quot;)), expression(paste(mu,&quot;=0,&quot;,sigma,&quot;=1&quot;)), expression(paste(mu,&quot;=1,&quot;, sigma,&quot;=1&quot;))) legend(&quot;topright&quot;, exbeta, lty = c(1, 2,3),col=c(1,2,3),lwd=2) Interpretation: - Changing \\(\\mu\\) shifts the curve left/right without changing shape (when \\(\\sigma\\) is fixed). 7.2.17.3 Solving for sigma numerically This code finds a \\(\\sigma\\) such that: \\[ P(120\\le X\\le 200) &lt; 0.80 \\quad \\text{for }X\\sim N(160,\\sigma^2), \\] using iterative search. sigma&lt;-1 repeat{ sigma&lt;-sigma+0.01 if (pnorm(200,160,sigma)-pnorm(120,160,sigma)&lt;0.80) break } sigma ## [1] 31.22 # alternative sigma&lt;-1 while( pnorm(200,160,sigma)-pnorm(120,160,sigma)&gt;=0.80){sigma&lt;-sigma+0.01} sigma ## [1] 31.22 Interpretation: - This is a simple numerical method (incremental search). - In more advanced work, you might solve it using uniroot(). 7.2.18 Distribution of random variable function 7.2.18.1 Discrete transformation Here we sample from a discrete distribution with weights, then examine the distributions of transformations \\(X^2\\) and \\(2X\\). x&lt;-c(-1,0,1,2,2.5) weight&lt;-c(0.2,0.1,0.1,0.3,0.3) toss&lt;-sample(x,10000,replace=T,weight) table(toss^2)/length(toss^2) 0 1 4 6.25 0.1011 0.3047 0.2949 0.2993 table(2*toss)/length(2*toss) -2 0 2 4 5 0.2018 0.1011 0.1029 0.2949 0.2993 Interpretation: - Transformations can change the support and probabilities. - This is a simulation-based way to study the distribution of \\(g(X)\\). 7.2.18.2 Continuous density via simulation This code compares an estimated density to a known “truth” piecewise density. x &lt;- seq(0,5,0.01) truth&lt;-rep(0,length(x)) truth[0&lt;=x&amp;x&lt;1]&lt;-2/3 truth[1&lt;=x&amp;x&lt;2]&lt;-1/3 plot(density(abs(runif(1000000,-1,2))),main=NA, ylim=c(0,1),lwd=3,lty=3) lines(x,truth,col=&quot;red&quot;,lwd=2) legend(&quot;topright&quot;,c(&quot;True Density&quot;,&quot;Estimated Density&quot;), col=c(&quot;red&quot;,&quot;black&quot;),lwd=3,lty=c(1,3)) Interpretation: - Kernel density estimation approximates a PDF from simulated samples. - This is a common strategy when the analytic density of a transformed variable is difficult to derive. 7.2.19 Joint and marginal probability (simulation approach) This example constructs a joint distribution by simulation: choose \\(X\\) uniformly from 1:4, then choose \\(Y\\) uniformly from 1:\\(X\\). p&lt;-function(x,y) { sim&lt;-10000 t&lt;-numeric(sim) for (i in 1:sim) { a&lt;-sample(1:4,1) b&lt;-sample(1:a,1) t[i]&lt;-(a==x)&amp;(b==y) } mean(t) } PF&lt;-matrix(0,4,4) for (i in 1:4) { for (j in 1:4) { PF[i,j]&lt;-p(i, j) } } PF 0.2610 0.0000 0.0000 0.0000 0.1260 0.1262 0.0000 0.0000 0.0858 0.0815 0.0847 0.0000 0.0617 0.0665 0.0648 0.0619 apply(PF,1,sum) ## [1] 0.2610 0.2522 0.2520 0.2549 apply(PF,2,sum) ## [1] 0.5345 0.2742 0.1495 0.0619 Interpretation: - PF approximates the joint PMF table \\(p_{ij}\\). - Row sums approximate the marginal distribution of \\(X\\). - Column sums approximate the marginal distribution of \\(Y\\). This is a useful template: if you can simulate from the joint mechanism, you can estimate joint/marginal/conditional distributions numerically. 7.2.20 Multiple random variables: derived distributions Here we simulate \\((X,Y)\\), then compute the distributions of several derived variables: \\(Z_1=X+Y\\) \\(Z_2=XY\\) \\(Z_3=\\max(X,Y)\\) \\(Z_4=X/Y\\) x&lt;-sample(1:4, 10000, replace=T, prob=c(1/4, 1/4, 1/4, 1/4)) y&lt;-numeric(10000) for(i in 1:10000) { if(x[i]==1) {y[i]&lt;-sample(1:4,1,replace=T, prob=c(1,0,0,0))} if(x[i]==2) {y[i]&lt;-sample(1:4,1,replace=T, prob=c(1/2,1/2,0,0))} if(x[i]==3) {y[i]&lt;-sample(1:4,1,replace=T, prob=c(1/3,1/3,1/3,0))} if(x[i]==4) {y[i]&lt;-sample(1:4,1,replace=T, prob=c(1/4,1/4,1/4,1/4))} } z1&lt;-x+y table(z1)/length(z1) 2 3 4 5 6 7 8 0.2461 0.126 0.2071 0.1475 0.1481 0.061 0.0642 z2&lt;-x*y table(z2)/length(z2) 1 2 3 4 6 8 9 12 16 0.2461 0.126 0.0826 0.1846 0.0874 0.0615 0.0866 0.061 0.0642 z3&lt;-pmax(x,y) table(z3)/length(z3) 1 2 3 4 0.2461 0.2505 0.2566 0.2468 z4&lt;-x/y table(z4)/length(z4) 1 1.33333333333333 1.5 2 3 4 0.5214 0.061 0.0874 0.1875 0.0826 0.0601 Interpretation: - This is a direct demonstration that even if \\((X,Y)\\) are discrete with a known mechanism, the distribution of \\(g(X,Y)\\) may be nontrivial. - Simulation gives a quick empirical picture of the resulting distribution. 7.2.21 Sum of two independent normal variables If \\(X\\sim N(0,1)\\) and \\(Y\\sim N(0,1)\\) independent, then: \\[ Z=X+Y \\sim N(0,2). \\] This code overlays simulated density with the true \\(N(0,2)\\) density. Z&lt;-function(n){ x&lt;-seq(-4,4,0.01) truth&lt;-dnorm(x,0,sqrt(2)) plot(density(rnorm(n)+rnorm(n)),main=&quot;Density Estimate of the Normal Addition Model&quot;,ylim=c(0,0.4),lwd=2,lty=2) lines(x,truth,col=&quot;red&quot;,lwd=2) legend(&quot;topright&quot;,c(&quot;True&quot;,&quot;Estimated&quot;),col=c(&quot;red&quot;,&quot;black&quot;),lwd=2,lty=c(1,2)) } Z(10000) Interpretation: - This is an important pattern: sums of independent normals remain normal, and variances add. - Many asymptotic results in statistics rely on this stability. 7.2.22 Generate a circle using simulated random dots Monte Carlo geometry: estimate regions defined by inequalities. Circle region: \\[ D=\\{(x,y): x^2+y^2\\le 1\\}. \\] x&lt;-runif(10000,-1,1) y&lt;-runif(10000,-1,1) a&lt;-x[x^2+y^2&lt;=1] b&lt;-y[x^2+y^2&lt;=1] plot(a,b,col=4) Oval (ellipse) region: \\[ \\frac{x^2}{a^2}+\\frac{y^2}{b^2}\\le 1. \\] a&lt;-3 b&lt;-1 x&lt;-runif(10000,-a,a) y&lt;-runif(10000,-b,b) x1&lt;-x[x^2/a^2+y^2/b^2&lt;=1] y1&lt;-y[x^2/a^2+y^2/b^2&lt;=1] plot(x1,y1,col=3) Interpretation: - This technique generalizes to Monte Carlo integration: “probability = area/volume proportion.” 7.2.23 Expectation (simulation) Expectation is the long-run average outcome. Simulation estimates it naturally by sample means. Example: piecewise payoff depending on an exponential random variable. sim&lt;-10000 t&lt;-numeric(sim) for (i in 1:sim) { Y&lt;-1500 X&lt;-rexp(1,rate=1/10) Y[1&lt;X&amp;X&lt;=2]&lt;-2000 Y[2&lt;X&amp;X&lt;=3]&lt;-2500 Y[3&lt;X]&lt;-3000 t[i]&lt;-Y } mean(t) ## [1] 2727.5 Interpretation: - This is a practical way to compute \\(E(Y)\\) when \\(Y\\) is a complicated function of a random input. - Analytically, you would integrate the payoff function against the density of \\(X\\). 7.2.24 Central Limit Theorem (simulation) 7.2.24.1 CLT for exponential distribution This code simulates the sampling distribution of \\(\\bar X\\) for \\(X\\sim\\mathrm{Exp}(\\lambda)\\), and overlays the normal approximation: \\[ \\bar X \\approx N\\left(\\mu,\\frac{\\sigma^2}{n}\\right), \\] with \\(\\mu=1/\\lambda\\) and \\(\\sigma=1/\\lambda\\) for the exponential. ####Central Limit Theorem for Expotential distribution layout(matrix(c(1,3,2,4 ),ncol=2)) r&lt;-1000 lambda&lt;-1/100 for (n in c(1,5,10,30)){ mu&lt;-1/lambda xbar&lt;-numeric(r) sxbar&lt;-1/(sqrt(n)*lambda) for(i in 1:r){ xbar[i]&lt;-mean(rexp(n,rate=lambda)) } hist(xbar,prob=T,main=paste(&#39;SampDist.Xbar,n=&#39;,n),col=gray(.8)) Npdf&lt;-dnorm(seq(mu-3*sxbar,mu+3*sxbar,0.01),mu,sxbar) lines(seq(mu-3*sxbar,mu+3*sxbar,0.01),Npdf,lty=2,col=2) box() } Interpretation: - For small \\(n\\), the distribution is skewed (reflecting the exponential). - As \\(n\\) increases, the histogram becomes closer to normal. 7.2.24.2 CLT for uniform distribution Uniform(0,10) has: \\[ \\mu=5,\\quad \\sigma=\\frac{10}{\\sqrt{12}}. \\] The code repeats the same idea for a different population distribution. #######The central limit theorem for uniform distribution layout(matrix(c(1,3,2,4),ncol=2)) r&lt;-10000 mu&lt;-5 sigma&lt;-10/sqrt(12) for (n in c(1,5,10,30)){ xbar&lt;-numeric(r) sxbar&lt;-sigma/sqrt(n) for (i in 1:r){ xbar[i]&lt;-mean(runif(n,0,10))} hist(xbar,prob=T,main=paste(&#39;SampDist.Xbar,n=&#39;,n),col=gray(0.8),ylim=c(0,1/(sqrt(1*pi)*sxbar))) XX&lt;-seq(mu-3*sxbar,mu+3*sxbar,0.01) Npdf&lt;-dnorm(XX,mu,sxbar) lines(XX,Npdf,lty=2,col=2) box()} 7.2.25 Law of large numbers The law of large numbers says the running average converges to the expected value. For a discrete uniform sample from 1:10, the theoretical mean is 5.5. N &lt;- 5000 set.seed(123) x &lt;- sample(1:10, N, replace = T) s &lt;- cumsum(x) r.avg &lt;- s/(1:N) options(scipen = 10) plot(r.avg, ylim=c(1, 10), type = &quot;l&quot;, xlab = &quot;Observations&quot; ,ylab = &quot;Probability&quot;, lwd = 2) lines(c(0,N), c(5.5,5.5),col=&quot;red&quot;, lwd = 2) Interpretation: - Early averages fluctuate widely. - As \\(N\\) grows, the curve stabilizes around 5.5. 7.2.26 Empirical distribution function (ECDF) The ECDF is the sample-based estimate of the CDF: \\[ \\hat F(x)=\\frac{1}{n}\\sum_{i=1}^n I(X_i\\le x). \\] x&lt;-c(-2,-1.2,1.5,2.3,3.5) plot(ecdf(x),col=2) abline(v=0,col=3) Interpretation: - The ECDF jumps by \\(1/n\\) at each observed value. - It is a nonparametric summary of the distribution. 7.2.27 Probability of mean &gt; 3 (simulation) If three numbers are from \\(N(2,9)\\), then \\(\\bar X\\) is normal, but simulation provides an easy estimate. A&lt;-matrix(rnorm(30000,2,3),10000,3) mean(apply(A,1,mean)&gt;3) ## [1] 0.2789 Interpretation: - Each row is one sample of size 3. - We estimate \\(P(\\bar X&gt;3)\\) by counting the fraction of rows with mean &gt; 3. (Analytically, \\(\\bar X\\sim N(2, 9/3)=N(2,3)\\), so \\(P(\\bar X&gt;3)=1-\\Phi((3-2)/\\sqrt{3})\\).) 7.2.28 Maximum likelihood estimate (MLE) MLE chooses parameters that maximize the likelihood (or log-likelihood). Your code estimates mean and variance for a normal model. sample&lt;-c(1.38, 3.96, -0.16, 8.12, 6.30, 2.61, -1.35, 0.03, 3.94, 1.11) n&lt;-length(sample) muhat&lt;-mean(sample) sigsqhat&lt;-sum((sample-muhat)^2)/n muhat ## [1] 2.594 sigsqhat ## [1] 8.133884 loglike&lt;-function(theta){ a&lt;--n/2*log(2*pi)-n/2*log(theta[2])-sum((sample-theta[1])^2)/(2*theta[2]) return(-a) } optim(c(2,2),loglike,method=&quot;BFGS&quot;)$par ## [1] 2.593942 8.130340 Interpretation: - The MLE for \\(\\mu\\) is the sample mean. - The MLE for \\(\\sigma^2\\) divides by \\(n\\) (not \\(n-1\\)), which differs from the unbiased sample variance. - optim() numerically confirms the closed-form solution, which is a useful habit when teaching likelihood. 7.2.29 t distribution, F distribution plots, and common distributions This section overlays simulated and theoretical densities to reinforce the idea that distributions can be understood both analytically and empirically. n&lt;-30 x&lt;-seq(-6,6,0.01) y&lt;-seq(-6,6,1) Truth&lt;-df(x,1,n) plot(density(rt(10000,n)^2),main=&quot;PDF&quot;,ylim=c(0,1),lty=2,xlim=c(-6,6)) #simulation lines(x, dt(x,n), col=3) #t dist lines(x, dchisq(x,2), col=4) #chisq dist lines(x,Truth,col=2) #f dist abline (v=0 ,col=7) points(y,dbinom(y, size=12, prob=0.2),col=1) #binomial dist points(y,dpois(y, 6),col=2) #poisson dist lines(y,dunif(-6:6,min=-6,max=6 ),col=5) #uniform Interpretation notes: - rt(... )^2 produces a distribution related to F (because ratios of chi-squares define F, and \\(t^2\\) is a special case). - Overlaying multiple distributions is primarily pedagogical: it builds intuition for how shapes differ (skew, tail heaviness, support). Closing perspective Probability is not an isolated topic; it is the foundation for: interpreting randomness and variation in real data, understanding why estimators fluctuate, quantifying uncertainty via standard errors and confidence intervals, and making principled decisions via hypothesis tests. The R practice section emphasizes a pragmatic approach: - If you can simulate a mechanism, you can estimate probabilities and expectations. - When analytic solutions exist, simulation becomes a powerful way to validate them and build intuition. "],["algorithms.html", "8 Algorithms 8.1 Maximum likelihood estimation 8.2 Gradient descent 8.3 Markov chain Monte Carlo 8.4 Expectation maximum 8.5 Combine estimates by pooling rules 8.6 Simulations and Bootstrapping", " 8 Algorithms In statistics and machine learning, algorithms provide the computational machinery that turns probability models into concrete numerical estimates. This chapter focuses on three closely related ideas: Maximum likelihood estimation (MLE) as a unifying estimation principle, Gradient-based optimization as a general-purpose numerical algorithm, and The connection between likelihood, optimization, and uncertainty quantification. The emphasis here is not on deriving every formula from scratch, but on understanding how the mathematics, statistical interpretation, and R implementations fit together. 8.1 Maximum likelihood estimation 8.1.1 Likelihood estimation (without random effects) Suppose we observe \\(n\\) independent observations \\(y_1,\\ldots,y_n\\), and we assume that the data are generated from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). The likelihood function is the joint density of the observed data, viewed as a function of the unknown parameters. For independent normal observations: \\[ L\\left(\\left(y_{1}, \\ldots, y_{n}\\right), \\mu, \\sigma^{2}\\right) =\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp\\left(-\\frac{(y_i-\\mu)^2}{2\\sigma^2}\\right). \\] Rather than maximizing \\(L\\) directly, we almost always work with the log-likelihood, because: products become sums, numerical stability improves, derivatives are easier to compute. In practice, optimization routines typically minimize the negative log-likelihood: \\[ \\ell = -\\log L. \\] 8.1.2 Negative log-likelihood (scalar form) Taking the logarithm and multiplying by \\(-1\\) gives: \\[ \\begin{aligned} \\ell\\left(\\left(y_{1}, \\ldots, y_{n}\\right), \\mu, \\sigma^{2}\\right) &amp;= -\\log\\left[ \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp\\left(-\\frac{(y_i-\\mu)^2}{2\\sigma^2}\\right) \\right] \\\\ &amp;= -\\sum_{i=1}^{n} \\left[ \\log\\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right) -\\frac{(y_i-\\mu)^2}{2\\sigma^2} \\right]. \\end{aligned} \\] This expression makes two things explicit: One part depends only on \\(\\sigma^2\\) (the normalization constant). The other part depends on squared deviations \\((y_i-\\mu)^2\\). 8.1.3 Matrix formulation The same likelihood can be written compactly using linear algebra. Let: \\(\\mathbf{y}\\) be the \\(n\\times 1\\) response vector, \\(\\mathbf{X}\\) be the design matrix, \\(\\beta\\) be the vector of regression coefficients, \\(\\mathbf{I}\\) be the identity matrix. Assuming independent errors with variance \\(\\sigma^2\\), the covariance matrix is \\(\\sigma^2\\mathbf{I}\\). The negative log-likelihood becomes: \\[ \\begin{aligned} \\ell(\\mathbf{y}, \\beta, \\gamma) &amp;=\\frac{1}{2}\\left\\{ n \\log (2 \\pi) +\\log \\left|\\sigma^{2} \\mathbf{I}\\right| +(\\mathbf{y}-\\mathbf{X} \\beta)^{\\prime} (\\sigma^{2} \\mathbf{I})^{-1} (\\mathbf{y}-\\mathbf{X} \\beta) \\right\\} \\\\ &amp;=\\frac{1}{2}\\left\\{ n \\log (2 \\pi) +n \\log (\\sigma^{2}) +(\\mathbf{y}-\\mathbf{X}\\beta)^{\\prime} (\\mathbf{y}-\\mathbf{X}\\beta)/\\sigma^{2} \\right\\}. \\end{aligned} \\] Here, \\(\\gamma\\) denotes variance-related parameters, derived from \\(\\sigma^2\\mathbf{I}\\). This form highlights an important idea: Ordinary least squares is maximum likelihood estimation under normal errors. 8.1.4 Minimizing the negative log-likelihood MLE finds parameter values that minimize \\(\\ell\\): \\[ \\arg\\min \\; \\ell(\\mathbf{y}, \\beta, \\gamma). \\] For the simple normal model with parameters \\((\\mu,\\sigma^2)\\), we can solve this analytically by taking derivatives. 8.1.5 Taking derivatives Derivative with respect to \\(\\mu\\): \\[ \\begin{aligned} \\frac{\\partial \\ell(\\mu, \\sigma^{2})}{\\partial \\mu} &amp;=\\frac{1}{2}\\sum_{i=1}^{n}(-2)(y_i-\\mu)/\\sigma^2 \\\\ &amp;=(n\\mu-\\sum_{i=1}^{n}y_i)/\\sigma^2. \\end{aligned} \\] Setting this equal to zero yields: \\[ \\hat{\\mu}=\\bar{y}. \\] Derivative with respect to \\(\\sigma^2\\): \\[ \\frac{\\partial \\ell(\\mu, \\sigma^{2})}{\\partial \\sigma^{2}} =\\frac{1}{2}\\left[ \\frac{n}{\\sigma^{2}} -\\frac{\\sum_{i=1}^{n}(y_i-\\mu)^2}{(\\sigma^{2})^2} \\right]. \\] Solving gives: \\[ \\hat{\\sigma}^{2} =\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\bar{y})^2. \\] This is the MLE of the variance, which divides by \\(n\\) rather than \\(n-1\\). 8.1.6 REML versus ML Under restricted maximum likelihood (REML), the variance estimator adjusts for the estimation of the mean: \\[ \\hat{\\sigma}^{2}_{\\text{REML}} =\\frac{1}{n-1}\\sum_{i=1}^{n}(y_i-\\bar{y})^2. \\] Conceptually: ML treats \\(\\mu\\) as fixed but unknown. REML accounts for the loss of one degree of freedom when estimating \\(\\mu\\). This distinction becomes crucial in linear mixed models, where REML is often preferred for variance components. 8.1.7 R demonstration 8.1.7.1 Toy data We simulate data from a linear regression model with normal errors. set.seed(123) n &lt;- 20000 x &lt;- rnorm(n, 2, sqrt(2)) s &lt;- rnorm(n, 0, 0.8) y &lt;- 1.5+x*3+s mydata &lt;- data.frame(y,x) 8.1.7.2 Linear regression (closed-form MLE) For normal errors, the MLE of regression coefficients coincides with ordinary least squares. lmfit &lt;- lm(y~., data=mydata) coefficients(lmfit) ## (Intercept) x ## 1.487701 3.003695 (summary(lmfit)$sigma**2) ## [1] 0.6397411 Interpretation: coefficients(lmfit) estimates \\(\\beta\\). (summary(lmfit)$sigma)^2 is the estimated variance of the errors. 8.1.7.3 MLE via explicit negative log-likelihood We now code the negative log-likelihood directly using matrix notation. minusloglik &lt;- function(param){ beta &lt;- param[-1] # regression coefficients sigma &lt;- param[1] # variance y &lt;- as.vector(mydata$y) x &lt;- cbind(1, mydata$x) mu &lt;- x %*% beta 0.5 * (n*log(2*pi) + n*log(sigma) + sum((y-mu)^2)/sigma) } This function implements exactly the mathematical expression derived earlier. Optimizing it numerically: MLoptimize &lt;- optim(c(1,1,1), minusloglik) MLoptimize$par ## [1] 0.6397201 1.4876186 3.0038675 The results agree with lm(), confirming that linear regression is an MLE problem. 8.1.7.4 Maximum likelihood via maxLik The maxLik package directly maximizes the log-likelihood. library(maxLik) ols.lf &lt;- function(param) { beta &lt;- param[-1] sigma &lt;- param[1] y &lt;- as.vector(mydata$y) x &lt;- cbind(1, mydata$x) mu &lt;- x %*% beta sum(dnorm(y, mu, sqrt(sigma), log = TRUE)) } mle_ols &lt;- maxLik(logLik = ols.lf, start = c(sigma = 1, beta1 = 1, beta2 = 1)) summary(mle_ols) ## -------------------------------------------- ## Maximum Likelihood estimation ## Newton-Raphson maximisation, 11 iterations ## Return code 8: successive function values within relative tolerance limit (reltol) ## Log-Likelihood: -23910.85 ## 3 free parameters ## Estimates: ## Estimate Std. error t value Pr(&gt; t) ## sigma 0.639677 0.006396 100.0 &lt;2e-16 *** ## beta1 1.487701 0.009768 152.3 &lt;2e-16 *** ## beta2 3.003695 0.003999 751.2 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## -------------------------------------------- This approach generalizes easily to models where no closed-form solution exists. 8.1.7.5 Another example (real data) We repeat the procedure for mtcars. ols.lf &lt;- function(param) { beta &lt;- param[-1] sigma &lt;- param[1] y &lt;- as.vector(mtcars$mpg) x &lt;- cbind(1, mtcars$cyl, mtcars$disp) mu &lt;- x %*% beta sum(dnorm(y, mu, sqrt(sigma), log = TRUE)) } mle_ols &lt;- maxLik(logLik = ols.lf, start = c(sigma = 1, beta1 = 1, beta2 = 1, beta3 = 1)) summary(mle_ols) ## -------------------------------------------- ## Maximum Likelihood estimation ## Newton-Raphson maximisation, 25 iterations ## Return code 2: successive function values within tolerance limit (tol) ## Log-Likelihood: -79.57282 ## 4 free parameters ## Estimates: ## Estimate Std. error t value Pr(&gt; t) ## sigma 8.460638 2.165929 3.906 0.0000937 *** ## beta1 34.660993 2.450186 14.146 &lt; 2e-16 *** ## beta2 -1.587276 0.683895 -2.321 0.0203 * ## beta3 -0.020584 0.009815 -2.097 0.0360 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## -------------------------------------------- lmfit2 &lt;- lm(mpg~cyl+disp, data=mtcars) lmfit2 ## ## Call: ## lm(formula = mpg ~ cyl + disp, data = mtcars) ## ## Coefficients: ## (Intercept) cyl disp ## 34.66099 -1.58728 -0.02058 (summary(lmfit2)$sigma**2) ## [1] 9.335872 Again, MLE and OLS agree under normal error assumptions. 8.1.8 Confidence intervals from likelihood Most classical confidence intervals rely on the asymptotic normality of MLEs: \\[ \\hat{\\theta}_i \\pm z_{1-\\alpha/2}\\, SE(\\hat{\\theta}_i). \\] The standard error is derived from the Hessian matrix (second derivative of the log-likelihood), also called the observed Fisher information. 8.1.8.1 Fisher information and Hessian For a parameter vector \\(\\theta\\): \\[ I(\\theta)=\\ell&#39;&#39;(\\theta). \\] The covariance matrix of \\(\\hat{\\theta}\\) is approximated by: \\[ I(\\hat{\\theta})^{-1}. \\] For the normal model with parameters \\((\\mu,\\sigma^2)\\), the Hessian is diagonal: \\[ \\frac{\\partial^2 \\ell}{\\partial \\mu^2}=\\frac{n}{\\sigma^2}, \\quad \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} =\\frac{n}{2(\\sigma^2)^2}, \\quad \\frac{\\partial^2 \\ell}{\\partial \\mu\\,\\partial\\sigma^2}=0. \\] Thus: \\[ SE_{\\hat{\\mu}}=\\sqrt{\\frac{\\hat{\\sigma}^2}{n}}, \\quad SE_{\\hat{\\sigma}^2}=\\hat{\\sigma}^2\\sqrt{\\frac{2}{n}}. \\] The corresponding Wald intervals follow immediately. 8.1.8.2 Likelihood ratio confidence intervals Wald intervals can perform poorly in small samples. A likelihood-based alternative defines the confidence set as: \\[ \\left\\{ \\theta \\;\\middle|\\; \\frac{L(\\theta)}{L(\\hat{\\theta})}&gt;\\exp(-3.84/2) \\right\\}, \\] where \\(3.84\\) is the 95% cutoff from a \\(\\chi^2_1\\) distribution. This approach often has better finite-sample behavior. 8.1.8.3 Profile likelihood The profile likelihood fixes one parameter and maximizes over the others. For example, profiling \\(\\mu\\): \\[ L_p(\\mu)=L\\left(\\mu,\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\mu)^2\\right). \\] Similarly, profiling \\(\\sigma^2\\): \\[ L_p(\\sigma^2)=L(\\bar{y},\\sigma^2). \\] Profile likelihoods are widely used for confidence intervals in mixed models and survival analysis. 8.1.9 Maximum likelihood estimate practice The following example revisits MLE numerically, reinforcing the connection between theory and optimization. sample&lt;-c(1.38, 3.96, -0.16, 8.12, 6.30, 2.61, -1.35, 0.03, 3.94, 1.11) n&lt;-length(sample) muhat&lt;-mean(sample) sigsqhat&lt;-sum((sample-muhat)^2)/n muhat ## [1] 2.594 sigsqhat ## [1] 8.133884 loglike&lt;-function(theta){ a&lt;--n/2*log(2*pi)-n/2*log(theta[2])- sum((sample-theta[1])^2)/(2*theta[2]) return(-a) } optim(c(2,2),loglike,method=&quot;BFGS&quot;)$par ## [1] 2.593942 8.130340 8.2 Gradient descent While MLE provides the objective function, gradient descent is a general optimization algorithm used to find minima numerically. 8.2.1 Linear regression example library(&quot;ggplot2&quot;) res &lt;- lm(hwy ~ cty ,data=mpg) summary(res) ## ## Call: ## lm(formula = hwy ~ cty, data = mpg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3408 -1.2790 0.0214 1.0338 4.0461 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.89204 0.46895 1.902 0.0584 . ## cty 1.33746 0.02697 49.585 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.752 on 232 degrees of freedom ## Multiple R-squared: 0.9138, Adjusted R-squared: 0.9134 ## F-statistic: 2459 on 1 and 232 DF, p-value: &lt; 2.2e-16 8.2.2 Cost function and gradient descent We define the squared-error cost: \\[ J(\\theta)=\\frac{1}{2n}\\sum_{i=1}^n (X_i\\theta-y_i)^2. \\] cost &lt;- function(X, y, theta) { sum((X %*% theta - y)^2) / (2*length(y)) } Gradient descent iteratively updates parameters in the direction of the negative gradient. alpha &lt;- 0.005 num_iters &lt;- 20000 theta &lt;- matrix(c(0,0), nrow=2) x &lt;- mpg$cty y &lt;- mpg$hwy X &lt;- cbind(1, matrix(x)) cost_history &lt;- double(num_iters) for (i in 1:num_iters) { error &lt;- (X %*% theta - y) delta &lt;- t(X) %*% error / length(y) theta &lt;- theta - alpha * delta cost_history[i] &lt;- cost(X, y, theta) } theta 0.8899161 1.3375742 tail(cost_history) ## [1] 1.522137 1.522137 1.522137 1.522137 1.522137 1.522137 8.2.3 Cost function convergence plot(cost_history, type=&#39;line&#39;, col=&#39;red&#39;, lwd=2, main=&#39;Cost function&#39;, ylab=&#39;cost&#39;, xlab=&#39;Iterations&#39;) ## Warning in plot.xy(xy, type, ...): plot type &#39;line&#39; will be truncated to first ## character This plot illustrates: monotone decrease of the objective, convergence toward the OLS solution. 8.2.4 Comparing gradient descent and linear regression x &lt;- mpg$cty yhat &lt;- mpg$cty*theta[2] + theta[1] plot(x,yhat, main=&#39;Linear regression by gradient descent&#39;) abline(lm(mpg$hwy ~ mpg$cty), col=&quot;blue&quot;, lwd=4) abline(res, col=&#39;red&#39;) The fitted line from gradient descent coincides with the closed-form OLS solution. Broader context 8.3 Markov chain Monte Carlo MCMC extends likelihood-based inference when direct optimization is infeasible. see here This report compares linear models, mixed-effects models (random intercept and random slope), a two-stage approach, and a Bayesian joint model (JAGS + MCMC) to predict the outcome NDSBWGT_fm024 using longitudinal EFW trajectories and covariates. Performance results indicate the two-stage model performs substantially better (median RMSE ≈ 252, median R² ≈ 0.612) whereas the implemented joint model shows near-zero explanatory power in the shown output (R² ≈ 0.003 with extremely large RMSE), suggesting the joint model specification/scale and outcome distribution need to be revisited. 8.4 Expectation maximum EM algorithms handle latent variables by alternating expectation and maximization steps. see here 8.5 Combine estimates by pooling rules Pooling rules combine estimates across multiple datasets (e.g., multiple imputation). see here 8.6 Simulations and Bootstrapping Bootstrapping replaces analytic sampling distributions with resampling-based approximations. see here Together, these algorithms form the computational backbone of modern statistical practice. The key takeaway is: Probability defines the model, likelihood defines the objective, and algorithms deliver the estimates. "],["sasmarkdown.html", "9 SASmarkdown 9.1 How to install SASmarkdown 9.2 Common statements 9.3 Using macros 9.4 Use SAS formats", " 9 SASmarkdown This section shows how to write SAS code inside an R Markdown workflow using the SASmarkdown package. The goal is to keep one reproducible document (HTML/PDF) while allowing SAS to do what SAS is good at: data steps, PROC routines, reporting, and macros. Two ideas drive the design: knitr engines: a code chunk can be executed by an external program (SAS) instead of R. chunk isolation: in most SASmarkdown workflows, each SAS chunk runs as its own SAS “session script,” so objects created in one chunk may not automatically exist in another chunk (unless you explicitly write/read datasets). This is why SASmarkdown feels different from pure SAS Studio and also different from pure R Markdown. 9.1 How to install SASmarkdown The SASmarkdown package and documentation are here: sasmarkdown The typical setup steps are: Install and load SASmarkdown in R. Tell knitr where SAS is installed (the sas.exe path). Set SAS options (log, line size, batch mode). Set the chunk engine to a SAS engine such as sas, saslog, sashtml, etc. 9.1.1 Use an engine The first chunk loads the package. You can also set global knitr options here, but the key is that SASmarkdown must be available. # knitr::opts_chunk$set(echo = TRUE) require(SASmarkdown) ## Loading required package: SASmarkdown ## Warning: package &#39;SASmarkdown&#39; was built under R version 4.4.3 ## SAS found at C:/Program Files/SASHome/SASFoundation/9.4/sas.exe ## SAS engines are now ready to use. Next, we define the SAS executable path and options, then tell knitr to use sashtml as the default engine. What this means operationally: knitr writes your SAS code to a temporary .sas file, calls sas.exe in batch mode, captures the HTML output produced by SAS, embeds that output into your knitted document. saspath &lt;- &quot;C:/Program Files/SASHome/SASFoundation/9.4/sas.exe&quot; sasopts &lt;- &quot;-nosplash -ls 75&quot; knitr::opts_chunk$set(engine=&#39;sashtml&#39;, engine.path=saspath, engine.opts=sasopts, comment=&quot;&quot;) Common troubleshooting notes (Windows): - If the path contains spaces, it must still be a valid Windows path string. - If you have multiple SAS versions, confirm the correct sas.exe. - Some corporate environments block batch execution; SASmarkdown may fail unless permissions allow it. 9.2 Common statements Two practical rules matter most when writing SAS inside R Markdown: Do not use R syntax inside SAS chunks SAS chunks are sent to SAS verbatim. R objects like mydata, paste(), or %&gt;% have no meaning there. Data typically cannot be used across chunks unless you persist it Many SASmarkdown setups run each SAS chunk independently. If you create data test; ... run; in one chunk, the next chunk may not “see” work.test unless the session is preserved or you write it out to a permanent library. A safe habit is: - treat each SAS chunk as self-contained, or - write datasets to a permanent location/library and read them back later. 9.2.1 Read in data using informats (date data) 9.2.1.1 Example 1: a minimal DATA step with inline data This example uses cards; ... ; to create a dataset directly. It is the fastest way to create toy data for teaching, testing, or demonstrations. /*import and export raw data using infile*/ data test; /*infile &quot;c:/document/data.text&quot;;*/ input name $ height weight; length height 4; cards ; Daniel 173 150 ; run; proc print;run; Obs name height weight 1 Daniel 173 150 Key points: - input name $ height weight; declares name as character ($) and the others numeric. - length height 4; sets storage length (bytes) for numeric variables (mainly useful for character variables; numeric is typically 8 bytes by default in SAS). - proc print; confirms what was read. 9.2.1.2 Example 2: reading mixed date strings using inputn() Real-world data often stores dates as text in inconsistent formats. Here we read two strings: one containing the date text and one containing the informat name. Then we convert dynamically using inputn(). data have; input char_date : $20. fmt : $20.; /* set a new variable format*/ num_char=inputn(char_date,fmt); format num_char date9. ; cards; 12-08-2015 DDMMYY12. 12/8/2016 MMDDYY12. 05/25/2015 MMDDYY12. ; run; proc print;run; Obs char_date fmt num_char 1 12-08-2015 DDMMYY12. 12AUG2015 2 12/8/2016 MMDDYY12. 08DEC2016 3 05/25/2015 MMDDYY12. 25MAY2015 What’s happening: - char_date is a character date string. - fmt is a character variable holding an informat name (like DDMMYY12.). - inputn(char_date, fmt) converts using the informat stored in fmt. - format num_char date9.; controls display (e.g., 25MAY2015). This pattern is extremely useful in messy ingestion pipelines. 9.2.1.3 Example 3: date informat directly in INPUT If the date format is consistent, you can apply the informat in the input statement directly: data have; input char_date MMDDYY10. ; /* output format */ format char_date date9. ; cards; 12-08-2015 12/8/2016 05/25/2015 ; run; proc print;run; Obs char_date 1 08DEC2015 2 08DEC2016 3 25MAY2015 Here: - SAS reads a date value using MMDDYY10. regardless of separator style. - The stored value is numeric (SAS date = days since 01JAN1960), but displayed via date9.. 9.2.1.4 Example 4: setting both input and output format consistently Sometimes you want to keep the same “look” as the input: data have; /* set input and output format*/ input char_date MMDDYY10. ; format char_date MMDDYY10. ; cards; 12-08-2015 12/8/2016 05/25/2015 ; run; proc print;run; Obs char_date 1 12/08/2015 2 12/08/2016 3 05/25/2015 This helps when your deliverable expects a specific format (for example, when exporting to CSV to be read by another system). 9.2.2 Compute mean and frequency A typical exploratory workflow in SAS uses proc means, proc freq, and proc contents. proc means data=sashelp.class maxdec=5 ; run; proc freq data=sashelp.class ; table sex; run; proc contents data=sashelp.class varnum ; run; The MEANS Procedure Variable N Mean Std Dev Minimum Maximum ------------------------------------------------------------------------- Age 19 13.31579 1.49267 11.00000 16.00000 Height 19 62.33684 5.12708 51.30000 72.00000 Weight 19 100.02632 22.77393 50.50000 150.00000 ------------------------------------------------------------------------- The FREQ Procedure Cumulative Cumulative Sex Frequency Percent Frequency Percent -------------------------------------------------------- F 9 47.37 9 47.37 M 10 52.63 19 100.00 The CONTENTS Procedure Data Set Name SASHELP.CLASS Observations 19 Member Type DATA Variables 5 Engine V9 Indexes 0 Created 09/06/2017 21:55:32 Observation Length 40 Last Modified 09/06/2017 21:55:32 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Student Data Data Representation WINDOWS_64 Encoding us-ascii ASCII (ANSI) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 1632 Obs in First Data Page 19 Number of Data Set Repairs 0 ExtendObsCounter YES Filename C:\\Program Files\\SASHome\\SASFoundation\\9. 4\\core\\sashelp\\class.sas7bdat Release Created 9.0401M5 Host Created X64_SR12R2 Owner Name BUILTIN\\Administrators File Size 128KB File Size (bytes) 131072 Variables in Creation Order # Variable Type Len 1 Name Char 8 2 Sex Char 1 3 Age Num 8 4 Height Num 8 5 Weight Num 8 Interpretation: - proc means summarizes numeric variables (mean, std, min, max, etc.). - proc freq tabulates categorical variables. - proc contents varnum shows dataset metadata ordered by variable position. These are often the first three procs you run when you open a new dataset. 9.2.3 Sort a dataset Sorting is required for many BY-group operations (including merges and BY processing). proc sort data=sashelp.class Out= name ; by name; Run; proc print data= name (obs=10) ; run; Obs Name Sex Age Height Weight 1 Alfred M 14 69.0 112.5 2 Alice F 13 56.5 84.0 3 Barbara F 13 65.3 98.0 4 Carol F 14 62.8 102.5 5 Henry M 14 63.5 102.5 6 James M 12 57.3 83.0 7 Jane F 12 59.8 84.5 8 Janet F 15 62.5 112.5 9 Jeffrey M 13 62.5 84.0 10 John M 12 59.0 99.5 Note: - The output dataset name name is legal but can be confusing because name is also a variable in sashelp.class. - A safer convention is out=class_sorted, but we keep your code unchanged. 9.2.4 Transpose or reshape Reshaping appears frequently in reporting and longitudinal data workflows. 9.2.4.1 Wide to long proc transpose converts columns into rows. proc transpose data= sashelp.class out= Field; by name; Run; proc print data= field (obs=10) ; run; Obs Name _NAME_ COL1 1 Alfred Age 14.0 2 Alfred Height 69.0 3 Alfred Weight 112.5 4 Alice Age 13.0 5 Alice Height 56.5 6 Alice Weight 84.0 7 Barbara Age 13.0 8 Barbara Height 65.3 9 Barbara Weight 98.0 10 Carol Age 14.0 A critical detail: - BY processing requires sorted data by name (or SAS will warn/error depending on settings). 9.2.4.2 Long to wide A second transpose can rebuild a wide layout, typically using: - id to name new columns, - var to specify the values to spread. proc transpose data= sashelp.class out= Field; by name; Run; proc transpose data=Field out=Field_wide ; by name ; id _name_; var col1; run; proc print data= field_wide (obs=10) ; run; Obs Name _NAME_ Age Height Weight 1 Alfred COL1 14 69.0 112.5 2 Alice COL1 13 56.5 84.0 3 Barbara COL1 13 65.3 98.0 4 Carol COL1 14 62.8 102.5 5 Henry COL1 14 63.5 102.5 6 James COL1 12 57.3 83.0 7 Jane COL1 12 59.8 84.5 8 Janet COL1 15 62.5 112.5 9 Jeffrey COL1 13 62.5 84.0 10 John COL1 12 59.0 99.5 Conceptually: - The first transpose creates a “longer” dataset with _name_ and col1. - The second transpose pivots _name_ levels into columns. 9.2.5 Conditional statements Conditional logic is fundamental in data steps: creating flags, categories, derived variables, and messages. 9.2.5.1 Simple IF/ELSE DATA ab; set sashelp.class; IF sex=&quot;F&quot; then message=&#39;A is greater&#39;; Else message=&#39;B is greater&#39;; Run; proc print data=ab (obs=10); run; Obs Name Sex Age Height Weight message 1 Alfred M 14 69.0 112.5 B is greater 2 Alice F 13 56.5 84.0 A is greater 3 Barbara F 13 65.3 98.0 A is greater 4 Carol F 14 62.8 102.5 A is greater 5 Henry M 14 63.5 102.5 B is greater 6 James M 12 57.3 83.0 B is greater 7 Jane F 12 59.8 84.5 A is greater 8 Janet F 15 62.5 112.5 A is greater 9 Jeffrey M 13 62.5 84.0 B is greater 10 John M 12 59.0 99.5 B is greater Even though the message text is arbitrary, the example demonstrates: - IF/ELSE structure, - character literals in quotes, - adding derived variables in a data step. 9.2.5.2 Multiple conditions (ELSE IF) Data class; set sashelp.class; if weight &gt;=100 then weight_cat =3; else if weight &gt;=90 and weight &lt;100 then weight_cat =2 ; else weight_cat =1; attrib weight_cat label = &quot;weight category, 1=Inad, 2=Adeq, 3=Exces&quot;; run; proc print data=class (obs=3); run; weight_ Obs Name Sex Age Height Weight cat 1 Alfred M 14 69.0 112.5 3 2 Alice F 13 56.5 84.0 1 3 Barbara F 13 65.3 98.0 2 This is a standard pattern for category creation: - ordered thresholds, - mutually exclusive categories, - labeling for readable tables. 9.2.5.3 index() for pattern detection data test; set sashelp.class; length gender $10 ; if index(sex,&quot;M&quot;)&gt;0 THEN gender=&quot;male&quot;; if index(sex,&quot;F&quot;)&gt;0 THEN gender=&quot;female&quot;; RUN; proc print data=test (obs=5); run; Obs Name Sex Age Height Weight gender 1 Alfred M 14 69.0 112.5 male 2 Alice F 13 56.5 84.0 female 3 Barbara F 13 65.3 98.0 female 4 Carol F 14 62.8 102.5 female 5 Henry M 14 63.5 102.5 male Note: - index() returns the position of a substring (0 if not found). - Because you used two separate IF statements (not IF/ELSE), the logic is still fine here because sex can’t be both “M” and “F”. 9.2.6 LIKE operation to select rows containing a pattern SAS where supports SQL-like pattern matching using like with % wildcards. proc print data =sashelp.class; where upcase(name) like &#39;%A%&#39;; run; proc print data =sashelp.class; where (name) like &#39;%A%&#39;; run; Obs Name Sex Age Height Weight 1 Alfred M 14 69.0 112.5 2 Alice F 13 56.5 84.0 3 Barbara F 13 65.3 98.0 4 Carol F 14 62.8 102.5 6 James M 12 57.3 83.0 7 Jane F 12 59.8 84.5 8 Janet F 15 62.5 112.5 14 Mary F 15 66.5 112.0 17 Ronald M 15 67.0 133.0 18 Thomas M 11 57.5 85.0 19 William M 15 66.5 112.0 Obs Name Sex Age Height Weight 1 Alfred M 14 69.0 112.5 2 Alice F 13 56.5 84.0 The first query is case-insensitive by forcing uppercase. The second is case-sensitive, depending on your SAS session settings. 9.2.7 Change format of a variable User-defined formats map raw values into labeled categories (often for tables). proc format; value AGEnew 11 = &#39;1: NEW&#39; 12 = &#39;2: NEW&#39; 13 = &#39;3: NEW&#39; 14 = &#39;4: NEW&#39; 15 = &#39;5: NEW&#39; 16 = &#39;6: NEW&#39; ; run; DATA ab; set sashelp.class; Format AGE AGEnew.; Run; proc freq data=ab; table AGE; run; The FREQ Procedure Cumulative Cumulative Age Frequency Percent Frequency Percent ----------------------------------------------------------- 1: NEW 2 10.53 2 10.53 2: NEW 5 26.32 7 36.84 3: NEW 3 15.79 10 52.63 4: NEW 4 21.05 14 73.68 5: NEW 4 21.05 18 94.74 6: NEW 1 5.26 19 100.00 This workflow is typical: 1) define format in proc format, 2) apply format in a data step, 3) display with proc freq. 9.2.8 Basic operations SAS provides many numeric functions directly in the data step. Data Mathdata; A= 10.12345; B=20; C= mean (a,b); D= Min(a,b); E= Max(a,b); F = log(a); G= round(a,0.02); H= floor(a ); Run; proc print data=Mathdata; run; Data mathdata; Set sashelp.Iris; Sum = sum (of SepalLength, SepalWIDTH); Diff = SepalLength- SepalWIDTH; Mult = SepalLength* SepalWIDTH; Div= SepalLength/ SepalWIDTH; Run; proc print data=mathdata (obs=10); run; Obs A B C D E F G H 1 10.1235 20 15.0617 10.1235 20 2.31485 10.12 10 Sepal Sepal Petal Petal Obs Species Length Width Length Width Sum Diff Mult Div 1 Setosa 50 33 14 2 83 17 1650 1.51515 2 Setosa 46 34 14 3 80 12 1564 1.35294 3 Setosa 46 36 10 2 82 10 1656 1.27778 4 Setosa 51 33 17 5 84 18 1683 1.54545 5 Setosa 55 35 13 2 90 20 1925 1.57143 6 Setosa 48 31 16 2 79 17 1488 1.54839 7 Setosa 52 34 14 2 86 18 1768 1.52941 8 Setosa 49 36 14 1 85 13 1764 1.36111 9 Setosa 44 32 13 2 76 12 1408 1.37500 10 Setosa 50 35 16 6 85 15 1750 1.42857 Two common teaching points: - SAS functions like mean() handle missing values differently than plain arithmetic. - sum(of ...) is a very SAS-specific idiom and scales well when many variables are involved. 9.2.9 Rename variables 9.2.9.1 Rename one variable Data AB; set sashelp.class; Rename AGE= AGENEW; Run; proc print data=AB; run; Obs Name Sex AGENEW Height Weight 1 Alfred M 14 69.0 112.5 2 Alice F 13 56.5 84.0 3 Barbara F 13 65.3 98.0 4 Carol F 14 62.8 102.5 5 Henry M 14 63.5 102.5 6 James M 12 57.3 83.0 7 Jane F 12 59.8 84.5 8 Janet F 15 62.5 112.5 9 Jeffrey M 13 62.5 84.0 10 John M 12 59.0 99.5 11 Joyce F 11 51.3 50.5 12 Judy F 14 64.3 90.0 13 Louise F 12 56.3 77.0 14 Mary F 15 66.5 112.0 15 Philip M 16 72.0 150.0 16 Robert M 12 64.8 128.0 17 Ronald M 15 67.0 133.0 18 Thomas M 11 57.5 85.0 19 William M 15 66.5 112.0 9.2.9.2 Rename multiple variables programmatically This advanced example shows how to build a rename list from dictionary.columns and apply it. 2 /*rename*/ 3 data AAA; 4 set sashelp.class; 5 run; NOTE: There were 19 observations read from the data set SASHELP.CLASS. NOTE: The data set WORK.AAA has 19 observations and 5 variables. NOTE: DATA statement used (Total process time): real time 0.03 seconds cpu time 0.00 seconds 6 7 /*create a list*/ 8 proc sql noprint; 9 select cats(name,&#39;=&#39;,name,&#39;_new&#39;) 10 into : lis 11 separated by &#39; &#39; 12 from dictionary.columns 13 where libname = &#39;WORK&#39; and memname = &#39;AAA&#39;; 14 quit; NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 15 %put &amp;lis; Name=Name_new Sex=Sex_new Age=Age_new Height=Height_new Weight=Weight_new 16 17 proc datasets library = work nolist; 18 modify AAA; 19 rename &amp;lis; NOTE: Renaming variable Name to Name_new. NOTE: Renaming variable Sex to Sex_new. NOTE: Renaming variable Age to Age_new. NOTE: Renaming variable Height to Height_new. NOTE: Renaming variable Weight to Weight_new. 20 quit; NOTE: MODIFY was successful for WORK.AAA.DATA. NOTE: PROCEDURE DATASETS used (Total process time): real time 0.03 seconds cpu time 0.00 seconds 21 22 proc print data=aaa (obs=2); 23 run; NOTE: There were 2 observations read from the data set WORK.AAA. NOTE: The PROCEDURE PRINT printed page 1. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 24 25 /*function to deal with text*/ 26 %scan(&amp;lis,2); NOTE: Line generated by the macro function &quot;SCAN&quot;. 26 Sex=Sex_new ___ 180 ERROR 180-322: Statement is not valid or it is used out of proper order. 27 /*#scan variable*/ 28 %substr(NAME, %length(NAME),1); NOTE: Line generated by the macro function &quot;SUBSTR&quot;. 28 E _ 180 ERROR 180-322: Statement is not valid or it is used out of proper order. 29 /*#extract character*/ 30 %substr(NAME, 2,1); NOTE: Line generated by the macro function &quot;SUBSTR&quot;. 30 A _ 180 ERROR 180-322: Statement is not valid or it is used out of proper order. 31 %index(&amp;lis,&quot;Age=Age_new&quot;) ; NOTE: Line generated by the macro function &quot;INDEX&quot;. 31 0 _ 180 ERROR 180-322: Statement is not valid or it is used out of proper order. 32 /*#identify this exists */ ERROR: Errors printed on page 1. Height_ Weight_ Obs Name_new Sex_new Age_new new new 1 Alfred M 14 69.0 112.5 2 Alice F 13 56.5 84.0 Why this matters: - In real clinical trial pipelines, you often receive datasets with inconsistent naming conventions. - Generating rename rules programmatically reduces manual error and improves reproducibility. 9.2.10 Text manipulation 9.2.10.1 Extract text from a character value Data Mathdata; Text = &quot;Hello World&quot;; Text1= substr(Text, 6, 2); Run; proc print data=mathdata ; run; Obs Text Text1 1 Hello World W substr() is one of the most-used string functions in clinical reporting. 9.2.10.2 Convert character to numeric and reverse Character → numeric: Data ABC; set sashelp.class; agenew= input (age, best.); Run; proc print data=abc ; run; Obs Name Sex Age Height Weight agenew 1 Alfred M 14 69.0 112.5 14 2 Alice F 13 56.5 84.0 13 3 Barbara F 13 65.3 98.0 13 4 Carol F 14 62.8 102.5 14 5 Henry M 14 63.5 102.5 14 6 James M 12 57.3 83.0 12 7 Jane F 12 59.8 84.5 12 8 Janet F 15 62.5 112.5 15 9 Jeffrey M 13 62.5 84.0 13 10 John M 12 59.0 99.5 12 11 Joyce F 11 51.3 50.5 11 12 Judy F 14 64.3 90.0 14 13 Louise F 12 56.3 77.0 12 14 Mary F 15 66.5 112.0 15 15 Philip M 16 72.0 150.0 16 16 Robert M 12 64.8 128.0 12 17 Ronald M 15 67.0 133.0 15 18 Thomas M 11 57.5 85.0 11 19 William M 15 66.5 112.0 15 Numeric → character: Data ABC; set sashelp.class; agenew= put (age, best.); Run; proc contents data=abc ; run; The CONTENTS Procedure Data Set Name WORK.ABC Observations 19 Member Type DATA Variables 6 Engine V9 Indexes 0 Created 02/20/2026 16:44:41 Observation Length 48 Last Modified 02/20/2026 16:44:41 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation WINDOWS_64 Encoding wlatin1 Western (Windows) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 1361 Obs in First Data Page 19 Number of Data Set Repairs 0 ExtendObsCounter YES Filename C:\\Users\\hed2\\AppData\\Local\\Temp\\SAS Temporary Files\\_TD19308_HDW02234493_\\abc.sas7bdat Release Created 9.0401M5 Host Created X64_10PRO Owner Name NIH\\hed2 File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 3 Age Num 8 4 Height Num 8 1 Name Char 8 2 Sex Char 1 5 Weight Num 8 6 agenew Char 12 In practice: - input() reads a character string into numeric. - put() formats numeric into character. 9.2.10.3 Change the length of a variable Data ABC; set sashelp.class; Length agenew $10.; Label agenew=“New age”; Run; proc contents data=abc ; run; The CONTENTS Procedure Data Set Name WORK.ABC Observations 19 Member Type DATA Variables 6 Engine V9 Indexes 0 Created 02/20/2026 16:44:42 Observation Length 48 Last Modified 02/20/2026 16:44:42 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation WINDOWS_64 Encoding wlatin1 Western (Windows) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 1361 Obs in First Data Page 19 Number of Data Set Repairs 0 ExtendObsCounter YES Filename C:\\Users\\hed2\\AppData\\Local\\Temp\\SAS Temporary Files\\_TD24572_HDW02234493_\\abc.sas7bdat Release Created 9.0401M5 Host Created X64_10PRO Owner Name NIH\\hed2 File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len Label 3 Age Num 8 4 Height Num 8 1 Name Char 8 2 Sex Char 1 5 Weight Num 8 6 agenew Char 10 âNew ageâ A key habit: define length before assignment when creating new character variables to avoid truncation. 9.2.11 Create a report proc report is a flexible reporting tool and can replace many proc print / proc tabulate workflows. proc report data=sashelp.class; Column age; Define age / display; Run; Age 14 13 13 14 14 12 12 15 13 12 11 14 12 15 16 12 15 11 15 Even this minimal example shows the structure: - column defines the layout, - define controls how variables appear. 9.2.12 Random variables SAS provides pseudo-random generators via rand(). data ab; set sashelp.class; num=rand(&quot;normal&quot;); run; proc print data=ab (obs=10); run; Obs Name Sex Age Height Weight num 1 Alfred M 14 69.0 112.5 -0.44832 2 Alice F 13 56.5 84.0 1.55149 3 Barbara F 13 65.3 98.0 -0.95175 4 Carol F 14 62.8 102.5 -0.66781 5 Henry M 14 63.5 102.5 0.25197 6 James M 12 57.3 83.0 -1.64294 7 Jane F 12 59.8 84.5 0.96083 8 Janet F 15 62.5 112.5 -0.45714 9 Jeffrey M 13 62.5 84.0 -0.63351 10 John M 12 59.0 99.5 -1.00366 This is useful for simulation, multiple imputation diagnostics, and resampling. 9.2.13 Combine two texts, compress spaces, locate a substring, change case These are common in cleaning messy character fields. Combine: Data Mathdata; Text = &quot;Hello&quot;; Text1= &quot;World&quot;; Text2= text || &quot; &quot; ||text1; Run; proc print data=Mathdata ; run; Obs Text Text1 Text2 1 Hello World Hello World Compress: Data Mathdata; Text = &quot;Hello World &quot;; Text1= trim(text); Text2= compress(text); Run; proc print data=Mathdata ; run; Obs Text Text1 Text2 1 Hello World Hello World HelloWorld Index: Data Mathdata; Text = &quot;Hello World&quot;; indextext= index(text, &quot;or&quot;); Run; proc print data=Mathdata ; run; Obs Text indextext 1 Hello World 8 Case functions: Data Mathdata; Text = &quot;Hello World&quot;; upcase= upcase(text ); lowcase= lowcase(text ); propcase= propcase(text ); Run; proc print data=Mathdata ; run; Obs Text upcase lowcase propcase 1 Hello World HELLO WORLD hello world Hello World 9.2.14 Deduplication Deduplication is a frequent preprocessing step, especially when identifying unique subjects, visits, or records. 9.2.14.1 Method 1: nodupkeys proc sort data = sashelp.class out = dedup nodupkeys; by height; run; proc print data= dedup; run; Obs Name Sex Age Height Weight 1 Joyce F 11 51.3 50.5 2 Louise F 12 56.3 77.0 3 Alice F 13 56.5 84.0 4 James M 12 57.3 83.0 5 Thomas M 11 57.5 85.0 6 John M 12 59.0 99.5 7 Jane F 12 59.8 84.5 8 Janet F 15 62.5 112.5 9 Carol F 14 62.8 102.5 10 Henry M 14 63.5 102.5 11 Judy F 14 64.3 90.0 12 Robert M 12 64.8 128.0 13 Barbara F 13 65.3 98.0 14 Mary F 15 66.5 112.0 15 Ronald M 15 67.0 133.0 16 Alfred M 14 69.0 112.5 17 Philip M 16 72.0 150.0 This keeps the first record for each unique height. 9.2.14.2 Method 2: BY-group with first. / last. proc sort data=sashelp.class out=data_rank; by height ; run; data outdata dropdata; set data_rank; by height ; if last.height then output outdata; else output dropdata; run; proc print data=dropdata;run; Obs Name Sex Age Height Weight 1 Janet F 15 62.5 112.5 2 Mary F 15 66.5 112.0 This version keeps the last record per height. Notice that the output depends on the current sort order. 9.2.15 Select a subset of rows data where; set sashelp.class; where sex ne &quot;F&quot;; /*if */ run; proc print data= where; run; Obs Name Sex Age Height Weight 1 Alfred M 14 69.0 112.5 2 Henry M 14 63.5 102.5 3 James M 12 57.3 83.0 4 Jeffrey M 13 62.5 84.0 5 John M 12 59.0 99.5 6 Philip M 16 72.0 150.0 7 Robert M 12 64.8 128.0 8 Ronald M 15 67.0 133.0 9 Thomas M 11 57.5 85.0 10 William M 15 66.5 112.0 The where statement filters rows during reading, which is typically more efficient than filtering after reading. 9.2.16 Create macros with DO loops Macros provide automation, especially when the same PROC is repeated across multiple variables. 9.2.16.1 A macro to calculate descriptive stats across multiple variables This example uses: - proc means to compute summaries by sex, - ODS output to capture results, - a macro loop to run across variables. 2 %macro means(var_avg) ; 3 4 /*calculate means*/ 5 proc means data=sashelp.class StackODSOutput n mean std min p5 5 ! p95 max nmiss; 6 var &amp;var_avg; 7 class sex; 8 ods output summary=result2; 9 run; 10 11 /*append then output*/ 12 data masterresult2; * combine results; 13 set masterresult2 result2; 14 run; 15 16 %mend means; 17 18 /*use macro to merge all descriptive stats */ 19 data masterresult2 ; 20 set _null_; 21 run; NOTE: The data set WORK.MASTERRESULT2 has 0 observations and 0 variables. NOTE: DATA statement used (Total process time): real time 0.02 seconds cpu time 0.00 seconds 22 23 %let vars= 24 age 25 height 26 weight 27 ; 28 29 %macro model ; 30 %do i=1 %to %sysfunc(countw(&amp;vars)); 31 32 %let x=%scan(&amp;vars,&amp;i); 33 %means( &amp;x ) 34 35 %end; 36 %mend model; 37 38 %model; NOTE: The data set WORK.RESULT2 has 2 observations and 12 variables. NOTE: There were 19 observations read from the data set SASHELP.CLASS. NOTE: The PROCEDURE MEANS printed page 1. NOTE: PROCEDURE MEANS used (Total process time): real time 0.07 seconds cpu time 0.00 seconds NOTE: There were 0 observations read from the data set WORK.MASTERRESULT2. NOTE: There were 2 observations read from the data set WORK.RESULT2. NOTE: The data set WORK.MASTERRESULT2 has 2 observations and 12 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds NOTE: The data set WORK.RESULT2 has 2 observations and 12 variables. NOTE: There were 19 observations read from the data set SASHELP.CLASS. NOTE: The PROCEDURE MEANS printed page 2. NOTE: PROCEDURE MEANS used (Total process time): real time 0.03 seconds cpu time 0.00 seconds WARNING: Multiple lengths were specified for the variable Variable by input data set(s). This can cause truncation of data. NOTE: There were 2 observations read from the data set WORK.MASTERRESULT2. NOTE: There were 2 observations read from the data set WORK.RESULT2. NOTE: The data set WORK.MASTERRESULT2 has 4 observations and 12 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.00 seconds NOTE: The data set WORK.RESULT2 has 2 observations and 12 variables. NOTE: There were 19 observations read from the data set SASHELP.CLASS. NOTE: The PROCEDURE MEANS printed page 3. NOTE: PROCEDURE MEANS used (Total process time): real time 0.01 seconds cpu time 0.00 seconds WARNING: Multiple lengths were specified for the variable Variable by input data set(s). This can cause truncation of data. NOTE: There were 4 observations read from the data set WORK.MASTERRESULT2. NOTE: There were 2 observations read from the data set WORK.RESULT2. NOTE: The data set WORK.MASTERRESULT2 has 6 observations and 12 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 39 40 proc print data= masterresult2; 41 run; NOTE: There were 6 observations read from the data set WORK.MASTERRESULT2. NOTE: The PROCEDURE PRINT printed page 4. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 42 The MEANS Procedure Analysis Variable : Age N Sex Obs N Mean Std Dev Minimum 5th Pctl -------------------------------------------------------------------------- F 9 9 13.222222 1.394433 11.000000 11.000000 M 10 10 13.400000 1.646545 11.000000 11.000000 -------------------------------------------------------------------------- Analysis Variable : Age N N Sex Obs 95th Pctl Maximum Miss ------------------------------------------------ F 9 15.000000 15.000000 0 M 10 16.000000 16.000000 0 ------------------------------------------------ The MEANS Procedure Analysis Variable : Height N Sex Obs N Mean Std Dev Minimum 5th Pctl -------------------------------------------------------------------------- F 9 9 60.588889 5.018328 51.300000 51.300000 M 10 10 63.910000 4.937937 57.300000 57.300000 -------------------------------------------------------------------------- Analysis Variable : Height N N Sex Obs 95th Pctl Maximum Miss ------------------------------------------------ F 9 66.500000 66.500000 0 M 10 72.000000 72.000000 0 ------------------------------------------------ The MEANS Procedure Analysis Variable : Weight N Sex Obs N Mean Std Dev Minimum 5th Pctl -------------------------------------------------------------------------- F 9 9 90.111111 19.383914 50.500000 50.500000 M 10 10 108.950000 22.727186 83.000000 83.000000 -------------------------------------------------------------------------- Analysis Variable : Weight N N Sex Obs 95th Pctl Maximum Miss ------------------------------------------------ F 9 112.500000 112.500000 0 M 10 150.000000 150.000000 0 ------------------------------------------------ Obs Sex NObs _control_ Variable N Mean StdDev 1 F 9 Age 9 13.222222 1.394433 2 M 10 1 Age 10 13.400000 1.646545 3 F 9 Hei 9 60.588889 5.018328 4 M 10 1 Hei 10 63.910000 4.937937 5 F 9 Wei 9 90.111111 19.383914 6 M 10 1 Wei 10 108.950000 22.727186 Obs Min P5 P95 Max NMiss 1 11.000000 11.000000 15.000000 15.000000 0 2 11.000000 11.000000 16.000000 16.000000 0 3 51.300000 51.300000 66.500000 66.500000 0 4 57.300000 57.300000 72.000000 72.000000 0 5 50.500000 50.500000 112.500000 112.500000 0 6 83.000000 83.000000 150.000000 150.000000 0 Practical interpretation: - &amp;vars defines the batch of variables. - %do iterates through them. - Each iteration runs proc means and appends results into a single dataset. This is exactly the type of pattern used in clinical reporting pipelines when building shells. 9.2.17 Output intermediate tables with ODS ODS is the bridge from SAS procedures to reusable datasets. 9.2.17.1 Discover table names: ods trace ods trace on; proc freq data=sashelp.class ; table sex; run; ods trace off; The FREQ Procedure Cumulative Cumulative Sex Frequency Percent Frequency Percent -------------------------------------------------------- F 9 47.37 9 47.37 M 10 52.63 19 100.00 ods trace tells you which internal table objects SAS produces. 9.2.17.2 Capture a specific output table ods listing close; ods trace on; ods output ParameterEstimates= ParameterEstimates ; proc glm data=sashelp.class; model height=age; run; ods trace off; ods listing; proc print data=ParameterEstimates; run; Obs Dependent Parameter Estimate StdErr tValue Probt 1 Height Intercept 25.22388451 6.52168912 3.87 0.0012 2 Height Age 2.78713911 0.48688163 5.72 &lt;.0001 This pattern is essential when you want to: - compute results in SAS, - feed them into later steps, - export them, or - compare them against R outputs. 9.2.18 Create sequence numbers Sequence variables are common in SDTM/ADaM creation and tracking. 9.2.18.1 Grouped sequence numbers /* grouped*/ proc sort data=sashelp.class out=class ; by name; run; data temp; set class; by name; count + 1; if first.name then count = 1; run; proc print data=temp (obs=3);run; proc sort data=class out=class2 ; by height; run; data class3; set class2; by height; retain count; if first.height then count=1; else count=count+1; run; proc print data=class3 (obs=3);run; /*nogrouped*/ data new; set class; seqno = _n_; run; proc print data=new (obs=3);run; Obs Name Sex Age Height Weight count 1 Alfred M 14 69.0 112.5 1 2 Alice F 13 56.5 84.0 1 3 Barbara F 13 65.3 98.0 1 Obs Name Sex Age Height Weight count 1 Joyce F 11 51.3 50.5 1 2 Louise F 12 56.3 77.0 1 3 Alice F 13 56.5 84.0 1 Obs Name Sex Age Height Weight seqno 1 Alfred M 14 69.0 112.5 1 2 Alice F 13 56.5 84.0 2 3 Barbara F 13 65.3 98.0 3 Key idea: - _n_ is the observation index within the data step. - first. and last. require sorted data and allow per-group logic. 9.2.19 Merge datasets SAS has two main approaches: - SQL joins (proc sql) — flexible and readable, - DATA step merge — fast and common in production, but requires sorting and careful logic. 9.2.19.1 Left and right join 2 data class2; 3 input name $ score; 4 datalines; NOTE: The data set WORK.CLASS2 has 3 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.04 seconds cpu time 0.00 seconds 8 ; 9 run; 10 11 proc sql; 12 create table class_c as 13 select * 14 from sashelp.class as a 15 left join class2 as b 16 on a.name = b.name; WARNING: Variable name already exists on file WORK.CLASS_C. NOTE: Table WORK.CLASS_C created, with 19 rows and 6 columns. 17 quit; NOTE: PROCEDURE SQL used (Total process time): real time 0.02 seconds cpu time 0.00 seconds 18 19 proc print data=class_c; 20 run; NOTE: There were 19 observations read from the data set WORK.CLASS_C. NOTE: The PROCEDURE PRINT printed page 1. NOTE: PROCEDURE PRINT used (Total process time): real time 0.04 seconds cpu time 0.00 seconds 21 22 proc sql; 23 create table class_d as 24 select * 25 from class2 as a 26 right join sashelp.class as b 27 on a.name = b.name; WARNING: Variable Name already exists on file WORK.CLASS_D. NOTE: Table WORK.CLASS_D created, with 19 rows and 6 columns. 28 quit; NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.00 seconds 29 30 proc print data=class_d; 31 run; NOTE: There were 19 observations read from the data set WORK.CLASS_D. NOTE: The PROCEDURE PRINT printed page 2. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 32 Obs Name Sex Age Height Weight score 1 Alfred M 14 69.0 112.5 85 2 Alice F 13 56.5 84.0 89 3 Barbara F 13 65.3 98.0 . 4 Carol F 14 62.8 102.5 . 5 Henry M 14 63.5 102.5 . 6 James M 12 57.3 83.0 . 7 Jane F 12 59.8 84.5 . 8 Janet F 15 62.5 112.5 . 9 Jeffrey M 13 62.5 84.0 . 10 John M 12 59.0 99.5 . 11 Joyce F 11 51.3 50.5 . 12 Judy F 14 64.3 90.0 . 13 Louise F 12 56.3 77.0 . 14 Mary F 15 66.5 112.0 . 15 Philip M 16 72.0 150.0 . 16 Robert M 12 64.8 128.0 . 17 Ronald M 15 67.0 133.0 . 18 Thomas M 11 57.5 85.0 . 19 William M 15 66.5 112.0 . Obs name score Sex Age Height Weight 1 Alfred 85 M 14 69.0 112.5 2 Alice 89 F 13 56.5 84.0 3 . F 13 65.3 98.0 4 . F 14 62.8 102.5 5 . M 14 63.5 102.5 6 . M 12 57.3 83.0 7 . F 12 59.8 84.5 8 . F 15 62.5 112.5 9 . M 13 62.5 84.0 10 . M 12 59.0 99.5 11 . F 11 51.3 50.5 12 . F 14 64.3 90.0 13 . F 12 56.3 77.0 14 . F 15 66.5 112.0 15 . M 16 72.0 150.0 16 . M 12 64.8 128.0 17 . M 15 67.0 133.0 18 . M 11 57.5 85.0 19 . M 15 66.5 112.0 9.2.19.2 Full join 2 data class2; 3 input name $ score; 4 datalines; NOTE: The data set WORK.CLASS2 has 3 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.03 seconds cpu time 0.00 seconds 8 ; 9 run; 10 11 proc sql; 12 create table class_e as 13 select * 14 from sashelp.class as a 15 full join class2 as b 16 on a.name = b.name; WARNING: Variable name already exists on file WORK.CLASS_E. NOTE: Table WORK.CLASS_E created, with 20 rows and 6 columns. 17 quit; NOTE: PROCEDURE SQL used (Total process time): real time 0.04 seconds cpu time 0.01 seconds 18 19 proc print data=class_e; 20 run; NOTE: There were 20 observations read from the data set WORK.CLASS_E. NOTE: The PROCEDURE PRINT printed page 1. NOTE: PROCEDURE PRINT used (Total process time): real time 0.06 seconds cpu time 0.06 seconds 21 Obs Name Sex Age Height Weight score 1 Alfred M 14 69.0 112.5 85 2 Alice F 13 56.5 84.0 89 3 Barbara F 13 65.3 98.0 . 4 Carol F 14 62.8 102.5 . 5 . . . 99 6 Henry M 14 63.5 102.5 . 7 James M 12 57.3 83.0 . 8 Jane F 12 59.8 84.5 . 9 Janet F 15 62.5 112.5 . 10 Jeffrey M 13 62.5 84.0 . 11 John M 12 59.0 99.5 . 12 Joyce F 11 51.3 50.5 . 13 Judy F 14 64.3 90.0 . 14 Louise F 12 56.3 77.0 . 15 Mary F 15 66.5 112.0 . 16 Philip M 16 72.0 150.0 . 17 Robert M 12 64.8 128.0 . 18 Ronald M 15 67.0 133.0 . 19 Thomas M 11 57.5 85.0 . 20 William M 15 66.5 112.0 . 9.2.19.3 DATA step merge (append-style merge by key) data class2; input name $ score; datalines; Alfred 85 Alice 89 Daniel 99 ; run; data class_f; merge sashelp.class class2; by name; /*if A and B; */ run; proc print data=class_f; run; Obs Name Sex Age Height Weight score 1 Alfred M 14 69.0 112.5 85 2 Alice F 13 56.5 84.0 89 3 Barbara F 13 65.3 98.0 . 4 Carol F 14 62.8 102.5 . 5 Daniel . . . 99 6 Henry M 14 63.5 102.5 . 7 James M 12 57.3 83.0 . 8 Jane F 12 59.8 84.5 . 9 Janet F 15 62.5 112.5 . 10 Jeffrey M 13 62.5 84.0 . 11 John M 12 59.0 99.5 . 12 Joyce F 11 51.3 50.5 . 13 Judy F 14 64.3 90.0 . 14 Louise F 12 56.3 77.0 . 15 Mary F 15 66.5 112.0 . 16 Philip M 16 72.0 150.0 . 17 Robert M 12 64.8 128.0 . 18 Ronald M 15 67.0 133.0 . 19 Thomas M 11 57.5 85.0 . 20 William M 15 66.5 112.0 . 9.2.19.4 Inner join 2 data class2; 3 input name $ score; 4 datalines; NOTE: The data set WORK.CLASS2 has 3 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.03 seconds cpu time 0.06 seconds 8 ; 9 run; 10 11 proc sql; 12 create table class_e as 13 select * 14 from sashelp.class as a 15 inner join class2 as b 16 on a.name = b.name; WARNING: Variable name already exists on file WORK.CLASS_E. NOTE: Table WORK.CLASS_E created, with 2 rows and 6 columns. 17 quit; NOTE: PROCEDURE SQL used (Total process time): real time 0.06 seconds cpu time 0.01 seconds 18 19 proc print data=class_e; 20 run; NOTE: There were 2 observations read from the data set WORK.CLASS_E. NOTE: The PROCEDURE PRINT printed page 1. NOTE: PROCEDURE PRINT used (Total process time): real time 0.05 seconds cpu time 0.04 seconds 21 Obs Name Sex Age Height Weight score 1 Alfred M 14 69.0 112.5 85 2 Alice F 13 56.5 84.0 89 9.2.19.5 Minus join (anti-join) 2 data class2; 3 input name $ score; 4 datalines; NOTE: The data set WORK.CLASS2 has 3 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.03 seconds cpu time 0.00 seconds 8 ; 9 run; 10 11 proc sql; 12 create table class_e as 13 select * 14 from sashelp.class as a 15 left join class2 as b 16 on a.name = b.name 17 where b.name is NULL; WARNING: Variable name already exists on file WORK.CLASS_E. NOTE: Table WORK.CLASS_E created, with 17 rows and 6 columns. 18 quit; NOTE: PROCEDURE SQL used (Total process time): real time 0.03 seconds cpu time 0.03 seconds 19 20 proc print data=class_e; 21 run; NOTE: There were 17 observations read from the data set WORK.CLASS_E. NOTE: The PROCEDURE PRINT printed page 1. NOTE: PROCEDURE PRINT used (Total process time): real time 0.03 seconds cpu time 0.00 seconds Obs Name Sex Age Height Weight score 1 Barbara F 13 65.3 98.0 . 2 Carol F 14 62.8 102.5 . 3 Henry M 14 63.5 102.5 . 4 James M 12 57.3 83.0 . 5 Jane F 12 59.8 84.5 . 6 Janet F 15 62.5 112.5 . 7 Jeffrey M 13 62.5 84.0 . 8 John M 12 59.0 99.5 . 9 Joyce F 11 51.3 50.5 . 10 Judy F 14 64.3 90.0 . 11 Louise F 12 56.3 77.0 . 12 Mary F 15 66.5 112.0 . 13 Philip M 16 72.0 150.0 . 14 Robert M 12 64.8 128.0 . 15 Ronald M 15 67.0 133.0 . 16 Thomas M 11 57.5 85.0 . 17 William M 15 66.5 112.0 . This is the SAS SQL equivalent of “keep rows in A that do not match B”. 9.2.20 Create Table 1 In clinical reporting, “Table 1” typically summarizes baseline demographics and characteristics by treatment group. Your code calls external macro files via %include. This is a realistic production pattern: statistical programming teams maintain shared macro libraries, and analyses import them. 2 *** Load utility macros; 3 %include 3 ! &quot;C:\\Users\\hed2\\Downloads\\mybook2\\mybook2\\sasmacro\\create_table1. 3 ! sas&quot;; WARNING: Physical file does not exist, C:\\Users\\hed2\\Downloads\\mybook2\\mybook2\\sasmacro\\create_table1.sas . ERROR: Cannot open %INCLUDE file C:\\Users\\hed2\\Downloads\\mybook2\\mybook2\\sasmacro\\create_table1.sas. 4 5 *** Specify input and output data sets, and the column 5 ! variable.; 6 %let INPUT_DATA = sashelp.class; 7 %let OUTPUT_DATA = Table1 ; 8 %let COLVAR = sex; 9 10 /*chort*/ 11 *** %AddText(text=Height); 12 *** %CategoricalRowVar2(rowVar=); 13 %ContinuousRowVar2(rowVar=height ); _ 180 WARNING: Apparent invocation of macro CONTINUOUSROWVAR2 not resolved. ERROR 180-322: Statement is not valid or it is used out of proper order. WARNING: Apparent invocation of macro CONTINUOUSROWVAR2 not resolved. 14 *** %AddText(text=); 15 16 %ContinuousRowVar2(rowVar=weight ); _ 180 ERROR 180-322: Statement is not valid or it is used out of proper order. WARNING: Apparent invocation of macro CONTINUOUSROWVAR2 not resolved. 17 %ContinuousRowVar2(rowVar=Age ); _ 180 ERROR 180-322: Statement is not valid or it is used out of proper order. 18 19 proc print data= table1; ERROR: File WORK.TABLE1.DATA does not exist. 20 run; NOTE: The SAS System stopped processing this step because of errors. NOTE: SAS set option OBS=0 and will continue to check statements. This might cause NOTE: No observations in data set. NOTE: PROCEDURE PRINT used (Total process time): real time 0.04 seconds cpu time 0.00 seconds 21 22 /* Export Table 1 as a CSV file*/ NOTE: PROCEDURE EXPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 23 proc export data=Table1 replace label 24 outfile=&quot;C:\\Users\\hed2\\Downloads\\mybook2\\mybook2\\sasmacro\\ta 24 ! ble1.csv&quot; 25 dbms=csv; 26 run; 27 ERROR: Errors printed on page 1. Interpretation: - Macro calls like %ContinuousRowVar2(...) likely append rows into the Table 1 dataset. - proc export creates a portable deliverable (CSV) for review, QA, or publication workflows. 9.3 Using macros Macros are the “automation layer” of SAS. They help you: - avoid copy/paste, - standardize reports, - scale analyses to many endpoints/variables. 9.3.0.1 Create a macro variable from a statistic 2 proc sql noprint; 3 select std(age) format=best32. 4 into :age_mean 5 from sashelp.class; 6 quit; NOTE: PROCEDURE SQL used (Total process time): real time 0.02 seconds cpu time 0.00 seconds 7 8 %put Mean of age: &amp;age_mean; Mean of age: 1.49267215939689 9 %putlog Mean of age: &amp;age_mean; _ 180 WARNING: Apparent invocation of macro PUTLOG not resolved. ERROR 180-322: Statement is not valid or it is used out of proper order. ERROR: Errors printed on page 1. This is a standard trick: - compute something with PROC SQL, - store into a macro variable, - reuse it later in titles, footnotes, conditions, or other macros. 9.3.0.2 Select variable names from metadata SAS metadata tables (dictionary.columns) are extremely powerful for programmatic workflows. proc sql; select name into :vars separated by &#39; &#39; from dictionary.columns where libname=&quot;SASHELP&quot; and memname=&quot;CLASS&quot; and varnum=4; ; quit; %put &amp;vars.; Column Name -------------------------------- Height This pattern is the foundation of “dynamic programming” in SAS: - detect variables automatically, - loop over them, - generate consistent outputs. 9.3.0.3 Iterative DO loops DATA step loops are for row-by-row computations and simulation-like constructions. data do_to; x=10; y=&quot;yes&quot;; do i=1 to 10; x=x+1; output; end; run; proc print data=do_to; run; Obs x y i 1 11 yes 1 2 12 yes 2 3 13 yes 3 4 14 yes 4 5 15 yes 5 6 16 yes 6 7 17 yes 7 8 18 yes 8 9 19 yes 9 10 20 yes 10 do while loops are useful for process-like simulations: data loan; balance=1000; payment=0; do while (balance&gt;0); balance=balance-100; payment=payment+1; output; end; run; proc print data=loan; run; Obs balance payment 1 900 1 2 800 2 3 700 3 4 600 4 5 500 5 6 400 6 7 300 7 8 200 8 9 100 9 10 0 10 9.3.0.4 DO loop inside a macro This example illustrates how macros generate SAS code and then run it. %macro run_calculation(amt, t, r); data customer_value; i=&amp;r./10.; do n=0 to &amp;t.; S=&amp;amt.*((1+i)*n - 1)/i; /*Power operator */ output; /*output s*/ end; file print; putlog s @@; put s @@; run; proc print data=customer_value; run; %mend; %run_calculation(amt=100, t=10, r=7); 2285.7142857 Obs i n S 1 0.7 0 -142.86 2 0.7 1 100.00 3 0.7 2 342.86 4 0.7 3 585.71 5 0.7 4 828.57 6 0.7 5 1071.43 7 0.7 6 1314.29 8 0.7 7 1557.14 9 0.7 8 1800.00 10 0.7 9 2042.86 11 0.7 10 2285.71 Even when the formula is toy-like, the structure mirrors production macro design: - macro parameters define the scenario, - data step creates a dataset, - output is printed or reported. 9.3.1 PROC REPORT for flexible summaries Here we generate a summary dataset with proc means, then format it with proc report. proc means data=sashelp.class; var _NUMERIC_; /*_CHARACTER_*/ output out=want mean= sum= max= /autoname; run; proc report data= want; Column _FREQ_; Column Age_Mean; Column Age_Sum; Column Age_max; Define _FREQ_ /&quot;The total number&quot; display; Define Age_mean /&quot;Mean of age&quot; display; Define Age_sum /&quot;Sum of age&quot; display; Define Age_max /&quot;Max of age&quot; display; Run; The MEANS Procedure Variable N Mean Std Dev Minimum Maximum ------------------------------------------------------------------------- Age 19 13.3157895 1.4926722 11.0000000 16.0000000 Height 19 62.3368421 5.1270752 51.3000000 72.0000000 Weight 19 100.0263158 22.7739335 50.5000000 150.0000000 ------------------------------------------------------------------------- The total Mean of Sum of Max of number age age age 19 13.315789 253 16 Teaching point: - proc means creates the statistics, - proc report controls presentation. This split is clean and scalable. 9.4 Use SAS formats Formats provide consistent labeling, grouping, and display standards—especially important in regulated reporting. 9.4.0.1 Creating labels for different values PROC FORMAT; VALUE LIKERT7_A 1,2,3 = &quot;Disagree&quot; 4 = &quot;Neither Agree nor Disagree&quot; 5,6,7 = &quot;Agree&quot; RUN; PROC FORMAT; VALUE INCOME LOW -&lt; 20000 = &quot;Low&quot; 20000 -&lt; 60000 = &quot;Middle&quot; 60000 - HIGH = &quot;High&quot;; RUN; PROC FORMAT; VALUE RACE 1 = &quot;White&quot; 2 = &quot;Black&quot; OTHER = &quot;Other&quot;; RUN; PROC FORMAT; VALUE $GENDERLABEL &quot;M&quot; = &quot;Male&quot; &quot;F&quot; = &quot;Female&quot;; RUN; DATA sample; SET sashelp.class; FORMAT sex GENDERLABEL. ; RUN; proc freq data=sample; table sex; run; The FREQ Procedure Cumulative Cumulative Sex Frequency Percent Frequency Percent ----------------------------------------------------------- Female 9 47.37 9 47.37 Male 10 52.63 19 100.00 Formats are not only cosmetic—they also define category behavior in many procedures. 9.4.1 Storing formats to a library When you create formats in work, they disappear after the session ends. A permanent format library allows reuse across projects and reports. 9.4.1.1 Way 1: store in a dedicated format library and use FMTSEARCH 2 LIBNAME format 2 ! &quot;C:\\Users\\hed2\\Downloads\\code-storage\\code\\format&quot;; NOTE: Library FORMAT does not exist. 3 4 PROC FORMAT LIBRARY=format ; ERROR: Library FORMAT does not exist. 5 VALUE $GENDER 6 &quot;M&quot; = &quot;Male_new&quot; 7 &quot;F&quot; = &quot;Female_new&quot;; NOTE: The previous statement has been deleted. 8 RUN; NOTE: PROCEDURE FORMAT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 9 10 OPTIONS FMTSEARCH=(format); 11 DATA sample; 12 SET sashelp.class; 13 FORMAT sex GENDER. ; _______ 48 ERROR 48-59: The format $GENDER was not found or could not be loaded. 14 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: SAS set option OBS=0 and will continue to check statements. This might cause NOTE: No observations in data set. WARNING: The data set WORK.SAMPLE may be incomplete. When this step was stopped there were 0 observations and 5 variables. NOTE: DATA statement used (Total process time): real time 0.04 seconds cpu time 0.04 seconds 15 16 proc freq data=sample; 17 table sex; 18 run; NOTE: PROCEDURE FREQ used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on page 1. Interpretation: - LIBNAME format points to a permanent folder. - PROC FORMAT LIBRARY=format stores formats there. - FMTSEARCH tells SAS where to look for formats. 9.4.1.2 Way 2: include a SAS file that defines formats 2 %INCLUDE 2 ! &#39;C:\\Users\\hed2\\Downloads\\code-storage\\code\\format\\format_test.sa 2 ! s&#39;; WARNING: Physical file does not exist, C:\\Users\\hed2\\Downloads\\code-storage\\code\\format\\format_test.sas. ERROR: Cannot open %INCLUDE file C:\\Users\\hed2\\Downloads\\code-storage\\code\\format\\format_test.sas. 3 DATA sample; 4 SET sashelp.class; 5 FORMAT sex GENDERsex. ; __________ 48 ERROR 48-59: The format $GENDERSEX was not found or could not be loaded. 6 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: SAS set option OBS=0 and will continue to check statements. This might cause NOTE: No observations in data set. WARNING: The data set WORK.SAMPLE may be incomplete. When this step was stopped there were 0 observations and 5 variables. NOTE: DATA statement used (Total process time): real time 0.02 seconds cpu time 0.00 seconds 7 8 proc freq data=sample; 9 table sex; 10 run; NOTE: PROCEDURE FREQ used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on page 1. This is common in shared programming environments: - formats are maintained in version-controlled .sas files, - projects include them as needed. 9.4.1.3 Way 3: add formats to an existing format library 2 LIBNAME format 2 ! &quot;C:\\Users\\hed2\\Downloads\\code-storage\\code\\format&quot;; NOTE: Library FORMAT does not exist. 3 4 PROC FORMAT LIBRARY=format ; ERROR: Library FORMAT does not exist. 5 VALUE $GENDER 6 &quot;M&quot; = &quot;Male_new&quot; 7 &quot;F&quot; = &quot;Female_new&quot;; NOTE: The previous statement has been deleted. 8 RUN; NOTE: PROCEDURE FORMAT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 9 10 PROC FORMAT LIBRARY=format ; ERROR: Library FORMAT does not exist. 11 VALUE $gendersextwo 12 &quot;M&quot; = &quot;Male_new2_two&quot; 13 &quot;F&quot; = &quot;Female_new2_two&quot;; NOTE: The previous statement has been deleted. 14 RUN; NOTE: PROCEDURE FORMAT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 15 16 OPTIONS FMTSEARCH=(format); 17 DATA sample; 18 SET sashelp.class; 19 FORMAT sex GENDERsextwo. ; _____________ 48 ERROR 48-59: The format $GENDERSEXTWO was not found or could not be loaded. 20 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: SAS set option OBS=0 and will continue to check statements. This might cause NOTE: No observations in data set. WARNING: The data set WORK.SAMPLE may be incomplete. When this step was stopped there were 0 observations and 5 variables. NOTE: DATA statement used (Total process time): real time 0.03 seconds cpu time 0.01 seconds 21 22 proc freq data=sample; 23 table sex; 24 run; NOTE: PROCEDURE FREQ used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on page 1. This demonstrates that format libraries are extendable: you can keep adding definitions over time. 9.4.1.4 Way 4: apply formats in batch via %include 2 LIBNAME format 2 ! &quot;C:\\Users\\hed2\\Downloads\\code-storage\\code\\format&quot;; NOTE: Library FORMAT does not exist. 3 4 PROC FORMAT LIBRARY=format ; ERROR: Library FORMAT does not exist. 5 VALUE $GENDER 6 &quot;M&quot; = &quot;Male_new&quot; 7 &quot;F&quot; = &quot;Female_new&quot;; NOTE: The previous statement has been deleted. 8 RUN; NOTE: PROCEDURE FORMAT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 9 10 OPTIONS FMTSEARCH=(format); 11 DATA sample; 12 SET sashelp.class; 13 %INCLUDE 13 ! &#39;C:\\Users\\hed2\\Downloads\\code-storage\\code\\format\\use_format.sas 13 ! &#39;; WARNING: Physical file does not exist, C:\\Users\\hed2\\Downloads\\code-storage\\code\\format\\use_format.sas. ERROR: Cannot open %INCLUDE file C:\\Users\\hed2\\Downloads\\code-storage\\code\\format\\use_format.sas. 14 RUN; NOTE: There were 19 observations read from the data set SASHELP.CLASS. NOTE: The data set WORK.SAMPLE has 19 observations and 5 variables. NOTE: DATA statement used (Total process time): real time 0.02 seconds cpu time 0.01 seconds 15 16 proc freq data=sample; 17 table sex; 18 run; NOTE: There were 19 observations read from the data set WORK.SAMPLE. NOTE: The PROCEDURE FREQ printed page 1. NOTE: PROCEDURE FREQ used (Total process time): real time 0.06 seconds cpu time 0.00 seconds ERROR: Errors printed on page 1. The FREQ Procedure Cumulative Cumulative Sex Frequency Percent Frequency Percent -------------------------------------------------------- F 9 47.37 9 47.37 M 10 52.63 19 100.00 This pattern is helpful when: - many variables require formatting, - format assignment rules are long, - you want to keep the main analysis code clean. 9.4.2 Modify formats User-defined formats cannot be edited “in place” like a database table; you typically redefine them. Your example also shows using other= to handle all remaining values. /*User-defined formats cannot be edited. to create a macro by myself ; or proc freq then use excel*/; PROC FORMAT ; VALUE $GENDER &quot;M&quot; = &quot;Male_new&quot; &quot;F&quot; = &quot;Female_new&quot;; RUN; proc format; value $sex_f &quot;F&quot;=&quot;Female_f&quot; other=[5.1]; run; DATA sample; SET sashelp.class; FORMAT sex $sex_f. ; RUN; proc freq data=sample; table sex; run; The FREQ Procedure Cumulative Cumulative Sex Frequency Percent Frequency Percent ------------------------------------------------------------- Female_f 9 47.37 9 47.37 M 10 52.63 19 100.00 9.4.3 Transform missing to character then to numeric again This technique appears in recoding tasks where you need numeric categories but start from character labels. PROC FORMAT ; VALUE $num &quot;M&quot; = 1 &quot;F&quot; = 2; RUN; DATA sample_2; SET sashelp.class; sex_2 = input(put(sex ,num.),best.); /*Converting variable types and keep*/ RUN; proc freq data=sample_2; table sex_2; run; The FREQ Procedure Cumulative Cumulative sex_2 Frequency Percent Frequency Percent ---------------------------------------------------------- 1 10 52.63 10 52.63 2 9 47.37 19 100.00 Mechanism: - put(sex, num.) maps “M”/“F” to “1”/“2” as character, - input(..., best.) converts that character into numeric. 9.4.4 Output format definition details When you need to inspect what a format contains, you can output it. PROC FORMAT ; VALUE $GENDER &quot;M&quot; = &quot;Male_new&quot; &quot;F&quot; = &quot;Female_new&quot;; RUN; proc format library = work.formats FMTLIB cntlout = cntlout; select $GENDER; run; --------------------------------------------------------------------------- - | FORMAT NAME: $GENDER LENGTH: 10 NUMBER OF VALUES: 2 | | MIN LENGTH: 1 MAX LENGTH: 40 DEFAULT LENGTH: 10 FUZZ: 0 | |-------------------------------------------------------------------------- | |START |END |LABEL (VER. V7|V8 20FEB2026:16:45:16)| |----------------+----------------+---------------------------------------- | |F |F |Female_new | |M |M |Male_new | --------------------------------------------------------------------------- - This creates cntlout, which is a dataset representation of the format—useful for QA and documentation. 9.4.5 A macro to copy an existing PROC FORMAT This longer macro demonstrates a common real need: reconstruct a format definition programmatically (for reuse, reporting, or migration). %macro formatd(data, var); data temp; set &amp;data; keep &amp;var; run; /*original freq*/ ods output OneWayFreqs=sample_b ; proc freq data=temp ; table &amp;var /missing ; run; ods output close; /*delete format freq*/ data sample2; set temp; Format _all_; run; proc freq data=sample2 ; table &amp;var /missing out=sample_c ; run; /*select original variable code*/ proc sql noprint; select name into :firstvar_b from dictionary.columns where libname=&#39;WORK&#39; and memname=&#39;SAMPLE_B&#39; and varnum=2; quit; data sample_b2; set sample_b; Keep &amp;firstvar_b ; run; /*select original variable label*/ proc sql noprint; select name into :firstvar_c from dictionary.columns where libname=&#39;WORK&#39; and memname=&#39;SAMPLE_C&#39; and varnum=1; quit; data sample_c2; set sample_c; Keep &amp;firstvar_c ; run; /*merge variable code and label*/ data sample_bc; set sample_b2 (RENAME=(&amp;firstvar_b=new_b)); set sample_c2 (RENAME=(&amp;firstvar_c=new_c)); run; /*create format format and output*/ data sample_bc; set sample_bc; Original_format = CATS(new_c,&quot;=&quot;,&quot;&#39;&quot;,new_b,&quot;&#39;&quot;); run; proc print data=sample_bc noobs; var Original_format; run; %mend formatd; PROC FORMAT ; VALUE $GENDER &quot;M&quot; = &quot;Male_new&quot; &quot;F&quot; = &quot;Female_new&quot;; RUN; DATA sample; SET sashelp.class; FORMAT sex $GENDER. ; RUN; %formatd(sample, sex ); The FREQ Procedure Cumulative Cumulative Sex Frequency Percent Frequency Percent --------------------------------------------------------------- Female_new 9 47.37 9 47.37 Male_new 10 52.63 19 100.00 The FREQ Procedure Cumulative Cumulative Sex Frequency Percent Frequency Percent -------------------------------------------------------- F 9 47.37 9 47.37 M 10 52.63 19 100.00 Original_ format F=&#39;Female_new&#39; M=&#39;Male_new&#39; Reading the intent: - sample_b captures formatted labels (via ODS output from PROC FREQ). - sample_c captures raw unformatted codes. - The macro merges them to reconstruct mapping rules like \"Male_new\"='M'. This is a clever QA/automation trick when you inherit legacy formats but need documentation. 9.4.6 A macro to view the list of variables Finally, a compact utility macro to list variable names in a dataset. %macro varnames (dat); proc contents data = &amp;dat noprint out = data_info (keep = name ); run; proc print data = data_info noobs; run; %mend; %varnames (sashelp.class) NAME Age Height Name Sex Weight This is particularly helpful inside SASmarkdown documents because it provides quick visibility when you cannot “click around” like in SAS Studio. 9.4.7 Practical workflow recommendations for SASmarkdown When you write SAS inside R Markdown, the biggest productivity gains come from being explicit: Make each SAS chunk self-contained, or write intermediate datasets to a permanent library. Use ODS OUTPUT whenever you need a procedure result as a dataset for downstream steps. Prefer PROC SQL joins for clarity unless you specifically need DATA step merge behavior. Store formats in a permanent library if the report will be knitted repeatedly. With these habits, SASmarkdown becomes a reliable way to produce reproducible, publication-ready analysis documents that combine narrative, code, and output in one place. "],["causal-inference.html", "10 Causal inference 10.1 Causal inference introduction 10.2 Inverse probability weighting 10.3 G formula 10.4 Double Robust Estimators 10.5 Instrumental variable regression 10.6 Mediation analysis 10.7 Confounding and effect measure modification 10.8 Causal inference and associated regression", " 10 Causal inference Chapter 10 provides a comprehensive overview of causal inference methods, beginning with foundational concepts and directed acyclic graphs (DAGs) for identifying confounding structures, followed by core estimation strategies including inverse probability weighting, matching, and the g-formula. It advances to double robust approaches—leveraging SuperLearner, TMLE, and longitudinal TMLE—for machine-learning–enhanced effect estimation, and further covers instrumental variable regression, mediation analysis, confounding and effect measure modification, and the integration of causal frameworks with regression modeling to support valid treatment effect estimation in observational and longitudinal data settings. 10.1 Causal inference introduction 10.1.1 Definitions and directed acyclic graph (DAG) see here see here 10.1.1.1 A online tool to draw and analyze causal digrams- DAGitty see here 10.2 Inverse probability weighting see here 10.2.1 Matching and weighting w/o imputation see here 10.3 G formula see here 10.4 Double Robust Estimators 10.4.1 SuperLearner see here see here 10.4.2 Double robust estimators using SuperLearner_sl3 see here 10.4.3 Double robust estimators using TMLE see here 10.4.4 Longitudinal targeted maximum likelihood estimation see here 10.5 Instrumental variable regression see here 10.6 Mediation analysis see here 10.7 Confounding and effect measure modification see here 10.8 Causal inference and associated regression see here "],["clinical-trial.html", "11 Clinical Trial 11.1 Statistics in clinical trial", " 11 Clinical Trial This chapter provides a comprehensive overview of statistics in clinical trials, outlining the clinical statistician’s role in protocol development, sample size determination, randomization, interim monitoring, and regulatory reporting; it details hypothesis testing for continuous and Poisson endpoints, derivation of sample size formulas under type I/II error constraints, classical group sequential methods and Bayesian sequential procedures for interim analyses, and emphasizes proper randomization, allocation concealment, and safety monitoring to ensure methodological rigor and regulatory compliance. 11.1 Statistics in clinical trial 11.1.1 Sample size, interim data reports and randomization of assignment see here see here "],["common-statistical-models.html", "12 Common statistical models 12.1 Survival analysis 12.2 Logistical regression 12.3 Poisson regression 12.4 Quantile regression 12.5 Principle components analysis 12.6 Which covariates should be adjusted 12.7 Variable selection 12.8 Fit regression model with a fan-shaped relation 12.9 Save And Finalize Your trained Model", " 12 Common statistical models This chapter reviews widely used statistical modeling approaches, including survival analysis for time-to-event data, logistic and Poisson regression for binary and count outcomes, quantile regression for distributional effects, and principal components analysis for dimensionality reduction; it also discusses practical modeling considerations such as covariate adjustment strategies, variable selection methods, and handling heteroscedastic or fan-shaped relationships in regression to ensure valid inference and robust model performance. 12.1 Survival analysis 12.1.0.1 Survival analysis This section introduces the core survival analysis framework—survival, hazard, and cumulative hazard functions under random censoring—and shows how to estimate and compare survival curves nonparametrically using the Kaplan–Meier estimator and the log-rank test. It then presents parametric models (exponential/Weibull) and the Cox proportional hazards model, covering likelihood/partial-likelihood estimation, interpretation of coefficients (time ratios or hazard ratios), and key diagnostics such as the proportional hazards assumption and influential observations. 12.1.0.2 Time-Dependent Cox Regression Model see here 12.1.0.3 Competing Risk Cox Model see here 12.1.0.4 Joint model with longitudinal and survival data This report demonstrates how to fit and evaluate joint models linking longitudinal biomarker trajectories (log serum bilirubin via LME) with time-to-event outcomes (Cox/competing risks), including PH diagnostics, dynamic survival prediction (AUC/ROC, survfitJM), and interpretation of biomarker–risk association parameters using the JM package. 12.2 Logistical regression see here 12.3 Poisson regression see here 12.4 Quantile regression see here 12.5 Principle components analysis see here 12.6 Which covariates should be adjusted see here 12.7 Variable selection see here 12.8 Fit regression model with a fan-shaped relation see here 12.9 Save And Finalize Your trained Model This report demonstrates how to train, save, reload, and deploy both a linear regression model and a random forest model in R using saveRDS() and readRDS(), enabling model persistence and reproducible prediction workflows. "],["bayesian-statistics.html", "13 Bayesian statistics 13.1 Bayesian Inference in R", " 13 Bayesian statistics This chapter introduces Bayesian inference in R by deriving posterior distributions as proportional to likelihood × prior, demonstrating grid-based estimation for continuous (normal) and binomial models with one or two parameters, computing joint and marginal posteriors, and illustrating practical applications such as diagnostic test updating via Bayes’ theorem; it also highlights computational scaling challenges and motivates the use of MCMC for higher-dimensional problems. 13.1 Bayesian Inference in R see here "],["epidemiolgy.html", "14 Epidemiolgy 14.1 Introduction 14.2 Bias analysis and control in R", " 14 Epidemiolgy 14.1 Introduction Epidemiology is the study of the distribution and determinants of health-related states in populations and the application of this knowledge to control health problems. It integrates measures of disease frequency (prevalence, incidence, mortality, survival, DALYs), measures of association (RR, OR, attributable risk), and study designs (cross-sectional, case-control, cohort, and experimental trials) to answer PECO-framed research questions. Landmark studies such as the Framingham Heart Study identified major cardiovascular risk factors, illustrating how observational research informs prevention. Epidemiology also emphasizes valid inference through appropriate statistical testing, confidence intervals, power and sample size planning, and careful interpretation of regression models. Core methodological concerns include bias (selection and information), confounding, and random error, with strategies such as matching, restriction, randomization, stratification, and multivariable adjustment to improve validity. In clinical trials, principles such as random assignment, masking, intention-to-treat analysis, and phased drug development ensure internal validity, while diagnostic testing relies on sensitivity, specificity, predictive values, and Bayes’ theorem, recognizing that disease prevalence strongly influences post-test probabilities. see here 14.2 Bias analysis and control in R This chapter reviews three major sources of bias and their analytical correction: selection bias (when the study population does not represent the target population due to nonresponse, loss to follow-up, or improper control selection), information bias/misclassification (measurement error in exposure, outcome, or covariates), and confounding bias (distortion from extraneous variables). Using the episensr package, it demonstrates how to quantify and correct selection bias via selection probabilities or bias factors, assess nondifferential and differential misclassification for exposures and covariates, and adjust for confounding through stratification, regression, or external bias parameters; it also summarizes common mechanisms (e.g., control selection bias, recall bias, healthy worker effect) and emphasizes sensitivity analysis as a structured approach to evaluating robustness of effect estimates. see here "],["bioinformation.html", "15 Bioinformation 15.1 Sequence Analysis", " 15 Bioinformation This section shows a compact R workflow for sequence analysis: perform multiple sequence alignment, run RNA-seq differential expression with DESeq2 (normalization, testing, volcano/PCA plots), build and optimize phylogenetic trees from aligned sequences (and compare trees with Robinson–Foulds distance plus bootstrap support), and finish with simple pathway/enrichment checks using Fisher’s exact test or rank-based tests like Wilcoxon. 15.1 Sequence Analysis see here "],["common-issues-in-statistics.html", "16 Common issues in Statistics", " 16 Common issues in Statistics This chapter reviews common statistical pitfalls and best practices, emphasizing the importance of plotting data before analysis, checking model assumptions (especially independence and equal variance), and distinguishing statistical from practical significance. It highlights issues such as biased or unrepresentative samples, underpowered or overpowered studies, misinterpretation of p-values, multiple comparisons without adjustment (FWER/FDR), data snooping, inappropriate categorization of continuous variables, misuse of stepwise selection, overinterpreting high R², pseudoreplication, and confusing confidence with prediction intervals. The chapter stresses careful study design—proper randomization, control of confounding, power analysis, handling missing data, and planning for multiple inference—as well as transparent reporting, consideration of bias, and involvement of statisticians early to ensure valid, interpretable, and ethically sound research. see here "],["miscellaneous.html", "17 Miscellaneous 17.1 Linear algebra 17.2 Calculus 17.3 Sample size calculation (simulation-based power) 17.4 How to evaluate a z score 17.5 Mathematical coupling 17.6 How to create a bookdown 17.7 How to create a blogdown 17.8 How to install tensorflow and keras 17.9 How to use GitHub in a team 17.10 How to insert a picture indirectly in markdown 17.11 R cheatsheets", " 17 Miscellaneous This chapter collects a set of “tools of the trade” that show up repeatedly in statistical practice: linear algebra, calculus, and a few practical workflows (sample size simulation, z-score evaluation, and reproducible reporting with bookdown/blogdown). The intention is not to present a rigorous proof-based treatment, but to provide a working reference with short explanations and runnable code. 17.1 Linear algebra Linear algebra is the language of most modern statistical modeling. Regression, mixed models, PCA, and many optimization algorithms are built on matrix representations: data are organized as matrices (design matrices), parameters are vectors, sums of squares become quadratic forms, estimation becomes solving linear systems or matrix decompositions. 17.1.1 Matrix basics 17.1.1.1 Dimensions of a matrix A matrix has two dimensions: number of rows and number of columns. In R, dim() returns both. seed=123 X=matrix(1:12,ncol=3,nrow=4) X 1 5 9 2 6 10 3 7 11 4 8 12 dim(X) [1] 4 3 dim(X)[1] [1] 4 ncol(X) [1] 3 Key ideas: - dim(X) returns (nrow, ncol). - dim(X)[1] extracts the number of rows. - ncol(X) is a convenience function for columns. 17.1.1.2 Change dimensions of a matrix In R, you can reshape a matrix by changing its dim. This does not change the values, only how they are arranged (by column, because R stores matrices column-wise). dim(X)=c(2,6) X 1 3 5 7 9 11 2 4 6 8 10 12 This is essentially a “reshape” of the underlying vector 1:12. 17.1.1.3 Change names of a matrix Row and column names improve readability and reduce mistakes, especially when matrices represent parameters or covariance structures. a &lt;- matrix(1:6,nrow=2,ncol=3,byrow=FALSE) b &lt;- matrix(1:6,nrow=3,ncol=2,byrow=T) c &lt;- matrix(1:6,nrow=3,ncol=2,byrow=T,dimnames=list(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;),c(&quot;boy&quot;,&quot;girl&quot;))) c boy girl A 1 2 B 3 4 C 5 6 Now we can extract and modify the names: rownames(c) [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; colnames(c) [1] &quot;boy&quot; &quot;girl&quot; rownames(c)=c(&quot;E&quot;,&quot;F&quot;,&quot;G&quot;) c boy girl E 1 2 F 3 4 G 5 6 17.1.1.4 Replace elements of a matrix Indexing uses [row, col]. Once you can index, you can update values. X=matrix(1:12,nrow=4,ncol=3) X[2,3] [1] 10 X[2,3]=1000 X 1 5 9 2 6 1000 3 7 11 4 8 12 17.1.1.5 Extract diagonal elements and replace them The diagonal is often special: identity matrices, variances, and many decompositions depend on it. diag(X) [1] 1 6 11 diag(X)=c(0,0,1) X 0 5 9 2 0 1000 3 7 1 4 8 12 17.1.1.6 Create diagonal and identity matrices diag(c(...)) creates a diagonal matrix with those values. diag(k) creates a k × k identity matrix. diag(c(1,2,3)) 1 0 0 0 2 0 0 0 3 diag(3) 1 0 0 0 1 0 0 0 1 17.1.1.7 Extract lower/upper triangle elements The lower and upper triangles are used heavily in covariance and triangular decompositions. X 0 5 9 2 0 1000 3 7 1 4 8 12 X[lower.tri(X)] [1] 2 3 4 7 8 12 X[upper.tri(X)] [1] 5 9 1000 17.1.1.8 Create lower/upper triangle matrices A common operation is to “zero out” part of a matrix. X[lower.tri(X)]=0 X 0 5 9 0 0 1000 0 0 1 0 0 0 This is useful for constructing triangular matrices or simplifying display. 17.1.2 Operations 17.1.2.1 Transpose The transpose swaps rows and columns. Many matrix formulas (e.g., normal equations) require transpose. t(X) 0 0 0 0 5 0 0 0 9 1000 1 0 17.1.2.2 Row/column summaries Row sums and means are common in data transformations and checking intermediate quantities. A=matrix(1:12,3,4) A 1 4 7 10 2 5 8 11 3 6 9 12 rowSums(A) [1] 22 26 30 rowMeans(A) [1] 5.5 6.5 7.5 17.1.2.3 Determinant The determinant summarizes properties of a square matrix: - If det(A)=0, the matrix is singular (not invertible). - Determinants also appear in multivariate normal likelihoods via |Σ|. X=matrix(rnorm(9),nrow=3,ncol=3) det(X) [1] 0.7165293 17.1.2.4 Addition Matrix addition requires the same dimensions. A=matrix(1:12,nrow=3,ncol=4) B=matrix(1:12,nrow=3,ncol=4) A+B #same dimensions (non-conformable arrays) 2 8 14 20 4 10 16 22 6 12 18 24 17.1.2.5 Addition by a scalar Adding a scalar shifts all entries. A=matrix(1:12,nrow=3,ncol=4) 2+A 3 6 9 12 4 7 10 13 5 8 11 14 17.1.2.6 Multiply by a scalar Scalar multiplication rescales all entries. A=matrix(1:12,nrow=3,ncol=4) 2*A 2 8 14 20 4 10 16 22 6 12 18 24 17.1.2.7 Matrix multiplication (dot product) Matrix multiplication requires conformable dimensions: - if A is m×k, B must be k×n. A=matrix(1:12,nrow=2,ncol=4) Warning in matrix(1:12, nrow = 2, ncol = 4): data length differs from size of matrix: [12 != 2 x 4] B=matrix(1:12,nrow=4,ncol=3) A%*%B 50 114 178 60 140 220 17.1.2.8 Kronecker product The Kronecker product appears in block matrix constructions and covariance modeling (e.g., separable covariance). A=matrix(1:4,2,2) B=matrix(rep(1,4),2,2) kronecker(A,B) 1 1 3 3 1 1 3 3 2 2 4 4 2 2 4 4 17.1.2.9 Inverse matrix A matrix must be square and non-singular to have an inverse. Inverse matrices appear in closed-form least squares and covariance transformations. A=matrix(rnorm(9),nrow=3,ncol=3) A -1.3843051 -1.310550 0.740038 0.0789580 1.051403 -0.711770 -0.5636427 0.738301 2.260475 AI=solve(A) AI -0.7582116 -0.9167069 -0.0404247 -0.0581822 0.7085465 0.2421523 -0.1700547 -0.4599988 0.3532150 # identity matrix (A%*%AI) 1 0 0 0 1 0 0 0 1 The matlib package provides a more explicit inv(). library(matlib) inv(A) -0.7582116 -0.9167069 -0.0404247 -0.0581822 0.7085465 0.2421523 -0.1700547 -0.4599988 0.3532150 17.1.2.10 Generalized inverse When a matrix is not square (or is singular), we can use a generalized inverse (Moore–Penrose pseudoinverse). This is common in least squares with rank deficiency. library(MASS) A2=matrix(1:12,nrow=3,ncol=4) A%*%ginv(A) 1 0 0 0 1 0 0 0 1 A%*%ginv(A)%*%A -1.3843051 -1.310550 0.740038 0.0789580 1.051403 -0.711770 -0.5636427 0.738301 2.260475 The product A %*% ginv(A) %*% A is a projection-like operation: it maps back into the column space of A. 17.1.2.11 crossprod crossprod(B) computes t(B) %*% B efficiently and with better numerical stability. B=matrix(1:12,nrow=4,ncol=3) B 1 5 9 2 6 10 3 7 11 4 8 12 crossprod(B) 30 70 110 70 174 278 110 278 446 t(B)%*%B 30 70 110 70 174 278 110 278 446 Generalized inverse utilities with crossprod: B=matrix(1:12,nrow=4,ncol=3) ginv(B) -0.375 -0.1458333 0.0833333 0.3125 -0.100 -0.0333333 0.0333333 0.1000 0.175 0.0791667 -0.0166667 -0.1125 ginv(crossprod(B)) 0.2664931 0.0763889 -0.1137153 0.0763889 0.0222222 -0.0319444 -0.1137153 -0.0319444 0.0498264 # solve(crossprod(B)) #is singular # solve((B)) #is not square 17.1.3 Eigen decomposition Eigen decomposition is defined for square matrices. It is central in: - PCA and spectral methods, - diagonalizing symmetric matrices, - understanding quadratic forms and covariance matrices. For an m×m matrix A, eigen decomposition takes the form: - values: eigenvalues (Λ) - vectors: eigenvectors (U) In many texts: A = U Λ U^{-1}. For symmetric matrices, U is orthogonal and U^{-1} = U', giving A = U Λ U'. 17.1.3.1 Eigen values A=matrix(1:9,nrow=3,ncol=3) Aeigen=eigen(A) Aeigen$values [1] 1.611684e+01 -1.116844e+00 -5.700691e-16 val &lt;- diag(Aeigen$values) val 16.11684 0.000000 0 0.00000 -1.116844 0 0.00000 0.000000 0 17.1.3.2 Eigen vectors Aeigen$vectors -0.4645473 -0.8829060 0.4082483 -0.5707955 -0.2395204 -0.8164966 -0.6770438 0.4038651 0.4082483 We can reconstruct A (approximately) using eigenvectors and eigenvalues: round(Aeigen$vectors%*%val%*%t(Aeigen$vectors)) 3 4 5 4 5 6 5 6 7 A 1 4 7 2 5 8 3 6 9 Small discrepancies may occur due to rounding and floating-point arithmetic. 17.1.4 Advanced operations 17.1.4.1 Cholesky factorization Cholesky factorization applies to symmetric positive definite matrices: - A = R'R (or sometimes A = LL' depending on convention), - chol(A) returns an upper triangular factor by default. Covariance matrices are typically positive semidefinite; in practice, estimated covariance matrices are often treated as positive definite (or adjusted to be so). A=diag(3)+1 A 2 1 1 1 2 1 1 1 2 chol(A) 1.414214 0.7071068 0.7071068 0.000000 1.2247449 0.4082483 0.000000 0.0000000 1.1547005 Check the reconstruction: t(chol(A))%*%chol(A) 2 1 1 1 2 1 1 1 2 Compute inverse via Cholesky: chol2inv(chol(A)) 0.75 -0.25 -0.25 -0.25 0.75 -0.25 -0.25 -0.25 0.75 inv(A) 0.75 -0.25 -0.25 -0.25 0.75 -0.25 -0.25 -0.25 0.75 Using Cholesky is often numerically preferable to directly calling solve() for symmetric positive definite matrices. 17.1.4.2 Singular value decomposition (SVD) SVD works for any m×n matrix and is among the most robust matrix decompositions. A = U D V' U: orthogonal (m×m or m×r) D: diagonal singular values (r×r) V: orthogonal (n×n or n×r) r: rank (number of non-zero singular values) A=matrix(1:18,3,6) svd(A) $d [1] 4.589453e+01 1.640705e+00 1.366522e-15 t(svd(A) $u)%*%svd(A) $u 1 0 0 0 1 0 0 0 1 t(svd(A) $v)%*%svd(A) $v 1 0 0 0 1 0 0 0 1 Reconstruction: svd(A) $u %*%diag(svd(A) $d)%*% t(svd(A) $v) 1 4 7 10 13 16 2 5 8 11 14 17 3 6 9 12 15 18 A 1 4 7 10 13 16 2 5 8 11 14 17 3 6 9 12 15 18 SVD is widely used for: - pseudoinverse, - PCA (via centered data matrix), - numerical stability in least squares. 17.1.4.3 QR decomposition QR is another general decomposition for m×n matrices: A = Q R Q: orthogonal (Q'Q = I) R: upper triangular QR is essential for numerically stable least squares. A=matrix(1:12,4,3) qr(A) $qr [,1] [,2] [,3] [1,] -5.4772256 -12.7801930 -2.008316e+01 [2,] 0.3651484 -3.2659863 -6.531973e+00 [3,] 0.5477226 -0.3781696 1.601186e-15 [4,] 0.7302967 -0.9124744 -5.547002e-01 $rank [1] 2 $qraux [1] 1.182574 1.156135 1.832050 $pivot [1] 1 2 3 attr(,&quot;class&quot;) [1] &quot;qr&quot; Extract the components: qr.Q(qr(A)) -0.1825742 -0.8164966 -0.4000874 -0.3651484 -0.4082483 0.2546329 -0.5477226 0.0000000 0.6909965 -0.7302967 0.4082483 -0.5455419 qr.R(qr(A)) -5.477226 -12.780193 -20.083160 0.000000 -3.265986 -6.531973 0.000000 0.000000 0.000000 17.1.5 Solve linear equations Many estimation problems reduce to solving Xβ = b. X=matrix(c(2, 2, 2, 0, 1, 1 ,2, 2, 0),ncol=3,nrow=3) X 2 0 2 2 1 2 2 1 0 b=1:3 b [1] 1 2 3 solve(X,b) # whether it is singular [1] 1.0 1.0 -0.5 If X is singular, solve() fails. In practice, that is a signal to use: - qr.solve() (least squares), - ginv() (pseudoinverse), - or to investigate collinearity/rank deficiency. 17.1.6 Summary A useful mental map: Eigen decomposition: square matrices; reveals directions of stretching/compression. SVD: works for any matrix; most robust; basis of pseudoinverse and PCA. QR: stable for regression/least squares. Cholesky: fastest for symmetric positive definite matrices (especially covariance matrices). 17.2 Calculus Calculus enters statistical practice in: - optimization (derivatives/gradients), - likelihood theory, - computing integrals for probabilities and expectations. 17.2.1 Derivation R’s deriv() can symbolically differentiate simple expressions. dx &lt;- deriv(y ~ x^3, &quot;x&quot;); dx expression({ .value &lt;- x^3 .grad &lt;- array(0, c(length(.value), 1L), list(NULL, c(&quot;x&quot;))) .grad[, &quot;x&quot;] &lt;- 3 * x^2 attr(.value, &quot;gradient&quot;) &lt;- .grad .value }) mode(dx) [1] &quot;expression&quot; x&lt;-1:2 eval(dx) [1] 1 8 attr(,&quot;gradient&quot;) x [1,] 3 [2,] 12 We can also request a function output: dx &lt;- deriv(y ~ sin(x), &quot;x&quot;, func= TRUE) ; mode(dx) [1] &quot;function&quot; dx(c(pi,4*pi)) [1] 1.224606e-16 -4.898425e-16 attr(,&quot;gradient&quot;) x [1,] -1 [2,] 1 And include constants: a&lt;-2 dx&lt;-deriv(y~a*cos(a*x),&quot;x&quot;,func = TRUE) dx(pi/3) [1] -1 attr(,&quot;gradient&quot;) x [1,] -3.464102 Partial derivatives: fxy = expression(2*x^2+y+3*x*y^2) dxy = deriv(fxy, c(&quot;x&quot;, &quot;y&quot;), func = TRUE) dxy(1,2) [1] 16 attr(,&quot;gradient&quot;) x y [1,] 16 13 In modeling, this kind of derivative structure is the foundation for gradients and Hessians used by optimizers. 17.2.2 Integration Many probabilities are integrals of density functions. integrate(dnorm, -1.96, 1.96) 0.9500042 with absolute error &lt; 0.00000000001 integrate(dnorm, -Inf, Inf) 1 with absolute error &lt; 0.000094 Improper integrals (infinite bounds) are supported: integrand &lt;- function(x) {1/((x+1)*sqrt(x))} integrate(integrand, lower = 0, upper = Inf) 3.141593 with absolute error &lt; 0.000027 Trigonometric integrals: integrand &lt;- function(x) {sin(x)} integrate(integrand, lower = 0, upper = pi/2) 1 with absolute error &lt; 0.000000000000011 17.3 Sample size calculation (simulation-based power) In complex designs, analytic power formulas may not be available or may be hard to trust. Simulation is often more transparent: Specify the data-generating process under the alternative, simulate many trials, apply the intended analysis method, estimate power as the proportion of significant results. Your code implements exactly this idea: for each candidate sample size t, simulate M trials and record whether prop.test() gives p &lt; 0.05. #given proportion px3=0.11 px4=0.07 px5=0.08 px6=0.06 px7=0.05 ################### Outcome0 = NULL for (t in seq(350, 600, by=2)){ # change the possible sample size graudally from 400 to 700. n=t count=0 M=500 # times of simulation for (i in 1:M){ # for a given sample size (400), for the first simulation, # we use rbinom function simulate the contingencey table as below with above given proportions, # and calculate the p value by using Chi square test for the simulation x3=rbinom(1,n,px3) x4=rbinom(1,n,px4) x5=rbinom(1,n,px5) x6=rbinom(1,n,px6) x7=rbinom(1,n,px7) data=matrix(c(x3,n-x3,x4,n-x4,x5,n-x5,x6,n-x6,x7,n-x7 ),ncol=2,byrow=T) # if the p value of this simulation is less than 0.05, it means we get the significant result for the given this sample size this time. it means we detected the difference in this simulation experiment when the difference is true. pv=prop.test(data)$p.value count=as.numeric(pv&lt;0.05)+count #sum of pv&lt;0.05 } #end loop of p value power0=count/M temp &lt;- data.frame(size=t,power=power0) Outcome0 = rbind(Outcome0, temp) } #end loop of power from different sample size # generate a new variable named &quot;power_loess&quot; by loess method because the curve of sample size and power is not enough smooth power_loess &lt;- round(predict(loess(Outcome0$power ~ Outcome0$size,data=Outcome0,span=0.6)),3) Outcome0 = data.frame(Outcome0, power_loess) # plot a line chart of the power and sample size, and smooth the curve by loess method plot (Outcome0$size, Outcome0$power,type = &quot;n&quot;, ylab = &quot;Power&quot;,xlab=&quot;Sample size&quot;, main = expression(paste(&quot;Power analysis for GDM&quot;))) abline(h=0.9,col=&#39;red&#39;,lwd=2,lty=2) abline(h=0.85, col=&#39;red&#39;,lwd=2,lty=2) abline(h=0.8,col=&#39;red&#39;,lwd=2,lty=2) lines(Outcome0$size,power_loess,col=&quot;blue&quot;,lwd=2) A practical interpretation of the plot: - The horizontal lines at 0.8 / 0.85 / 0.9 help you read off the required sample size range. - The loess smoothing stabilizes Monte Carlo noise from finite M. 17.4 How to evaluate a z score This section demonstrates a common pattern in applied modeling: load parameter estimates (here stored in a vector PE), extract the right “row” of estimates for a particular scenario (Ultra), compute mean and variance at a specific time/value i, compute a z-score comparing an observed value to the model-implied distribution. 17.4.1 Call the parameter estimates file You define which scenario to use: Then you load a long parameter vector. Conceptually, this is like reading from a saved model object, but stored explicitly as numeric values. Next, you reshape PE into rows and then select the relevant row using Ultra. PE= matrix(PE,nrow=5,byrow = T) index= Ultra-3; info= PE[index,] fcoef=info[3:9]; sigma=info[10];Sigmab= matrix(info[11:26],4,4);Zeta=info[27:29];varfixed= matrix(info[30:78],7,7) #abstract specific parameters to calculate mean and SD Interpretation of the objects you create: - fcoef: fixed-effect coefficients for a spline-like mean function (7 terms). - sigma: residual SD (or residual scale parameter, depending on model). - Sigmab: random-effect covariance matrix (4×4), used with Rxxi. - Zeta: knot locations for truncated power spline terms. - varfixed: covariance matrix of fixed effects (7×7) used for uncertainty in the mean function. This decomposition matches how mixed models often separate: - residual variance, - random effect variance, - and uncertainty of estimated fixed effects. 17.4.2 Calculate mean and standard deviation at a specific value You set i = 28.29 and build two design vectors: - Fxxi: 7 columns (fixed effects, including spline terms), - Rxxi: 4 columns (random effects). i = 28.29 int &lt;- 1 t1 &lt;- i t2 &lt;- i**2 t3 &lt;- i**3 tt1 &lt;- (i - Zeta[1])**3 * (i &gt; Zeta[1]) tt2 &lt;- (i - Zeta[2])**3 * (i &gt; Zeta[2]) tt3 &lt;- (i - Zeta[3])**3 * (i &gt; Zeta[3]) Fxxi = cbind(int, t1, t2, t3, tt1, tt2, tt3) Rxxi = cbind(int, t1, t2, t3) mean &lt;- Fxxi%*%fcoef var &lt;- sigma**2 + Rxxi%*%Sigmab%*%t(Rxxi) + Fxxi%*%varfixed%*%t(Fxxi) std &lt;- sqrt(var) What the variance formula is doing: - sigma^2: measurement noise / residual variance. - Rxxi Σ_b Rxxi': variance contributed by random effects at point i. - Fxxi Var(β) Fxxi': uncertainty from estimating fixed effects (mean curve uncertainty). 17.4.3 Output the calculated mean (on exp scale) The model seems to operate on the log scale, so exp(mean) returns the mean on the original scale. cat(&quot;Actual Mean=&quot;,exp(mean)) Actual Mean= 9.358071 17.4.4 Calculate z score You compare an observed value (here 289) to the modeled mean and standard deviation on the log scale: cat(&quot;Z score=&quot;,((log(289 ))-(mean))/std) Z score= 0.4419324 This is the standard “(observed − expected) / SD” idea. 17.4.5 Result checking Convert the z score to a probability using the standard normal CDF: pnorm(0.4465193, mean=0, sd=1) [1] 0.6723889 This returns P(Z ≤ 0.4465) under a standard normal assumption. 17.5 Mathematical coupling This is a classic example of “leakage”: you create a feature (cuty) derived from the outcome and then include it as a predictor. Because cuty is essentially a coarsened version of y1to9, it explains a large fraction of the variation and can make the genuine predictors look nonsignificant. set.seed(1234) # Random seed x1 &lt;- rnorm(1000) #Normal IVs x2 &lt;- rnorm(1000) x3 &lt;- rnorm(1000) yraw &lt;- x1 + x2 + x3 + rnorm(1000,0, 10) y1to9 &lt;- (yraw + -1*min(yraw) + 1) y1to9 &lt;- trunc(y1to9*(9/(max(y1to9)))) #Create a Y variable that goes 1 to 9 cuty &lt;- cut(y1to9,5) head(cuty) [1] (1.8,3.6] (3.6,5.4] (3.6,5.4] (5.4,7.2] (5.4,7.2] (1.8,3.6] Levels: (-0.009,1.8] (1.8,3.6] (3.6,5.4] (5.4,7.2] (7.2,9.01] model1 &lt;- lm(y1to9~x1 + x2 + x3) # Regular model summary(model1) #Gives sensible values Call: lm(formula = y1to9 ~ x1 + x2 + x3) Residuals: Min 1Q Median 3Q Max -4.0798 -0.8990 0.0888 1.0473 4.8964 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.79492 0.04470 84.899 &lt; 2e-16 *** x1 0.13592 0.04488 3.029 0.00252 ** x2 0.14269 0.04564 3.126 0.00182 ** x3 0.12522 0.04417 2.835 0.00468 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.412 on 996 degrees of freedom Multiple R-squared: 0.02715, Adjusted R-squared: 0.02422 F-statistic: 9.265 on 3 and 996 DF, p-value: 0.000004786 model2 &lt;- lm(y1to9~x1 + x2 + x3 + cuty) # Adding the cut of y summary(model2) #X1, x2, x3 not sig; R^2 is 0.75 Call: lm(formula = y1to9 ~ x1 + x2 + x3 + cuty) Residuals: Min 1Q Median 3Q Max -0.8743 -0.4458 0.1291 0.4295 0.8184 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.84267 0.06897 12.218 &lt;2e-16 *** x1 0.02187 0.01540 1.420 0.156 x2 0.01360 0.01567 0.868 0.386 x3 0.01324 0.01516 0.873 0.383 cuty(1.8,3.6] 1.77791 0.07342 24.214 &lt;2e-16 *** cuty(3.6,5.4] 3.59343 0.07243 49.611 &lt;2e-16 *** cuty(5.4,7.2] 5.40203 0.08464 63.825 &lt;2e-16 *** cuty(7.2,9.01] 7.37863 0.25190 29.292 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4821 on 992 degrees of freedom Multiple R-squared: 0.8871, Adjusted R-squared: 0.8863 F-statistic: 1113 on 7 and 992 DF, p-value: &lt; 2.2e-16 The plot illustrates why: cuty is nearly a deterministic summary of the response. df &lt;- data.frame(cuty, y1to9) # it causes the coefficient of x1 is 1; others equal 0. library(ggplot2) ggplot(data = df, mapping = aes(as.numeric(cuty), y1to9)) + geom_point() + geom_smooth( method = &quot;lm&quot;) `geom_smooth()` using formula = &#39;y ~ x&#39; Practical takeaway: - Never include variables derived from the outcome as predictors (unless your modeling framework explicitly accounts for it, such as certain joint models or measurement models). - This problem is common in ML workflows too (target leakage). 17.6 How to create a bookdown Bookdown is a workflow for writing a multi-chapter book using R Markdown. The main benefits are: - consistent formatting across chapters, - automatic cross-references and numbering, - easy publishing via GitHub + Netlify (or other hosting). Your steps outline a typical production path: create a GitHub repository create a new RStudio project using version control create and organize .Rmd chapter files (chapter order determined by filenames) configure theme and output options (often via _bookdown.yml and _output.yml) build locally commit and push deploy on Netlify, then map to your website Because bookdown chapters are file-based, naming conventions matter (e.g., 01-intro.Rmd, 02-methods.Rmd, …). 17.6.1 How to git up a project into GitHub Your notes reflect the standard flow: initialize version control in RStudio create an empty GitHub repo (no README if you want to push your existing structure cleanly) connect local → remote, commit, push 17.6.2 How to git down a project into your PC Cloning a repo is the reproducible way to move projects across machines: - create project from version control - paste the repo URL - pull and work locally 17.6.3 How to return to a previous version Downloading and copying works, but in professional workflows you typically: - use git tags/releases, - or git checkout &lt;commit&gt; / git revert, - then commit the rollback. Your “download then replace” approach is simple and safe for non-technical users, especially if you want to avoid dealing with git history commands. 17.7 How to create a blogdown Blogdown is more configuration-heavy than bookdown because websites require: - theme configuration, - content organization (posts, pages), - an index page, - and often Hugo theme settings. You already listed a tutorial and your own blog post as references: tutorial: https://www.youtube.com/watch?v=BHpkLJieXPE your post: https://danielhe.netlify.app/post/how-to-create-a-blog/ 17.8 How to install tensorflow and keras Your steps reflect the classic RStudio workflow: - install R packages (tensorflow, keras, reticulate) - install miniconda - create/manage a Python environment - install TensorFlow/Keras into that environment The key operational points: - reticulate::install_miniconda() sets up Python. - install_tensorflow() / install_keras() install Python dependencies. You included a YouTube tutorial link: https://www.youtube.com/watch?v=cIUg11mAmK4 17.9 How to use GitHub in a team Your checklist covers the core collaboration pattern: clone the repository configure username/email create a branch per feature or task commit and push to that branch open a pull request merge after review pull main regularly And a set of practical “misc” commands for cleanup and recovery: - delete branches, - check status, - merge, - manage remotes, - use reflog/reset/revert cautiously, - reinitialize or remove .git when needed. This is the same workflow used by most industry teams. 17.10 How to insert a picture indirectly in markdown This chunk loads tooling for clipboard-based image insertion. library(rstudioapi) library(rmarkdown) # library(imageclipr) You included the project link: [link] (https://github.com/Toniiiio/imageclipr) 17.11 R cheatsheets RStudio cheatsheets are practical, compact references for common packages (dplyr, ggplot2, tidyr, rmarkdown, etc.): cheatsheets "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
