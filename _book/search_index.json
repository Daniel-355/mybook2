[["index.html", "My Little Handbook About", " My Little Handbook Daniel He 2022-05-26 About This is my first book. “In God we Trust, all others bring data”. "],["data-wrangling.html", "Chapter 1 Data Wrangling 1.1 How to do data wrangling 1.2 How to do aggregation/ summarization", " Chapter 1 Data Wrangling 1.1 How to do data wrangling We will use tidyverse package to work with data. 1.1.1 Load data and package head (iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✔ ggplot2 3.3.5 ✔ purrr 0.3.4 ## ✔ tibble 3.1.6 ✔ dplyr 1.0.8 ## ✔ tidyr 1.2.0 ✔ stringr 1.4.0 ## ✔ readr 2.1.2 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 1.1.2 Select certain rows setosa &lt;- filter(iris, Species == &#39;setosa&#39;) setosa ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## 11 5.4 3.7 1.5 0.2 setosa ## 12 4.8 3.4 1.6 0.2 setosa ## 13 4.8 3.0 1.4 0.1 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 15 5.8 4.0 1.2 0.2 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 17 5.4 3.9 1.3 0.4 setosa ## 18 5.1 3.5 1.4 0.3 setosa ## 19 5.7 3.8 1.7 0.3 setosa ## 20 5.1 3.8 1.5 0.3 setosa ## 21 5.4 3.4 1.7 0.2 setosa ## 22 5.1 3.7 1.5 0.4 setosa ## 23 4.6 3.6 1.0 0.2 setosa ## 24 5.1 3.3 1.7 0.5 setosa ## 25 4.8 3.4 1.9 0.2 setosa ## 26 5.0 3.0 1.6 0.2 setosa ## 27 5.0 3.4 1.6 0.4 setosa ## 28 5.2 3.5 1.5 0.2 setosa ## 29 5.2 3.4 1.4 0.2 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 31 4.8 3.1 1.6 0.2 setosa ## 32 5.4 3.4 1.5 0.4 setosa ## 33 5.2 4.1 1.5 0.1 setosa ## 34 5.5 4.2 1.4 0.2 setosa ## 35 4.9 3.1 1.5 0.2 setosa ## 36 5.0 3.2 1.2 0.2 setosa ## 37 5.5 3.5 1.3 0.2 setosa ## 38 4.9 3.6 1.4 0.1 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 40 5.1 3.4 1.5 0.2 setosa ## 41 5.0 3.5 1.3 0.3 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 44 5.0 3.5 1.6 0.6 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 47 5.1 3.8 1.6 0.2 setosa ## 48 4.6 3.2 1.4 0.2 setosa ## 49 5.3 3.7 1.5 0.2 setosa ## 50 5.0 3.3 1.4 0.2 setosa 1.1.3 Select certain columns select(iris, Sepal.Length, Species) ## Sepal.Length Species ## 1 5.1 setosa ## 2 4.9 setosa ## 3 4.7 setosa ## 4 4.6 setosa ## 5 5.0 setosa ## 6 5.4 setosa ## 7 4.6 setosa ## 8 5.0 setosa ## 9 4.4 setosa ## 10 4.9 setosa ## 11 5.4 setosa ## 12 4.8 setosa ## 13 4.8 setosa ## 14 4.3 setosa ## 15 5.8 setosa ## 16 5.7 setosa ## 17 5.4 setosa ## 18 5.1 setosa ## 19 5.7 setosa ## 20 5.1 setosa ## 21 5.4 setosa ## 22 5.1 setosa ## 23 4.6 setosa ## 24 5.1 setosa ## 25 4.8 setosa ## 26 5.0 setosa ## 27 5.0 setosa ## 28 5.2 setosa ## 29 5.2 setosa ## 30 4.7 setosa ## 31 4.8 setosa ## 32 5.4 setosa ## 33 5.2 setosa ## 34 5.5 setosa ## 35 4.9 setosa ## 36 5.0 setosa ## 37 5.5 setosa ## 38 4.9 setosa ## 39 4.4 setosa ## 40 5.1 setosa ## 41 5.0 setosa ## 42 4.5 setosa ## 43 4.4 setosa ## 44 5.0 setosa ## 45 5.1 setosa ## 46 4.8 setosa ## 47 5.1 setosa ## 48 4.6 setosa ## 49 5.3 setosa ## 50 5.0 setosa ## 51 7.0 versicolor ## 52 6.4 versicolor ## 53 6.9 versicolor ## 54 5.5 versicolor ## 55 6.5 versicolor ## 56 5.7 versicolor ## 57 6.3 versicolor ## 58 4.9 versicolor ## 59 6.6 versicolor ## 60 5.2 versicolor ## 61 5.0 versicolor ## 62 5.9 versicolor ## 63 6.0 versicolor ## 64 6.1 versicolor ## 65 5.6 versicolor ## 66 6.7 versicolor ## 67 5.6 versicolor ## 68 5.8 versicolor ## 69 6.2 versicolor ## 70 5.6 versicolor ## 71 5.9 versicolor ## 72 6.1 versicolor ## 73 6.3 versicolor ## 74 6.1 versicolor ## 75 6.4 versicolor ## 76 6.6 versicolor ## 77 6.8 versicolor ## 78 6.7 versicolor ## 79 6.0 versicolor ## 80 5.7 versicolor ## 81 5.5 versicolor ## 82 5.5 versicolor ## 83 5.8 versicolor ## 84 6.0 versicolor ## 85 5.4 versicolor ## 86 6.0 versicolor ## 87 6.7 versicolor ## 88 6.3 versicolor ## 89 5.6 versicolor ## 90 5.5 versicolor ## 91 5.5 versicolor ## 92 6.1 versicolor ## 93 5.8 versicolor ## 94 5.0 versicolor ## 95 5.6 versicolor ## 96 5.7 versicolor ## 97 5.7 versicolor ## 98 6.2 versicolor ## 99 5.1 versicolor ## 100 5.7 versicolor ## 101 6.3 virginica ## 102 5.8 virginica ## 103 7.1 virginica ## 104 6.3 virginica ## 105 6.5 virginica ## 106 7.6 virginica ## 107 4.9 virginica ## 108 7.3 virginica ## 109 6.7 virginica ## 110 7.2 virginica ## 111 6.5 virginica ## 112 6.4 virginica ## 113 6.8 virginica ## 114 5.7 virginica ## 115 5.8 virginica ## 116 6.4 virginica ## 117 6.5 virginica ## 118 7.7 virginica ## 119 7.7 virginica ## 120 6.0 virginica ## 121 6.9 virginica ## 122 5.6 virginica ## 123 7.7 virginica ## 124 6.3 virginica ## 125 6.7 virginica ## 126 7.2 virginica ## 127 6.2 virginica ## 128 6.1 virginica ## 129 6.4 virginica ## 130 7.2 virginica ## 131 7.4 virginica ## 132 7.9 virginica ## 133 6.4 virginica ## 134 6.3 virginica ## 135 6.1 virginica ## 136 7.7 virginica ## 137 6.3 virginica ## 138 6.4 virginica ## 139 6.0 virginica ## 140 6.9 virginica ## 141 6.7 virginica ## 142 6.9 virginica ## 143 5.8 virginica ## 144 6.8 virginica ## 145 6.7 virginica ## 146 6.7 virginica ## 147 6.3 virginica ## 148 6.5 virginica ## 149 6.2 virginica ## 150 5.9 virginica select(iris, -Sepal.Length, -Species) ## Sepal.Width Petal.Length Petal.Width ## 1 3.5 1.4 0.2 ## 2 3.0 1.4 0.2 ## 3 3.2 1.3 0.2 ## 4 3.1 1.5 0.2 ## 5 3.6 1.4 0.2 ## 6 3.9 1.7 0.4 ## 7 3.4 1.4 0.3 ## 8 3.4 1.5 0.2 ## 9 2.9 1.4 0.2 ## 10 3.1 1.5 0.1 ## 11 3.7 1.5 0.2 ## 12 3.4 1.6 0.2 ## 13 3.0 1.4 0.1 ## 14 3.0 1.1 0.1 ## 15 4.0 1.2 0.2 ## 16 4.4 1.5 0.4 ## 17 3.9 1.3 0.4 ## 18 3.5 1.4 0.3 ## 19 3.8 1.7 0.3 ## 20 3.8 1.5 0.3 ## 21 3.4 1.7 0.2 ## 22 3.7 1.5 0.4 ## 23 3.6 1.0 0.2 ## 24 3.3 1.7 0.5 ## 25 3.4 1.9 0.2 ## 26 3.0 1.6 0.2 ## 27 3.4 1.6 0.4 ## 28 3.5 1.5 0.2 ## 29 3.4 1.4 0.2 ## 30 3.2 1.6 0.2 ## 31 3.1 1.6 0.2 ## 32 3.4 1.5 0.4 ## 33 4.1 1.5 0.1 ## 34 4.2 1.4 0.2 ## 35 3.1 1.5 0.2 ## 36 3.2 1.2 0.2 ## 37 3.5 1.3 0.2 ## 38 3.6 1.4 0.1 ## 39 3.0 1.3 0.2 ## 40 3.4 1.5 0.2 ## 41 3.5 1.3 0.3 ## 42 2.3 1.3 0.3 ## 43 3.2 1.3 0.2 ## 44 3.5 1.6 0.6 ## 45 3.8 1.9 0.4 ## 46 3.0 1.4 0.3 ## 47 3.8 1.6 0.2 ## 48 3.2 1.4 0.2 ## 49 3.7 1.5 0.2 ## 50 3.3 1.4 0.2 ## 51 3.2 4.7 1.4 ## 52 3.2 4.5 1.5 ## 53 3.1 4.9 1.5 ## 54 2.3 4.0 1.3 ## 55 2.8 4.6 1.5 ## 56 2.8 4.5 1.3 ## 57 3.3 4.7 1.6 ## 58 2.4 3.3 1.0 ## 59 2.9 4.6 1.3 ## 60 2.7 3.9 1.4 ## 61 2.0 3.5 1.0 ## 62 3.0 4.2 1.5 ## 63 2.2 4.0 1.0 ## 64 2.9 4.7 1.4 ## 65 2.9 3.6 1.3 ## 66 3.1 4.4 1.4 ## 67 3.0 4.5 1.5 ## 68 2.7 4.1 1.0 ## 69 2.2 4.5 1.5 ## 70 2.5 3.9 1.1 ## 71 3.2 4.8 1.8 ## 72 2.8 4.0 1.3 ## 73 2.5 4.9 1.5 ## 74 2.8 4.7 1.2 ## 75 2.9 4.3 1.3 ## 76 3.0 4.4 1.4 ## 77 2.8 4.8 1.4 ## 78 3.0 5.0 1.7 ## 79 2.9 4.5 1.5 ## 80 2.6 3.5 1.0 ## 81 2.4 3.8 1.1 ## 82 2.4 3.7 1.0 ## 83 2.7 3.9 1.2 ## 84 2.7 5.1 1.6 ## 85 3.0 4.5 1.5 ## 86 3.4 4.5 1.6 ## 87 3.1 4.7 1.5 ## 88 2.3 4.4 1.3 ## 89 3.0 4.1 1.3 ## 90 2.5 4.0 1.3 ## 91 2.6 4.4 1.2 ## 92 3.0 4.6 1.4 ## 93 2.6 4.0 1.2 ## 94 2.3 3.3 1.0 ## 95 2.7 4.2 1.3 ## 96 3.0 4.2 1.2 ## 97 2.9 4.2 1.3 ## 98 2.9 4.3 1.3 ## 99 2.5 3.0 1.1 ## 100 2.8 4.1 1.3 ## 101 3.3 6.0 2.5 ## 102 2.7 5.1 1.9 ## 103 3.0 5.9 2.1 ## 104 2.9 5.6 1.8 ## 105 3.0 5.8 2.2 ## 106 3.0 6.6 2.1 ## 107 2.5 4.5 1.7 ## 108 2.9 6.3 1.8 ## 109 2.5 5.8 1.8 ## 110 3.6 6.1 2.5 ## 111 3.2 5.1 2.0 ## 112 2.7 5.3 1.9 ## 113 3.0 5.5 2.1 ## 114 2.5 5.0 2.0 ## 115 2.8 5.1 2.4 ## 116 3.2 5.3 2.3 ## 117 3.0 5.5 1.8 ## 118 3.8 6.7 2.2 ## 119 2.6 6.9 2.3 ## 120 2.2 5.0 1.5 ## 121 3.2 5.7 2.3 ## 122 2.8 4.9 2.0 ## 123 2.8 6.7 2.0 ## 124 2.7 4.9 1.8 ## 125 3.3 5.7 2.1 ## 126 3.2 6.0 1.8 ## 127 2.8 4.8 1.8 ## 128 3.0 4.9 1.8 ## 129 2.8 5.6 2.1 ## 130 3.0 5.8 1.6 ## 131 2.8 6.1 1.9 ## 132 3.8 6.4 2.0 ## 133 2.8 5.6 2.2 ## 134 2.8 5.1 1.5 ## 135 2.6 5.6 1.4 ## 136 3.0 6.1 2.3 ## 137 3.4 5.6 2.4 ## 138 3.1 5.5 1.8 ## 139 3.0 4.8 1.8 ## 140 3.1 5.4 2.1 ## 141 3.1 5.6 2.4 ## 142 3.1 5.1 2.3 ## 143 2.7 5.1 1.9 ## 144 3.2 5.9 2.3 ## 145 3.3 5.7 2.5 ## 146 3.0 5.2 2.3 ## 147 2.5 5.0 1.9 ## 148 3.0 5.2 2.0 ## 149 3.4 5.4 2.3 ## 150 3.0 5.1 1.8 1.1.4 Rename variables rename(iris, Sepal_Width= Sepal.Width, Sepal_Length= Sepal.Length ) ## Sepal_Length Sepal_Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## 11 5.4 3.7 1.5 0.2 setosa ## 12 4.8 3.4 1.6 0.2 setosa ## 13 4.8 3.0 1.4 0.1 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 15 5.8 4.0 1.2 0.2 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 17 5.4 3.9 1.3 0.4 setosa ## 18 5.1 3.5 1.4 0.3 setosa ## 19 5.7 3.8 1.7 0.3 setosa ## 20 5.1 3.8 1.5 0.3 setosa ## 21 5.4 3.4 1.7 0.2 setosa ## 22 5.1 3.7 1.5 0.4 setosa ## 23 4.6 3.6 1.0 0.2 setosa ## 24 5.1 3.3 1.7 0.5 setosa ## 25 4.8 3.4 1.9 0.2 setosa ## 26 5.0 3.0 1.6 0.2 setosa ## 27 5.0 3.4 1.6 0.4 setosa ## 28 5.2 3.5 1.5 0.2 setosa ## 29 5.2 3.4 1.4 0.2 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 31 4.8 3.1 1.6 0.2 setosa ## 32 5.4 3.4 1.5 0.4 setosa ## 33 5.2 4.1 1.5 0.1 setosa ## 34 5.5 4.2 1.4 0.2 setosa ## 35 4.9 3.1 1.5 0.2 setosa ## 36 5.0 3.2 1.2 0.2 setosa ## 37 5.5 3.5 1.3 0.2 setosa ## 38 4.9 3.6 1.4 0.1 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 40 5.1 3.4 1.5 0.2 setosa ## 41 5.0 3.5 1.3 0.3 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 44 5.0 3.5 1.6 0.6 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 47 5.1 3.8 1.6 0.2 setosa ## 48 4.6 3.2 1.4 0.2 setosa ## 49 5.3 3.7 1.5 0.2 setosa ## 50 5.0 3.3 1.4 0.2 setosa ## 51 7.0 3.2 4.7 1.4 versicolor ## 52 6.4 3.2 4.5 1.5 versicolor ## 53 6.9 3.1 4.9 1.5 versicolor ## 54 5.5 2.3 4.0 1.3 versicolor ## 55 6.5 2.8 4.6 1.5 versicolor ## 56 5.7 2.8 4.5 1.3 versicolor ## 57 6.3 3.3 4.7 1.6 versicolor ## 58 4.9 2.4 3.3 1.0 versicolor ## 59 6.6 2.9 4.6 1.3 versicolor ## 60 5.2 2.7 3.9 1.4 versicolor ## 61 5.0 2.0 3.5 1.0 versicolor ## 62 5.9 3.0 4.2 1.5 versicolor ## 63 6.0 2.2 4.0 1.0 versicolor ## 64 6.1 2.9 4.7 1.4 versicolor ## 65 5.6 2.9 3.6 1.3 versicolor ## 66 6.7 3.1 4.4 1.4 versicolor ## 67 5.6 3.0 4.5 1.5 versicolor ## 68 5.8 2.7 4.1 1.0 versicolor ## 69 6.2 2.2 4.5 1.5 versicolor ## 70 5.6 2.5 3.9 1.1 versicolor ## 71 5.9 3.2 4.8 1.8 versicolor ## 72 6.1 2.8 4.0 1.3 versicolor ## 73 6.3 2.5 4.9 1.5 versicolor ## 74 6.1 2.8 4.7 1.2 versicolor ## 75 6.4 2.9 4.3 1.3 versicolor ## 76 6.6 3.0 4.4 1.4 versicolor ## 77 6.8 2.8 4.8 1.4 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 79 6.0 2.9 4.5 1.5 versicolor ## 80 5.7 2.6 3.5 1.0 versicolor ## 81 5.5 2.4 3.8 1.1 versicolor ## 82 5.5 2.4 3.7 1.0 versicolor ## 83 5.8 2.7 3.9 1.2 versicolor ## 84 6.0 2.7 5.1 1.6 versicolor ## 85 5.4 3.0 4.5 1.5 versicolor ## 86 6.0 3.4 4.5 1.6 versicolor ## 87 6.7 3.1 4.7 1.5 versicolor ## 88 6.3 2.3 4.4 1.3 versicolor ## 89 5.6 3.0 4.1 1.3 versicolor ## 90 5.5 2.5 4.0 1.3 versicolor ## 91 5.5 2.6 4.4 1.2 versicolor ## 92 6.1 3.0 4.6 1.4 versicolor ## 93 5.8 2.6 4.0 1.2 versicolor ## 94 5.0 2.3 3.3 1.0 versicolor ## 95 5.6 2.7 4.2 1.3 versicolor ## 96 5.7 3.0 4.2 1.2 versicolor ## 97 5.7 2.9 4.2 1.3 versicolor ## 98 6.2 2.9 4.3 1.3 versicolor ## 99 5.1 2.5 3.0 1.1 versicolor ## 100 5.7 2.8 4.1 1.3 versicolor ## 101 6.3 3.3 6.0 2.5 virginica ## 102 5.8 2.7 5.1 1.9 virginica ## 103 7.1 3.0 5.9 2.1 virginica ## 104 6.3 2.9 5.6 1.8 virginica ## 105 6.5 3.0 5.8 2.2 virginica ## 106 7.6 3.0 6.6 2.1 virginica ## 107 4.9 2.5 4.5 1.7 virginica ## 108 7.3 2.9 6.3 1.8 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 110 7.2 3.6 6.1 2.5 virginica ## 111 6.5 3.2 5.1 2.0 virginica ## 112 6.4 2.7 5.3 1.9 virginica ## 113 6.8 3.0 5.5 2.1 virginica ## 114 5.7 2.5 5.0 2.0 virginica ## 115 5.8 2.8 5.1 2.4 virginica ## 116 6.4 3.2 5.3 2.3 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 120 6.0 2.2 5.0 1.5 virginica ## 121 6.9 3.2 5.7 2.3 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 124 6.3 2.7 4.9 1.8 virginica ## 125 6.7 3.3 5.7 2.1 virginica ## 126 7.2 3.2 6.0 1.8 virginica ## 127 6.2 2.8 4.8 1.8 virginica ## 128 6.1 3.0 4.9 1.8 virginica ## 129 6.4 2.8 5.6 2.1 virginica ## 130 7.2 3.0 5.8 1.6 virginica ## 131 7.4 2.8 6.1 1.9 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 133 6.4 2.8 5.6 2.2 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 virginica ## 136 7.7 3.0 6.1 2.3 virginica ## 137 6.3 3.4 5.6 2.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica ## 139 6.0 3.0 4.8 1.8 virginica ## 140 6.9 3.1 5.4 2.1 virginica ## 141 6.7 3.1 5.6 2.4 virginica ## 142 6.9 3.1 5.1 2.3 virginica ## 143 5.8 2.7 5.1 1.9 virginica ## 144 6.8 3.2 5.9 2.3 virginica ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica 1.1.5 Sorting in ascending or descending order put a minus in front of a variable for descending order arrange(iris, Petal.Length, -Petal.Width) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 4.6 3.6 1.0 0.2 setosa ## 2 4.3 3.0 1.1 0.1 setosa ## 3 5.8 4.0 1.2 0.2 setosa ## 4 5.0 3.2 1.2 0.2 setosa ## 5 5.4 3.9 1.3 0.4 setosa ## 6 5.0 3.5 1.3 0.3 setosa ## 7 4.5 2.3 1.3 0.3 setosa ## 8 4.7 3.2 1.3 0.2 setosa ## 9 5.5 3.5 1.3 0.2 setosa ## 10 4.4 3.0 1.3 0.2 setosa ## 11 4.4 3.2 1.3 0.2 setosa ## 12 4.6 3.4 1.4 0.3 setosa ## 13 5.1 3.5 1.4 0.3 setosa ## 14 4.8 3.0 1.4 0.3 setosa ## 15 5.1 3.5 1.4 0.2 setosa ## 16 4.9 3.0 1.4 0.2 setosa ## 17 5.0 3.6 1.4 0.2 setosa ## 18 4.4 2.9 1.4 0.2 setosa ## 19 5.2 3.4 1.4 0.2 setosa ## 20 5.5 4.2 1.4 0.2 setosa ## 21 4.6 3.2 1.4 0.2 setosa ## 22 5.0 3.3 1.4 0.2 setosa ## 23 4.8 3.0 1.4 0.1 setosa ## 24 4.9 3.6 1.4 0.1 setosa ## 25 5.7 4.4 1.5 0.4 setosa ## 26 5.1 3.7 1.5 0.4 setosa ## 27 5.4 3.4 1.5 0.4 setosa ## 28 5.1 3.8 1.5 0.3 setosa ## 29 4.6 3.1 1.5 0.2 setosa ## 30 5.0 3.4 1.5 0.2 setosa ## 31 5.4 3.7 1.5 0.2 setosa ## 32 5.2 3.5 1.5 0.2 setosa ## 33 4.9 3.1 1.5 0.2 setosa ## 34 5.1 3.4 1.5 0.2 setosa ## 35 5.3 3.7 1.5 0.2 setosa ## 36 4.9 3.1 1.5 0.1 setosa ## 37 5.2 4.1 1.5 0.1 setosa ## 38 5.0 3.5 1.6 0.6 setosa ## 39 5.0 3.4 1.6 0.4 setosa ## 40 4.8 3.4 1.6 0.2 setosa ## 41 5.0 3.0 1.6 0.2 setosa ## 42 4.7 3.2 1.6 0.2 setosa ## 43 4.8 3.1 1.6 0.2 setosa ## 44 5.1 3.8 1.6 0.2 setosa ## 45 5.1 3.3 1.7 0.5 setosa ## 46 5.4 3.9 1.7 0.4 setosa ## 47 5.7 3.8 1.7 0.3 setosa ## 48 5.4 3.4 1.7 0.2 setosa ## 49 5.1 3.8 1.9 0.4 setosa ## 50 4.8 3.4 1.9 0.2 setosa ## 51 5.1 2.5 3.0 1.1 versicolor ## 52 4.9 2.4 3.3 1.0 versicolor ## 53 5.0 2.3 3.3 1.0 versicolor ## 54 5.0 2.0 3.5 1.0 versicolor ## 55 5.7 2.6 3.5 1.0 versicolor ## 56 5.6 2.9 3.6 1.3 versicolor ## 57 5.5 2.4 3.7 1.0 versicolor ## 58 5.5 2.4 3.8 1.1 versicolor ## 59 5.2 2.7 3.9 1.4 versicolor ## 60 5.8 2.7 3.9 1.2 versicolor ## 61 5.6 2.5 3.9 1.1 versicolor ## 62 5.5 2.3 4.0 1.3 versicolor ## 63 6.1 2.8 4.0 1.3 versicolor ## 64 5.5 2.5 4.0 1.3 versicolor ## 65 5.8 2.6 4.0 1.2 versicolor ## 66 6.0 2.2 4.0 1.0 versicolor ## 67 5.6 3.0 4.1 1.3 versicolor ## 68 5.7 2.8 4.1 1.3 versicolor ## 69 5.8 2.7 4.1 1.0 versicolor ## 70 5.9 3.0 4.2 1.5 versicolor ## 71 5.6 2.7 4.2 1.3 versicolor ## 72 5.7 2.9 4.2 1.3 versicolor ## 73 5.7 3.0 4.2 1.2 versicolor ## 74 6.4 2.9 4.3 1.3 versicolor ## 75 6.2 2.9 4.3 1.3 versicolor ## 76 6.7 3.1 4.4 1.4 versicolor ## 77 6.6 3.0 4.4 1.4 versicolor ## 78 6.3 2.3 4.4 1.3 versicolor ## 79 5.5 2.6 4.4 1.2 versicolor ## 80 4.9 2.5 4.5 1.7 virginica ## 81 6.0 3.4 4.5 1.6 versicolor ## 82 6.4 3.2 4.5 1.5 versicolor ## 83 5.6 3.0 4.5 1.5 versicolor ## 84 6.2 2.2 4.5 1.5 versicolor ## 85 6.0 2.9 4.5 1.5 versicolor ## 86 5.4 3.0 4.5 1.5 versicolor ## 87 5.7 2.8 4.5 1.3 versicolor ## 88 6.5 2.8 4.6 1.5 versicolor ## 89 6.1 3.0 4.6 1.4 versicolor ## 90 6.6 2.9 4.6 1.3 versicolor ## 91 6.3 3.3 4.7 1.6 versicolor ## 92 6.7 3.1 4.7 1.5 versicolor ## 93 7.0 3.2 4.7 1.4 versicolor ## 94 6.1 2.9 4.7 1.4 versicolor ## 95 6.1 2.8 4.7 1.2 versicolor ## 96 5.9 3.2 4.8 1.8 versicolor ## 97 6.2 2.8 4.8 1.8 virginica ## 98 6.0 3.0 4.8 1.8 virginica ## 99 6.8 2.8 4.8 1.4 versicolor ## 100 5.6 2.8 4.9 2.0 virginica ## 101 6.3 2.7 4.9 1.8 virginica ## 102 6.1 3.0 4.9 1.8 virginica ## 103 6.9 3.1 4.9 1.5 versicolor ## 104 6.3 2.5 4.9 1.5 versicolor ## 105 5.7 2.5 5.0 2.0 virginica ## 106 6.3 2.5 5.0 1.9 virginica ## 107 6.7 3.0 5.0 1.7 versicolor ## 108 6.0 2.2 5.0 1.5 virginica ## 109 5.8 2.8 5.1 2.4 virginica ## 110 6.9 3.1 5.1 2.3 virginica ## 111 6.5 3.2 5.1 2.0 virginica ## 112 5.8 2.7 5.1 1.9 virginica ## 113 5.8 2.7 5.1 1.9 virginica ## 114 5.9 3.0 5.1 1.8 virginica ## 115 6.0 2.7 5.1 1.6 versicolor ## 116 6.3 2.8 5.1 1.5 virginica ## 117 6.7 3.0 5.2 2.3 virginica ## 118 6.5 3.0 5.2 2.0 virginica ## 119 6.4 3.2 5.3 2.3 virginica ## 120 6.4 2.7 5.3 1.9 virginica ## 121 6.2 3.4 5.4 2.3 virginica ## 122 6.9 3.1 5.4 2.1 virginica ## 123 6.8 3.0 5.5 2.1 virginica ## 124 6.5 3.0 5.5 1.8 virginica ## 125 6.4 3.1 5.5 1.8 virginica ## 126 6.3 3.4 5.6 2.4 virginica ## 127 6.7 3.1 5.6 2.4 virginica ## 128 6.4 2.8 5.6 2.2 virginica ## 129 6.4 2.8 5.6 2.1 virginica ## 130 6.3 2.9 5.6 1.8 virginica ## 131 6.1 2.6 5.6 1.4 virginica ## 132 6.7 3.3 5.7 2.5 virginica ## 133 6.9 3.2 5.7 2.3 virginica ## 134 6.7 3.3 5.7 2.1 virginica ## 135 6.5 3.0 5.8 2.2 virginica ## 136 6.7 2.5 5.8 1.8 virginica ## 137 7.2 3.0 5.8 1.6 virginica ## 138 6.8 3.2 5.9 2.3 virginica ## 139 7.1 3.0 5.9 2.1 virginica ## 140 6.3 3.3 6.0 2.5 virginica ## 141 7.2 3.2 6.0 1.8 virginica ## 142 7.2 3.6 6.1 2.5 virginica ## 143 7.7 3.0 6.1 2.3 virginica ## 144 7.4 2.8 6.1 1.9 virginica ## 145 7.3 2.9 6.3 1.8 virginica ## 146 7.9 3.8 6.4 2.0 virginica ## 147 7.6 3.0 6.6 2.1 virginica ## 148 7.7 3.8 6.7 2.2 virginica ## 149 7.7 2.8 6.7 2.0 virginica ## 150 7.7 2.6 6.9 2.3 virginica 1.1.6 Transform variables mutate(iris, newvar= Sepal.Width*10, Petal.Length=Petal.Length/100 ) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species newvar ## 1 5.1 3.5 0.014 0.2 setosa 35 ## 2 4.9 3.0 0.014 0.2 setosa 30 ## 3 4.7 3.2 0.013 0.2 setosa 32 ## 4 4.6 3.1 0.015 0.2 setosa 31 ## 5 5.0 3.6 0.014 0.2 setosa 36 ## 6 5.4 3.9 0.017 0.4 setosa 39 ## 7 4.6 3.4 0.014 0.3 setosa 34 ## 8 5.0 3.4 0.015 0.2 setosa 34 ## 9 4.4 2.9 0.014 0.2 setosa 29 ## 10 4.9 3.1 0.015 0.1 setosa 31 ## 11 5.4 3.7 0.015 0.2 setosa 37 ## 12 4.8 3.4 0.016 0.2 setosa 34 ## 13 4.8 3.0 0.014 0.1 setosa 30 ## 14 4.3 3.0 0.011 0.1 setosa 30 ## 15 5.8 4.0 0.012 0.2 setosa 40 ## 16 5.7 4.4 0.015 0.4 setosa 44 ## 17 5.4 3.9 0.013 0.4 setosa 39 ## 18 5.1 3.5 0.014 0.3 setosa 35 ## 19 5.7 3.8 0.017 0.3 setosa 38 ## 20 5.1 3.8 0.015 0.3 setosa 38 ## 21 5.4 3.4 0.017 0.2 setosa 34 ## 22 5.1 3.7 0.015 0.4 setosa 37 ## 23 4.6 3.6 0.010 0.2 setosa 36 ## 24 5.1 3.3 0.017 0.5 setosa 33 ## 25 4.8 3.4 0.019 0.2 setosa 34 ## 26 5.0 3.0 0.016 0.2 setosa 30 ## 27 5.0 3.4 0.016 0.4 setosa 34 ## 28 5.2 3.5 0.015 0.2 setosa 35 ## 29 5.2 3.4 0.014 0.2 setosa 34 ## 30 4.7 3.2 0.016 0.2 setosa 32 ## 31 4.8 3.1 0.016 0.2 setosa 31 ## 32 5.4 3.4 0.015 0.4 setosa 34 ## 33 5.2 4.1 0.015 0.1 setosa 41 ## 34 5.5 4.2 0.014 0.2 setosa 42 ## 35 4.9 3.1 0.015 0.2 setosa 31 ## 36 5.0 3.2 0.012 0.2 setosa 32 ## 37 5.5 3.5 0.013 0.2 setosa 35 ## 38 4.9 3.6 0.014 0.1 setosa 36 ## 39 4.4 3.0 0.013 0.2 setosa 30 ## 40 5.1 3.4 0.015 0.2 setosa 34 ## 41 5.0 3.5 0.013 0.3 setosa 35 ## 42 4.5 2.3 0.013 0.3 setosa 23 ## 43 4.4 3.2 0.013 0.2 setosa 32 ## 44 5.0 3.5 0.016 0.6 setosa 35 ## 45 5.1 3.8 0.019 0.4 setosa 38 ## 46 4.8 3.0 0.014 0.3 setosa 30 ## 47 5.1 3.8 0.016 0.2 setosa 38 ## 48 4.6 3.2 0.014 0.2 setosa 32 ## 49 5.3 3.7 0.015 0.2 setosa 37 ## 50 5.0 3.3 0.014 0.2 setosa 33 ## 51 7.0 3.2 0.047 1.4 versicolor 32 ## 52 6.4 3.2 0.045 1.5 versicolor 32 ## 53 6.9 3.1 0.049 1.5 versicolor 31 ## 54 5.5 2.3 0.040 1.3 versicolor 23 ## 55 6.5 2.8 0.046 1.5 versicolor 28 ## 56 5.7 2.8 0.045 1.3 versicolor 28 ## 57 6.3 3.3 0.047 1.6 versicolor 33 ## 58 4.9 2.4 0.033 1.0 versicolor 24 ## 59 6.6 2.9 0.046 1.3 versicolor 29 ## 60 5.2 2.7 0.039 1.4 versicolor 27 ## 61 5.0 2.0 0.035 1.0 versicolor 20 ## 62 5.9 3.0 0.042 1.5 versicolor 30 ## 63 6.0 2.2 0.040 1.0 versicolor 22 ## 64 6.1 2.9 0.047 1.4 versicolor 29 ## 65 5.6 2.9 0.036 1.3 versicolor 29 ## 66 6.7 3.1 0.044 1.4 versicolor 31 ## 67 5.6 3.0 0.045 1.5 versicolor 30 ## 68 5.8 2.7 0.041 1.0 versicolor 27 ## 69 6.2 2.2 0.045 1.5 versicolor 22 ## 70 5.6 2.5 0.039 1.1 versicolor 25 ## 71 5.9 3.2 0.048 1.8 versicolor 32 ## 72 6.1 2.8 0.040 1.3 versicolor 28 ## 73 6.3 2.5 0.049 1.5 versicolor 25 ## 74 6.1 2.8 0.047 1.2 versicolor 28 ## 75 6.4 2.9 0.043 1.3 versicolor 29 ## 76 6.6 3.0 0.044 1.4 versicolor 30 ## 77 6.8 2.8 0.048 1.4 versicolor 28 ## 78 6.7 3.0 0.050 1.7 versicolor 30 ## 79 6.0 2.9 0.045 1.5 versicolor 29 ## 80 5.7 2.6 0.035 1.0 versicolor 26 ## 81 5.5 2.4 0.038 1.1 versicolor 24 ## 82 5.5 2.4 0.037 1.0 versicolor 24 ## 83 5.8 2.7 0.039 1.2 versicolor 27 ## 84 6.0 2.7 0.051 1.6 versicolor 27 ## 85 5.4 3.0 0.045 1.5 versicolor 30 ## 86 6.0 3.4 0.045 1.6 versicolor 34 ## 87 6.7 3.1 0.047 1.5 versicolor 31 ## 88 6.3 2.3 0.044 1.3 versicolor 23 ## 89 5.6 3.0 0.041 1.3 versicolor 30 ## 90 5.5 2.5 0.040 1.3 versicolor 25 ## 91 5.5 2.6 0.044 1.2 versicolor 26 ## 92 6.1 3.0 0.046 1.4 versicolor 30 ## 93 5.8 2.6 0.040 1.2 versicolor 26 ## 94 5.0 2.3 0.033 1.0 versicolor 23 ## 95 5.6 2.7 0.042 1.3 versicolor 27 ## 96 5.7 3.0 0.042 1.2 versicolor 30 ## 97 5.7 2.9 0.042 1.3 versicolor 29 ## 98 6.2 2.9 0.043 1.3 versicolor 29 ## 99 5.1 2.5 0.030 1.1 versicolor 25 ## 100 5.7 2.8 0.041 1.3 versicolor 28 ## 101 6.3 3.3 0.060 2.5 virginica 33 ## 102 5.8 2.7 0.051 1.9 virginica 27 ## 103 7.1 3.0 0.059 2.1 virginica 30 ## 104 6.3 2.9 0.056 1.8 virginica 29 ## 105 6.5 3.0 0.058 2.2 virginica 30 ## 106 7.6 3.0 0.066 2.1 virginica 30 ## 107 4.9 2.5 0.045 1.7 virginica 25 ## 108 7.3 2.9 0.063 1.8 virginica 29 ## 109 6.7 2.5 0.058 1.8 virginica 25 ## 110 7.2 3.6 0.061 2.5 virginica 36 ## 111 6.5 3.2 0.051 2.0 virginica 32 ## 112 6.4 2.7 0.053 1.9 virginica 27 ## 113 6.8 3.0 0.055 2.1 virginica 30 ## 114 5.7 2.5 0.050 2.0 virginica 25 ## 115 5.8 2.8 0.051 2.4 virginica 28 ## 116 6.4 3.2 0.053 2.3 virginica 32 ## 117 6.5 3.0 0.055 1.8 virginica 30 ## 118 7.7 3.8 0.067 2.2 virginica 38 ## 119 7.7 2.6 0.069 2.3 virginica 26 ## 120 6.0 2.2 0.050 1.5 virginica 22 ## 121 6.9 3.2 0.057 2.3 virginica 32 ## 122 5.6 2.8 0.049 2.0 virginica 28 ## 123 7.7 2.8 0.067 2.0 virginica 28 ## 124 6.3 2.7 0.049 1.8 virginica 27 ## 125 6.7 3.3 0.057 2.1 virginica 33 ## 126 7.2 3.2 0.060 1.8 virginica 32 ## 127 6.2 2.8 0.048 1.8 virginica 28 ## 128 6.1 3.0 0.049 1.8 virginica 30 ## 129 6.4 2.8 0.056 2.1 virginica 28 ## 130 7.2 3.0 0.058 1.6 virginica 30 ## 131 7.4 2.8 0.061 1.9 virginica 28 ## 132 7.9 3.8 0.064 2.0 virginica 38 ## 133 6.4 2.8 0.056 2.2 virginica 28 ## 134 6.3 2.8 0.051 1.5 virginica 28 ## 135 6.1 2.6 0.056 1.4 virginica 26 ## 136 7.7 3.0 0.061 2.3 virginica 30 ## 137 6.3 3.4 0.056 2.4 virginica 34 ## 138 6.4 3.1 0.055 1.8 virginica 31 ## 139 6.0 3.0 0.048 1.8 virginica 30 ## 140 6.9 3.1 0.054 2.1 virginica 31 ## 141 6.7 3.1 0.056 2.4 virginica 31 ## 142 6.9 3.1 0.051 2.3 virginica 31 ## 143 5.8 2.7 0.051 1.9 virginica 27 ## 144 6.8 3.2 0.059 2.3 virginica 32 ## 145 6.7 3.3 0.057 2.5 virginica 33 ## 146 6.7 3.0 0.052 2.3 virginica 30 ## 147 6.3 2.5 0.050 1.9 virginica 25 ## 148 6.5 3.0 0.052 2.0 virginica 30 ## 149 6.2 3.4 0.054 2.3 virginica 34 ## 150 5.9 3.0 0.051 1.8 virginica 30 1.1.7 Working with pipes %&gt;% iris %&gt;% filter(Species==&quot;setosa&quot;) %&gt;% mutate (newvar=Sepal.Width*10) %&gt;% select (-Sepal.Width, -Petal.Width) %&gt;% arrange(-Sepal.Length, newvar) ## Sepal.Length Petal.Length Species newvar ## 1 5.8 1.2 setosa 40 ## 2 5.7 1.7 setosa 38 ## 3 5.7 1.5 setosa 44 ## 4 5.5 1.3 setosa 35 ## 5 5.5 1.4 setosa 42 ## 6 5.4 1.7 setosa 34 ## 7 5.4 1.5 setosa 34 ## 8 5.4 1.5 setosa 37 ## 9 5.4 1.7 setosa 39 ## 10 5.4 1.3 setosa 39 ## 11 5.3 1.5 setosa 37 ## 12 5.2 1.4 setosa 34 ## 13 5.2 1.5 setosa 35 ## 14 5.2 1.5 setosa 41 ## 15 5.1 1.7 setosa 33 ## 16 5.1 1.5 setosa 34 ## 17 5.1 1.4 setosa 35 ## 18 5.1 1.4 setosa 35 ## 19 5.1 1.5 setosa 37 ## 20 5.1 1.5 setosa 38 ## 21 5.1 1.9 setosa 38 ## 22 5.1 1.6 setosa 38 ## 23 5.0 1.6 setosa 30 ## 24 5.0 1.2 setosa 32 ## 25 5.0 1.4 setosa 33 ## 26 5.0 1.5 setosa 34 ## 27 5.0 1.6 setosa 34 ## 28 5.0 1.3 setosa 35 ## 29 5.0 1.6 setosa 35 ## 30 5.0 1.4 setosa 36 ## 31 4.9 1.4 setosa 30 ## 32 4.9 1.5 setosa 31 ## 33 4.9 1.5 setosa 31 ## 34 4.9 1.4 setosa 36 ## 35 4.8 1.4 setosa 30 ## 36 4.8 1.4 setosa 30 ## 37 4.8 1.6 setosa 31 ## 38 4.8 1.6 setosa 34 ## 39 4.8 1.9 setosa 34 ## 40 4.7 1.3 setosa 32 ## 41 4.7 1.6 setosa 32 ## 42 4.6 1.5 setosa 31 ## 43 4.6 1.4 setosa 32 ## 44 4.6 1.4 setosa 34 ## 45 4.6 1.0 setosa 36 ## 46 4.5 1.3 setosa 23 ## 47 4.4 1.4 setosa 29 ## 48 4.4 1.3 setosa 30 ## 49 4.4 1.3 setosa 32 ## 50 4.3 1.1 setosa 30 1.1.8 Pivot wider (long to wide) If no unique identifier row in each group doesn’t work iris %&gt;% pivot_wider( names_from=Species, values_from= c(Sepal.Length)) ## Warning: Values from `Sepal.Length` are not uniquely identified; output will contain list-cols. ## * Use `values_fn = list` to suppress this warning. ## * Use `values_fn = {summary_fun}` to summarise duplicates. ## * Use the following dplyr code to identify duplicates. ## {data} %&gt;% ## dplyr::group_by(Sepal.Width, Petal.Length, Petal.Width, Species) %&gt;% ## dplyr::summarise(n = dplyr::n(), .groups = &quot;drop&quot;) %&gt;% ## dplyr::filter(n &gt; 1L) ## # A tibble: 143 × 6 ## Sepal.Width Petal.Length Petal.Width setosa versicolor virginica ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 3.5 1.4 0.2 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 2 3 1.4 0.2 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 3 3.2 1.3 0.2 &lt;dbl [2]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 4 3.1 1.5 0.2 &lt;dbl [2]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 5 3.6 1.4 0.2 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 6 3.9 1.7 0.4 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 7 3.4 1.4 0.3 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 8 3.4 1.5 0.2 &lt;dbl [2]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 9 2.9 1.4 0.2 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 10 3.1 1.5 0.1 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## # … with 133 more rows Create a unique identifier row for each name and then use pivot_wider widedata &lt;- iris %&gt;% # create groups then assign unique identifier row number in each group group_by(Species) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from=Species, values_from= c(Petal.Length,Sepal.Length,Petal.Width,Sepal.Width)) widedata ## # A tibble: 50 × 13 ## row Petal.Length_setosa Petal.Length_ver… Petal.Length_vi… Sepal.Length_se… ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.4 4.7 6 5.1 ## 2 2 1.4 4.5 5.1 4.9 ## 3 3 1.3 4.9 5.9 4.7 ## 4 4 1.5 4 5.6 4.6 ## 5 5 1.4 4.6 5.8 5 ## 6 6 1.7 4.5 6.6 5.4 ## 7 7 1.4 4.7 4.5 4.6 ## 8 8 1.5 3.3 6.3 5 ## 9 9 1.4 4.6 5.8 4.4 ## 10 10 1.5 3.9 6.1 4.9 ## # … with 40 more rows, and 8 more variables: Sepal.Length_versicolor &lt;dbl&gt;, ## # Sepal.Length_virginica &lt;dbl&gt;, Petal.Width_setosa &lt;dbl&gt;, ## # Petal.Width_versicolor &lt;dbl&gt;, Petal.Width_virginica &lt;dbl&gt;, ## # Sepal.Width_setosa &lt;dbl&gt;, Sepal.Width_versicolor &lt;dbl&gt;, ## # Sepal.Width_virginica &lt;dbl&gt; iris %&gt;% group_by(Species) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider( names_from=Species, values_from= c(Petal.Length, Petal.Width)) ## # A tibble: 150 × 9 ## Sepal.Length Sepal.Width row Petal.Length_setosa Petal.Length_versicolor ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1 1.4 NA ## 2 4.9 3 2 1.4 NA ## 3 4.7 3.2 3 1.3 NA ## 4 4.6 3.1 4 1.5 NA ## 5 5 3.6 5 1.4 NA ## 6 5.4 3.9 6 1.7 NA ## 7 4.6 3.4 7 1.4 NA ## 8 5 3.4 8 1.5 NA ## 9 4.4 2.9 9 1.4 NA ## 10 4.9 3.1 10 1.5 NA ## # … with 140 more rows, and 4 more variables: Petal.Length_virginica &lt;dbl&gt;, ## # Petal.Width_setosa &lt;dbl&gt;, Petal.Width_versicolor &lt;dbl&gt;, ## # Petal.Width_virginica &lt;dbl&gt; 1.1.9 Pivot longer (wide to long) longdata = pivot_longer(widedata, - c( &quot;row&quot; , &quot;Petal.Length_setosa&quot; , &quot;Petal.Length_versicolor&quot;, &quot;Petal.Length_virginica&quot;, &quot;Sepal.Length_setosa&quot; , &quot;Sepal.Length_versicolor&quot;, &quot;Sepal.Length_virginica&quot; , &quot;Petal.Width_setosa&quot; , &quot;Petal.Width_versicolor&quot; , &quot;Petal.Width_virginica&quot; ) , names_to=&quot;Sepal.Width&quot;, values_to=&quot;Sepal.Width.value&quot;) longdata ## # A tibble: 150 × 12 ## row Petal.Length_setosa Petal.Length_ver… Petal.Length_vi… Sepal.Length_se… ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.4 4.7 6 5.1 ## 2 1 1.4 4.7 6 5.1 ## 3 1 1.4 4.7 6 5.1 ## 4 2 1.4 4.5 5.1 4.9 ## 5 2 1.4 4.5 5.1 4.9 ## 6 2 1.4 4.5 5.1 4.9 ## 7 3 1.3 4.9 5.9 4.7 ## 8 3 1.3 4.9 5.9 4.7 ## 9 3 1.3 4.9 5.9 4.7 ## 10 4 1.5 4 5.6 4.6 ## # … with 140 more rows, and 7 more variables: Sepal.Length_versicolor &lt;dbl&gt;, ## # Sepal.Length_virginica &lt;dbl&gt;, Petal.Width_setosa &lt;dbl&gt;, ## # Petal.Width_versicolor &lt;dbl&gt;, Petal.Width_virginica &lt;dbl&gt;, ## # Sepal.Width &lt;chr&gt;, Sepal.Width.value &lt;dbl&gt; Pivot wider again (long to wide) pivot_wider(longdata, names_from=Sepal.Width, values_from= c(Sepal.Width.value)) ## # A tibble: 50 × 13 ## row Petal.Length_setosa Petal.Length_ver… Petal.Length_vi… Sepal.Length_se… ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.4 4.7 6 5.1 ## 2 2 1.4 4.5 5.1 4.9 ## 3 3 1.3 4.9 5.9 4.7 ## 4 4 1.5 4 5.6 4.6 ## 5 5 1.4 4.6 5.8 5 ## 6 6 1.7 4.5 6.6 5.4 ## 7 7 1.4 4.7 4.5 4.6 ## 8 8 1.5 3.3 6.3 5 ## 9 9 1.4 4.6 5.8 4.4 ## 10 10 1.5 3.9 6.1 4.9 ## # … with 40 more rows, and 8 more variables: Sepal.Length_versicolor &lt;dbl&gt;, ## # Sepal.Length_virginica &lt;dbl&gt;, Petal.Width_setosa &lt;dbl&gt;, ## # Petal.Width_versicolor &lt;dbl&gt;, Petal.Width_virginica &lt;dbl&gt;, ## # Sepal.Width_setosa &lt;dbl&gt;, Sepal.Width_versicolor &lt;dbl&gt;, ## # Sepal.Width_virginica &lt;dbl&gt; 1.1.10 Separate columns separate(iris, Species, into = c(&quot;integer&quot;,&quot;decimal&quot;,&quot;third&quot;), sep=&quot;o&quot;) ## Warning: Expected 3 pieces. Missing pieces filled with `NA` in 100 rows [1, 2, ## 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...]. ## Sepal.Length Sepal.Width Petal.Length Petal.Width integer decimal third ## 1 5.1 3.5 1.4 0.2 set sa &lt;NA&gt; ## 2 4.9 3.0 1.4 0.2 set sa &lt;NA&gt; ## 3 4.7 3.2 1.3 0.2 set sa &lt;NA&gt; ## 4 4.6 3.1 1.5 0.2 set sa &lt;NA&gt; ## 5 5.0 3.6 1.4 0.2 set sa &lt;NA&gt; ## 6 5.4 3.9 1.7 0.4 set sa &lt;NA&gt; ## 7 4.6 3.4 1.4 0.3 set sa &lt;NA&gt; ## 8 5.0 3.4 1.5 0.2 set sa &lt;NA&gt; ## 9 4.4 2.9 1.4 0.2 set sa &lt;NA&gt; ## 10 4.9 3.1 1.5 0.1 set sa &lt;NA&gt; ## 11 5.4 3.7 1.5 0.2 set sa &lt;NA&gt; ## 12 4.8 3.4 1.6 0.2 set sa &lt;NA&gt; ## 13 4.8 3.0 1.4 0.1 set sa &lt;NA&gt; ## 14 4.3 3.0 1.1 0.1 set sa &lt;NA&gt; ## 15 5.8 4.0 1.2 0.2 set sa &lt;NA&gt; ## 16 5.7 4.4 1.5 0.4 set sa &lt;NA&gt; ## 17 5.4 3.9 1.3 0.4 set sa &lt;NA&gt; ## 18 5.1 3.5 1.4 0.3 set sa &lt;NA&gt; ## 19 5.7 3.8 1.7 0.3 set sa &lt;NA&gt; ## 20 5.1 3.8 1.5 0.3 set sa &lt;NA&gt; ## 21 5.4 3.4 1.7 0.2 set sa &lt;NA&gt; ## 22 5.1 3.7 1.5 0.4 set sa &lt;NA&gt; ## 23 4.6 3.6 1.0 0.2 set sa &lt;NA&gt; ## 24 5.1 3.3 1.7 0.5 set sa &lt;NA&gt; ## 25 4.8 3.4 1.9 0.2 set sa &lt;NA&gt; ## 26 5.0 3.0 1.6 0.2 set sa &lt;NA&gt; ## 27 5.0 3.4 1.6 0.4 set sa &lt;NA&gt; ## 28 5.2 3.5 1.5 0.2 set sa &lt;NA&gt; ## 29 5.2 3.4 1.4 0.2 set sa &lt;NA&gt; ## 30 4.7 3.2 1.6 0.2 set sa &lt;NA&gt; ## 31 4.8 3.1 1.6 0.2 set sa &lt;NA&gt; ## 32 5.4 3.4 1.5 0.4 set sa &lt;NA&gt; ## 33 5.2 4.1 1.5 0.1 set sa &lt;NA&gt; ## 34 5.5 4.2 1.4 0.2 set sa &lt;NA&gt; ## 35 4.9 3.1 1.5 0.2 set sa &lt;NA&gt; ## 36 5.0 3.2 1.2 0.2 set sa &lt;NA&gt; ## 37 5.5 3.5 1.3 0.2 set sa &lt;NA&gt; ## 38 4.9 3.6 1.4 0.1 set sa &lt;NA&gt; ## 39 4.4 3.0 1.3 0.2 set sa &lt;NA&gt; ## 40 5.1 3.4 1.5 0.2 set sa &lt;NA&gt; ## 41 5.0 3.5 1.3 0.3 set sa &lt;NA&gt; ## 42 4.5 2.3 1.3 0.3 set sa &lt;NA&gt; ## 43 4.4 3.2 1.3 0.2 set sa &lt;NA&gt; ## 44 5.0 3.5 1.6 0.6 set sa &lt;NA&gt; ## 45 5.1 3.8 1.9 0.4 set sa &lt;NA&gt; ## 46 4.8 3.0 1.4 0.3 set sa &lt;NA&gt; ## 47 5.1 3.8 1.6 0.2 set sa &lt;NA&gt; ## 48 4.6 3.2 1.4 0.2 set sa &lt;NA&gt; ## 49 5.3 3.7 1.5 0.2 set sa &lt;NA&gt; ## 50 5.0 3.3 1.4 0.2 set sa &lt;NA&gt; ## 51 7.0 3.2 4.7 1.4 versic l r ## 52 6.4 3.2 4.5 1.5 versic l r ## 53 6.9 3.1 4.9 1.5 versic l r ## 54 5.5 2.3 4.0 1.3 versic l r ## 55 6.5 2.8 4.6 1.5 versic l r ## 56 5.7 2.8 4.5 1.3 versic l r ## 57 6.3 3.3 4.7 1.6 versic l r ## 58 4.9 2.4 3.3 1.0 versic l r ## 59 6.6 2.9 4.6 1.3 versic l r ## 60 5.2 2.7 3.9 1.4 versic l r ## 61 5.0 2.0 3.5 1.0 versic l r ## 62 5.9 3.0 4.2 1.5 versic l r ## 63 6.0 2.2 4.0 1.0 versic l r ## 64 6.1 2.9 4.7 1.4 versic l r ## 65 5.6 2.9 3.6 1.3 versic l r ## 66 6.7 3.1 4.4 1.4 versic l r ## 67 5.6 3.0 4.5 1.5 versic l r ## 68 5.8 2.7 4.1 1.0 versic l r ## 69 6.2 2.2 4.5 1.5 versic l r ## 70 5.6 2.5 3.9 1.1 versic l r ## 71 5.9 3.2 4.8 1.8 versic l r ## 72 6.1 2.8 4.0 1.3 versic l r ## 73 6.3 2.5 4.9 1.5 versic l r ## 74 6.1 2.8 4.7 1.2 versic l r ## 75 6.4 2.9 4.3 1.3 versic l r ## 76 6.6 3.0 4.4 1.4 versic l r ## 77 6.8 2.8 4.8 1.4 versic l r ## 78 6.7 3.0 5.0 1.7 versic l r ## 79 6.0 2.9 4.5 1.5 versic l r ## 80 5.7 2.6 3.5 1.0 versic l r ## 81 5.5 2.4 3.8 1.1 versic l r ## 82 5.5 2.4 3.7 1.0 versic l r ## 83 5.8 2.7 3.9 1.2 versic l r ## 84 6.0 2.7 5.1 1.6 versic l r ## 85 5.4 3.0 4.5 1.5 versic l r ## 86 6.0 3.4 4.5 1.6 versic l r ## 87 6.7 3.1 4.7 1.5 versic l r ## 88 6.3 2.3 4.4 1.3 versic l r ## 89 5.6 3.0 4.1 1.3 versic l r ## 90 5.5 2.5 4.0 1.3 versic l r ## 91 5.5 2.6 4.4 1.2 versic l r ## 92 6.1 3.0 4.6 1.4 versic l r ## 93 5.8 2.6 4.0 1.2 versic l r ## 94 5.0 2.3 3.3 1.0 versic l r ## 95 5.6 2.7 4.2 1.3 versic l r ## 96 5.7 3.0 4.2 1.2 versic l r ## 97 5.7 2.9 4.2 1.3 versic l r ## 98 6.2 2.9 4.3 1.3 versic l r ## 99 5.1 2.5 3.0 1.1 versic l r ## 100 5.7 2.8 4.1 1.3 versic l r ## 101 6.3 3.3 6.0 2.5 virginica &lt;NA&gt; &lt;NA&gt; ## 102 5.8 2.7 5.1 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 103 7.1 3.0 5.9 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 104 6.3 2.9 5.6 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 105 6.5 3.0 5.8 2.2 virginica &lt;NA&gt; &lt;NA&gt; ## 106 7.6 3.0 6.6 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 107 4.9 2.5 4.5 1.7 virginica &lt;NA&gt; &lt;NA&gt; ## 108 7.3 2.9 6.3 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 109 6.7 2.5 5.8 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 110 7.2 3.6 6.1 2.5 virginica &lt;NA&gt; &lt;NA&gt; ## 111 6.5 3.2 5.1 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 112 6.4 2.7 5.3 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 113 6.8 3.0 5.5 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 114 5.7 2.5 5.0 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 115 5.8 2.8 5.1 2.4 virginica &lt;NA&gt; &lt;NA&gt; ## 116 6.4 3.2 5.3 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 117 6.5 3.0 5.5 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 118 7.7 3.8 6.7 2.2 virginica &lt;NA&gt; &lt;NA&gt; ## 119 7.7 2.6 6.9 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 120 6.0 2.2 5.0 1.5 virginica &lt;NA&gt; &lt;NA&gt; ## 121 6.9 3.2 5.7 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 122 5.6 2.8 4.9 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 123 7.7 2.8 6.7 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 124 6.3 2.7 4.9 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 125 6.7 3.3 5.7 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 126 7.2 3.2 6.0 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 127 6.2 2.8 4.8 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 128 6.1 3.0 4.9 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 129 6.4 2.8 5.6 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 130 7.2 3.0 5.8 1.6 virginica &lt;NA&gt; &lt;NA&gt; ## 131 7.4 2.8 6.1 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 132 7.9 3.8 6.4 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 133 6.4 2.8 5.6 2.2 virginica &lt;NA&gt; &lt;NA&gt; ## 134 6.3 2.8 5.1 1.5 virginica &lt;NA&gt; &lt;NA&gt; ## 135 6.1 2.6 5.6 1.4 virginica &lt;NA&gt; &lt;NA&gt; ## 136 7.7 3.0 6.1 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 137 6.3 3.4 5.6 2.4 virginica &lt;NA&gt; &lt;NA&gt; ## 138 6.4 3.1 5.5 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 139 6.0 3.0 4.8 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 140 6.9 3.1 5.4 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 141 6.7 3.1 5.6 2.4 virginica &lt;NA&gt; &lt;NA&gt; ## 142 6.9 3.1 5.1 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 143 5.8 2.7 5.1 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 144 6.8 3.2 5.9 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 145 6.7 3.3 5.7 2.5 virginica &lt;NA&gt; &lt;NA&gt; ## 146 6.7 3.0 5.2 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 147 6.3 2.5 5.0 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 148 6.5 3.0 5.2 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 149 6.2 3.4 5.4 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 150 5.9 3.0 5.1 1.8 virginica &lt;NA&gt; &lt;NA&gt; 1.1.11 Recode/relabel data mutate(iris, Species2 = recode(Species, &quot;setosa&quot;=&quot;seto&quot;, &quot;versicolor&quot;=&quot;versi&quot;)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Species2 ## 1 5.1 3.5 1.4 0.2 setosa seto ## 2 4.9 3.0 1.4 0.2 setosa seto ## 3 4.7 3.2 1.3 0.2 setosa seto ## 4 4.6 3.1 1.5 0.2 setosa seto ## 5 5.0 3.6 1.4 0.2 setosa seto ## 6 5.4 3.9 1.7 0.4 setosa seto ## 7 4.6 3.4 1.4 0.3 setosa seto ## 8 5.0 3.4 1.5 0.2 setosa seto ## 9 4.4 2.9 1.4 0.2 setosa seto ## 10 4.9 3.1 1.5 0.1 setosa seto ## 11 5.4 3.7 1.5 0.2 setosa seto ## 12 4.8 3.4 1.6 0.2 setosa seto ## 13 4.8 3.0 1.4 0.1 setosa seto ## 14 4.3 3.0 1.1 0.1 setosa seto ## 15 5.8 4.0 1.2 0.2 setosa seto ## 16 5.7 4.4 1.5 0.4 setosa seto ## 17 5.4 3.9 1.3 0.4 setosa seto ## 18 5.1 3.5 1.4 0.3 setosa seto ## 19 5.7 3.8 1.7 0.3 setosa seto ## 20 5.1 3.8 1.5 0.3 setosa seto ## 21 5.4 3.4 1.7 0.2 setosa seto ## 22 5.1 3.7 1.5 0.4 setosa seto ## 23 4.6 3.6 1.0 0.2 setosa seto ## 24 5.1 3.3 1.7 0.5 setosa seto ## 25 4.8 3.4 1.9 0.2 setosa seto ## 26 5.0 3.0 1.6 0.2 setosa seto ## 27 5.0 3.4 1.6 0.4 setosa seto ## 28 5.2 3.5 1.5 0.2 setosa seto ## 29 5.2 3.4 1.4 0.2 setosa seto ## 30 4.7 3.2 1.6 0.2 setosa seto ## 31 4.8 3.1 1.6 0.2 setosa seto ## 32 5.4 3.4 1.5 0.4 setosa seto ## 33 5.2 4.1 1.5 0.1 setosa seto ## 34 5.5 4.2 1.4 0.2 setosa seto ## 35 4.9 3.1 1.5 0.2 setosa seto ## 36 5.0 3.2 1.2 0.2 setosa seto ## 37 5.5 3.5 1.3 0.2 setosa seto ## 38 4.9 3.6 1.4 0.1 setosa seto ## 39 4.4 3.0 1.3 0.2 setosa seto ## 40 5.1 3.4 1.5 0.2 setosa seto ## 41 5.0 3.5 1.3 0.3 setosa seto ## 42 4.5 2.3 1.3 0.3 setosa seto ## 43 4.4 3.2 1.3 0.2 setosa seto ## 44 5.0 3.5 1.6 0.6 setosa seto ## 45 5.1 3.8 1.9 0.4 setosa seto ## 46 4.8 3.0 1.4 0.3 setosa seto ## 47 5.1 3.8 1.6 0.2 setosa seto ## 48 4.6 3.2 1.4 0.2 setosa seto ## 49 5.3 3.7 1.5 0.2 setosa seto ## 50 5.0 3.3 1.4 0.2 setosa seto ## 51 7.0 3.2 4.7 1.4 versicolor versi ## 52 6.4 3.2 4.5 1.5 versicolor versi ## 53 6.9 3.1 4.9 1.5 versicolor versi ## 54 5.5 2.3 4.0 1.3 versicolor versi ## 55 6.5 2.8 4.6 1.5 versicolor versi ## 56 5.7 2.8 4.5 1.3 versicolor versi ## 57 6.3 3.3 4.7 1.6 versicolor versi ## 58 4.9 2.4 3.3 1.0 versicolor versi ## 59 6.6 2.9 4.6 1.3 versicolor versi ## 60 5.2 2.7 3.9 1.4 versicolor versi ## 61 5.0 2.0 3.5 1.0 versicolor versi ## 62 5.9 3.0 4.2 1.5 versicolor versi ## 63 6.0 2.2 4.0 1.0 versicolor versi ## 64 6.1 2.9 4.7 1.4 versicolor versi ## 65 5.6 2.9 3.6 1.3 versicolor versi ## 66 6.7 3.1 4.4 1.4 versicolor versi ## 67 5.6 3.0 4.5 1.5 versicolor versi ## 68 5.8 2.7 4.1 1.0 versicolor versi ## 69 6.2 2.2 4.5 1.5 versicolor versi ## 70 5.6 2.5 3.9 1.1 versicolor versi ## 71 5.9 3.2 4.8 1.8 versicolor versi ## 72 6.1 2.8 4.0 1.3 versicolor versi ## 73 6.3 2.5 4.9 1.5 versicolor versi ## 74 6.1 2.8 4.7 1.2 versicolor versi ## 75 6.4 2.9 4.3 1.3 versicolor versi ## 76 6.6 3.0 4.4 1.4 versicolor versi ## 77 6.8 2.8 4.8 1.4 versicolor versi ## 78 6.7 3.0 5.0 1.7 versicolor versi ## 79 6.0 2.9 4.5 1.5 versicolor versi ## 80 5.7 2.6 3.5 1.0 versicolor versi ## 81 5.5 2.4 3.8 1.1 versicolor versi ## 82 5.5 2.4 3.7 1.0 versicolor versi ## 83 5.8 2.7 3.9 1.2 versicolor versi ## 84 6.0 2.7 5.1 1.6 versicolor versi ## 85 5.4 3.0 4.5 1.5 versicolor versi ## 86 6.0 3.4 4.5 1.6 versicolor versi ## 87 6.7 3.1 4.7 1.5 versicolor versi ## 88 6.3 2.3 4.4 1.3 versicolor versi ## 89 5.6 3.0 4.1 1.3 versicolor versi ## 90 5.5 2.5 4.0 1.3 versicolor versi ## 91 5.5 2.6 4.4 1.2 versicolor versi ## 92 6.1 3.0 4.6 1.4 versicolor versi ## 93 5.8 2.6 4.0 1.2 versicolor versi ## 94 5.0 2.3 3.3 1.0 versicolor versi ## 95 5.6 2.7 4.2 1.3 versicolor versi ## 96 5.7 3.0 4.2 1.2 versicolor versi ## 97 5.7 2.9 4.2 1.3 versicolor versi ## 98 6.2 2.9 4.3 1.3 versicolor versi ## 99 5.1 2.5 3.0 1.1 versicolor versi ## 100 5.7 2.8 4.1 1.3 versicolor versi ## 101 6.3 3.3 6.0 2.5 virginica virginica ## 102 5.8 2.7 5.1 1.9 virginica virginica ## 103 7.1 3.0 5.9 2.1 virginica virginica ## 104 6.3 2.9 5.6 1.8 virginica virginica ## 105 6.5 3.0 5.8 2.2 virginica virginica ## 106 7.6 3.0 6.6 2.1 virginica virginica ## 107 4.9 2.5 4.5 1.7 virginica virginica ## 108 7.3 2.9 6.3 1.8 virginica virginica ## 109 6.7 2.5 5.8 1.8 virginica virginica ## 110 7.2 3.6 6.1 2.5 virginica virginica ## 111 6.5 3.2 5.1 2.0 virginica virginica ## 112 6.4 2.7 5.3 1.9 virginica virginica ## 113 6.8 3.0 5.5 2.1 virginica virginica ## 114 5.7 2.5 5.0 2.0 virginica virginica ## 115 5.8 2.8 5.1 2.4 virginica virginica ## 116 6.4 3.2 5.3 2.3 virginica virginica ## 117 6.5 3.0 5.5 1.8 virginica virginica ## 118 7.7 3.8 6.7 2.2 virginica virginica ## 119 7.7 2.6 6.9 2.3 virginica virginica ## 120 6.0 2.2 5.0 1.5 virginica virginica ## 121 6.9 3.2 5.7 2.3 virginica virginica ## 122 5.6 2.8 4.9 2.0 virginica virginica ## 123 7.7 2.8 6.7 2.0 virginica virginica ## 124 6.3 2.7 4.9 1.8 virginica virginica ## 125 6.7 3.3 5.7 2.1 virginica virginica ## 126 7.2 3.2 6.0 1.8 virginica virginica ## 127 6.2 2.8 4.8 1.8 virginica virginica ## 128 6.1 3.0 4.9 1.8 virginica virginica ## 129 6.4 2.8 5.6 2.1 virginica virginica ## 130 7.2 3.0 5.8 1.6 virginica virginica ## 131 7.4 2.8 6.1 1.9 virginica virginica ## 132 7.9 3.8 6.4 2.0 virginica virginica ## 133 6.4 2.8 5.6 2.2 virginica virginica ## 134 6.3 2.8 5.1 1.5 virginica virginica ## 135 6.1 2.6 5.6 1.4 virginica virginica ## 136 7.7 3.0 6.1 2.3 virginica virginica ## 137 6.3 3.4 5.6 2.4 virginica virginica ## 138 6.4 3.1 5.5 1.8 virginica virginica ## 139 6.0 3.0 4.8 1.8 virginica virginica ## 140 6.9 3.1 5.4 2.1 virginica virginica ## 141 6.7 3.1 5.6 2.4 virginica virginica ## 142 6.9 3.1 5.1 2.3 virginica virginica ## 143 5.8 2.7 5.1 1.9 virginica virginica ## 144 6.8 3.2 5.9 2.3 virginica virginica ## 145 6.7 3.3 5.7 2.5 virginica virginica ## 146 6.7 3.0 5.2 2.3 virginica virginica ## 147 6.3 2.5 5.0 1.9 virginica virginica ## 148 6.5 3.0 5.2 2.0 virginica virginica ## 149 6.2 3.4 5.4 2.3 virginica virginica ## 150 5.9 3.0 5.1 1.8 virginica virginica 1.1.12 Combine data sets prepare data sets data1 &lt;- data.frame(ID = 1:4, X1 = c(&quot;a1&quot;, &quot;a2&quot;,&quot;a3&quot;, &quot;a4&quot;), stringsAsFactors = FALSE) data2 &lt;- data.frame(ID = 2:5, X2 = c(&quot;b1&quot;, &quot;b2&quot;,&quot;b3&quot;, &quot;b4&quot;), stringsAsFactors = FALSE) inner join inner_join(data1, data2, by = &quot;ID&quot;) ## ID X1 X2 ## 1 2 a2 b1 ## 2 3 a3 b2 ## 3 4 a4 b3 left join left_join(data1, data2, by = &quot;ID&quot;) ## ID X1 X2 ## 1 1 a1 &lt;NA&gt; ## 2 2 a2 b1 ## 3 3 a3 b2 ## 4 4 a4 b3 right join right_join(data1, data2, by = &quot;ID&quot;) ## ID X1 X2 ## 1 2 a2 b1 ## 2 3 a3 b2 ## 3 4 a4 b3 ## 4 5 &lt;NA&gt; b4 full join full_join(data1, data2, by = &quot;ID&quot;) ## ID X1 X2 ## 1 1 a1 &lt;NA&gt; ## 2 2 a2 b1 ## 3 3 a3 b2 ## 4 4 a4 b3 ## 5 5 &lt;NA&gt; b4 keep cases of left data table without in right data table anti_join(data1, data2, by = &quot;ID&quot;) ## ID X1 ## 1 1 a1 keep cases of left data table in right data table semi_join(data1, data2, by = &quot;ID&quot;) ## ID X1 ## 1 2 a2 ## 2 3 a3 ## 3 4 a4 multiple full join full_join(data1, data2, by = &quot;ID&quot;) %&gt;% full_join(., data2, by = &quot;ID&quot;) ## ID X1 X2.x X2.y ## 1 1 a1 &lt;NA&gt; &lt;NA&gt; ## 2 2 a2 b1 b1 ## 3 3 a3 b2 b2 ## 4 4 a4 b3 b3 ## 5 5 &lt;NA&gt; b4 b4 rbind doesn’t work # df1 &lt;- data.frame(col1 = LETTERS[1:6], # col2a = c(5:10), # col3a = TRUE) # # df2 &lt;- data.frame(col1 = LETTERS[4:8], # col2b= c(4:8), # col3b = FALSE) # rbind(df1,df2) append two data tables by using join and merge data_frame1 &lt;- data.frame(col1 = c(6:8), col2 = letters[1:3], col3 = c(1,4,NA)) data_frame2 &lt;- data.frame(col1 = c(5:6), col5 = letters[7:8]) data_frame_merge &lt;- merge(data_frame1, data_frame2, by = &#39;col1&#39;, all = TRUE) print (data_frame_merge) ## col1 col2 col3 col5 ## 1 5 &lt;NA&gt; NA g ## 2 6 a 1 h ## 3 7 b 4 &lt;NA&gt; ## 4 8 c NA &lt;NA&gt; full_join(data_frame1,data_frame2, by=c(&quot;col1&quot;),) ## col1 col2 col3 col5 ## 1 6 a 1 h ## 2 7 b 4 &lt;NA&gt; ## 3 8 c NA &lt;NA&gt; ## 4 5 &lt;NA&gt; NA g 1.2 How to do aggregation/ summarization 1.2.1 Summarization after grouping library(tidyverse) iris %&gt;% group_by(Species) %&gt;% summarize(Support = mean(Sepal.Length)) %&gt;% # average arrange(-Support) # sort ## # A tibble: 3 × 2 ## Species Support ## &lt;fct&gt; &lt;dbl&gt; ## 1 virginica 6.59 ## 2 versicolor 5.94 ## 3 setosa 5.01 iris %&gt;% group_by(Species) %&gt;% summarize(mean_s = mean(Sepal.Width), meas_p = mean(Petal.Length), diff = mean(Sepal.Width-Petal.Length)) %&gt;% arrange(-diff) ## # A tibble: 3 × 4 ## Species mean_s meas_p diff ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 3.43 1.46 1.97 ## 2 versicolor 2.77 4.26 -1.49 ## 3 virginica 2.97 5.55 -2.58 iris %&gt;% group_by(Species) %&gt;% summarize(n = n(), meas_p = mean(Petal.Length), sd = sd(Petal.Length)) ## # A tibble: 3 × 4 ## Species n meas_p sd ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 50 1.46 0.174 ## 2 versicolor 50 4.26 0.470 ## 3 virginica 50 5.55 0.552 1.2.2 Summarization with upgroup iris %&gt;% ungroup( ) %&gt;% summarize(n = n(), meas_p = mean(Petal.Length), sd = sd(Petal.Length)) ## n meas_p sd ## 1 150 3.758 1.765298 1.2.3 Mutate new variables after grouping iris %&gt;% group_by(Species) %&gt;% mutate(n = n(), meas_p = mean(Petal.Length), sd = sd(Petal.Length)) ## # A tibble: 150 × 8 ## # Groups: Species [3] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species n meas_p sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 setosa 50 1.46 0.174 ## 2 4.9 3 1.4 0.2 setosa 50 1.46 0.174 ## 3 4.7 3.2 1.3 0.2 setosa 50 1.46 0.174 ## 4 4.6 3.1 1.5 0.2 setosa 50 1.46 0.174 ## 5 5 3.6 1.4 0.2 setosa 50 1.46 0.174 ## 6 5.4 3.9 1.7 0.4 setosa 50 1.46 0.174 ## 7 4.6 3.4 1.4 0.3 setosa 50 1.46 0.174 ## 8 5 3.4 1.5 0.2 setosa 50 1.46 0.174 ## 9 4.4 2.9 1.4 0.2 setosa 50 1.46 0.174 ## 10 4.9 3.1 1.5 0.1 setosa 50 1.46 0.174 ## # … with 140 more rows iris %&gt;% group_by(Species) %&gt;% mutate(n = n(), meas_p = mean(Petal.Length, na.rm = T), sd = sd(Petal.Length)) %&gt;% summarize (n_mean = paste (&quot;sample size:&quot;,mean(n)), meas_p = mean(Petal.Length), sd = sd(Petal.Length)) ## # A tibble: 3 × 4 ## Species n_mean meas_p sd ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa sample size: 50 1.46 0.174 ## 2 versicolor sample size: 50 4.26 0.470 ## 3 virginica sample size: 50 5.55 0.552 1.2.4 Recode and generate new variables, then value label irisifelse &lt;- iris %&gt;% mutate(Species2 = ifelse(Species == &quot;setosa&quot;, NA, Species)) # relabel values irisifelse$Species2 &lt;- factor(irisifelse$Species2,labels = c( &quot;versi&quot;,&quot;virg&quot;)) irisifelse ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Species2 ## 1 5.1 3.5 1.4 0.2 setosa &lt;NA&gt; ## 2 4.9 3.0 1.4 0.2 setosa &lt;NA&gt; ## 3 4.7 3.2 1.3 0.2 setosa &lt;NA&gt; ## 4 4.6 3.1 1.5 0.2 setosa &lt;NA&gt; ## 5 5.0 3.6 1.4 0.2 setosa &lt;NA&gt; ## 6 5.4 3.9 1.7 0.4 setosa &lt;NA&gt; ## 7 4.6 3.4 1.4 0.3 setosa &lt;NA&gt; ## 8 5.0 3.4 1.5 0.2 setosa &lt;NA&gt; ## 9 4.4 2.9 1.4 0.2 setosa &lt;NA&gt; ## 10 4.9 3.1 1.5 0.1 setosa &lt;NA&gt; ## 11 5.4 3.7 1.5 0.2 setosa &lt;NA&gt; ## 12 4.8 3.4 1.6 0.2 setosa &lt;NA&gt; ## 13 4.8 3.0 1.4 0.1 setosa &lt;NA&gt; ## 14 4.3 3.0 1.1 0.1 setosa &lt;NA&gt; ## 15 5.8 4.0 1.2 0.2 setosa &lt;NA&gt; ## 16 5.7 4.4 1.5 0.4 setosa &lt;NA&gt; ## 17 5.4 3.9 1.3 0.4 setosa &lt;NA&gt; ## 18 5.1 3.5 1.4 0.3 setosa &lt;NA&gt; ## 19 5.7 3.8 1.7 0.3 setosa &lt;NA&gt; ## 20 5.1 3.8 1.5 0.3 setosa &lt;NA&gt; ## 21 5.4 3.4 1.7 0.2 setosa &lt;NA&gt; ## 22 5.1 3.7 1.5 0.4 setosa &lt;NA&gt; ## 23 4.6 3.6 1.0 0.2 setosa &lt;NA&gt; ## 24 5.1 3.3 1.7 0.5 setosa &lt;NA&gt; ## 25 4.8 3.4 1.9 0.2 setosa &lt;NA&gt; ## 26 5.0 3.0 1.6 0.2 setosa &lt;NA&gt; ## 27 5.0 3.4 1.6 0.4 setosa &lt;NA&gt; ## 28 5.2 3.5 1.5 0.2 setosa &lt;NA&gt; ## 29 5.2 3.4 1.4 0.2 setosa &lt;NA&gt; ## 30 4.7 3.2 1.6 0.2 setosa &lt;NA&gt; ## 31 4.8 3.1 1.6 0.2 setosa &lt;NA&gt; ## 32 5.4 3.4 1.5 0.4 setosa &lt;NA&gt; ## 33 5.2 4.1 1.5 0.1 setosa &lt;NA&gt; ## 34 5.5 4.2 1.4 0.2 setosa &lt;NA&gt; ## 35 4.9 3.1 1.5 0.2 setosa &lt;NA&gt; ## 36 5.0 3.2 1.2 0.2 setosa &lt;NA&gt; ## 37 5.5 3.5 1.3 0.2 setosa &lt;NA&gt; ## 38 4.9 3.6 1.4 0.1 setosa &lt;NA&gt; ## 39 4.4 3.0 1.3 0.2 setosa &lt;NA&gt; ## 40 5.1 3.4 1.5 0.2 setosa &lt;NA&gt; ## 41 5.0 3.5 1.3 0.3 setosa &lt;NA&gt; ## 42 4.5 2.3 1.3 0.3 setosa &lt;NA&gt; ## 43 4.4 3.2 1.3 0.2 setosa &lt;NA&gt; ## 44 5.0 3.5 1.6 0.6 setosa &lt;NA&gt; ## 45 5.1 3.8 1.9 0.4 setosa &lt;NA&gt; ## 46 4.8 3.0 1.4 0.3 setosa &lt;NA&gt; ## 47 5.1 3.8 1.6 0.2 setosa &lt;NA&gt; ## 48 4.6 3.2 1.4 0.2 setosa &lt;NA&gt; ## 49 5.3 3.7 1.5 0.2 setosa &lt;NA&gt; ## 50 5.0 3.3 1.4 0.2 setosa &lt;NA&gt; ## 51 7.0 3.2 4.7 1.4 versicolor versi ## 52 6.4 3.2 4.5 1.5 versicolor versi ## 53 6.9 3.1 4.9 1.5 versicolor versi ## 54 5.5 2.3 4.0 1.3 versicolor versi ## 55 6.5 2.8 4.6 1.5 versicolor versi ## 56 5.7 2.8 4.5 1.3 versicolor versi ## 57 6.3 3.3 4.7 1.6 versicolor versi ## 58 4.9 2.4 3.3 1.0 versicolor versi ## 59 6.6 2.9 4.6 1.3 versicolor versi ## 60 5.2 2.7 3.9 1.4 versicolor versi ## 61 5.0 2.0 3.5 1.0 versicolor versi ## 62 5.9 3.0 4.2 1.5 versicolor versi ## 63 6.0 2.2 4.0 1.0 versicolor versi ## 64 6.1 2.9 4.7 1.4 versicolor versi ## 65 5.6 2.9 3.6 1.3 versicolor versi ## 66 6.7 3.1 4.4 1.4 versicolor versi ## 67 5.6 3.0 4.5 1.5 versicolor versi ## 68 5.8 2.7 4.1 1.0 versicolor versi ## 69 6.2 2.2 4.5 1.5 versicolor versi ## 70 5.6 2.5 3.9 1.1 versicolor versi ## 71 5.9 3.2 4.8 1.8 versicolor versi ## 72 6.1 2.8 4.0 1.3 versicolor versi ## 73 6.3 2.5 4.9 1.5 versicolor versi ## 74 6.1 2.8 4.7 1.2 versicolor versi ## 75 6.4 2.9 4.3 1.3 versicolor versi ## 76 6.6 3.0 4.4 1.4 versicolor versi ## 77 6.8 2.8 4.8 1.4 versicolor versi ## 78 6.7 3.0 5.0 1.7 versicolor versi ## 79 6.0 2.9 4.5 1.5 versicolor versi ## 80 5.7 2.6 3.5 1.0 versicolor versi ## 81 5.5 2.4 3.8 1.1 versicolor versi ## 82 5.5 2.4 3.7 1.0 versicolor versi ## 83 5.8 2.7 3.9 1.2 versicolor versi ## 84 6.0 2.7 5.1 1.6 versicolor versi ## 85 5.4 3.0 4.5 1.5 versicolor versi ## 86 6.0 3.4 4.5 1.6 versicolor versi ## 87 6.7 3.1 4.7 1.5 versicolor versi ## 88 6.3 2.3 4.4 1.3 versicolor versi ## 89 5.6 3.0 4.1 1.3 versicolor versi ## 90 5.5 2.5 4.0 1.3 versicolor versi ## 91 5.5 2.6 4.4 1.2 versicolor versi ## 92 6.1 3.0 4.6 1.4 versicolor versi ## 93 5.8 2.6 4.0 1.2 versicolor versi ## 94 5.0 2.3 3.3 1.0 versicolor versi ## 95 5.6 2.7 4.2 1.3 versicolor versi ## 96 5.7 3.0 4.2 1.2 versicolor versi ## 97 5.7 2.9 4.2 1.3 versicolor versi ## 98 6.2 2.9 4.3 1.3 versicolor versi ## 99 5.1 2.5 3.0 1.1 versicolor versi ## 100 5.7 2.8 4.1 1.3 versicolor versi ## 101 6.3 3.3 6.0 2.5 virginica virg ## 102 5.8 2.7 5.1 1.9 virginica virg ## 103 7.1 3.0 5.9 2.1 virginica virg ## 104 6.3 2.9 5.6 1.8 virginica virg ## 105 6.5 3.0 5.8 2.2 virginica virg ## 106 7.6 3.0 6.6 2.1 virginica virg ## 107 4.9 2.5 4.5 1.7 virginica virg ## 108 7.3 2.9 6.3 1.8 virginica virg ## 109 6.7 2.5 5.8 1.8 virginica virg ## 110 7.2 3.6 6.1 2.5 virginica virg ## 111 6.5 3.2 5.1 2.0 virginica virg ## 112 6.4 2.7 5.3 1.9 virginica virg ## 113 6.8 3.0 5.5 2.1 virginica virg ## 114 5.7 2.5 5.0 2.0 virginica virg ## 115 5.8 2.8 5.1 2.4 virginica virg ## 116 6.4 3.2 5.3 2.3 virginica virg ## 117 6.5 3.0 5.5 1.8 virginica virg ## 118 7.7 3.8 6.7 2.2 virginica virg ## 119 7.7 2.6 6.9 2.3 virginica virg ## 120 6.0 2.2 5.0 1.5 virginica virg ## 121 6.9 3.2 5.7 2.3 virginica virg ## 122 5.6 2.8 4.9 2.0 virginica virg ## 123 7.7 2.8 6.7 2.0 virginica virg ## 124 6.3 2.7 4.9 1.8 virginica virg ## 125 6.7 3.3 5.7 2.1 virginica virg ## 126 7.2 3.2 6.0 1.8 virginica virg ## 127 6.2 2.8 4.8 1.8 virginica virg ## 128 6.1 3.0 4.9 1.8 virginica virg ## 129 6.4 2.8 5.6 2.1 virginica virg ## 130 7.2 3.0 5.8 1.6 virginica virg ## 131 7.4 2.8 6.1 1.9 virginica virg ## 132 7.9 3.8 6.4 2.0 virginica virg ## 133 6.4 2.8 5.6 2.2 virginica virg ## 134 6.3 2.8 5.1 1.5 virginica virg ## 135 6.1 2.6 5.6 1.4 virginica virg ## 136 7.7 3.0 6.1 2.3 virginica virg ## 137 6.3 3.4 5.6 2.4 virginica virg ## 138 6.4 3.1 5.5 1.8 virginica virg ## 139 6.0 3.0 4.8 1.8 virginica virg ## 140 6.9 3.1 5.4 2.1 virginica virg ## 141 6.7 3.1 5.6 2.4 virginica virg ## 142 6.9 3.1 5.1 2.3 virginica virg ## 143 5.8 2.7 5.1 1.9 virginica virg ## 144 6.8 3.2 5.9 2.3 virginica virg ## 145 6.7 3.3 5.7 2.5 virginica virg ## 146 6.7 3.0 5.2 2.3 virginica virg ## 147 6.3 2.5 5.0 1.9 virginica virg ## 148 6.5 3.0 5.2 2.0 virginica virg ## 149 6.2 3.4 5.4 2.3 virginica virg ## 150 5.9 3.0 5.1 1.8 virginica virg str(irisifelse) ## &#39;data.frame&#39;: 150 obs. of 6 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Species2 : Factor w/ 2 levels &quot;versi&quot;,&quot;virg&quot;: NA NA NA NA NA NA NA NA NA NA ... "],["machine-learning.html", "Chapter 2 Machine learning 2.1 Machine learning workflow 2.2 KNN Classifier 2.3 KNN regression", " Chapter 2 Machine learning 2.1 Machine learning workflow 2.1.1 Loading packages and datasets # load the Pima Indians dataset from the mlbench dataset library(mlbench) data(PimaIndiansDiabetes) # rename dataset to have shorter name because lazy diabetes &lt;- PimaIndiansDiabetes look at the data set # install.packages(c(&#39;caret&#39;, &#39;skimr&#39;, &#39;RANN&#39;, &#39;randomForest&#39;, &#39;fastAdaboost&#39;, &#39;gbm&#39;, &#39;xgboost&#39;, &#39;caretEnsemble&#39;, &#39;C50&#39;, &#39;earth&#39;)) # Load the caret package library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift # Structure of the dataframe str(diabetes) ## &#39;data.frame&#39;: 768 obs. of 9 variables: ## $ pregnant: num 6 1 8 1 0 5 3 10 2 8 ... ## $ glucose : num 148 85 183 89 137 116 78 115 197 125 ... ## $ pressure: num 72 66 64 66 40 74 50 0 70 96 ... ## $ triceps : num 35 29 0 23 35 0 32 0 45 0 ... ## $ insulin : num 0 0 0 94 168 0 88 0 543 0 ... ## $ mass : num 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ... ## $ pedigree: num 0.627 0.351 0.672 0.167 2.288 ... ## $ age : num 50 31 32 21 33 30 26 29 53 54 ... ## $ diabetes: Factor w/ 2 levels &quot;neg&quot;,&quot;pos&quot;: 2 1 2 1 2 1 2 1 2 2 ... # See top 6 rows head(diabetes ) ## pregnant glucose pressure triceps insulin mass pedigree age diabetes ## 1 6 148 72 35 0 33.6 0.627 50 pos ## 2 1 85 66 29 0 26.6 0.351 31 neg ## 3 8 183 64 0 0 23.3 0.672 32 pos ## 4 1 89 66 23 94 28.1 0.167 21 neg ## 5 0 137 40 35 168 43.1 2.288 33 pos ## 6 5 116 74 0 0 25.6 0.201 30 neg 2.1.2 Spliting the dataset into training and test data sets # Create the training and test datasets set.seed(100) # Step 1: Get row numbers for the training data trainRowNumbers &lt;- createDataPartition(diabetes$diabetes, p=0.8, list=FALSE) # Step 2: Create the training dataset trainData &lt;- diabetes[trainRowNumbers,] # Step 3: Create the test dataset testData &lt;- diabetes[-trainRowNumbers,] # Store X and Y for later use. # x = trainData[, -1] y = trainData$diabetes have a look training data set library(skimr) skimmed &lt;- skim (trainData) skimmed Table 2.1: Data summary Name trainData Number of rows 615 Number of columns 9 _______________________ Column type frequency: factor 1 numeric 8 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts diabetes 0 1 FALSE 2 neg: 400, pos: 215 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist pregnant 0 1 3.88 3.42 0.00 1.00 3.00 6.00 17.00 ▇▃▂▁▁ glucose 0 1 120.87 32.58 0.00 99.00 116.00 140.00 199.00 ▁▁▇▅▂ pressure 0 1 69.33 19.44 0.00 62.00 72.00 80.00 122.00 ▁▁▇▇▁ triceps 0 1 19.97 15.87 0.00 0.00 22.00 32.00 99.00 ▇▇▂▁▁ insulin 0 1 78.00 114.39 0.00 0.00 18.00 125.00 846.00 ▇▁▁▁▁ mass 0 1 32.08 7.97 0.00 27.50 32.00 36.80 67.10 ▁▂▇▂▁ pedigree 0 1 0.46 0.33 0.08 0.24 0.36 0.61 2.29 ▇▃▁▁▁ age 0 1 33.41 11.77 21.00 24.00 29.00 41.00 81.00 ▇▃▁▁▁ 2.1.3 Implement data imputation compiling knnimpute model # Create the knn imputation model on the training data preProcess_missingdata_model &lt;- preProcess(trainData, method=&#39;knnImpute&#39;) preProcess_missingdata_model ## Created from 615 samples and 9 variables ## ## Pre-processing: ## - centered (8) ## - ignored (1) ## - 5 nearest neighbor imputation (8) ## - scaled (8) check missingness # Use the imputation model to predict the values of missing data points library(RANN) # required for knnInpute trainData &lt;- predict(preProcess_missingdata_model, newdata = trainData) anyNA(trainData) ## [1] FALSE 2.1.4 One-hot-endcoding Y (dependent) will not be encoded as one-hot-encoding # One-Hot Encoding # Creating dummy variables is converting a categorical variable to as many binary variables as here are categories. dummies_model &lt;- dummyVars(diabetes ~ ., data=trainData) # Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat. trainData_mat &lt;- predict(dummies_model, newdata = trainData) ## Warning in model.frame.default(Terms, newdata, na.action = na.action, xlev = ## object$lvls): variable &#39;diabetes&#39; is not a factor # # Convert to dataframe trainData &lt;- data.frame(trainData_mat) # # See the structure of the new dataset str(trainData) ## &#39;data.frame&#39;: 615 obs. of 8 variables: ## $ pregnant: num -0.843 1.205 -0.843 -1.136 0.327 ... ## $ glucose : num -1.101 1.907 -0.978 0.495 -0.15 ... ## $ pressure: num -0.171 -0.274 -0.171 -1.508 0.24 ... ## $ triceps : num 0.569 -1.258 0.191 0.947 -1.258 ... ## $ insulin : num -0.682 -0.682 0.14 0.787 -0.682 ... ## $ mass : num -0.687 -1.101 -0.499 1.382 -0.812 ... ## $ pedigree: num -0.349 0.637 -0.915 5.601 -0.81 ... ## $ age : num -0.205 -0.1201 -1.0548 -0.0351 -0.29 ... 2.1.5 Normalizing features preProcess_range_model &lt;- preProcess(trainData, method=&#39;range&#39;) trainData &lt;- predict(preProcess_range_model, newdata = trainData) # Append the Y variable instead of normalized data trainData$diabetes &lt;- y # Look the dataset apply(trainData[, -1], 2, FUN=function(x){c(&#39;min&#39;=min(x), &#39;max&#39;=max(x))}) ## glucose pressure triceps insulin mass pedigree ## min &quot;0.0000000&quot; &quot;0.0000000&quot; &quot;0.00000000&quot; &quot;0.00000000&quot; &quot;0.0000000&quot; &quot;0.000000000&quot; ## max &quot;1.0000000&quot; &quot;1.0000000&quot; &quot;1.00000000&quot; &quot;1.00000000&quot; &quot;1.0000000&quot; &quot;1.000000000&quot; ## age diabetes ## min &quot;0.00000000&quot; &quot;neg&quot; ## max &quot;1.00000000&quot; &quot;pos&quot; str(trainData) ## &#39;data.frame&#39;: 615 obs. of 9 variables: ## $ pregnant: num 0.0588 0.4706 0.0588 0 0.2941 ... ## $ glucose : num 0.427 0.92 0.447 0.688 0.583 ... ## $ pressure: num 0.541 0.525 0.541 0.328 0.607 ... ## $ triceps : num 0.293 0 0.232 0.354 0 ... ## $ insulin : num 0 0 0.111 0.199 0 ... ## $ mass : num 0.396 0.347 0.419 0.642 0.382 ... ## $ pedigree: num 0.1235 0.2688 0.0403 1 0.0557 ... ## $ age : num 0.167 0.183 0 0.2 0.15 ... ## $ diabetes: Factor w/ 2 levels &quot;neg&quot;,&quot;pos&quot;: 1 2 1 2 1 2 1 2 2 2 ... 2.1.6 Plot features featurePlot(x = trainData[, 1:8], y = trainData$diabetes, plot = &quot;box&quot;, strip=strip.custom(par.strip.text=list(cex=.7)), scales = list(x = list(relation=&quot;free&quot;), y = list(relation=&quot;free&quot;))) featurePlot(x = trainData[, 1:8], y = trainData$diabetes, plot = &quot;density&quot;, strip=strip.custom(par.strip.text=list(cex=.7)), scales = list(x = list(relation=&quot;free&quot;), y = list(relation=&quot;free&quot;))) library(corrplot) ## corrplot 0.92 loaded corrplot(cor((trainData[,-9] ))) 2.1.7 Recursive feature elimination (rfe) In some scenarios, we just have to include the significant features into the following model. A good choice of selecting the important features is the recursive feature elimination (RFE). the final subset model is marked with a starisk in the last column, here it is 8th. though it is not wise to neglect the other predictors. set.seed(100) options(warn=-1) subsets &lt;- c(1:8) ctrl &lt;- rfeControl(functions = rfFuncs, #random forest algorithm method = &quot;repeatedcv&quot;, #k fold cross validation repeated 5 times repeats = 5, verbose = FALSE) lmProfile &lt;- rfe(x=trainData[, 1:8], y=trainData$diabetes, sizes = subsets, rfeControl = ctrl) lmProfile ## ## Recursive feature selection ## ## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) ## ## Resampling performance over subset size: ## ## Variables Accuracy Kappa AccuracySD KappaSD Selected ## 1 0.6952 0.2841 0.04915 0.10605 ## 2 0.7275 0.3815 0.04359 0.09609 ## 3 0.7642 0.4673 0.04216 0.09490 ## 4 0.7620 0.4631 0.04762 0.10945 ## 5 0.7571 0.4534 0.05152 0.11813 ## 6 0.7627 0.4679 0.04949 0.11218 ## 7 0.7620 0.4619 0.05210 0.12019 ## 8 0.7682 0.4728 0.04620 0.10576 * ## ## The top 5 variables (out of 8): ## glucose, mass, age, pregnant, insulin look up features of all models in R # See available algorithms in caret modelnames &lt;- paste(names(getModelInfo()), collapse=&#39;, &#39;) modelLookup(&#39;xgbTree&#39;) ## model parameter label forReg forClass ## 1 xgbTree nrounds # Boosting Iterations TRUE TRUE ## 2 xgbTree max_depth Max Tree Depth TRUE TRUE ## 3 xgbTree eta Shrinkage TRUE TRUE ## 4 xgbTree gamma Minimum Loss Reduction TRUE TRUE ## 5 xgbTree colsample_bytree Subsample Ratio of Columns TRUE TRUE ## 6 xgbTree min_child_weight Minimum Sum of Instance Weight TRUE TRUE ## 7 xgbTree subsample Subsample Percentage TRUE TRUE ## probModel ## 1 TRUE ## 2 TRUE ## 3 TRUE ## 4 TRUE ## 5 TRUE ## 6 TRUE ## 7 TRUE 2.1.8 Training a model Multivariate Adaptive Regression Splines (MARS) # Set the seed for reproducibility set.seed(100) # Train the model using randomForest and predict on the training data itself. model_mars = train(diabetes ~ ., data=trainData, method=&#39;earth&#39;) ## Loading required package: earth ## Loading required package: Formula ## Loading required package: plotmo ## Loading required package: plotrix ## Loading required package: TeachingDemos fitted &lt;- predict(model_mars) the default of resampling (Bootstrapped) is 25 reps model_mars ## Multivariate Adaptive Regression Spline ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 615, 615, 615, 615, 615, 615, ... ## Resampling results across tuning parameters: ## ## nprune Accuracy Kappa ## 2 0.7451922 0.4023855 ## 8 0.7680686 0.4748261 ## 14 0.7603116 0.4581491 ## ## Tuning parameter &#39;degree&#39; was held constant at a value of 1 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were nprune = 8 and degree = 1. plot the Accuracy of various combinations of the hyper parameters - interaction.depth and n.trees. plot(model_mars, main=&quot;Model Accuracies with MARS&quot;) calculate the importance of variable varimp_mars &lt;- varImp(model_mars) plot(varimp_mars, main=&quot;Variable Importance with MARS&quot;) 2.1.9 Prepare the test data set imputation,dummy, and normalization # Step 1: Impute missing values testData2 &lt;- predict(preProcess_missingdata_model, testData) # Step 2: Create one-hot encodings (dummy variables) testData3 &lt;- predict(dummies_model, testData2) # Step 3: Transform the features to range between 0 and 1 testData4 &lt;- predict(preProcess_range_model, testData3) # View head(testData4 ) ## pregnant glucose pressure triceps insulin mass pedigree ## 1 0.35294118 0.7437186 0.5901639 0.3535354 0.0000000 0.5007452 0.24841629 ## 11 0.23529412 0.5527638 0.7540984 0.0000000 0.0000000 0.5603577 0.05113122 ## 21 0.17647059 0.6331658 0.7213115 0.4141414 0.2777778 0.5856930 0.28325792 ## 24 0.52941176 0.5979899 0.6557377 0.3535354 0.0000000 0.4321908 0.08371041 ## 28 0.05882353 0.4874372 0.5409836 0.1515152 0.1654846 0.3457526 0.18506787 ## 37 0.64705882 0.6934673 0.6229508 0.0000000 0.0000000 0.4947839 0.15475113 ## age ## 1 0.48333333 ## 11 0.15000000 ## 21 0.10000000 ## 24 0.13333333 ## 28 0.01666667 ## 37 0.23333333 2.1.10 Prediction uisng testdata # Predict on testData predicted &lt;- predict(model_mars, testData4) head(predicted) ## [1] pos neg neg neg neg pos ## Levels: neg pos 2.1.11 Compute confusion matrix # Compute the confusion matrix confusionMatrix(reference = as.factor(testData$diabetes), data = predicted ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 86 22 ## pos 14 31 ## ## Accuracy : 0.7647 ## 95% CI : (0.6894, 0.8294) ## No Information Rate : 0.6536 ## P-Value [Acc &gt; NIR] : 0.001988 ## ## Kappa : 0.4613 ## ## Mcnemar&#39;s Test P-Value : 0.243345 ## ## Sensitivity : 0.8600 ## Specificity : 0.5849 ## Pos Pred Value : 0.7963 ## Neg Pred Value : 0.6889 ## Prevalence : 0.6536 ## Detection Rate : 0.5621 ## Detection Prevalence : 0.7059 ## Balanced Accuracy : 0.7225 ## ## &#39;Positive&#39; Class : neg ## 2.1.12 Tuning hyperparameter to optimize the model setting up hyper parameter tuneLength, tuneGrid # Define the training control fitControl &lt;- trainControl( method = &#39;cv&#39;, # k-fold cross validation number = 5, # number of folds savePredictions = &#39;final&#39;, # saves predictions for optimal tuning parameter classProbs = T, # should class probabilities be returned summaryFunction=twoClassSummary # results summary function ) # Step 1: Define the tuneGrid marsGrid &lt;- expand.grid(nprune = c(2, 4, 6, 8, 10), degree = c(1, 2, 3)) # Step 2: Tune hyper parameters by setting tuneGrid set.seed(100) model_mars3 = train(diabetes ~ ., data=trainData, method=&#39;earth&#39;, metric=&#39;ROC&#39;, tuneGrid = marsGrid, trControl = fitControl) model_mars3 ## Multivariate Adaptive Regression Spline ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results across tuning parameters: ## ## degree nprune ROC Sens Spec ## 1 2 0.7962500 0.8850 0.4976744 ## 1 4 0.8400581 0.8725 0.6046512 ## 1 6 0.8410465 0.8825 0.6046512 ## 1 8 0.8471512 0.8850 0.5860465 ## 1 10 0.8437209 0.8775 0.6093023 ## 2 2 0.7962500 0.8850 0.4976744 ## 2 4 0.8284593 0.8775 0.6000000 ## 2 6 0.8224419 0.8725 0.5488372 ## 2 8 0.8237209 0.8700 0.5395349 ## 2 10 0.8212209 0.8650 0.5395349 ## 3 2 0.7962500 0.8850 0.4976744 ## 3 4 0.8245058 0.8825 0.6000000 ## 3 6 0.8205814 0.8750 0.5627907 ## 3 8 0.8191860 0.8725 0.5581395 ## 3 10 0.8195349 0.8650 0.5627907 ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were nprune = 8 and degree = 1. # Step 3: Predict on testData and Compute the confusion matrix predicted3 &lt;- predict(model_mars3, testData4) confusionMatrix(reference = as.factor(testData$diabetes), data = predicted3 ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 86 22 ## pos 14 31 ## ## Accuracy : 0.7647 ## 95% CI : (0.6894, 0.8294) ## No Information Rate : 0.6536 ## P-Value [Acc &gt; NIR] : 0.001988 ## ## Kappa : 0.4613 ## ## Mcnemar&#39;s Test P-Value : 0.243345 ## ## Sensitivity : 0.8600 ## Specificity : 0.5849 ## Pos Pred Value : 0.7963 ## Neg Pred Value : 0.6889 ## Prevalence : 0.6536 ## Detection Rate : 0.5621 ## Detection Prevalence : 0.7059 ## Balanced Accuracy : 0.7225 ## ## &#39;Positive&#39; Class : neg ## 2.1.13 Other marchine learning algorithms 2.1.13.1 adaboost algorithm set.seed(100) # Train the model using adaboost model_adaboost = train(diabetes ~ ., data=trainData, method=&#39;adaboost&#39;, tuneLength=2, trControl = fitControl) model_adaboost ## AdaBoost Classification Trees ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results across tuning parameters: ## ## nIter method ROC Sens Spec ## 50 Adaboost.M1 0.7856395 0.8025 0.5906977 ## 50 Real adaboost 0.6250000 0.8350 0.5534884 ## 100 Adaboost.M1 0.7852907 0.8050 0.6325581 ## 100 Real adaboost 0.6051163 0.8450 0.5581395 ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were nIter = 50 and method = Adaboost.M1. 2.1.13.2 random forest set.seed(100) # Train the model using rf model_rf = train(diabetes ~ ., data=trainData, method=&#39;rf&#39;, tuneLength=5, trControl = fitControl) model_rf ## Random Forest ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results across tuning parameters: ## ## mtry ROC Sens Spec ## 2 0.8210756 0.8600 0.5906977 ## 3 0.8217733 0.8575 0.6046512 ## 5 0.8145640 0.8550 0.5906977 ## 6 0.8152616 0.8575 0.6093023 ## 8 0.8145349 0.8500 0.6000000 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 3. 2.1.13.3 xgbDART algorithm # set.seed(100) # # # Train the model using MARS # model_xgbDART = train(Purchase ~ ., data=trainData, method=&#39;xgbDART&#39;, tuneLength=5, trControl = fitControl, verbose=F) # model_xgbDART 2.1.13.4 Support Vector Machines (SVM) set.seed(100) # Train the model using MARS model_svmRadial = train(diabetes ~ ., data=trainData, method=&#39;svmRadial&#39;, tuneLength=15, trControl = fitControl) model_svmRadial ## Support Vector Machines with Radial Basis Function Kernel ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results across tuning parameters: ## ## C ROC Sens Spec ## 0.25 0.8306395 0.8650 0.5813953 ## 0.50 0.8308140 0.8800 0.5720930 ## 1.00 0.8279070 0.8750 0.5348837 ## 2.00 0.8216860 0.8825 0.4976744 ## 4.00 0.8204070 0.8925 0.4883721 ## 8.00 0.8080814 0.8800 0.4790698 ## 16.00 0.7892442 0.8825 0.4651163 ## 32.00 0.7677326 0.8875 0.4093023 ## 64.00 0.7430814 0.8925 0.3674419 ## 128.00 0.7165698 0.8825 0.3255814 ## 256.00 0.7062209 0.8975 0.3255814 ## 512.00 0.7051163 0.9100 0.2930233 ## 1024.00 0.7005814 0.9025 0.3023256 ## 2048.00 0.6955233 0.9000 0.3162791 ## 4096.00 0.6948837 0.8925 0.3348837 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.1161195 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.1161195 and C = 0.5. 2.1.13.5 K-Nearest Neighbors set.seed(100) # Train the model using MARS model_knn = train(diabetes ~ ., data=trainData, method=&#39;knn&#39;, tuneLength=15, trControl = fitControl) model_knn ## k-Nearest Neighbors ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results across tuning parameters: ## ## k ROC Sens Spec ## 5 0.7401744 0.8250 0.4651163 ## 7 0.7655523 0.8425 0.4744186 ## 9 0.7707849 0.8500 0.4930233 ## 11 0.7797384 0.8800 0.4976744 ## 13 0.7876744 0.8725 0.4790698 ## 15 0.7951163 0.8800 0.4837209 ## 17 0.7933721 0.8775 0.4651163 ## 19 0.7997965 0.8825 0.4465116 ## 21 0.8001163 0.8975 0.4418605 ## 23 0.8024709 0.9050 0.4651163 ## 25 0.8037791 0.9050 0.4744186 ## 27 0.8082267 0.9050 0.4790698 ## 29 0.8069767 0.9150 0.4697674 ## 31 0.8083430 0.9100 0.4418605 ## 33 0.8064244 0.9175 0.4372093 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was k = 31. 2.1.14 Comparisons of different models # Compare model performances using resample() models_compare &lt;- resamples(list(ADABOOST=model_adaboost, RF=model_rf, knn=model_knn, MARS=model_mars3, SVM=model_svmRadial)) # Summary of the models performances summary(models_compare) ## ## Call: ## summary.resamples(object = models_compare) ## ## Models: ADABOOST, RF, knn, MARS, SVM ## Number of resamples: 5 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## ADABOOST 0.7436047 0.7750000 0.7851744 0.7856395 0.7889535 0.8354651 0 ## RF 0.7697674 0.8155523 0.8170058 0.8217733 0.8497093 0.8568314 0 ## knn 0.7646802 0.7680233 0.8209302 0.8083430 0.8297965 0.8582849 0 ## MARS 0.8238372 0.8430233 0.8450581 0.8471512 0.8613372 0.8625000 0 ## SVM 0.7648256 0.8279070 0.8436047 0.8308140 0.8441860 0.8735465 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## ADABOOST 0.7500 0.7750 0.8125 0.8025 0.8250 0.8500 0 ## RF 0.8250 0.8375 0.8625 0.8575 0.8750 0.8875 0 ## knn 0.8875 0.9000 0.9000 0.9100 0.9125 0.9500 0 ## MARS 0.8500 0.8625 0.8875 0.8850 0.9125 0.9125 0 ## SVM 0.8625 0.8750 0.8750 0.8800 0.8875 0.9000 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## ADABOOST 0.5348837 0.5348837 0.5581395 0.5906977 0.6046512 0.7209302 0 ## RF 0.5581395 0.5581395 0.5813953 0.6046512 0.6511628 0.6744186 0 ## knn 0.4186047 0.4418605 0.4418605 0.4418605 0.4418605 0.4651163 0 ## MARS 0.5348837 0.5348837 0.5813953 0.5860465 0.6046512 0.6744186 0 ## SVM 0.5116279 0.5581395 0.5813953 0.5720930 0.6046512 0.6046512 0 2.1.15 Plot comparisons of models # Draw box plots to compare models scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) bwplot(models_compare, scales=scales) 2.1.16 Ensemble predictions from multiple models create multiple models library(caretEnsemble) ## ## Attaching package: &#39;caretEnsemble&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## autoplot # Stacking Algorithms - Run multiple algos in one call. trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE) algorithmList &lt;- c(&#39;rf&#39;, &#39;adaboost&#39;, &#39;earth&#39;, &#39;knn&#39;, &#39;svmRadial&#39;) set.seed(100) models &lt;- caretList(diabetes ~ ., data=trainData, trControl=trainControl, methodList=algorithmList) results &lt;- resamples(models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rf, adaboost, earth, knn, svmRadial ## Number of resamples: 30 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rf 0.6774194 0.7540984 0.7704918 0.7723956 0.8056584 0.8548387 0 ## adaboost 0.6612903 0.7224352 0.7540984 0.7539221 0.7868852 0.8387097 0 ## earth 0.6612903 0.7419355 0.7741935 0.7745725 0.8032787 0.8709677 0 ## knn 0.6229508 0.7049180 0.7398202 0.7372466 0.7704918 0.8360656 0 ## svmRadial 0.6935484 0.7387626 0.7805394 0.7712938 0.8000397 0.8387097 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rf 0.2654028 0.4262678 0.4828423 0.4872370 0.5579818 0.6796785 0 ## adaboost 0.2203593 0.3688623 0.4526472 0.4369009 0.4864705 0.6403712 0 ## earth 0.2368113 0.4089641 0.4935398 0.4820558 0.5378447 0.7122970 0 ## knn 0.1553281 0.3014894 0.4019217 0.3890542 0.4644021 0.6007853 0 ## svmRadial 0.3237658 0.3923780 0.5012975 0.4765574 0.5300695 0.6403712 0 comparison by visualization # Box plots to compare models scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) bwplot(results, scales=scales) ensemble predictions on testdata # Create the trainControl set.seed(101) stackControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE) # Ensemble the predictions of `models` to form a new combined prediction based on glm stack.glm &lt;- caretStack(models, method=&quot;glm&quot;, metric=&quot;Accuracy&quot;, trControl=stackControl) print(stack.glm) ## A glm ensemble of 5 base models: rf, adaboost, earth, knn, svmRadial ## ## Ensemble results: ## Generalized Linear Model ## ## 1845 samples ## 5 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 1660, 1660, 1661, 1661, 1661, 1660, ... ## Resampling results: ## ## Accuracy Kappa ## 0.7799569 0.4938476 compute confusion matrix # Predict on testData stack_predicteds &lt;- predict(stack.glm, newdata=testData4) confusionMatrix(reference = as.factor(testData$diabetes), data = stack_predicteds ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 84 22 ## pos 16 31 ## ## Accuracy : 0.7516 ## 95% CI : (0.6754, 0.8179) ## No Information Rate : 0.6536 ## P-Value [Acc &gt; NIR] : 0.005891 ## ## Kappa : 0.4365 ## ## Mcnemar&#39;s Test P-Value : 0.417304 ## ## Sensitivity : 0.8400 ## Specificity : 0.5849 ## Pos Pred Value : 0.7925 ## Neg Pred Value : 0.6596 ## Prevalence : 0.6536 ## Detection Rate : 0.5490 ## Detection Prevalence : 0.6928 ## Balanced Accuracy : 0.7125 ## ## &#39;Positive&#39; Class : neg ## 2.2 KNN Classifier # Loading package # library(e1071) library(caTools) library(class) 2.2.1 Splitting data # load the Pima Indians dataset from the mlbench dataset library(mlbench) data(PimaIndiansDiabetes) # rename dataset to have shorter name because lazy diabetes &lt;- PimaIndiansDiabetes # Splitting data into train and test data set.seed(100) split &lt;- sample.split(diabetes, SplitRatio = 0.8) train_cl &lt;- subset(diabetes, split == &quot;TRUE&quot;) test_cl &lt;- subset(diabetes, split == &quot;FALSE&quot;) # Feature Scaling train_scale &lt;- scale(train_cl[, 1:8]) test_scale &lt;- scale(test_cl[, 1:8]) # train_y &lt;- scale(train_cl[, 5]) # test_y &lt;- scale(test_cl[, 5]) 2.2.2 Creating KNN model # Fitting KNN Model to training dataset classifier_knn &lt;- knn(train = train_scale, cl = train_cl$diabetes, test = test_scale, k = 1) classifier_knn ## [1] pos neg neg neg pos neg neg neg neg neg neg neg pos neg neg pos neg pos ## [19] neg neg neg neg pos neg neg pos neg pos pos neg neg neg neg pos neg pos ## [37] neg neg neg pos neg pos pos neg neg neg pos pos neg pos neg neg neg neg ## [55] pos neg pos neg pos neg neg neg pos pos pos pos neg pos neg pos pos neg ## [73] pos neg neg pos neg neg neg pos pos neg neg pos neg pos pos neg neg neg ## [91] neg neg neg pos pos neg neg neg pos neg neg pos neg neg pos neg pos neg ## [109] neg neg neg neg pos pos pos pos pos pos neg pos pos pos neg neg neg neg ## [127] neg neg neg neg neg neg pos neg pos neg pos pos neg pos neg pos neg pos ## [145] neg neg neg pos neg neg neg pos pos pos neg pos neg pos neg neg neg neg ## [163] neg neg pos pos neg pos neg neg neg ## Levels: neg pos 2.2.3 Model Evaluation Creat confusion matrix # Confusion Matrix cm &lt;- table(test_cl$diabetes, classifier_knn) cm ## classifier_knn ## neg pos ## neg 79 32 ## pos 27 33 2.2.4 Calculate accuracy with different K # Model Evaluation - Choosing K =1 # Calculate out of Sample error misClassError &lt;- mean(classifier_knn != test_cl$diabetes) print(paste(&#39;Accuracy =&#39;, 1-misClassError)) ## [1] &quot;Accuracy = 0.654970760233918&quot; # K = 7 classifier_knn &lt;- knn(train = train_scale, test = test_scale, cl = train_cl$diabetes, k = 23) misClassError &lt;- mean(classifier_knn != test_cl$diabetes) print(paste(&#39;Accuracy =&#39;, 1-misClassError)) ## [1] &quot;Accuracy = 0.795321637426901&quot; 2.2.5 Optimization search better k parameter i=1 k.optm=1 for (i in 1:39){ y_pred = knn(train = train_scale, test = test_scale, cl = train_cl$diabetes, k = i ) k.optm[i] &lt;- 1- mean(y_pred != test_cl$diabetes) k=i cat(k,&#39;=&#39;,k.optm[i],&#39;&#39;) } ## 1 = 0.6549708 2 = 0.6666667 3 = 0.7426901 4 = 0.6900585 5 = 0.7309942 6 = 0.748538 7 = 0.7368421 8 = 0.7309942 9 = 0.7368421 10 = 0.7251462 11 = 0.7602339 12 = 0.748538 13 = 0.7719298 14 = 0.748538 15 = 0.754386 16 = 0.754386 17 = 0.7602339 18 = 0.7192982 19 = 0.7719298 20 = 0.754386 21 = 0.7836257 22 = 0.7777778 23 = 0.7953216 24 = 0.7719298 25 = 0.7777778 26 = 0.7836257 27 = 0.7719298 28 = 0.7660819 29 = 0.7777778 30 = 0.7660819 31 = 0.7660819 32 = 0.7602339 33 = 0.7719298 34 = 0.754386 35 = 0.7602339 36 = 0.7719298 37 = 0.7777778 38 = 0.7836257 39 = 0.7836257 Accuracy plot k=15 plot(k.optm, type=&quot;b&quot;, xlab=&quot;K- Value&quot;,ylab=&quot;RMSE level&quot;) 2.2.6 Visualization # Visualising the Training set results # Install ElemStatLearn if not present 2.3 KNN regression 2.3.1 Data exploring library(&quot;Amelia&quot;) data(&quot;Boston&quot;, package = &quot;MASS&quot;) missmap(Boston,col=c(&#39;yellow&#39;,&#39;black&#39;),y.at=1,y.labels=&#39;&#39;,legend=TRUE) library(corrplot) corrplot(cor((Boston))) library(Hmisc) describe(Boston) ## Boston ## ## 14 Variables 506 Observations ## -------------------------------------------------------------------------------- ## crim ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 504 1 3.614 5.794 0.02791 0.03819 ## .25 .50 .75 .90 .95 ## 0.08204 0.25651 3.67708 10.75300 15.78915 ## ## lowest : 0.00632 0.00906 0.01096 0.01301 0.01311 ## highest: 45.74610 51.13580 67.92080 73.53410 88.97620 ## -------------------------------------------------------------------------------- ## zn ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 26 0.603 11.36 18.77 0.0 0.0 ## .25 .50 .75 .90 .95 ## 0.0 0.0 12.5 42.5 80.0 ## ## lowest : 0.0 12.5 17.5 18.0 20.0, highest: 82.5 85.0 90.0 95.0 100.0 ## -------------------------------------------------------------------------------- ## indus ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 76 0.982 11.14 7.705 2.18 2.91 ## .25 .50 .75 .90 .95 ## 5.19 9.69 18.10 19.58 21.89 ## ## lowest : 0.46 0.74 1.21 1.22 1.25, highest: 18.10 19.58 21.89 25.65 27.74 ## -------------------------------------------------------------------------------- ## chas ## n missing distinct Info Sum Mean Gmd ## 506 0 2 0.193 35 0.06917 0.129 ## ## -------------------------------------------------------------------------------- ## nox ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 81 1 0.5547 0.1295 0.4092 0.4270 ## .25 .50 .75 .90 .95 ## 0.4490 0.5380 0.6240 0.7130 0.7400 ## ## lowest : 0.385 0.389 0.392 0.394 0.398, highest: 0.713 0.718 0.740 0.770 0.871 ## -------------------------------------------------------------------------------- ## rm ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 446 1 6.285 0.7515 5.314 5.594 ## .25 .50 .75 .90 .95 ## 5.886 6.208 6.623 7.152 7.588 ## ## lowest : 3.561 3.863 4.138 4.368 4.519, highest: 8.375 8.398 8.704 8.725 8.780 ## -------------------------------------------------------------------------------- ## age ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 356 0.999 68.57 31.52 17.72 26.95 ## .25 .50 .75 .90 .95 ## 45.02 77.50 94.07 98.80 100.00 ## ## lowest : 2.9 6.0 6.2 6.5 6.6, highest: 98.8 98.9 99.1 99.3 100.0 ## -------------------------------------------------------------------------------- ## dis ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 412 1 3.795 2.298 1.462 1.628 ## .25 .50 .75 .90 .95 ## 2.100 3.207 5.188 6.817 7.828 ## ## lowest : 1.1296 1.1370 1.1691 1.1742 1.1781 ## highest: 9.2203 9.2229 10.5857 10.7103 12.1265 ## -------------------------------------------------------------------------------- ## rad ## n missing distinct Info Mean Gmd ## 506 0 9 0.959 9.549 8.518 ## ## lowest : 1 2 3 4 5, highest: 5 6 7 8 24 ## ## Value 1 2 3 4 5 6 7 8 24 ## Frequency 20 24 38 110 115 26 17 24 132 ## Proportion 0.040 0.047 0.075 0.217 0.227 0.051 0.034 0.047 0.261 ## -------------------------------------------------------------------------------- ## tax ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 66 0.981 408.2 181.7 222 233 ## .25 .50 .75 .90 .95 ## 279 330 666 666 666 ## ## lowest : 187 188 193 198 216, highest: 432 437 469 666 711 ## -------------------------------------------------------------------------------- ## ptratio ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 46 0.978 18.46 2.383 14.70 14.75 ## .25 .50 .75 .90 .95 ## 17.40 19.05 20.20 20.90 21.00 ## ## lowest : 12.6 13.0 13.6 14.4 14.7, highest: 20.9 21.0 21.1 21.2 22.0 ## -------------------------------------------------------------------------------- ## black ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 357 0.986 356.7 65.5 84.59 290.27 ## .25 .50 .75 .90 .95 ## 375.38 391.44 396.23 396.90 396.90 ## ## lowest : 0.32 2.52 2.60 3.50 3.65, highest: 396.28 396.30 396.33 396.42 396.90 ## -------------------------------------------------------------------------------- ## lstat ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 455 1 12.65 7.881 3.708 4.680 ## .25 .50 .75 .90 .95 ## 6.950 11.360 16.955 23.035 26.808 ## ## lowest : 1.73 1.92 1.98 2.47 2.87, highest: 34.37 34.41 34.77 36.98 37.97 ## -------------------------------------------------------------------------------- ## medv ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 229 1 22.53 9.778 10.20 12.75 ## .25 .50 .75 .90 .95 ## 17.02 21.20 25.00 34.80 43.40 ## ## lowest : 5.0 5.6 6.3 7.0 7.2, highest: 46.7 48.3 48.5 48.8 50.0 ## -------------------------------------------------------------------------------- 2.3.2 Prepareing data Boston &lt;- dplyr::select (Boston ,medv , crim , rm , tax , lstat) # Splitting the dataset into # the Training set and Test set # install.packages(&#39;caTools&#39;) library(caTools) set.seed(123) split = sample.split(Boston$medv, SplitRatio = 0.75) training_set_origi = subset(Boston, split == TRUE) test_set_origi = subset(Boston, split == FALSE) # Feature Scaling training_set = scale(training_set_origi[,-1] ) test_set = scale(test_set_origi [,-1]) 2.3.3 Creating model # Fitting K-NN to the Training set # and Predicting the Test set results # library(class) y_pred = knn(train = training_set[, -1], test = test_set[, -1], cl = training_set_origi[, 1], k = 15 ) # 2.3.4 Evaluation # converting factor into character then into numeric error &lt;- test_set_origi[,1]-as.numeric (as.character(y_pred)) head(error) ## [1] -3.5 -0.5 -1.4 -2.6 1.0 -8.8 rmse &lt;- sqrt(mean(error)^2) rmse ## [1] 0.8487179 plot(error) head(cbind(test_set_origi[,1], as.numeric (as.character(y_pred)))) ## [,1] [,2] ## [1,] 18.2 21.7 ## [2,] 19.9 20.4 ## [3,] 17.5 18.9 ## [4,] 15.2 17.8 ## [5,] 14.5 13.5 ## [6,] 15.6 24.4 2.3.5 Optimization search better k parameter i=1 k.optm=1 for (i in 1:29){ y_pred = knn(train = training_set[, -1], test = test_set[, -1], cl = training_set_origi[, 1], k = i ) k.optm[i] &lt;- sqrt(mean( test_set_origi[,1]-as.numeric (as.character(y_pred)) )^2) k=i cat(k,&#39;=&#39;,k.optm[i],&#39;&#39;) } ## 1 = 0.35 2 = 0.5371795 3 = 0.9705128 4 = 1.105128 5 = 1.373077 6 = 0.4512821 7 = 0.6230769 8 = 0.575641 9 = 1.325641 10 = 1.176923 11 = 0.6628205 12 = 0.15 13 = 0.04358974 14 = 0.724359 15 = 0.3551282 16 = 0.07820513 17 = 0.07820513 18 = 0.6346154 19 = 0.2628205 20 = 0.4769231 21 = 0.9294872 22 = 0.6423077 23 = 0.4333333 24 = 0.4320513 25 = 0.3807692 26 = 1.061538 27 = 0.924359 28 = 0.7230769 29 = 0.03461538 Accuracy plot k=15 plot(k.optm, type=&quot;b&quot;, xlab=&quot;K- Value&quot;,ylab=&quot;RMSE level&quot;) "],["deep-learning.html", "Chapter 3 Deep learning 3.1 Deep neural network 3.2 Deep neural networks for regression 3.3 Convolutional neural netwrok", " Chapter 3 Deep learning 3.1 Deep neural network 3.1.1 Load data # load the Pima Indians dataset from the mlbench dataset library(mlbench) data(PimaIndiansDiabetes) # rename dataset to have shorter name because lazy diabetes &lt;- PimaIndiansDiabetes data.set &lt;- diabetes # datatable(data.set[sample(nrow(data.set), # replace = FALSE, # size = 0.005 * nrow(data.set)), ]) summary(data.set) ## pregnant glucose pressure triceps ## Min. : 0.000 Min. : 0.0 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 1.000 1st Qu.: 99.0 1st Qu.: 62.00 1st Qu.: 0.00 ## Median : 3.000 Median :117.0 Median : 72.00 Median :23.00 ## Mean : 3.845 Mean :120.9 Mean : 69.11 Mean :20.54 ## 3rd Qu.: 6.000 3rd Qu.:140.2 3rd Qu.: 80.00 3rd Qu.:32.00 ## Max. :17.000 Max. :199.0 Max. :122.00 Max. :99.00 ## insulin mass pedigree age diabetes ## Min. : 0.0 Min. : 0.00 Min. :0.0780 Min. :21.00 neg:500 ## 1st Qu.: 0.0 1st Qu.:27.30 1st Qu.:0.2437 1st Qu.:24.00 pos:268 ## Median : 30.5 Median :32.00 Median :0.3725 Median :29.00 ## Mean : 79.8 Mean :31.99 Mean :0.4719 Mean :33.24 ## 3rd Qu.:127.2 3rd Qu.:36.60 3rd Qu.:0.6262 3rd Qu.:41.00 ## Max. :846.0 Max. :67.10 Max. :2.4200 Max. :81.00 3.1.2 Process data and variable data.set$diabetes &lt;- as.numeric(data.set$diabetes) data.set$diabetes=data.set$diabetes-1 head(data.set$diabetes) ## [1] 1 0 1 0 1 0 head(data.set) ## pregnant glucose pressure triceps insulin mass pedigree age diabetes ## 1 6 148 72 35 0 33.6 0.627 50 1 ## 2 1 85 66 29 0 26.6 0.351 31 0 ## 3 8 183 64 0 0 23.3 0.672 32 1 ## 4 1 89 66 23 94 28.1 0.167 21 0 ## 5 0 137 40 35 168 43.1 2.288 33 1 ## 6 5 116 74 0 0 25.6 0.201 30 0 str(data.set) ## &#39;data.frame&#39;: 768 obs. of 9 variables: ## $ pregnant: num 6 1 8 1 0 5 3 10 2 8 ... ## $ glucose : num 148 85 183 89 137 116 78 115 197 125 ... ## $ pressure: num 72 66 64 66 40 74 50 0 70 96 ... ## $ triceps : num 35 29 0 23 35 0 32 0 45 0 ... ## $ insulin : num 0 0 0 94 168 0 88 0 543 0 ... ## $ mass : num 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ... ## $ pedigree: num 0.627 0.351 0.672 0.167 2.288 ... ## $ age : num 50 31 32 21 33 30 26 29 53 54 ... ## $ diabetes: num 1 0 1 0 1 0 1 0 1 1 ... transform dataframe into matrix # Cast dataframe as a matrix data.set &lt;- as.matrix(data.set) # Remove column names dimnames(data.set) = NULL head(data.set) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 6 148 72 35 0 33.6 0.627 50 1 ## [2,] 1 85 66 29 0 26.6 0.351 31 0 ## [3,] 8 183 64 0 0 23.3 0.672 32 1 ## [4,] 1 89 66 23 94 28.1 0.167 21 0 ## [5,] 0 137 40 35 168 43.1 2.288 33 1 ## [6,] 5 116 74 0 0 25.6 0.201 30 0 3.1.3 Split data into training and test datasets including xtrain ytrian xtest ytest # Split for train and test data set.seed(100) indx &lt;- sample(2, nrow(data.set), replace = TRUE, prob = c(0.8, 0.2)) # Makes index with values 1 and 2 # Select only the feature variables # Take rows with index = 1 x_train &lt;- data.set[indx == 1, 1:8] x_test &lt;- data.set[indx == 2, 1:8] # Feature Scaling x_train &lt;- scale(x_train ) x_test &lt;- scale(x_test ) y_test_actual &lt;- data.set[indx == 2, 9] transform target as on-hot-coding format # Using similar indices to correspond to the training and test set y_train &lt;- to_categorical(data.set[indx == 1, 9]) ## Loaded Tensorflow version 2.8.0 y_test &lt;- to_categorical(data.set[indx == 2, 9]) head(y_train) ## [,1] [,2] ## [1,] 0 1 ## [2,] 1 0 ## [3,] 0 1 ## [4,] 1 0 ## [5,] 0 1 ## [6,] 1 0 head(data.set[indx == 1, 9],20) ## [1] 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 dimension of four splitting data sets dim(x_train) ## [1] 609 8 dim(y_train) ## [1] 609 2 dim(x_test) ## [1] 159 8 dim(y_test) ## [1] 159 2 3.1.4 Creating neural network model 3.1.4.1 construction of model the output layer contains 3 levels # Creating the model model &lt;- keras_model_sequential() model %&gt;% layer_dense(name = &quot;DeepLayer1&quot;, units = 10, activation = &quot;relu&quot;, input_shape = c(8)) %&gt;% # input 4 features layer_dense(name = &quot;DeepLayer2&quot;, units = 10, activation = &quot;relu&quot;) %&gt;% layer_dense(name = &quot;OutputLayer&quot;, units = 2, activation = &quot;softmax&quot;) # output 4 categories using one-hot-coding summary(model) ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## DeepLayer1 (Dense) (None, 10) 90 ## DeepLayer2 (Dense) (None, 10) 110 ## OutputLayer (Dense) (None, 2) 22 ## ================================================================================ ## Total params: 222 ## Trainable params: 222 ## Non-trainable params: 0 ## ________________________________________________________________________________ 3.1.4.2 Compiling the model # Compiling the model model %&gt;% compile(loss = &quot;categorical_crossentropy&quot;, optimizer = &quot;adam&quot;, metrics = c(&quot;accuracy&quot;)) 3.1.4.3 Fitting the data and plot history &lt;- model %&gt;% fit(x_train, y_train, # adjusting number of epoch epoch = 60, # adjusting number of batch size batch_size = 64, validation_split = 0.15, verbose = 2) plot(history) 3.1.5 Evaluation 3.1.5.1 Output loss and accuracy using xtest and ytest data sets to evaluate the built model directly model %&gt;% evaluate(x_test, y_test) ## loss accuracy ## 0.4394172 0.7798742 3.1.5.2 Output the predicted classes and confusion matrix pred &lt;- model %&gt;% predict(x_test) %&gt;% k_argmax() %&gt;% k_get_value() head(pred) ## [1] 0 1 0 1 0 0 table(Predicted = pred, Actual = y_test_actual) ## Actual ## Predicted 0 1 ## 0 85 16 ## 1 19 39 3.1.5.3 Output the predicted values prob &lt;- model %&gt;% predict(x_test) %&gt;% k_get_value() head(prob) ## [,1] [,2] ## [1,] 0.9483572 0.05164286 ## [2,] 0.1468039 0.85319614 ## [3,] 0.9818276 0.01817242 ## [4,] 0.3916492 0.60835081 ## [5,] 0.9184096 0.08159041 ## [6,] 0.7314926 0.26850742 3.1.5.4 Comparison between prob, pred, and ytest comparison &lt;- cbind(prob , pred , y_test_actual ) head(comparison) ## pred y_test_actual ## [1,] 0.9483572 0.05164286 0 1 ## [2,] 0.1468039 0.85319614 1 1 ## [3,] 0.9818276 0.01817242 0 0 ## [4,] 0.3916492 0.60835081 1 1 ## [5,] 0.9184096 0.08159041 0 0 ## [6,] 0.7314926 0.26850742 0 0 3.2 Deep neural networks for regression 3.2.1 Loading packages and data sets library(readr) library(keras) library(plotly) data(&quot;Boston&quot;, package = &quot;MASS&quot;) data.set &lt;- Boston dim(data.set) ## [1] 506 14 3.2.2 Convert dataframe to matrix without dimnames library(DT) # Cast dataframe as a matrix data.set &lt;- as.matrix(data.set) # Remove column names dimnames(data.set) = NULL head(data.set) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## [2,] 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## [3,] 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## [4,] 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## [5,] 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## [6,] 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## [,13] [,14] ## [1,] 4.98 24.0 ## [2,] 9.14 21.6 ## [3,] 4.03 34.7 ## [4,] 2.94 33.4 ## [5,] 5.33 36.2 ## [6,] 5.21 28.7 summary(data.set[, 14]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.00 17.02 21.20 22.53 25.00 50.00 hist( data.set[, 14]) (#fig:target variable histogram)Fig 1 Histogram of the target variable 3.2.3 Spiting training and test data # Split for train and test data set.seed(123) indx &lt;- sample(2, nrow(data.set), replace = TRUE, prob = c(0.75, 0.25)) # Makes index with values 1 and 2 x_train &lt;- data.set[indx == 1, 1:13] x_test &lt;- data.set[indx == 2, 1:13] y_train &lt;- data.set[indx == 1, 14] y_test &lt;- data.set[indx == 2, 14] 3.2.4 Normalizing xtrain and xtest data x_train &lt;- scale(x_train) x_test &lt;- scale(x_test) 3.2.5 Creating the model model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 25, activation = &quot;relu&quot;, input_shape = c(13)) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(units = 25, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(units = 25, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(units = 1) model %&gt;% summary() ## Model: &quot;sequential_1&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_3 (Dense) (None, 25) 350 ## dropout_2 (Dropout) (None, 25) 0 ## dense_2 (Dense) (None, 25) 650 ## dropout_1 (Dropout) (None, 25) 0 ## dense_1 (Dense) (None, 25) 650 ## dropout (Dropout) (None, 25) 0 ## dense (Dense) (None, 1) 26 ## ================================================================================ ## Total params: 1,676 ## Trainable params: 1,676 ## Non-trainable params: 0 ## ________________________________________________________________________________ model %&gt;% get_config() ## {&#39;name&#39;: &#39;sequential_1&#39;, &#39;layers&#39;: [{&#39;class_name&#39;: &#39;InputLayer&#39;, &#39;config&#39;: {&#39;batch_input_shape&#39;: (None, 13), &#39;dtype&#39;: &#39;float32&#39;, &#39;sparse&#39;: False, &#39;ragged&#39;: False, &#39;name&#39;: &#39;dense_3_input&#39;}}, {&#39;class_name&#39;: &#39;Dense&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dense_3&#39;, &#39;trainable&#39;: True, &#39;batch_input_shape&#39;: (None, 13), &#39;dtype&#39;: &#39;float32&#39;, &#39;units&#39;: 25, &#39;activation&#39;: &#39;relu&#39;, &#39;use_bias&#39;: True, &#39;kernel_initializer&#39;: {&#39;class_name&#39;: &#39;GlorotUniform&#39;, &#39;config&#39;: {&#39;seed&#39;: None}}, &#39;bias_initializer&#39;: {&#39;class_name&#39;: &#39;Zeros&#39;, &#39;config&#39;: {}}, &#39;kernel_regularizer&#39;: None, &#39;bias_regularizer&#39;: None, &#39;activity_regularizer&#39;: None, &#39;kernel_constraint&#39;: None, &#39;bias_constraint&#39;: None}}, {&#39;class_name&#39;: &#39;Dropout&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dropout_2&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;rate&#39;: 0.2, &#39;noise_shape&#39;: None, &#39;seed&#39;: None}}, {&#39;class_name&#39;: &#39;Dense&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dense_2&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;units&#39;: 25, &#39;activation&#39;: &#39;relu&#39;, &#39;use_bias&#39;: True, &#39;kernel_initializer&#39;: {&#39;class_name&#39;: &#39;GlorotUniform&#39;, &#39;config&#39;: {&#39;seed&#39;: None}}, &#39;bias_initializer&#39;: {&#39;class_name&#39;: &#39;Zeros&#39;, &#39;config&#39;: {}}, &#39;kernel_regularizer&#39;: None, &#39;bias_regularizer&#39;: None, &#39;activity_regularizer&#39;: None, &#39;kernel_constraint&#39;: None, &#39;bias_constraint&#39;: None}}, {&#39;class_name&#39;: &#39;Dropout&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dropout_1&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;rate&#39;: 0.2, &#39;noise_shape&#39;: None, &#39;seed&#39;: None}}, {&#39;class_name&#39;: &#39;Dense&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dense_1&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;units&#39;: 25, &#39;activation&#39;: &#39;relu&#39;, &#39;use_bias&#39;: True, &#39;kernel_initializer&#39;: {&#39;class_name&#39;: &#39;GlorotUniform&#39;, &#39;config&#39;: {&#39;seed&#39;: None}}, &#39;bias_initializer&#39;: {&#39;class_name&#39;: &#39;Zeros&#39;, &#39;config&#39;: {}}, &#39;kernel_regularizer&#39;: None, &#39;bias_regularizer&#39;: None, &#39;activity_regularizer&#39;: None, &#39;kernel_constraint&#39;: None, &#39;bias_constraint&#39;: None}}, {&#39;class_name&#39;: &#39;Dropout&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dropout&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;rate&#39;: 0.2, &#39;noise_shape&#39;: None, &#39;seed&#39;: None}}, {&#39;class_name&#39;: &#39;Dense&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dense&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;units&#39;: 1, &#39;activation&#39;: &#39;linear&#39;, &#39;use_bias&#39;: True, &#39;kernel_initializer&#39;: {&#39;class_name&#39;: &#39;GlorotUniform&#39;, &#39;config&#39;: {&#39;seed&#39;: None}}, &#39;bias_initializer&#39;: {&#39;class_name&#39;: &#39;Zeros&#39;, &#39;config&#39;: {}}, &#39;kernel_regularizer&#39;: None, &#39;bias_regularizer&#39;: None, &#39;activity_regularizer&#39;: None, &#39;kernel_constraint&#39;: None, &#39;bias_constraint&#39;: None}}]} 3.2.6 Compiling the model model %&gt;% compile(loss = &quot;mse&quot;, optimizer = optimizer_rmsprop(), metrics = c(&quot;mean_absolute_error&quot;)) 3.2.7 Fitting the model history &lt;- model %&gt;% fit(x_train, y_train, epoch = 100, batch_size = 64, validation_split = 0.1, callbacks = c(callback_early_stopping(monitor = &quot;val_mean_absolute_error&quot;, patience = 5)), verbose = 2) c(loss, mae) %&lt;-% (model %&gt;% evaluate(x_test, y_test, verbose = 0)) paste0(&quot;Mean absolute error on test set: &quot;, sprintf(&quot;%.2f&quot;, mae)) ## [1] &quot;Mean absolute error on test set: 2.91&quot; 3.2.8 Plot the training process plot(history) ### Calculating the predicted values on test data pred2 &lt;- model %&gt;% predict(x_test) %&gt;% k_get_value() head(cbind(pred2,y_test)) ## y_test ## [1,] 26.50778 21.6 ## [2,] 33.88538 33.4 ## [3,] 33.37019 36.2 ## [4,] 15.23457 27.1 ## [5,] 15.98161 15.0 ## [6,] 18.01249 19.9 calculating mean absolute error and root mean square error and ploting error &lt;- y_test-pred2 head(error) ## [,1] ## [1,] -4.9077763 ## [2,] -0.4853836 ## [3,] 2.8298065 ## [4,] 11.8654295 ## [5,] -0.9816074 ## [6,] 1.8875126 rmse &lt;- sqrt(mean(error)^2) rmse ## [1] 0.8488308 plot(error) 3.3 Convolutional neural netwrok 3.3.1 Import library library(keras) 3.3.2 Importing the data mnist &lt;- dataset_mnist() mnist is list; it contains trainx, trainy, testx, testy class(mnist) ## [1] &quot;list&quot; the dim of “mnist\\(train\\)x” is 60000 28 28 # head(mnist) 3.3.3 preparing the data randomly sampling 1000 cases for training and 100 for testing set.seed(123) index &lt;- sample(nrow(mnist$train$x), 1000) x_train &lt;- mnist$train$x[index,,] y_train &lt;- (mnist$train$y[index]) index &lt;- sample(nrow(mnist$test$x), 100) x_test &lt;- mnist$test$x[index,,] y_test &lt;- (mnist$test$y[index]) dim of four data sets dim(x_train) ## [1] 1000 28 28 dim(y_train) ## [1] 1000 dim(x_test) ## [1] 100 28 28 dim(y_test) ## [1] 100 3.3.3.1 Generate tensors each image is 28*28 pixel size; pass these values to computer img_rows &lt;- 28 img_cols &lt;- 28 using array_reshape() function to transform list data into tensors x_train &lt;- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1)) x_test &lt;- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1)) input_shape &lt;- c(img_rows, img_cols, 1) this below is tensor data dim(x_train) ## [1] 1000 28 28 1 3.3.3.2 Normalization and one-hot-encoded (dummy) training (features) data is rescaled by dividing the maxmimum to be normalized x_train &lt;- x_train / 255 x_test &lt;- x_test / 255 converse targets into one-hot-encoded (dummy) type using to_categorical() function num_classes = 10 y_train &lt;- to_categorical(y_train, num_classes) y_test &lt;- to_categorical(y_test, num_classes) y_train[1,] ## [1] 0 0 0 0 0 0 1 0 0 0 3.3.4 Creating the model model &lt;- keras_model_sequential() %&gt;% layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = &#39;relu&#39;, input_shape = input_shape) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = &#39;relu&#39;) %&gt;% layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% layer_dropout(rate = 0.25) %&gt;% layer_flatten() %&gt;% layer_dense(units = 128, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_dense(units = num_classes, activation = &#39;softmax&#39;) summary of model model %&gt;% summary() ## Model: &quot;sequential_2&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## conv2d_1 (Conv2D) (None, 26, 26, 32) 320 ## conv2d (Conv2D) (None, 24, 24, 64) 18496 ## max_pooling2d (MaxPooling2D) (None, 12, 12, 64) 0 ## dropout_4 (Dropout) (None, 12, 12, 64) 0 ## flatten (Flatten) (None, 9216) 0 ## dense_5 (Dense) (None, 128) 1179776 ## dropout_3 (Dropout) (None, 128) 0 ## dense_4 (Dense) (None, 10) 1290 ## ================================================================================ ## Total params: 1,199,882 ## Trainable params: 1,199,882 ## Non-trainable params: 0 ## ________________________________________________________________________________ 3.3.4.1 compiling loss function is categorical crossentropy; the gradient descent will be optimized by adadelta; model %&gt;% compile( loss = loss_categorical_crossentropy, optimizer = optimizer_adadelta(), metrics = c(&#39;accuracy&#39;) ) 3.3.5 Training batch_size &lt;- 128 epochs &lt;- 10 # Train model history &lt;- model %&gt;% fit( x_train, y_train, batch_size = batch_size, epochs = epochs, validation_split = 0.2 ) plot(history) 3.3.6 Evaluating the accuracy score &lt;- model %&gt;% evaluate(x_test, y_test) score ## loss accuracy ## 0.2477634 0.9300000 "],["data-visualization.html", "Chapter 4 Data visualization 4.1 Data visualization introduction 4.2 Scatter plot 4.3 Bar chart 4.4 Line charts", " Chapter 4 Data visualization 4.1 Data visualization introduction 4.1.1 Summarization library(tidyverse) library(dplyr) mtcars %&gt;% mutate( kml = mpg * 0.42) %&gt;% group_by(cyl) %&gt;% summarise(avg_US = mean(mpg), avg_metric = mean(kml)) ## # A tibble: 3 × 3 ## cyl avg_US avg_metric ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 26.7 11.2 ## 2 6 19.7 8.29 ## 3 8 15.1 6.34 mpg %&gt;% group_by(manufacturer, year) %&gt;% summarise_at(vars(cty, hwy), mean) ## # A tibble: 30 × 4 ## # Groups: manufacturer [15] ## manufacturer year cty hwy ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi 1999 17.1 26.1 ## 2 audi 2008 18.1 26.8 ## 3 chevrolet 1999 15.1 21.6 ## 4 chevrolet 2008 14.9 22.1 ## 5 dodge 1999 13.4 18.4 ## 6 dodge 2008 13.0 17.6 ## 7 ford 1999 13.9 18.6 ## 8 ford 2008 14.1 20.5 ## 9 honda 1999 24.8 31.6 ## 10 honda 2008 24 33.8 ## # … with 20 more rows change layout mpg %&gt;% count(class, year)%&gt;% spread(class, n) ## # A tibble: 2 × 8 ## year `2seater` compact midsize minivan pickup subcompact suv ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1999 2 25 20 6 16 19 29 ## 2 2008 3 22 21 5 17 16 33 change all characters into factors mpg &lt;- mpg %&gt;% mutate_if(is.character, as.factor) #if a column is a character, change to a factor wide to long data mpg1 &lt;- mpg %&gt;% gather(&quot;key&quot;, &quot;value&quot;, cty, hwy) convert wide data to long data using pivot_longer ## Your code here. Naming choices for 1 and 2 are yours dta &lt;- mpg %&gt;% pivot_longer(cty:hwy, names_to = &quot;var&quot;, values_to = &quot;value&quot;) %&gt;% # Both of those are # value label mutate(var = ifelse( var == &#39;cty&#39;, &#39;city&#39;,&#39;highway&#39;)) ggplot(dta, aes(x = displ, y = value)) + geom_point(aes(color = var)) + geom_smooth(aes(color = var), se = F) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; explore distribution library(DataExplorer) library(psych) ## ## Attaching package: &#39;psych&#39; ## The following object is masked from &#39;package:Hmisc&#39;: ## ## describe ## The following object is masked from &#39;package:plotrix&#39;: ## ## rescale ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha library(naniar) ## ## Attaching package: &#39;naniar&#39; ## The following object is masked from &#39;package:skimr&#39;: ## ## n_complete plot_histogram(riskfactors) explore relationship/correlation library(psych) pairs.panels(riskfactors[,1:10]) create a individual theme my_theme &lt;- function(){ theme_bw() + theme(axis.title = element_text(size=16), axis.text = element_text(size=14), text = element_text(size = 14)) } 4.1.2 Explore missing values # install.packages(&quot;naniar&quot;) library(naniar) # head(riskfactors) riskfactors &lt;- riskfactors gg_miss_upset(riskfactors,nsets=10) # install.packages(&quot;DataExplorer&quot;) plot_missing(riskfactors) # take a quick look at the data types of each column visdat::vis_dat(riskfactors) 4.1.3 Add statistical test library(ggpubr) plt &lt;- ggplot( data=mpg, mapping= aes(x = as.factor(year), y = cty, color = as.factor(year) ) )+ geom_boxplot() + geom_jitter(width=0.1)+ labs(x = &#39;Year&#39;, y = &quot;City mpg&quot;) + my_theme()+ facet_wrap( ~ manufacturer,nrow = 2) # add statistical test my_comparisons &lt;- list(c(&#39;1999&#39;,&#39;2008&#39;)) plt + stat_compare_means() + stat_compare_means(comparisons = my_comparisons) 4.1.4 Add texts to dots USArrests &lt;- USArrests %&gt;% rownames_to_column(&#39;State&#39;) ggplot(USArrests, aes( x=UrbanPop,y=Murder))+ geom_point() + labs(x = &quot;Percent of population that is urban&quot;, y = &quot;Murder arrests (per 100,000)&quot;, caption = &quot;McNeil (1997). Interactive Data Analysis&quot;)+ geom_text(aes(label=State),size=3) 4.1.5 Set the legend ggplot(iris, aes(x= Sepal.Length , fill= as.factor( Species)) ) + #whole plot&#39;s option geom_histogram(aes(y=..density..),alpha=0.5, position=&quot;identity&quot; , bins = 50)+ geom_density(aes(linetype=as.factor(Species)),alpha=.1 )+ #aesthetic&#39;s option scale_fill_manual( name = &quot;Groups&quot;,values = c(&quot;grey&quot;, &quot;black&quot;, &quot;skyblue&quot;),labels = c(&quot;setosa&quot;, &quot;versicolor&quot; , &quot;virginica&quot; ))+ scale_linetype_manual( name = &quot;Groups&quot; ,values = c(1,3,5),labels = c(&quot;setosa&quot;, &quot;versicolor&quot; , &quot;virginica&quot;) )+ # common legend labs(x = &quot;Sepal.Length&quot;, y = &quot;Density&quot;, title = &quot;&quot;) 4.1.6 Create a panel of plots combine multiple plots into one p1=ggplot(data=riskfactors,aes(x=age))+ geom_histogram(bins = 30 ) p2=ggplot(data=riskfactors,aes(x=sex))+ geom_bar (aes(x=sex) ) p3=ggplot(riskfactors,aes(x = education, y = bmi))+ geom_boxplot ( ) p4=ggplot(riskfactors, aes(x = marital )) + geom_bar(aes(group = education, y = (..count..)/sum(..count..),fill = education)) + scale_y_continuous(labels=scales::percent) # install.packages(&quot;ggpubr&quot;) library(ggpubr) ggarrange(p1, p2, p3, p4, ncol = 2, nrow=2) 4.1.7 Plots in regression create linear regression model data(&quot;Boston&quot;, package = &quot;MASS&quot;) linear_reg &lt;- glm(medv ~ ., data=Boston , family = gaussian()) summary(linear_reg) ## ## Call: ## glm(formula = medv ~ ., family = gaussian(), data = Boston) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -15.595 -2.730 -0.518 1.777 26.199 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.646e+01 5.103e+00 7.144 3.28e-12 *** ## crim -1.080e-01 3.286e-02 -3.287 0.001087 ** ## zn 4.642e-02 1.373e-02 3.382 0.000778 *** ## indus 2.056e-02 6.150e-02 0.334 0.738288 ## chas 2.687e+00 8.616e-01 3.118 0.001925 ** ## nox -1.777e+01 3.820e+00 -4.651 4.25e-06 *** ## rm 3.810e+00 4.179e-01 9.116 &lt; 2e-16 *** ## age 6.922e-04 1.321e-02 0.052 0.958229 ## dis -1.476e+00 1.995e-01 -7.398 6.01e-13 *** ## rad 3.060e-01 6.635e-02 4.613 5.07e-06 *** ## tax -1.233e-02 3.760e-03 -3.280 0.001112 ** ## ptratio -9.527e-01 1.308e-01 -7.283 1.31e-12 *** ## black 9.312e-03 2.686e-03 3.467 0.000573 *** ## lstat -5.248e-01 5.072e-02 -10.347 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 22.51785) ## ## Null deviance: 42716 on 505 degrees of freedom ## Residual deviance: 11079 on 492 degrees of freedom ## AIC: 3027.6 ## ## Number of Fisher Scoring iterations: 2 summary knitr::kable(broom::tidy(linear_reg)) term estimate std.error statistic p.value (Intercept) 36.4594884 5.1034588 7.1440742 0.0000000 crim -0.1080114 0.0328650 -3.2865169 0.0010868 zn 0.0464205 0.0137275 3.3815763 0.0007781 indus 0.0205586 0.0614957 0.3343100 0.7382881 chas 2.6867338 0.8615798 3.1183809 0.0019250 nox -17.7666112 3.8197437 -4.6512574 0.0000042 rm 3.8098652 0.4179253 9.1161402 0.0000000 age 0.0006922 0.0132098 0.0524024 0.9582293 dis -1.4755668 0.1994547 -7.3980036 0.0000000 rad 0.3060495 0.0663464 4.6128998 0.0000051 tax -0.0123346 0.0037605 -3.2800091 0.0011116 ptratio -0.9527472 0.1308268 -7.2825106 0.0000000 black 0.0093117 0.0026860 3.4667926 0.0005729 lstat -0.5247584 0.0507153 -10.3471458 0.0000000 create logistical regression # load the Pima Indians dataset from the mlbench dataset library(mlbench) data(PimaIndiansDiabetes) # rename dataset to have shorter name because lazy diabetes &lt;- PimaIndiansDiabetes logistic_reg &lt;- glm(diabetes ~ ., data=diabetes, family = binomial) summary(logistic_reg) ## ## Call: ## glm(formula = diabetes ~ ., family = binomial, data = diabetes) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.5566 -0.7274 -0.4159 0.7267 2.9297 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.4046964 0.7166359 -11.728 &lt; 2e-16 *** ## pregnant 0.1231823 0.0320776 3.840 0.000123 *** ## glucose 0.0351637 0.0037087 9.481 &lt; 2e-16 *** ## pressure -0.0132955 0.0052336 -2.540 0.011072 * ## triceps 0.0006190 0.0068994 0.090 0.928515 ## insulin -0.0011917 0.0009012 -1.322 0.186065 ## mass 0.0897010 0.0150876 5.945 2.76e-09 *** ## pedigree 0.9451797 0.2991475 3.160 0.001580 ** ## age 0.0148690 0.0093348 1.593 0.111192 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 993.48 on 767 degrees of freedom ## Residual deviance: 723.45 on 759 degrees of freedom ## AIC: 741.45 ## ## Number of Fisher Scoring iterations: 5 summary knitr::kable(broom::tidy(logistic_reg)) term estimate std.error statistic p.value (Intercept) -8.4046964 0.7166359 -11.7279870 0.0000000 pregnant 0.1231823 0.0320776 3.8401403 0.0001230 glucose 0.0351637 0.0037087 9.4813935 0.0000000 pressure -0.0132955 0.0052336 -2.5404160 0.0110721 triceps 0.0006190 0.0068994 0.0897131 0.9285152 insulin -0.0011917 0.0009012 -1.3223094 0.1860652 mass 0.0897010 0.0150876 5.9453340 0.0000000 pedigree 0.9451797 0.2991475 3.1595780 0.0015800 age 0.0148690 0.0093348 1.5928584 0.1111920 4.1.7.1 Create forest plots for coefficients or OR library(sjPlot) ## Registered S3 methods overwritten by &#39;effectsize&#39;: ## method from ## standardize.Surv datawizard ## standardize.bcplm datawizard ## standardize.clm2 datawizard ## standardize.default datawizard ## standardize.mediate datawizard ## standardize.wbgee datawizard ## standardize.wbm datawizard ## #refugeeswelcome plot_model(linear_reg, show.values = TRUE, value.offset = 0.5) plot_model(logistic_reg, show.values = TRUE, value.offset = .5, vline.color = &quot;black&quot;) another way library(finalfit) explanatory = c( &quot;crim&quot; , &quot;zn&quot; , &quot;indus&quot; , &quot;nox&quot; , &quot;rm&quot; , &quot;age&quot; , &quot;dis&quot; , &quot;rad&quot; , &quot;tax&quot; ,&quot;ptratio&quot; ,&quot;black&quot; , &quot;lstat&quot; ) dependent = &quot;medv&quot; Boston %&gt;% coefficient_plot(dependent, explanatory, table_text_size=3, title_text_size=12, plot_opts=list(xlab(&quot;Beta, 95% CI&quot;), theme(axis.title = element_text(size=12)))) library(finalfit) explanatory = c( &quot;pregnant&quot;, &quot;glucose&quot; , &quot;pressure&quot;, &quot;triceps&quot; ,&quot;insulin&quot; , &quot;mass&quot; , &quot;pedigree&quot;, &quot;age&quot; ) dependent = &quot;diabetes&quot; diabetes %&gt;% or_plot(dependent, explanatory, table_text_size=3, title_text_size=12, plot_opts=list(xlab(&quot;OR, 95% CI&quot;), theme(axis.title = element_text(size=12)))) ## Waiting for profiling to be done... ## Waiting for profiling to be done... ## Waiting for profiling to be done... qq plot ggqqplot( (Boston$medv)) Loading data set library(printr) ## Registered S3 method overwritten by &#39;printr&#39;: ## method from ## knit_print.data.frame rmarkdown library(tidyverse) head(iris) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.2 Scatter plot 4.2.1 Create a empty canvas then create aesthetic mapping tell the function which dataset and variables to use ggplot(data = iris, # which data set? canvas? aes(x=Sepal.Length , y=Petal.Length )) # which variables as aesthetics? x and y are mapped to columns of the data; different geoms can have different aesthetics (different variables). 4.2.2 Add a layer/geom of points to the canvas ggplot(data = iris, mapping = aes(x=Sepal.Length , y=Petal.Length )) + geom_point() # adding the geometrical representation # same plot as above ggplot(data = iris) + geom_point( aes(x=Sepal.Length , y=Petal.Length )) 4.2.3 Add another aesthetic add a curve/straight line to fit these points geom provides the aesthetic to ggplot # Loess curve ggplot(data = iris, mapping = aes(x=Sepal.Length , y=Petal.Length )) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; # Linear regression line ggplot(data = iris, mapping = aes(x=Sepal.Length , y=Petal.Length)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 4.2.4 Add other aesthetic set other aesthetics colour, alpha (transparency), and size of points ggplot(data = iris) + geom_point(aes(x=Sepal.Length , y=Petal.Length, size = Sepal.Width ), alpha = .5, colour = &quot;red&quot;) ggplot(data = iris) + geom_point(aes(x=Sepal.Length , y=Petal.Length, size = Sepal.Width , colour=Species), #white is a variable here alpha=.9) categorize Petal.Width then map colour to this new variable iris &lt;- iris %&gt;% mutate(growth = ifelse(Petal.Width &gt; 1.5, &quot;Wide&quot;, &quot;Normal&quot;)) ggplot(data=iris) + geom_point(aes(x=Sepal.Length , y=Petal.Length, size = Sepal.Width , colour=growth), alpha=.9) 4.3 Bar chart ggplot(data = iris) + geom_bar(aes(x = growth)) bar chart after group_by then use stat='identity' library(dplyr) results &lt;- iris %&gt;% group_by(Species, growth) %&gt;% summarise(Sepal.Length.mean=mean (Sepal.Length )) ## `summarise()` has grouped output by &#39;Species&#39;. You can override using the ## `.groups` argument. gop &lt;- results %&gt;% filter(Species != &quot;setosa_null&quot; ) gop ## # A tibble: 5 × 3 ## # Groups: Species [3] ## Species growth Sepal.Length.mean ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 setosa Normal 5.01 ## 2 versicolor Normal 5.91 ## 3 versicolor Wide 6.18 ## 4 virginica Normal 6.13 ## 5 virginica Wide 6.62 though meaningless below until line chart (just use the mean as the sum for demonstration) # We can also store parts of a plot in an object plot1 &lt;- ggplot(gop) + geom_bar(aes(x=growth , y=Sepal.Length.mean), stat=&#39;identity&#39;) plot1 ### Add some options for the whole ggplot rather than layers - switch the x and y axes plot1 + coord_flip() reorder x categories (-means descending) ggplot( gop) + geom_bar(aes(x=reorder(growth, -Sepal.Length.mean), y=Sepal.Length.mean, fill=growth), stat=&#39;identity&#39;) + coord_flip() add x axis label and a theme ggplot(gop) + geom_bar(aes(x=reorder(growth, -Sepal.Length.mean), y=Sepal.Length.mean, fill=growth), stat=&#39;identity&#39;) + coord_flip() + xlab(&quot;Growth categories&quot;) + guides(fill=F) + theme_minimal() set theme library(ggthemes) ggplot(data = iris) + geom_bar(aes(x = growth)) + theme_economist() 4.3.1 Grouped bar chart -bar chart with different panels ggplot(mpg, aes(x = class)) + geom_bar() + facet_wrap( ~ year) actual number (groups are stacked by default) ggplot(gop) + geom_bar(aes(x=growth, y=Sepal.Length.mean, fill=Species) , stat=&#39;identity&#39; ) ggplot(mpg, aes(x = class )) + geom_bar(aes(group = year, fill = year), position = &quot;stack&quot;) percentage ggplot(gop) + geom_bar(aes(x=growth, y=Sepal.Length.mean, fill=Species), stat=&#39;identity&#39;, position=&#39;fill&#39;) groups are dodge with actual number ggplot(gop) + geom_bar(aes(x=growth, y=Sepal.Length.mean, fill=Species), stat=&#39;identity&#39;, position=&#39;dodge&#39;) - groups are dodge with percentage gop2 &lt;- gop %&gt;% group_by(growth ) %&gt;% mutate(Sepal.Length.prop=Sepal.Length.mean/sum(Sepal.Length.mean)) ggplot(gop2) + geom_bar(aes(x=growth, y=Sepal.Length.prop, fill=Species), stat=&#39;identity&#39;, position=&#39;dodge&#39;) + ylab(&quot;Votes (%)&quot;) 4.4 Line charts ggplot(iris) + geom_line(aes(x=Sepal.Length , y=Petal.Length)) 4.4.1 Grouped by colour variable ggplot(iris) + geom_line(aes(x=Sepal.Length , y=Petal.Length, colour = Species)) grouped by state then set how many rows or columns ggplot(iris) + geom_line(aes(x=Sepal.Length , y=Petal.Length) ) + facet_wrap(~Species, nrow = 1) + #set how many rows coord_flip() 4.4.2 Multiple aesthetics iris &lt;- iris %&gt;% mutate(growth = ifelse(Petal.Width &gt; 1.5, &quot;Wide&quot;, &quot;Normal&quot;)) ggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) + geom_line(size=2,color=&quot;purple&quot;)+ # number format scale_x_log10(labels = scales::label_number())+ geom_point( aes(size = Sepal.Length,colour = as.factor(growth)),show.legend = F)+ facet_wrap(~ Species) "],["basic-statistics.html", "Chapter 5 Basic statistics 5.1 The essentials of R", " Chapter 5 Basic statistics 5.1 The essentials of R 5.1.1 Manipulation of vector library(tidyverse) library(dplyr) vec &lt;- c(3,5,2,1,5,&quot;O&quot;,NA) length(unique(vec)) ## [1] 6 num_vec &lt;- as.numeric(vec) log(num_vec) ## [1] 1.0986123 1.6094379 0.6931472 0.0000000 1.6094379 NA NA sum(c(num_vec, NA), na.rm=T) ## [1] 16 sort(num_vec, decreasing = T) ## [1] 5 5 3 2 1 is.na(num_vec) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE num_vec[!is.na(num_vec)] ## [1] 3 5 2 1 5 c(5,6) %in% vec ## [1] TRUE FALSE grepl(&quot;5&quot;, vec) ## [1] FALSE TRUE FALSE FALSE TRUE FALSE FALSE 5.1.2 Generate sequence or repeted sequece seq(from = 0, to = 10, by = 0.5) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 ## [16] 7.5 8.0 8.5 9.0 9.5 10.0 rep(x = 1:3, times = 4) ## [1] 1 2 3 1 2 3 1 2 3 1 2 3 rep(x = 1:3, each = 4) ## [1] 1 1 1 1 2 2 2 2 3 3 3 3 5.1.3 Get directory and write data out and in getwd() ## [1] &quot;C:/Users/hed2/Downloads/mybook2/mybook2&quot; setwd(getwd()) write.csv(cars, &quot;cars.csv&quot;, row.names=F) dataframe &lt;- read.csv(&quot;cars.csv&quot;) 5.1.4 Function my_func &lt;- function(x){ x_mod &lt;- (x + 7) * 4 return(x_mod) } my_func(num_vec) ## [1] 40 48 36 32 48 NA NA 5.1.5 Plot plot(dist ~ speed, data=cars) hist(cars$dist ) ### Build model and plot model &lt;- lm(dist ~ speed, data=cars) plot(dist ~ speed, data=cars) abline(model) abline(v = 25) abline(h = 15) ### Rename names of columns names(cars) ## [1] &quot;speed&quot; &quot;dist&quot; names(cars) &lt;- c(&quot;speed per hour&quot;, &quot;total dist&quot;) 5.1.6 Class of dataframe matrix &lt;- as.matrix(cars) df &lt;- as.data.frame(matrix) class(matrix) ## [1] &quot;matrix&quot; &quot;array&quot; class(df) ## [1] &quot;data.frame&quot; # tranform t(matrix) speed per hour 4 4 7 7 8 9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25 total dist 2 10 4 22 16 10 18 26 34 17 28 14 20 24 28 26 34 34 46 26 36 60 80 20 26 54 32 40 32 40 50 42 56 76 84 36 46 68 32 48 52 56 64 66 54 70 92 93 120 85 5.1.7 Generate new variable for dataframe (character) paste0(&quot;raster_&quot;, 1:10) ## [1] &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_6&quot; ## [7] &quot;raster_7&quot; &quot;raster_8&quot; &quot;raster_9&quot; &quot;raster_10&quot; paste0(&quot;raster_&quot;, rep(x = 1:5, times = 10)) ## [1] &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; ## [7] &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; ## [13] &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; ## [19] &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; ## [25] &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; ## [31] &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; ## [37] &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; ## [43] &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; ## [49] &quot;raster_4&quot; &quot;raster_5&quot; df$group &lt;- paste0(&quot;raster_&quot;, rep(x = 1:5, times = 10)) df$id &lt;- paste0(&quot;raster_&quot;, 1:50) 5.1.8 Create a new dataframe using ‘rnorm’ - random number from distribution sample &lt;- round((rnorm(50,0, 1)),2) group &lt;- paste0(&quot;raster_&quot;, rep(x = 1:5, times = 10)) df_join &lt;- data.frame(sample, group) df_join$id &lt;- paste0(&quot;raster_&quot;, 1:50) 5.1.9 Left join two dataframes library(dplyr) data_all &lt;- left_join(df, df_join, by=&quot;id&quot;) head(data_all) speed per hour total dist group.x id sample group.y 4 2 raster_1 raster_1 -0.58 raster_1 4 10 raster_2 raster_2 -0.01 raster_2 7 4 raster_3 raster_3 -1.78 raster_3 7 22 raster_4 raster_4 -0.78 raster_4 8 16 raster_5 raster_5 0.13 raster_5 9 10 raster_1 raster_6 -0.71 raster_1 5.1.10 Select variables select(data_all, group.x, id ) group.x id raster_1 raster_1 raster_2 raster_2 raster_3 raster_3 raster_4 raster_4 raster_5 raster_5 raster_1 raster_6 raster_2 raster_7 raster_3 raster_8 raster_4 raster_9 raster_5 raster_10 raster_1 raster_11 raster_2 raster_12 raster_3 raster_13 raster_4 raster_14 raster_5 raster_15 raster_1 raster_16 raster_2 raster_17 raster_3 raster_18 raster_4 raster_19 raster_5 raster_20 raster_1 raster_21 raster_2 raster_22 raster_3 raster_23 raster_4 raster_24 raster_5 raster_25 raster_1 raster_26 raster_2 raster_27 raster_3 raster_28 raster_4 raster_29 raster_5 raster_30 raster_1 raster_31 raster_2 raster_32 raster_3 raster_33 raster_4 raster_34 raster_5 raster_35 raster_1 raster_36 raster_2 raster_37 raster_3 raster_38 raster_4 raster_39 raster_5 raster_40 raster_1 raster_41 raster_2 raster_42 raster_3 raster_43 raster_4 raster_44 raster_5 raster_45 raster_1 raster_46 raster_2 raster_47 raster_3 raster_48 raster_4 raster_49 raster_5 raster_50 5.1.11 Filter observations raster_1 &lt;- filter(data_all, group.x == &quot;raster_1&quot;) raster_1 speed per hour total dist group.x id sample group.y 4 2 raster_1 raster_1 -0.58 raster_1 9 10 raster_1 raster_6 -0.71 raster_1 11 28 raster_1 raster_11 -0.82 raster_1 13 26 raster_1 raster_16 2.06 raster_1 14 36 raster_1 raster_21 -0.65 raster_1 15 54 raster_1 raster_26 -0.76 raster_1 17 50 raster_1 raster_31 -0.99 raster_1 19 36 raster_1 raster_36 -0.43 raster_1 20 52 raster_1 raster_41 -2.17 raster_1 24 70 raster_1 raster_46 -0.99 raster_1 speed_dist &lt;- filter(data_all, data_all$`speed per hour` &lt; 11 &amp; data_all$`total dist` &gt;= 10) speed_dist speed per hour total dist group.x id sample group.y 4 10 raster_2 raster_2 -0.01 raster_2 7 22 raster_4 raster_4 -0.78 raster_4 8 16 raster_5 raster_5 0.13 raster_5 9 10 raster_1 raster_6 -0.71 raster_1 10 18 raster_2 raster_7 -0.04 raster_2 10 26 raster_3 raster_8 -0.47 raster_3 10 34 raster_4 raster_9 0.61 raster_4 5.1.12 Append rows rbind(raster_1,speed_dist) speed per hour total dist group.x id sample group.y 4 2 raster_1 raster_1 -0.58 raster_1 9 10 raster_1 raster_6 -0.71 raster_1 11 28 raster_1 raster_11 -0.82 raster_1 13 26 raster_1 raster_16 2.06 raster_1 14 36 raster_1 raster_21 -0.65 raster_1 15 54 raster_1 raster_26 -0.76 raster_1 17 50 raster_1 raster_31 -0.99 raster_1 19 36 raster_1 raster_36 -0.43 raster_1 20 52 raster_1 raster_41 -2.17 raster_1 24 70 raster_1 raster_46 -0.99 raster_1 4 10 raster_2 raster_2 -0.01 raster_2 7 22 raster_4 raster_4 -0.78 raster_4 8 16 raster_5 raster_5 0.13 raster_5 9 10 raster_1 raster_6 -0.71 raster_1 10 18 raster_2 raster_7 -0.04 raster_2 10 26 raster_3 raster_8 -0.47 raster_3 10 34 raster_4 raster_9 0.61 raster_4 5.1.13 Create new variables instead of old variables mutate(data_all, sample = round(sample,1)) speed per hour total dist group.x id sample group.y 4 2 raster_1 raster_1 -0.6 raster_1 4 10 raster_2 raster_2 0.0 raster_2 7 4 raster_3 raster_3 -1.8 raster_3 7 22 raster_4 raster_4 -0.8 raster_4 8 16 raster_5 raster_5 0.1 raster_5 9 10 raster_1 raster_6 -0.7 raster_1 10 18 raster_2 raster_7 0.0 raster_2 10 26 raster_3 raster_8 -0.5 raster_3 10 34 raster_4 raster_9 0.6 raster_4 11 17 raster_5 raster_10 1.2 raster_5 11 28 raster_1 raster_11 -0.8 raster_1 12 14 raster_2 raster_12 -0.3 raster_2 12 20 raster_3 raster_13 1.4 raster_3 12 24 raster_4 raster_14 -2.2 raster_4 12 28 raster_5 raster_15 -0.3 raster_5 13 26 raster_1 raster_16 2.1 raster_1 13 34 raster_2 raster_17 2.2 raster_2 13 34 raster_3 raster_18 0.2 raster_3 13 46 raster_4 raster_19 -0.9 raster_4 14 26 raster_5 raster_20 0.2 raster_5 14 36 raster_1 raster_21 -0.7 raster_1 14 60 raster_2 raster_22 1.4 raster_2 14 80 raster_3 raster_23 -0.8 raster_3 15 20 raster_4 raster_24 0.4 raster_4 15 26 raster_5 raster_25 0.6 raster_5 15 54 raster_1 raster_26 -0.8 raster_1 16 32 raster_2 raster_27 0.8 raster_2 16 40 raster_3 raster_28 -0.8 raster_3 17 32 raster_4 raster_29 0.6 raster_4 17 40 raster_5 raster_30 1.1 raster_5 17 50 raster_1 raster_31 -1.0 raster_1 18 42 raster_2 raster_32 1.1 raster_2 18 56 raster_3 raster_33 -0.5 raster_3 18 76 raster_4 raster_34 0.3 raster_4 18 84 raster_5 raster_35 0.2 raster_5 19 36 raster_1 raster_36 -0.4 raster_1 19 46 raster_2 raster_37 0.3 raster_2 19 68 raster_3 raster_38 -1.2 raster_3 20 32 raster_4 raster_39 -0.1 raster_4 20 48 raster_5 raster_40 0.8 raster_5 20 52 raster_1 raster_41 -2.2 raster_1 20 56 raster_2 raster_42 -1.5 raster_2 20 64 raster_3 raster_43 -1.2 raster_3 22 66 raster_4 raster_44 -1.6 raster_4 23 54 raster_5 raster_45 0.4 raster_5 24 70 raster_1 raster_46 -1.0 raster_1 24 92 raster_2 raster_47 -2.2 raster_2 24 93 raster_3 raster_48 -0.6 raster_3 24 120 raster_4 raster_49 -0.4 raster_4 25 85 raster_5 raster_50 0.9 raster_5 5.1.14 summarise statistics summarise(data_all, mean_speed = mean(sample), max_dist = max( &quot;total dist&quot; )) mean_speed max_dist -0.1758 total dist 5.1.15 Group dataframe then summarise statistics data_all_group &lt;- group_by(data_all, group.x) summarise(data_all_group, mean_speed = mean(sample), max_dist = max( &quot;total dist&quot; )) group.x mean_speed max_dist raster_1 -0.604 total dist raster_2 0.186 total dist raster_3 -0.573 total dist raster_4 -0.406 total dist raster_5 0.518 total dist 5.1.16 Ungroup then summarise statistics ungroup_data &lt;- ungroup( data_all_group) summarise( ungroup_data , mean_speed = mean(sample), max_dist = max( &quot;total dist&quot; )) mean_speed max_dist -0.1758 total dist 5.1.17 Summary linear regression model mod1 &lt;- lm(cars$`total dist` ~ cars$`speed per hour` ) summary(mod1) ## ## Call: ## lm(formula = cars$`total dist` ~ cars$`speed per hour`) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## cars$`speed per hour` 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 5.1.18 Create frequency table table(data_all_group$`speed per hour`,data_all_group$group.x ) / raster_1 raster_2 raster_3 raster_4 raster_5 4 1 1 0 0 0 7 0 0 1 1 0 8 0 0 0 0 1 9 1 0 0 0 0 10 0 1 1 1 0 11 1 0 0 0 1 12 0 1 1 1 1 13 1 1 1 1 0 14 1 1 1 0 1 15 1 0 0 1 1 16 0 1 1 0 0 17 1 0 0 1 1 18 0 1 1 1 1 19 1 1 1 0 0 20 1 1 1 1 1 22 0 0 0 1 0 23 0 0 0 0 1 24 1 1 1 1 0 25 0 0 0 0 1 5.1.19 Value and variable label table(iris$Species) setosa versicolor virginica 50 50 50 iris$Species &lt;- factor(iris$Species,labels = c( &quot;setosanew&quot;,&quot;versicolornew&quot;,&quot;virginianew&quot;)) table(iris$Species) setosanew versicolornew virginianew 50 50 50 library(Hmisc) label(iris$Species) &lt;- &quot;Species types&quot; table(iris$Species) setosanew versicolornew virginianew 50 50 50 5.1.20 Recode a variable irisifelse &lt;- iris%&gt;% mutate(Sepal.Length2 = ifelse(Sepal.Length &lt; 6 , &quot;level1&quot;, ifelse(Sepal.Length &lt; 7 , &quot;level2&quot;, Sepal.Length))) table(irisifelse$Sepal.Length2) 7 7.1 7.2 7.3 7.4 7.6 7.7 7.9 level1 level2 1 1 3 1 1 1 4 1 83 54 "],["statistical-models.html", "Chapter 6 Statistical models", " Chapter 6 Statistical models Stay tuned! "],["probability.html", "Chapter 7 Probability", " Chapter 7 Probability Stay tuned! "],["algorithms.html", "Chapter 8 Algorithms", " Chapter 8 Algorithms Stay tuned! "],["sasmarkdown.html", "Chapter 9 SASmarkdown", " Chapter 9 SASmarkdown Stay tuned! "],["miscellaneous.html", "Chapter 10 Miscellaneous", " Chapter 10 Miscellaneous Stay tuned! "],["english.html", "Chapter 11 English", " Chapter 11 English Stay tuned! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
