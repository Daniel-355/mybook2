[["index.html", "My Little Handbook About", " My Little Handbook Daniel He 2022-07-08 About This is my first book. First of all, sincerely thanks where thanks are due. “In God we Trust, all others bring data”. "],["data-wrangling.html", "1 Data Wrangling 1.1 How to do data wrangling 1.2 How to do aggregation/ summarization", " 1 Data Wrangling 1.1 How to do data wrangling We will use tidyverse package to work with data. 1.1.1 Load data and package head (iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✔ ggplot2 3.3.5 ✔ purrr 0.3.4 ## ✔ tibble 3.1.6 ✔ dplyr 1.0.8 ## ✔ tidyr 1.2.0 ✔ stringr 1.4.0 ## ✔ readr 2.1.2 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 1.1.2 Select certain rows setosa &lt;- filter(iris, Species == &#39;setosa&#39;) setosa ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## 11 5.4 3.7 1.5 0.2 setosa ## 12 4.8 3.4 1.6 0.2 setosa ## 13 4.8 3.0 1.4 0.1 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 15 5.8 4.0 1.2 0.2 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 17 5.4 3.9 1.3 0.4 setosa ## 18 5.1 3.5 1.4 0.3 setosa ## 19 5.7 3.8 1.7 0.3 setosa ## 20 5.1 3.8 1.5 0.3 setosa ## 21 5.4 3.4 1.7 0.2 setosa ## 22 5.1 3.7 1.5 0.4 setosa ## 23 4.6 3.6 1.0 0.2 setosa ## 24 5.1 3.3 1.7 0.5 setosa ## 25 4.8 3.4 1.9 0.2 setosa ## 26 5.0 3.0 1.6 0.2 setosa ## 27 5.0 3.4 1.6 0.4 setosa ## 28 5.2 3.5 1.5 0.2 setosa ## 29 5.2 3.4 1.4 0.2 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 31 4.8 3.1 1.6 0.2 setosa ## 32 5.4 3.4 1.5 0.4 setosa ## 33 5.2 4.1 1.5 0.1 setosa ## 34 5.5 4.2 1.4 0.2 setosa ## 35 4.9 3.1 1.5 0.2 setosa ## 36 5.0 3.2 1.2 0.2 setosa ## 37 5.5 3.5 1.3 0.2 setosa ## 38 4.9 3.6 1.4 0.1 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 40 5.1 3.4 1.5 0.2 setosa ## 41 5.0 3.5 1.3 0.3 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 44 5.0 3.5 1.6 0.6 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 47 5.1 3.8 1.6 0.2 setosa ## 48 4.6 3.2 1.4 0.2 setosa ## 49 5.3 3.7 1.5 0.2 setosa ## 50 5.0 3.3 1.4 0.2 setosa 1.1.3 Select certain columns select(iris, Sepal.Length, Species) ## Sepal.Length Species ## 1 5.1 setosa ## 2 4.9 setosa ## 3 4.7 setosa ## 4 4.6 setosa ## 5 5.0 setosa ## 6 5.4 setosa ## 7 4.6 setosa ## 8 5.0 setosa ## 9 4.4 setosa ## 10 4.9 setosa ## 11 5.4 setosa ## 12 4.8 setosa ## 13 4.8 setosa ## 14 4.3 setosa ## 15 5.8 setosa ## 16 5.7 setosa ## 17 5.4 setosa ## 18 5.1 setosa ## 19 5.7 setosa ## 20 5.1 setosa ## 21 5.4 setosa ## 22 5.1 setosa ## 23 4.6 setosa ## 24 5.1 setosa ## 25 4.8 setosa ## 26 5.0 setosa ## 27 5.0 setosa ## 28 5.2 setosa ## 29 5.2 setosa ## 30 4.7 setosa ## 31 4.8 setosa ## 32 5.4 setosa ## 33 5.2 setosa ## 34 5.5 setosa ## 35 4.9 setosa ## 36 5.0 setosa ## 37 5.5 setosa ## 38 4.9 setosa ## 39 4.4 setosa ## 40 5.1 setosa ## 41 5.0 setosa ## 42 4.5 setosa ## 43 4.4 setosa ## 44 5.0 setosa ## 45 5.1 setosa ## 46 4.8 setosa ## 47 5.1 setosa ## 48 4.6 setosa ## 49 5.3 setosa ## 50 5.0 setosa ## 51 7.0 versicolor ## 52 6.4 versicolor ## 53 6.9 versicolor ## 54 5.5 versicolor ## 55 6.5 versicolor ## 56 5.7 versicolor ## 57 6.3 versicolor ## 58 4.9 versicolor ## 59 6.6 versicolor ## 60 5.2 versicolor ## 61 5.0 versicolor ## 62 5.9 versicolor ## 63 6.0 versicolor ## 64 6.1 versicolor ## 65 5.6 versicolor ## 66 6.7 versicolor ## 67 5.6 versicolor ## 68 5.8 versicolor ## 69 6.2 versicolor ## 70 5.6 versicolor ## 71 5.9 versicolor ## 72 6.1 versicolor ## 73 6.3 versicolor ## 74 6.1 versicolor ## 75 6.4 versicolor ## 76 6.6 versicolor ## 77 6.8 versicolor ## 78 6.7 versicolor ## 79 6.0 versicolor ## 80 5.7 versicolor ## 81 5.5 versicolor ## 82 5.5 versicolor ## 83 5.8 versicolor ## 84 6.0 versicolor ## 85 5.4 versicolor ## 86 6.0 versicolor ## 87 6.7 versicolor ## 88 6.3 versicolor ## 89 5.6 versicolor ## 90 5.5 versicolor ## 91 5.5 versicolor ## 92 6.1 versicolor ## 93 5.8 versicolor ## 94 5.0 versicolor ## 95 5.6 versicolor ## 96 5.7 versicolor ## 97 5.7 versicolor ## 98 6.2 versicolor ## 99 5.1 versicolor ## 100 5.7 versicolor ## 101 6.3 virginica ## 102 5.8 virginica ## 103 7.1 virginica ## 104 6.3 virginica ## 105 6.5 virginica ## 106 7.6 virginica ## 107 4.9 virginica ## 108 7.3 virginica ## 109 6.7 virginica ## 110 7.2 virginica ## 111 6.5 virginica ## 112 6.4 virginica ## 113 6.8 virginica ## 114 5.7 virginica ## 115 5.8 virginica ## 116 6.4 virginica ## 117 6.5 virginica ## 118 7.7 virginica ## 119 7.7 virginica ## 120 6.0 virginica ## 121 6.9 virginica ## 122 5.6 virginica ## 123 7.7 virginica ## 124 6.3 virginica ## 125 6.7 virginica ## 126 7.2 virginica ## 127 6.2 virginica ## 128 6.1 virginica ## 129 6.4 virginica ## 130 7.2 virginica ## 131 7.4 virginica ## 132 7.9 virginica ## 133 6.4 virginica ## 134 6.3 virginica ## 135 6.1 virginica ## 136 7.7 virginica ## 137 6.3 virginica ## 138 6.4 virginica ## 139 6.0 virginica ## 140 6.9 virginica ## 141 6.7 virginica ## 142 6.9 virginica ## 143 5.8 virginica ## 144 6.8 virginica ## 145 6.7 virginica ## 146 6.7 virginica ## 147 6.3 virginica ## 148 6.5 virginica ## 149 6.2 virginica ## 150 5.9 virginica select(iris, -Sepal.Length, -Species) ## Sepal.Width Petal.Length Petal.Width ## 1 3.5 1.4 0.2 ## 2 3.0 1.4 0.2 ## 3 3.2 1.3 0.2 ## 4 3.1 1.5 0.2 ## 5 3.6 1.4 0.2 ## 6 3.9 1.7 0.4 ## 7 3.4 1.4 0.3 ## 8 3.4 1.5 0.2 ## 9 2.9 1.4 0.2 ## 10 3.1 1.5 0.1 ## 11 3.7 1.5 0.2 ## 12 3.4 1.6 0.2 ## 13 3.0 1.4 0.1 ## 14 3.0 1.1 0.1 ## 15 4.0 1.2 0.2 ## 16 4.4 1.5 0.4 ## 17 3.9 1.3 0.4 ## 18 3.5 1.4 0.3 ## 19 3.8 1.7 0.3 ## 20 3.8 1.5 0.3 ## 21 3.4 1.7 0.2 ## 22 3.7 1.5 0.4 ## 23 3.6 1.0 0.2 ## 24 3.3 1.7 0.5 ## 25 3.4 1.9 0.2 ## 26 3.0 1.6 0.2 ## 27 3.4 1.6 0.4 ## 28 3.5 1.5 0.2 ## 29 3.4 1.4 0.2 ## 30 3.2 1.6 0.2 ## 31 3.1 1.6 0.2 ## 32 3.4 1.5 0.4 ## 33 4.1 1.5 0.1 ## 34 4.2 1.4 0.2 ## 35 3.1 1.5 0.2 ## 36 3.2 1.2 0.2 ## 37 3.5 1.3 0.2 ## 38 3.6 1.4 0.1 ## 39 3.0 1.3 0.2 ## 40 3.4 1.5 0.2 ## 41 3.5 1.3 0.3 ## 42 2.3 1.3 0.3 ## 43 3.2 1.3 0.2 ## 44 3.5 1.6 0.6 ## 45 3.8 1.9 0.4 ## 46 3.0 1.4 0.3 ## 47 3.8 1.6 0.2 ## 48 3.2 1.4 0.2 ## 49 3.7 1.5 0.2 ## 50 3.3 1.4 0.2 ## 51 3.2 4.7 1.4 ## 52 3.2 4.5 1.5 ## 53 3.1 4.9 1.5 ## 54 2.3 4.0 1.3 ## 55 2.8 4.6 1.5 ## 56 2.8 4.5 1.3 ## 57 3.3 4.7 1.6 ## 58 2.4 3.3 1.0 ## 59 2.9 4.6 1.3 ## 60 2.7 3.9 1.4 ## 61 2.0 3.5 1.0 ## 62 3.0 4.2 1.5 ## 63 2.2 4.0 1.0 ## 64 2.9 4.7 1.4 ## 65 2.9 3.6 1.3 ## 66 3.1 4.4 1.4 ## 67 3.0 4.5 1.5 ## 68 2.7 4.1 1.0 ## 69 2.2 4.5 1.5 ## 70 2.5 3.9 1.1 ## 71 3.2 4.8 1.8 ## 72 2.8 4.0 1.3 ## 73 2.5 4.9 1.5 ## 74 2.8 4.7 1.2 ## 75 2.9 4.3 1.3 ## 76 3.0 4.4 1.4 ## 77 2.8 4.8 1.4 ## 78 3.0 5.0 1.7 ## 79 2.9 4.5 1.5 ## 80 2.6 3.5 1.0 ## 81 2.4 3.8 1.1 ## 82 2.4 3.7 1.0 ## 83 2.7 3.9 1.2 ## 84 2.7 5.1 1.6 ## 85 3.0 4.5 1.5 ## 86 3.4 4.5 1.6 ## 87 3.1 4.7 1.5 ## 88 2.3 4.4 1.3 ## 89 3.0 4.1 1.3 ## 90 2.5 4.0 1.3 ## 91 2.6 4.4 1.2 ## 92 3.0 4.6 1.4 ## 93 2.6 4.0 1.2 ## 94 2.3 3.3 1.0 ## 95 2.7 4.2 1.3 ## 96 3.0 4.2 1.2 ## 97 2.9 4.2 1.3 ## 98 2.9 4.3 1.3 ## 99 2.5 3.0 1.1 ## 100 2.8 4.1 1.3 ## 101 3.3 6.0 2.5 ## 102 2.7 5.1 1.9 ## 103 3.0 5.9 2.1 ## 104 2.9 5.6 1.8 ## 105 3.0 5.8 2.2 ## 106 3.0 6.6 2.1 ## 107 2.5 4.5 1.7 ## 108 2.9 6.3 1.8 ## 109 2.5 5.8 1.8 ## 110 3.6 6.1 2.5 ## 111 3.2 5.1 2.0 ## 112 2.7 5.3 1.9 ## 113 3.0 5.5 2.1 ## 114 2.5 5.0 2.0 ## 115 2.8 5.1 2.4 ## 116 3.2 5.3 2.3 ## 117 3.0 5.5 1.8 ## 118 3.8 6.7 2.2 ## 119 2.6 6.9 2.3 ## 120 2.2 5.0 1.5 ## 121 3.2 5.7 2.3 ## 122 2.8 4.9 2.0 ## 123 2.8 6.7 2.0 ## 124 2.7 4.9 1.8 ## 125 3.3 5.7 2.1 ## 126 3.2 6.0 1.8 ## 127 2.8 4.8 1.8 ## 128 3.0 4.9 1.8 ## 129 2.8 5.6 2.1 ## 130 3.0 5.8 1.6 ## 131 2.8 6.1 1.9 ## 132 3.8 6.4 2.0 ## 133 2.8 5.6 2.2 ## 134 2.8 5.1 1.5 ## 135 2.6 5.6 1.4 ## 136 3.0 6.1 2.3 ## 137 3.4 5.6 2.4 ## 138 3.1 5.5 1.8 ## 139 3.0 4.8 1.8 ## 140 3.1 5.4 2.1 ## 141 3.1 5.6 2.4 ## 142 3.1 5.1 2.3 ## 143 2.7 5.1 1.9 ## 144 3.2 5.9 2.3 ## 145 3.3 5.7 2.5 ## 146 3.0 5.2 2.3 ## 147 2.5 5.0 1.9 ## 148 3.0 5.2 2.0 ## 149 3.4 5.4 2.3 ## 150 3.0 5.1 1.8 1.1.4 Rename variables rename(iris, Sepal_Width= Sepal.Width, Sepal_Length= Sepal.Length ) ## Sepal_Length Sepal_Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## 11 5.4 3.7 1.5 0.2 setosa ## 12 4.8 3.4 1.6 0.2 setosa ## 13 4.8 3.0 1.4 0.1 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 15 5.8 4.0 1.2 0.2 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 17 5.4 3.9 1.3 0.4 setosa ## 18 5.1 3.5 1.4 0.3 setosa ## 19 5.7 3.8 1.7 0.3 setosa ## 20 5.1 3.8 1.5 0.3 setosa ## 21 5.4 3.4 1.7 0.2 setosa ## 22 5.1 3.7 1.5 0.4 setosa ## 23 4.6 3.6 1.0 0.2 setosa ## 24 5.1 3.3 1.7 0.5 setosa ## 25 4.8 3.4 1.9 0.2 setosa ## 26 5.0 3.0 1.6 0.2 setosa ## 27 5.0 3.4 1.6 0.4 setosa ## 28 5.2 3.5 1.5 0.2 setosa ## 29 5.2 3.4 1.4 0.2 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 31 4.8 3.1 1.6 0.2 setosa ## 32 5.4 3.4 1.5 0.4 setosa ## 33 5.2 4.1 1.5 0.1 setosa ## 34 5.5 4.2 1.4 0.2 setosa ## 35 4.9 3.1 1.5 0.2 setosa ## 36 5.0 3.2 1.2 0.2 setosa ## 37 5.5 3.5 1.3 0.2 setosa ## 38 4.9 3.6 1.4 0.1 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 40 5.1 3.4 1.5 0.2 setosa ## 41 5.0 3.5 1.3 0.3 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 44 5.0 3.5 1.6 0.6 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 47 5.1 3.8 1.6 0.2 setosa ## 48 4.6 3.2 1.4 0.2 setosa ## 49 5.3 3.7 1.5 0.2 setosa ## 50 5.0 3.3 1.4 0.2 setosa ## 51 7.0 3.2 4.7 1.4 versicolor ## 52 6.4 3.2 4.5 1.5 versicolor ## 53 6.9 3.1 4.9 1.5 versicolor ## 54 5.5 2.3 4.0 1.3 versicolor ## 55 6.5 2.8 4.6 1.5 versicolor ## 56 5.7 2.8 4.5 1.3 versicolor ## 57 6.3 3.3 4.7 1.6 versicolor ## 58 4.9 2.4 3.3 1.0 versicolor ## 59 6.6 2.9 4.6 1.3 versicolor ## 60 5.2 2.7 3.9 1.4 versicolor ## 61 5.0 2.0 3.5 1.0 versicolor ## 62 5.9 3.0 4.2 1.5 versicolor ## 63 6.0 2.2 4.0 1.0 versicolor ## 64 6.1 2.9 4.7 1.4 versicolor ## 65 5.6 2.9 3.6 1.3 versicolor ## 66 6.7 3.1 4.4 1.4 versicolor ## 67 5.6 3.0 4.5 1.5 versicolor ## 68 5.8 2.7 4.1 1.0 versicolor ## 69 6.2 2.2 4.5 1.5 versicolor ## 70 5.6 2.5 3.9 1.1 versicolor ## 71 5.9 3.2 4.8 1.8 versicolor ## 72 6.1 2.8 4.0 1.3 versicolor ## 73 6.3 2.5 4.9 1.5 versicolor ## 74 6.1 2.8 4.7 1.2 versicolor ## 75 6.4 2.9 4.3 1.3 versicolor ## 76 6.6 3.0 4.4 1.4 versicolor ## 77 6.8 2.8 4.8 1.4 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 79 6.0 2.9 4.5 1.5 versicolor ## 80 5.7 2.6 3.5 1.0 versicolor ## 81 5.5 2.4 3.8 1.1 versicolor ## 82 5.5 2.4 3.7 1.0 versicolor ## 83 5.8 2.7 3.9 1.2 versicolor ## 84 6.0 2.7 5.1 1.6 versicolor ## 85 5.4 3.0 4.5 1.5 versicolor ## 86 6.0 3.4 4.5 1.6 versicolor ## 87 6.7 3.1 4.7 1.5 versicolor ## 88 6.3 2.3 4.4 1.3 versicolor ## 89 5.6 3.0 4.1 1.3 versicolor ## 90 5.5 2.5 4.0 1.3 versicolor ## 91 5.5 2.6 4.4 1.2 versicolor ## 92 6.1 3.0 4.6 1.4 versicolor ## 93 5.8 2.6 4.0 1.2 versicolor ## 94 5.0 2.3 3.3 1.0 versicolor ## 95 5.6 2.7 4.2 1.3 versicolor ## 96 5.7 3.0 4.2 1.2 versicolor ## 97 5.7 2.9 4.2 1.3 versicolor ## 98 6.2 2.9 4.3 1.3 versicolor ## 99 5.1 2.5 3.0 1.1 versicolor ## 100 5.7 2.8 4.1 1.3 versicolor ## 101 6.3 3.3 6.0 2.5 virginica ## 102 5.8 2.7 5.1 1.9 virginica ## 103 7.1 3.0 5.9 2.1 virginica ## 104 6.3 2.9 5.6 1.8 virginica ## 105 6.5 3.0 5.8 2.2 virginica ## 106 7.6 3.0 6.6 2.1 virginica ## 107 4.9 2.5 4.5 1.7 virginica ## 108 7.3 2.9 6.3 1.8 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 110 7.2 3.6 6.1 2.5 virginica ## 111 6.5 3.2 5.1 2.0 virginica ## 112 6.4 2.7 5.3 1.9 virginica ## 113 6.8 3.0 5.5 2.1 virginica ## 114 5.7 2.5 5.0 2.0 virginica ## 115 5.8 2.8 5.1 2.4 virginica ## 116 6.4 3.2 5.3 2.3 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 120 6.0 2.2 5.0 1.5 virginica ## 121 6.9 3.2 5.7 2.3 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 124 6.3 2.7 4.9 1.8 virginica ## 125 6.7 3.3 5.7 2.1 virginica ## 126 7.2 3.2 6.0 1.8 virginica ## 127 6.2 2.8 4.8 1.8 virginica ## 128 6.1 3.0 4.9 1.8 virginica ## 129 6.4 2.8 5.6 2.1 virginica ## 130 7.2 3.0 5.8 1.6 virginica ## 131 7.4 2.8 6.1 1.9 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 133 6.4 2.8 5.6 2.2 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 virginica ## 136 7.7 3.0 6.1 2.3 virginica ## 137 6.3 3.4 5.6 2.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica ## 139 6.0 3.0 4.8 1.8 virginica ## 140 6.9 3.1 5.4 2.1 virginica ## 141 6.7 3.1 5.6 2.4 virginica ## 142 6.9 3.1 5.1 2.3 virginica ## 143 5.8 2.7 5.1 1.9 virginica ## 144 6.8 3.2 5.9 2.3 virginica ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica 1.1.5 Sorting in ascending or descending order put a minus in front of a variable for descending order arrange(iris, Petal.Length, -Petal.Width) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 4.6 3.6 1.0 0.2 setosa ## 2 4.3 3.0 1.1 0.1 setosa ## 3 5.8 4.0 1.2 0.2 setosa ## 4 5.0 3.2 1.2 0.2 setosa ## 5 5.4 3.9 1.3 0.4 setosa ## 6 5.0 3.5 1.3 0.3 setosa ## 7 4.5 2.3 1.3 0.3 setosa ## 8 4.7 3.2 1.3 0.2 setosa ## 9 5.5 3.5 1.3 0.2 setosa ## 10 4.4 3.0 1.3 0.2 setosa ## 11 4.4 3.2 1.3 0.2 setosa ## 12 4.6 3.4 1.4 0.3 setosa ## 13 5.1 3.5 1.4 0.3 setosa ## 14 4.8 3.0 1.4 0.3 setosa ## 15 5.1 3.5 1.4 0.2 setosa ## 16 4.9 3.0 1.4 0.2 setosa ## 17 5.0 3.6 1.4 0.2 setosa ## 18 4.4 2.9 1.4 0.2 setosa ## 19 5.2 3.4 1.4 0.2 setosa ## 20 5.5 4.2 1.4 0.2 setosa ## 21 4.6 3.2 1.4 0.2 setosa ## 22 5.0 3.3 1.4 0.2 setosa ## 23 4.8 3.0 1.4 0.1 setosa ## 24 4.9 3.6 1.4 0.1 setosa ## 25 5.7 4.4 1.5 0.4 setosa ## 26 5.1 3.7 1.5 0.4 setosa ## 27 5.4 3.4 1.5 0.4 setosa ## 28 5.1 3.8 1.5 0.3 setosa ## 29 4.6 3.1 1.5 0.2 setosa ## 30 5.0 3.4 1.5 0.2 setosa ## 31 5.4 3.7 1.5 0.2 setosa ## 32 5.2 3.5 1.5 0.2 setosa ## 33 4.9 3.1 1.5 0.2 setosa ## 34 5.1 3.4 1.5 0.2 setosa ## 35 5.3 3.7 1.5 0.2 setosa ## 36 4.9 3.1 1.5 0.1 setosa ## 37 5.2 4.1 1.5 0.1 setosa ## 38 5.0 3.5 1.6 0.6 setosa ## 39 5.0 3.4 1.6 0.4 setosa ## 40 4.8 3.4 1.6 0.2 setosa ## 41 5.0 3.0 1.6 0.2 setosa ## 42 4.7 3.2 1.6 0.2 setosa ## 43 4.8 3.1 1.6 0.2 setosa ## 44 5.1 3.8 1.6 0.2 setosa ## 45 5.1 3.3 1.7 0.5 setosa ## 46 5.4 3.9 1.7 0.4 setosa ## 47 5.7 3.8 1.7 0.3 setosa ## 48 5.4 3.4 1.7 0.2 setosa ## 49 5.1 3.8 1.9 0.4 setosa ## 50 4.8 3.4 1.9 0.2 setosa ## 51 5.1 2.5 3.0 1.1 versicolor ## 52 4.9 2.4 3.3 1.0 versicolor ## 53 5.0 2.3 3.3 1.0 versicolor ## 54 5.0 2.0 3.5 1.0 versicolor ## 55 5.7 2.6 3.5 1.0 versicolor ## 56 5.6 2.9 3.6 1.3 versicolor ## 57 5.5 2.4 3.7 1.0 versicolor ## 58 5.5 2.4 3.8 1.1 versicolor ## 59 5.2 2.7 3.9 1.4 versicolor ## 60 5.8 2.7 3.9 1.2 versicolor ## 61 5.6 2.5 3.9 1.1 versicolor ## 62 5.5 2.3 4.0 1.3 versicolor ## 63 6.1 2.8 4.0 1.3 versicolor ## 64 5.5 2.5 4.0 1.3 versicolor ## 65 5.8 2.6 4.0 1.2 versicolor ## 66 6.0 2.2 4.0 1.0 versicolor ## 67 5.6 3.0 4.1 1.3 versicolor ## 68 5.7 2.8 4.1 1.3 versicolor ## 69 5.8 2.7 4.1 1.0 versicolor ## 70 5.9 3.0 4.2 1.5 versicolor ## 71 5.6 2.7 4.2 1.3 versicolor ## 72 5.7 2.9 4.2 1.3 versicolor ## 73 5.7 3.0 4.2 1.2 versicolor ## 74 6.4 2.9 4.3 1.3 versicolor ## 75 6.2 2.9 4.3 1.3 versicolor ## 76 6.7 3.1 4.4 1.4 versicolor ## 77 6.6 3.0 4.4 1.4 versicolor ## 78 6.3 2.3 4.4 1.3 versicolor ## 79 5.5 2.6 4.4 1.2 versicolor ## 80 4.9 2.5 4.5 1.7 virginica ## 81 6.0 3.4 4.5 1.6 versicolor ## 82 6.4 3.2 4.5 1.5 versicolor ## 83 5.6 3.0 4.5 1.5 versicolor ## 84 6.2 2.2 4.5 1.5 versicolor ## 85 6.0 2.9 4.5 1.5 versicolor ## 86 5.4 3.0 4.5 1.5 versicolor ## 87 5.7 2.8 4.5 1.3 versicolor ## 88 6.5 2.8 4.6 1.5 versicolor ## 89 6.1 3.0 4.6 1.4 versicolor ## 90 6.6 2.9 4.6 1.3 versicolor ## 91 6.3 3.3 4.7 1.6 versicolor ## 92 6.7 3.1 4.7 1.5 versicolor ## 93 7.0 3.2 4.7 1.4 versicolor ## 94 6.1 2.9 4.7 1.4 versicolor ## 95 6.1 2.8 4.7 1.2 versicolor ## 96 5.9 3.2 4.8 1.8 versicolor ## 97 6.2 2.8 4.8 1.8 virginica ## 98 6.0 3.0 4.8 1.8 virginica ## 99 6.8 2.8 4.8 1.4 versicolor ## 100 5.6 2.8 4.9 2.0 virginica ## 101 6.3 2.7 4.9 1.8 virginica ## 102 6.1 3.0 4.9 1.8 virginica ## 103 6.9 3.1 4.9 1.5 versicolor ## 104 6.3 2.5 4.9 1.5 versicolor ## 105 5.7 2.5 5.0 2.0 virginica ## 106 6.3 2.5 5.0 1.9 virginica ## 107 6.7 3.0 5.0 1.7 versicolor ## 108 6.0 2.2 5.0 1.5 virginica ## 109 5.8 2.8 5.1 2.4 virginica ## 110 6.9 3.1 5.1 2.3 virginica ## 111 6.5 3.2 5.1 2.0 virginica ## 112 5.8 2.7 5.1 1.9 virginica ## 113 5.8 2.7 5.1 1.9 virginica ## 114 5.9 3.0 5.1 1.8 virginica ## 115 6.0 2.7 5.1 1.6 versicolor ## 116 6.3 2.8 5.1 1.5 virginica ## 117 6.7 3.0 5.2 2.3 virginica ## 118 6.5 3.0 5.2 2.0 virginica ## 119 6.4 3.2 5.3 2.3 virginica ## 120 6.4 2.7 5.3 1.9 virginica ## 121 6.2 3.4 5.4 2.3 virginica ## 122 6.9 3.1 5.4 2.1 virginica ## 123 6.8 3.0 5.5 2.1 virginica ## 124 6.5 3.0 5.5 1.8 virginica ## 125 6.4 3.1 5.5 1.8 virginica ## 126 6.3 3.4 5.6 2.4 virginica ## 127 6.7 3.1 5.6 2.4 virginica ## 128 6.4 2.8 5.6 2.2 virginica ## 129 6.4 2.8 5.6 2.1 virginica ## 130 6.3 2.9 5.6 1.8 virginica ## 131 6.1 2.6 5.6 1.4 virginica ## 132 6.7 3.3 5.7 2.5 virginica ## 133 6.9 3.2 5.7 2.3 virginica ## 134 6.7 3.3 5.7 2.1 virginica ## 135 6.5 3.0 5.8 2.2 virginica ## 136 6.7 2.5 5.8 1.8 virginica ## 137 7.2 3.0 5.8 1.6 virginica ## 138 6.8 3.2 5.9 2.3 virginica ## 139 7.1 3.0 5.9 2.1 virginica ## 140 6.3 3.3 6.0 2.5 virginica ## 141 7.2 3.2 6.0 1.8 virginica ## 142 7.2 3.6 6.1 2.5 virginica ## 143 7.7 3.0 6.1 2.3 virginica ## 144 7.4 2.8 6.1 1.9 virginica ## 145 7.3 2.9 6.3 1.8 virginica ## 146 7.9 3.8 6.4 2.0 virginica ## 147 7.6 3.0 6.6 2.1 virginica ## 148 7.7 3.8 6.7 2.2 virginica ## 149 7.7 2.8 6.7 2.0 virginica ## 150 7.7 2.6 6.9 2.3 virginica 1.1.6 Transform variables mutate(iris, newvar= Sepal.Width*10, Petal.Length=Petal.Length/100 ) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species newvar ## 1 5.1 3.5 0.014 0.2 setosa 35 ## 2 4.9 3.0 0.014 0.2 setosa 30 ## 3 4.7 3.2 0.013 0.2 setosa 32 ## 4 4.6 3.1 0.015 0.2 setosa 31 ## 5 5.0 3.6 0.014 0.2 setosa 36 ## 6 5.4 3.9 0.017 0.4 setosa 39 ## 7 4.6 3.4 0.014 0.3 setosa 34 ## 8 5.0 3.4 0.015 0.2 setosa 34 ## 9 4.4 2.9 0.014 0.2 setosa 29 ## 10 4.9 3.1 0.015 0.1 setosa 31 ## 11 5.4 3.7 0.015 0.2 setosa 37 ## 12 4.8 3.4 0.016 0.2 setosa 34 ## 13 4.8 3.0 0.014 0.1 setosa 30 ## 14 4.3 3.0 0.011 0.1 setosa 30 ## 15 5.8 4.0 0.012 0.2 setosa 40 ## 16 5.7 4.4 0.015 0.4 setosa 44 ## 17 5.4 3.9 0.013 0.4 setosa 39 ## 18 5.1 3.5 0.014 0.3 setosa 35 ## 19 5.7 3.8 0.017 0.3 setosa 38 ## 20 5.1 3.8 0.015 0.3 setosa 38 ## 21 5.4 3.4 0.017 0.2 setosa 34 ## 22 5.1 3.7 0.015 0.4 setosa 37 ## 23 4.6 3.6 0.010 0.2 setosa 36 ## 24 5.1 3.3 0.017 0.5 setosa 33 ## 25 4.8 3.4 0.019 0.2 setosa 34 ## 26 5.0 3.0 0.016 0.2 setosa 30 ## 27 5.0 3.4 0.016 0.4 setosa 34 ## 28 5.2 3.5 0.015 0.2 setosa 35 ## 29 5.2 3.4 0.014 0.2 setosa 34 ## 30 4.7 3.2 0.016 0.2 setosa 32 ## 31 4.8 3.1 0.016 0.2 setosa 31 ## 32 5.4 3.4 0.015 0.4 setosa 34 ## 33 5.2 4.1 0.015 0.1 setosa 41 ## 34 5.5 4.2 0.014 0.2 setosa 42 ## 35 4.9 3.1 0.015 0.2 setosa 31 ## 36 5.0 3.2 0.012 0.2 setosa 32 ## 37 5.5 3.5 0.013 0.2 setosa 35 ## 38 4.9 3.6 0.014 0.1 setosa 36 ## 39 4.4 3.0 0.013 0.2 setosa 30 ## 40 5.1 3.4 0.015 0.2 setosa 34 ## 41 5.0 3.5 0.013 0.3 setosa 35 ## 42 4.5 2.3 0.013 0.3 setosa 23 ## 43 4.4 3.2 0.013 0.2 setosa 32 ## 44 5.0 3.5 0.016 0.6 setosa 35 ## 45 5.1 3.8 0.019 0.4 setosa 38 ## 46 4.8 3.0 0.014 0.3 setosa 30 ## 47 5.1 3.8 0.016 0.2 setosa 38 ## 48 4.6 3.2 0.014 0.2 setosa 32 ## 49 5.3 3.7 0.015 0.2 setosa 37 ## 50 5.0 3.3 0.014 0.2 setosa 33 ## 51 7.0 3.2 0.047 1.4 versicolor 32 ## 52 6.4 3.2 0.045 1.5 versicolor 32 ## 53 6.9 3.1 0.049 1.5 versicolor 31 ## 54 5.5 2.3 0.040 1.3 versicolor 23 ## 55 6.5 2.8 0.046 1.5 versicolor 28 ## 56 5.7 2.8 0.045 1.3 versicolor 28 ## 57 6.3 3.3 0.047 1.6 versicolor 33 ## 58 4.9 2.4 0.033 1.0 versicolor 24 ## 59 6.6 2.9 0.046 1.3 versicolor 29 ## 60 5.2 2.7 0.039 1.4 versicolor 27 ## 61 5.0 2.0 0.035 1.0 versicolor 20 ## 62 5.9 3.0 0.042 1.5 versicolor 30 ## 63 6.0 2.2 0.040 1.0 versicolor 22 ## 64 6.1 2.9 0.047 1.4 versicolor 29 ## 65 5.6 2.9 0.036 1.3 versicolor 29 ## 66 6.7 3.1 0.044 1.4 versicolor 31 ## 67 5.6 3.0 0.045 1.5 versicolor 30 ## 68 5.8 2.7 0.041 1.0 versicolor 27 ## 69 6.2 2.2 0.045 1.5 versicolor 22 ## 70 5.6 2.5 0.039 1.1 versicolor 25 ## 71 5.9 3.2 0.048 1.8 versicolor 32 ## 72 6.1 2.8 0.040 1.3 versicolor 28 ## 73 6.3 2.5 0.049 1.5 versicolor 25 ## 74 6.1 2.8 0.047 1.2 versicolor 28 ## 75 6.4 2.9 0.043 1.3 versicolor 29 ## 76 6.6 3.0 0.044 1.4 versicolor 30 ## 77 6.8 2.8 0.048 1.4 versicolor 28 ## 78 6.7 3.0 0.050 1.7 versicolor 30 ## 79 6.0 2.9 0.045 1.5 versicolor 29 ## 80 5.7 2.6 0.035 1.0 versicolor 26 ## 81 5.5 2.4 0.038 1.1 versicolor 24 ## 82 5.5 2.4 0.037 1.0 versicolor 24 ## 83 5.8 2.7 0.039 1.2 versicolor 27 ## 84 6.0 2.7 0.051 1.6 versicolor 27 ## 85 5.4 3.0 0.045 1.5 versicolor 30 ## 86 6.0 3.4 0.045 1.6 versicolor 34 ## 87 6.7 3.1 0.047 1.5 versicolor 31 ## 88 6.3 2.3 0.044 1.3 versicolor 23 ## 89 5.6 3.0 0.041 1.3 versicolor 30 ## 90 5.5 2.5 0.040 1.3 versicolor 25 ## 91 5.5 2.6 0.044 1.2 versicolor 26 ## 92 6.1 3.0 0.046 1.4 versicolor 30 ## 93 5.8 2.6 0.040 1.2 versicolor 26 ## 94 5.0 2.3 0.033 1.0 versicolor 23 ## 95 5.6 2.7 0.042 1.3 versicolor 27 ## 96 5.7 3.0 0.042 1.2 versicolor 30 ## 97 5.7 2.9 0.042 1.3 versicolor 29 ## 98 6.2 2.9 0.043 1.3 versicolor 29 ## 99 5.1 2.5 0.030 1.1 versicolor 25 ## 100 5.7 2.8 0.041 1.3 versicolor 28 ## 101 6.3 3.3 0.060 2.5 virginica 33 ## 102 5.8 2.7 0.051 1.9 virginica 27 ## 103 7.1 3.0 0.059 2.1 virginica 30 ## 104 6.3 2.9 0.056 1.8 virginica 29 ## 105 6.5 3.0 0.058 2.2 virginica 30 ## 106 7.6 3.0 0.066 2.1 virginica 30 ## 107 4.9 2.5 0.045 1.7 virginica 25 ## 108 7.3 2.9 0.063 1.8 virginica 29 ## 109 6.7 2.5 0.058 1.8 virginica 25 ## 110 7.2 3.6 0.061 2.5 virginica 36 ## 111 6.5 3.2 0.051 2.0 virginica 32 ## 112 6.4 2.7 0.053 1.9 virginica 27 ## 113 6.8 3.0 0.055 2.1 virginica 30 ## 114 5.7 2.5 0.050 2.0 virginica 25 ## 115 5.8 2.8 0.051 2.4 virginica 28 ## 116 6.4 3.2 0.053 2.3 virginica 32 ## 117 6.5 3.0 0.055 1.8 virginica 30 ## 118 7.7 3.8 0.067 2.2 virginica 38 ## 119 7.7 2.6 0.069 2.3 virginica 26 ## 120 6.0 2.2 0.050 1.5 virginica 22 ## 121 6.9 3.2 0.057 2.3 virginica 32 ## 122 5.6 2.8 0.049 2.0 virginica 28 ## 123 7.7 2.8 0.067 2.0 virginica 28 ## 124 6.3 2.7 0.049 1.8 virginica 27 ## 125 6.7 3.3 0.057 2.1 virginica 33 ## 126 7.2 3.2 0.060 1.8 virginica 32 ## 127 6.2 2.8 0.048 1.8 virginica 28 ## 128 6.1 3.0 0.049 1.8 virginica 30 ## 129 6.4 2.8 0.056 2.1 virginica 28 ## 130 7.2 3.0 0.058 1.6 virginica 30 ## 131 7.4 2.8 0.061 1.9 virginica 28 ## 132 7.9 3.8 0.064 2.0 virginica 38 ## 133 6.4 2.8 0.056 2.2 virginica 28 ## 134 6.3 2.8 0.051 1.5 virginica 28 ## 135 6.1 2.6 0.056 1.4 virginica 26 ## 136 7.7 3.0 0.061 2.3 virginica 30 ## 137 6.3 3.4 0.056 2.4 virginica 34 ## 138 6.4 3.1 0.055 1.8 virginica 31 ## 139 6.0 3.0 0.048 1.8 virginica 30 ## 140 6.9 3.1 0.054 2.1 virginica 31 ## 141 6.7 3.1 0.056 2.4 virginica 31 ## 142 6.9 3.1 0.051 2.3 virginica 31 ## 143 5.8 2.7 0.051 1.9 virginica 27 ## 144 6.8 3.2 0.059 2.3 virginica 32 ## 145 6.7 3.3 0.057 2.5 virginica 33 ## 146 6.7 3.0 0.052 2.3 virginica 30 ## 147 6.3 2.5 0.050 1.9 virginica 25 ## 148 6.5 3.0 0.052 2.0 virginica 30 ## 149 6.2 3.4 0.054 2.3 virginica 34 ## 150 5.9 3.0 0.051 1.8 virginica 30 1.1.7 Working with pipes %&gt;% iris %&gt;% filter(Species==&quot;setosa&quot;) %&gt;% mutate (newvar=Sepal.Width*10) %&gt;% select (-Sepal.Width, -Petal.Width) %&gt;% arrange(-Sepal.Length, newvar) ## Sepal.Length Petal.Length Species newvar ## 1 5.8 1.2 setosa 40 ## 2 5.7 1.7 setosa 38 ## 3 5.7 1.5 setosa 44 ## 4 5.5 1.3 setosa 35 ## 5 5.5 1.4 setosa 42 ## 6 5.4 1.7 setosa 34 ## 7 5.4 1.5 setosa 34 ## 8 5.4 1.5 setosa 37 ## 9 5.4 1.7 setosa 39 ## 10 5.4 1.3 setosa 39 ## 11 5.3 1.5 setosa 37 ## 12 5.2 1.4 setosa 34 ## 13 5.2 1.5 setosa 35 ## 14 5.2 1.5 setosa 41 ## 15 5.1 1.7 setosa 33 ## 16 5.1 1.5 setosa 34 ## 17 5.1 1.4 setosa 35 ## 18 5.1 1.4 setosa 35 ## 19 5.1 1.5 setosa 37 ## 20 5.1 1.5 setosa 38 ## 21 5.1 1.9 setosa 38 ## 22 5.1 1.6 setosa 38 ## 23 5.0 1.6 setosa 30 ## 24 5.0 1.2 setosa 32 ## 25 5.0 1.4 setosa 33 ## 26 5.0 1.5 setosa 34 ## 27 5.0 1.6 setosa 34 ## 28 5.0 1.3 setosa 35 ## 29 5.0 1.6 setosa 35 ## 30 5.0 1.4 setosa 36 ## 31 4.9 1.4 setosa 30 ## 32 4.9 1.5 setosa 31 ## 33 4.9 1.5 setosa 31 ## 34 4.9 1.4 setosa 36 ## 35 4.8 1.4 setosa 30 ## 36 4.8 1.4 setosa 30 ## 37 4.8 1.6 setosa 31 ## 38 4.8 1.6 setosa 34 ## 39 4.8 1.9 setosa 34 ## 40 4.7 1.3 setosa 32 ## 41 4.7 1.6 setosa 32 ## 42 4.6 1.5 setosa 31 ## 43 4.6 1.4 setosa 32 ## 44 4.6 1.4 setosa 34 ## 45 4.6 1.0 setosa 36 ## 46 4.5 1.3 setosa 23 ## 47 4.4 1.4 setosa 29 ## 48 4.4 1.3 setosa 30 ## 49 4.4 1.3 setosa 32 ## 50 4.3 1.1 setosa 30 1.1.8 Pivot wider (long to wide) If no unique identifier row in each group doesn’t work iris %&gt;% pivot_wider( names_from=Species, values_from= c(Sepal.Length)) ## Warning: Values from `Sepal.Length` are not uniquely identified; output will contain list-cols. ## * Use `values_fn = list` to suppress this warning. ## * Use `values_fn = {summary_fun}` to summarise duplicates. ## * Use the following dplyr code to identify duplicates. ## {data} %&gt;% ## dplyr::group_by(Sepal.Width, Petal.Length, Petal.Width, Species) %&gt;% ## dplyr::summarise(n = dplyr::n(), .groups = &quot;drop&quot;) %&gt;% ## dplyr::filter(n &gt; 1L) ## # A tibble: 143 × 6 ## Sepal.Width Petal.Length Petal.Width setosa versicolor virginica ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 3.5 1.4 0.2 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 2 3 1.4 0.2 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 3 3.2 1.3 0.2 &lt;dbl [2]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 4 3.1 1.5 0.2 &lt;dbl [2]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 5 3.6 1.4 0.2 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 6 3.9 1.7 0.4 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 7 3.4 1.4 0.3 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 8 3.4 1.5 0.2 &lt;dbl [2]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 9 2.9 1.4 0.2 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## 10 3.1 1.5 0.1 &lt;dbl [1]&gt; &lt;NULL&gt; &lt;NULL&gt; ## # … with 133 more rows Create a unique identifier row for each name and then use pivot_wider widedata &lt;- iris %&gt;% # create groups then assign unique identifier row number in each group group_by(Species) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from=Species, values_from= c(Petal.Length,Sepal.Length,Petal.Width,Sepal.Width)) widedata ## # A tibble: 50 × 13 ## row Petal.Length_setosa Petal.Length_ver… Petal.Length_vi… Sepal.Length_se… ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.4 4.7 6 5.1 ## 2 2 1.4 4.5 5.1 4.9 ## 3 3 1.3 4.9 5.9 4.7 ## 4 4 1.5 4 5.6 4.6 ## 5 5 1.4 4.6 5.8 5 ## 6 6 1.7 4.5 6.6 5.4 ## 7 7 1.4 4.7 4.5 4.6 ## 8 8 1.5 3.3 6.3 5 ## 9 9 1.4 4.6 5.8 4.4 ## 10 10 1.5 3.9 6.1 4.9 ## # … with 40 more rows, and 8 more variables: Sepal.Length_versicolor &lt;dbl&gt;, ## # Sepal.Length_virginica &lt;dbl&gt;, Petal.Width_setosa &lt;dbl&gt;, ## # Petal.Width_versicolor &lt;dbl&gt;, Petal.Width_virginica &lt;dbl&gt;, ## # Sepal.Width_setosa &lt;dbl&gt;, Sepal.Width_versicolor &lt;dbl&gt;, ## # Sepal.Width_virginica &lt;dbl&gt; iris %&gt;% group_by(Species) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider( names_from=Species, values_from= c(Petal.Length, Petal.Width)) ## # A tibble: 150 × 9 ## Sepal.Length Sepal.Width row Petal.Length_setosa Petal.Length_versicolor ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1 1.4 NA ## 2 4.9 3 2 1.4 NA ## 3 4.7 3.2 3 1.3 NA ## 4 4.6 3.1 4 1.5 NA ## 5 5 3.6 5 1.4 NA ## 6 5.4 3.9 6 1.7 NA ## 7 4.6 3.4 7 1.4 NA ## 8 5 3.4 8 1.5 NA ## 9 4.4 2.9 9 1.4 NA ## 10 4.9 3.1 10 1.5 NA ## # … with 140 more rows, and 4 more variables: Petal.Length_virginica &lt;dbl&gt;, ## # Petal.Width_setosa &lt;dbl&gt;, Petal.Width_versicolor &lt;dbl&gt;, ## # Petal.Width_virginica &lt;dbl&gt; 1.1.9 Pivot longer (wide to long) longdata = pivot_longer(widedata, - c( &quot;row&quot; , &quot;Petal.Length_setosa&quot; , &quot;Petal.Length_versicolor&quot;, &quot;Petal.Length_virginica&quot;, &quot;Sepal.Length_setosa&quot; , &quot;Sepal.Length_versicolor&quot;, &quot;Sepal.Length_virginica&quot; , &quot;Petal.Width_setosa&quot; , &quot;Petal.Width_versicolor&quot; , &quot;Petal.Width_virginica&quot; ) , names_to=&quot;Sepal.Width&quot;, values_to=&quot;Sepal.Width.value&quot;) longdata ## # A tibble: 150 × 12 ## row Petal.Length_setosa Petal.Length_ver… Petal.Length_vi… Sepal.Length_se… ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.4 4.7 6 5.1 ## 2 1 1.4 4.7 6 5.1 ## 3 1 1.4 4.7 6 5.1 ## 4 2 1.4 4.5 5.1 4.9 ## 5 2 1.4 4.5 5.1 4.9 ## 6 2 1.4 4.5 5.1 4.9 ## 7 3 1.3 4.9 5.9 4.7 ## 8 3 1.3 4.9 5.9 4.7 ## 9 3 1.3 4.9 5.9 4.7 ## 10 4 1.5 4 5.6 4.6 ## # … with 140 more rows, and 7 more variables: Sepal.Length_versicolor &lt;dbl&gt;, ## # Sepal.Length_virginica &lt;dbl&gt;, Petal.Width_setosa &lt;dbl&gt;, ## # Petal.Width_versicolor &lt;dbl&gt;, Petal.Width_virginica &lt;dbl&gt;, ## # Sepal.Width &lt;chr&gt;, Sepal.Width.value &lt;dbl&gt; Pivot wider again (long to wide) pivot_wider(longdata, names_from=Sepal.Width, values_from= c(Sepal.Width.value)) ## # A tibble: 50 × 13 ## row Petal.Length_setosa Petal.Length_ver… Petal.Length_vi… Sepal.Length_se… ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.4 4.7 6 5.1 ## 2 2 1.4 4.5 5.1 4.9 ## 3 3 1.3 4.9 5.9 4.7 ## 4 4 1.5 4 5.6 4.6 ## 5 5 1.4 4.6 5.8 5 ## 6 6 1.7 4.5 6.6 5.4 ## 7 7 1.4 4.7 4.5 4.6 ## 8 8 1.5 3.3 6.3 5 ## 9 9 1.4 4.6 5.8 4.4 ## 10 10 1.5 3.9 6.1 4.9 ## # … with 40 more rows, and 8 more variables: Sepal.Length_versicolor &lt;dbl&gt;, ## # Sepal.Length_virginica &lt;dbl&gt;, Petal.Width_setosa &lt;dbl&gt;, ## # Petal.Width_versicolor &lt;dbl&gt;, Petal.Width_virginica &lt;dbl&gt;, ## # Sepal.Width_setosa &lt;dbl&gt;, Sepal.Width_versicolor &lt;dbl&gt;, ## # Sepal.Width_virginica &lt;dbl&gt; 1.1.10 Separate columns separate(iris, Species, into = c(&quot;integer&quot;,&quot;decimal&quot;,&quot;third&quot;), sep=&quot;o&quot;) ## Warning: Expected 3 pieces. Missing pieces filled with `NA` in 100 rows [1, 2, ## 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...]. ## Sepal.Length Sepal.Width Petal.Length Petal.Width integer decimal third ## 1 5.1 3.5 1.4 0.2 set sa &lt;NA&gt; ## 2 4.9 3.0 1.4 0.2 set sa &lt;NA&gt; ## 3 4.7 3.2 1.3 0.2 set sa &lt;NA&gt; ## 4 4.6 3.1 1.5 0.2 set sa &lt;NA&gt; ## 5 5.0 3.6 1.4 0.2 set sa &lt;NA&gt; ## 6 5.4 3.9 1.7 0.4 set sa &lt;NA&gt; ## 7 4.6 3.4 1.4 0.3 set sa &lt;NA&gt; ## 8 5.0 3.4 1.5 0.2 set sa &lt;NA&gt; ## 9 4.4 2.9 1.4 0.2 set sa &lt;NA&gt; ## 10 4.9 3.1 1.5 0.1 set sa &lt;NA&gt; ## 11 5.4 3.7 1.5 0.2 set sa &lt;NA&gt; ## 12 4.8 3.4 1.6 0.2 set sa &lt;NA&gt; ## 13 4.8 3.0 1.4 0.1 set sa &lt;NA&gt; ## 14 4.3 3.0 1.1 0.1 set sa &lt;NA&gt; ## 15 5.8 4.0 1.2 0.2 set sa &lt;NA&gt; ## 16 5.7 4.4 1.5 0.4 set sa &lt;NA&gt; ## 17 5.4 3.9 1.3 0.4 set sa &lt;NA&gt; ## 18 5.1 3.5 1.4 0.3 set sa &lt;NA&gt; ## 19 5.7 3.8 1.7 0.3 set sa &lt;NA&gt; ## 20 5.1 3.8 1.5 0.3 set sa &lt;NA&gt; ## 21 5.4 3.4 1.7 0.2 set sa &lt;NA&gt; ## 22 5.1 3.7 1.5 0.4 set sa &lt;NA&gt; ## 23 4.6 3.6 1.0 0.2 set sa &lt;NA&gt; ## 24 5.1 3.3 1.7 0.5 set sa &lt;NA&gt; ## 25 4.8 3.4 1.9 0.2 set sa &lt;NA&gt; ## 26 5.0 3.0 1.6 0.2 set sa &lt;NA&gt; ## 27 5.0 3.4 1.6 0.4 set sa &lt;NA&gt; ## 28 5.2 3.5 1.5 0.2 set sa &lt;NA&gt; ## 29 5.2 3.4 1.4 0.2 set sa &lt;NA&gt; ## 30 4.7 3.2 1.6 0.2 set sa &lt;NA&gt; ## 31 4.8 3.1 1.6 0.2 set sa &lt;NA&gt; ## 32 5.4 3.4 1.5 0.4 set sa &lt;NA&gt; ## 33 5.2 4.1 1.5 0.1 set sa &lt;NA&gt; ## 34 5.5 4.2 1.4 0.2 set sa &lt;NA&gt; ## 35 4.9 3.1 1.5 0.2 set sa &lt;NA&gt; ## 36 5.0 3.2 1.2 0.2 set sa &lt;NA&gt; ## 37 5.5 3.5 1.3 0.2 set sa &lt;NA&gt; ## 38 4.9 3.6 1.4 0.1 set sa &lt;NA&gt; ## 39 4.4 3.0 1.3 0.2 set sa &lt;NA&gt; ## 40 5.1 3.4 1.5 0.2 set sa &lt;NA&gt; ## 41 5.0 3.5 1.3 0.3 set sa &lt;NA&gt; ## 42 4.5 2.3 1.3 0.3 set sa &lt;NA&gt; ## 43 4.4 3.2 1.3 0.2 set sa &lt;NA&gt; ## 44 5.0 3.5 1.6 0.6 set sa &lt;NA&gt; ## 45 5.1 3.8 1.9 0.4 set sa &lt;NA&gt; ## 46 4.8 3.0 1.4 0.3 set sa &lt;NA&gt; ## 47 5.1 3.8 1.6 0.2 set sa &lt;NA&gt; ## 48 4.6 3.2 1.4 0.2 set sa &lt;NA&gt; ## 49 5.3 3.7 1.5 0.2 set sa &lt;NA&gt; ## 50 5.0 3.3 1.4 0.2 set sa &lt;NA&gt; ## 51 7.0 3.2 4.7 1.4 versic l r ## 52 6.4 3.2 4.5 1.5 versic l r ## 53 6.9 3.1 4.9 1.5 versic l r ## 54 5.5 2.3 4.0 1.3 versic l r ## 55 6.5 2.8 4.6 1.5 versic l r ## 56 5.7 2.8 4.5 1.3 versic l r ## 57 6.3 3.3 4.7 1.6 versic l r ## 58 4.9 2.4 3.3 1.0 versic l r ## 59 6.6 2.9 4.6 1.3 versic l r ## 60 5.2 2.7 3.9 1.4 versic l r ## 61 5.0 2.0 3.5 1.0 versic l r ## 62 5.9 3.0 4.2 1.5 versic l r ## 63 6.0 2.2 4.0 1.0 versic l r ## 64 6.1 2.9 4.7 1.4 versic l r ## 65 5.6 2.9 3.6 1.3 versic l r ## 66 6.7 3.1 4.4 1.4 versic l r ## 67 5.6 3.0 4.5 1.5 versic l r ## 68 5.8 2.7 4.1 1.0 versic l r ## 69 6.2 2.2 4.5 1.5 versic l r ## 70 5.6 2.5 3.9 1.1 versic l r ## 71 5.9 3.2 4.8 1.8 versic l r ## 72 6.1 2.8 4.0 1.3 versic l r ## 73 6.3 2.5 4.9 1.5 versic l r ## 74 6.1 2.8 4.7 1.2 versic l r ## 75 6.4 2.9 4.3 1.3 versic l r ## 76 6.6 3.0 4.4 1.4 versic l r ## 77 6.8 2.8 4.8 1.4 versic l r ## 78 6.7 3.0 5.0 1.7 versic l r ## 79 6.0 2.9 4.5 1.5 versic l r ## 80 5.7 2.6 3.5 1.0 versic l r ## 81 5.5 2.4 3.8 1.1 versic l r ## 82 5.5 2.4 3.7 1.0 versic l r ## 83 5.8 2.7 3.9 1.2 versic l r ## 84 6.0 2.7 5.1 1.6 versic l r ## 85 5.4 3.0 4.5 1.5 versic l r ## 86 6.0 3.4 4.5 1.6 versic l r ## 87 6.7 3.1 4.7 1.5 versic l r ## 88 6.3 2.3 4.4 1.3 versic l r ## 89 5.6 3.0 4.1 1.3 versic l r ## 90 5.5 2.5 4.0 1.3 versic l r ## 91 5.5 2.6 4.4 1.2 versic l r ## 92 6.1 3.0 4.6 1.4 versic l r ## 93 5.8 2.6 4.0 1.2 versic l r ## 94 5.0 2.3 3.3 1.0 versic l r ## 95 5.6 2.7 4.2 1.3 versic l r ## 96 5.7 3.0 4.2 1.2 versic l r ## 97 5.7 2.9 4.2 1.3 versic l r ## 98 6.2 2.9 4.3 1.3 versic l r ## 99 5.1 2.5 3.0 1.1 versic l r ## 100 5.7 2.8 4.1 1.3 versic l r ## 101 6.3 3.3 6.0 2.5 virginica &lt;NA&gt; &lt;NA&gt; ## 102 5.8 2.7 5.1 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 103 7.1 3.0 5.9 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 104 6.3 2.9 5.6 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 105 6.5 3.0 5.8 2.2 virginica &lt;NA&gt; &lt;NA&gt; ## 106 7.6 3.0 6.6 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 107 4.9 2.5 4.5 1.7 virginica &lt;NA&gt; &lt;NA&gt; ## 108 7.3 2.9 6.3 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 109 6.7 2.5 5.8 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 110 7.2 3.6 6.1 2.5 virginica &lt;NA&gt; &lt;NA&gt; ## 111 6.5 3.2 5.1 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 112 6.4 2.7 5.3 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 113 6.8 3.0 5.5 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 114 5.7 2.5 5.0 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 115 5.8 2.8 5.1 2.4 virginica &lt;NA&gt; &lt;NA&gt; ## 116 6.4 3.2 5.3 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 117 6.5 3.0 5.5 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 118 7.7 3.8 6.7 2.2 virginica &lt;NA&gt; &lt;NA&gt; ## 119 7.7 2.6 6.9 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 120 6.0 2.2 5.0 1.5 virginica &lt;NA&gt; &lt;NA&gt; ## 121 6.9 3.2 5.7 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 122 5.6 2.8 4.9 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 123 7.7 2.8 6.7 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 124 6.3 2.7 4.9 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 125 6.7 3.3 5.7 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 126 7.2 3.2 6.0 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 127 6.2 2.8 4.8 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 128 6.1 3.0 4.9 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 129 6.4 2.8 5.6 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 130 7.2 3.0 5.8 1.6 virginica &lt;NA&gt; &lt;NA&gt; ## 131 7.4 2.8 6.1 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 132 7.9 3.8 6.4 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 133 6.4 2.8 5.6 2.2 virginica &lt;NA&gt; &lt;NA&gt; ## 134 6.3 2.8 5.1 1.5 virginica &lt;NA&gt; &lt;NA&gt; ## 135 6.1 2.6 5.6 1.4 virginica &lt;NA&gt; &lt;NA&gt; ## 136 7.7 3.0 6.1 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 137 6.3 3.4 5.6 2.4 virginica &lt;NA&gt; &lt;NA&gt; ## 138 6.4 3.1 5.5 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 139 6.0 3.0 4.8 1.8 virginica &lt;NA&gt; &lt;NA&gt; ## 140 6.9 3.1 5.4 2.1 virginica &lt;NA&gt; &lt;NA&gt; ## 141 6.7 3.1 5.6 2.4 virginica &lt;NA&gt; &lt;NA&gt; ## 142 6.9 3.1 5.1 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 143 5.8 2.7 5.1 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 144 6.8 3.2 5.9 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 145 6.7 3.3 5.7 2.5 virginica &lt;NA&gt; &lt;NA&gt; ## 146 6.7 3.0 5.2 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 147 6.3 2.5 5.0 1.9 virginica &lt;NA&gt; &lt;NA&gt; ## 148 6.5 3.0 5.2 2.0 virginica &lt;NA&gt; &lt;NA&gt; ## 149 6.2 3.4 5.4 2.3 virginica &lt;NA&gt; &lt;NA&gt; ## 150 5.9 3.0 5.1 1.8 virginica &lt;NA&gt; &lt;NA&gt; 1.1.11 Recode/relabel data mutate(iris, Species2 = recode(Species, &quot;setosa&quot;=&quot;seto&quot;, &quot;versicolor&quot;=&quot;versi&quot;)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Species2 ## 1 5.1 3.5 1.4 0.2 setosa seto ## 2 4.9 3.0 1.4 0.2 setosa seto ## 3 4.7 3.2 1.3 0.2 setosa seto ## 4 4.6 3.1 1.5 0.2 setosa seto ## 5 5.0 3.6 1.4 0.2 setosa seto ## 6 5.4 3.9 1.7 0.4 setosa seto ## 7 4.6 3.4 1.4 0.3 setosa seto ## 8 5.0 3.4 1.5 0.2 setosa seto ## 9 4.4 2.9 1.4 0.2 setosa seto ## 10 4.9 3.1 1.5 0.1 setosa seto ## 11 5.4 3.7 1.5 0.2 setosa seto ## 12 4.8 3.4 1.6 0.2 setosa seto ## 13 4.8 3.0 1.4 0.1 setosa seto ## 14 4.3 3.0 1.1 0.1 setosa seto ## 15 5.8 4.0 1.2 0.2 setosa seto ## 16 5.7 4.4 1.5 0.4 setosa seto ## 17 5.4 3.9 1.3 0.4 setosa seto ## 18 5.1 3.5 1.4 0.3 setosa seto ## 19 5.7 3.8 1.7 0.3 setosa seto ## 20 5.1 3.8 1.5 0.3 setosa seto ## 21 5.4 3.4 1.7 0.2 setosa seto ## 22 5.1 3.7 1.5 0.4 setosa seto ## 23 4.6 3.6 1.0 0.2 setosa seto ## 24 5.1 3.3 1.7 0.5 setosa seto ## 25 4.8 3.4 1.9 0.2 setosa seto ## 26 5.0 3.0 1.6 0.2 setosa seto ## 27 5.0 3.4 1.6 0.4 setosa seto ## 28 5.2 3.5 1.5 0.2 setosa seto ## 29 5.2 3.4 1.4 0.2 setosa seto ## 30 4.7 3.2 1.6 0.2 setosa seto ## 31 4.8 3.1 1.6 0.2 setosa seto ## 32 5.4 3.4 1.5 0.4 setosa seto ## 33 5.2 4.1 1.5 0.1 setosa seto ## 34 5.5 4.2 1.4 0.2 setosa seto ## 35 4.9 3.1 1.5 0.2 setosa seto ## 36 5.0 3.2 1.2 0.2 setosa seto ## 37 5.5 3.5 1.3 0.2 setosa seto ## 38 4.9 3.6 1.4 0.1 setosa seto ## 39 4.4 3.0 1.3 0.2 setosa seto ## 40 5.1 3.4 1.5 0.2 setosa seto ## 41 5.0 3.5 1.3 0.3 setosa seto ## 42 4.5 2.3 1.3 0.3 setosa seto ## 43 4.4 3.2 1.3 0.2 setosa seto ## 44 5.0 3.5 1.6 0.6 setosa seto ## 45 5.1 3.8 1.9 0.4 setosa seto ## 46 4.8 3.0 1.4 0.3 setosa seto ## 47 5.1 3.8 1.6 0.2 setosa seto ## 48 4.6 3.2 1.4 0.2 setosa seto ## 49 5.3 3.7 1.5 0.2 setosa seto ## 50 5.0 3.3 1.4 0.2 setosa seto ## 51 7.0 3.2 4.7 1.4 versicolor versi ## 52 6.4 3.2 4.5 1.5 versicolor versi ## 53 6.9 3.1 4.9 1.5 versicolor versi ## 54 5.5 2.3 4.0 1.3 versicolor versi ## 55 6.5 2.8 4.6 1.5 versicolor versi ## 56 5.7 2.8 4.5 1.3 versicolor versi ## 57 6.3 3.3 4.7 1.6 versicolor versi ## 58 4.9 2.4 3.3 1.0 versicolor versi ## 59 6.6 2.9 4.6 1.3 versicolor versi ## 60 5.2 2.7 3.9 1.4 versicolor versi ## 61 5.0 2.0 3.5 1.0 versicolor versi ## 62 5.9 3.0 4.2 1.5 versicolor versi ## 63 6.0 2.2 4.0 1.0 versicolor versi ## 64 6.1 2.9 4.7 1.4 versicolor versi ## 65 5.6 2.9 3.6 1.3 versicolor versi ## 66 6.7 3.1 4.4 1.4 versicolor versi ## 67 5.6 3.0 4.5 1.5 versicolor versi ## 68 5.8 2.7 4.1 1.0 versicolor versi ## 69 6.2 2.2 4.5 1.5 versicolor versi ## 70 5.6 2.5 3.9 1.1 versicolor versi ## 71 5.9 3.2 4.8 1.8 versicolor versi ## 72 6.1 2.8 4.0 1.3 versicolor versi ## 73 6.3 2.5 4.9 1.5 versicolor versi ## 74 6.1 2.8 4.7 1.2 versicolor versi ## 75 6.4 2.9 4.3 1.3 versicolor versi ## 76 6.6 3.0 4.4 1.4 versicolor versi ## 77 6.8 2.8 4.8 1.4 versicolor versi ## 78 6.7 3.0 5.0 1.7 versicolor versi ## 79 6.0 2.9 4.5 1.5 versicolor versi ## 80 5.7 2.6 3.5 1.0 versicolor versi ## 81 5.5 2.4 3.8 1.1 versicolor versi ## 82 5.5 2.4 3.7 1.0 versicolor versi ## 83 5.8 2.7 3.9 1.2 versicolor versi ## 84 6.0 2.7 5.1 1.6 versicolor versi ## 85 5.4 3.0 4.5 1.5 versicolor versi ## 86 6.0 3.4 4.5 1.6 versicolor versi ## 87 6.7 3.1 4.7 1.5 versicolor versi ## 88 6.3 2.3 4.4 1.3 versicolor versi ## 89 5.6 3.0 4.1 1.3 versicolor versi ## 90 5.5 2.5 4.0 1.3 versicolor versi ## 91 5.5 2.6 4.4 1.2 versicolor versi ## 92 6.1 3.0 4.6 1.4 versicolor versi ## 93 5.8 2.6 4.0 1.2 versicolor versi ## 94 5.0 2.3 3.3 1.0 versicolor versi ## 95 5.6 2.7 4.2 1.3 versicolor versi ## 96 5.7 3.0 4.2 1.2 versicolor versi ## 97 5.7 2.9 4.2 1.3 versicolor versi ## 98 6.2 2.9 4.3 1.3 versicolor versi ## 99 5.1 2.5 3.0 1.1 versicolor versi ## 100 5.7 2.8 4.1 1.3 versicolor versi ## 101 6.3 3.3 6.0 2.5 virginica virginica ## 102 5.8 2.7 5.1 1.9 virginica virginica ## 103 7.1 3.0 5.9 2.1 virginica virginica ## 104 6.3 2.9 5.6 1.8 virginica virginica ## 105 6.5 3.0 5.8 2.2 virginica virginica ## 106 7.6 3.0 6.6 2.1 virginica virginica ## 107 4.9 2.5 4.5 1.7 virginica virginica ## 108 7.3 2.9 6.3 1.8 virginica virginica ## 109 6.7 2.5 5.8 1.8 virginica virginica ## 110 7.2 3.6 6.1 2.5 virginica virginica ## 111 6.5 3.2 5.1 2.0 virginica virginica ## 112 6.4 2.7 5.3 1.9 virginica virginica ## 113 6.8 3.0 5.5 2.1 virginica virginica ## 114 5.7 2.5 5.0 2.0 virginica virginica ## 115 5.8 2.8 5.1 2.4 virginica virginica ## 116 6.4 3.2 5.3 2.3 virginica virginica ## 117 6.5 3.0 5.5 1.8 virginica virginica ## 118 7.7 3.8 6.7 2.2 virginica virginica ## 119 7.7 2.6 6.9 2.3 virginica virginica ## 120 6.0 2.2 5.0 1.5 virginica virginica ## 121 6.9 3.2 5.7 2.3 virginica virginica ## 122 5.6 2.8 4.9 2.0 virginica virginica ## 123 7.7 2.8 6.7 2.0 virginica virginica ## 124 6.3 2.7 4.9 1.8 virginica virginica ## 125 6.7 3.3 5.7 2.1 virginica virginica ## 126 7.2 3.2 6.0 1.8 virginica virginica ## 127 6.2 2.8 4.8 1.8 virginica virginica ## 128 6.1 3.0 4.9 1.8 virginica virginica ## 129 6.4 2.8 5.6 2.1 virginica virginica ## 130 7.2 3.0 5.8 1.6 virginica virginica ## 131 7.4 2.8 6.1 1.9 virginica virginica ## 132 7.9 3.8 6.4 2.0 virginica virginica ## 133 6.4 2.8 5.6 2.2 virginica virginica ## 134 6.3 2.8 5.1 1.5 virginica virginica ## 135 6.1 2.6 5.6 1.4 virginica virginica ## 136 7.7 3.0 6.1 2.3 virginica virginica ## 137 6.3 3.4 5.6 2.4 virginica virginica ## 138 6.4 3.1 5.5 1.8 virginica virginica ## 139 6.0 3.0 4.8 1.8 virginica virginica ## 140 6.9 3.1 5.4 2.1 virginica virginica ## 141 6.7 3.1 5.6 2.4 virginica virginica ## 142 6.9 3.1 5.1 2.3 virginica virginica ## 143 5.8 2.7 5.1 1.9 virginica virginica ## 144 6.8 3.2 5.9 2.3 virginica virginica ## 145 6.7 3.3 5.7 2.5 virginica virginica ## 146 6.7 3.0 5.2 2.3 virginica virginica ## 147 6.3 2.5 5.0 1.9 virginica virginica ## 148 6.5 3.0 5.2 2.0 virginica virginica ## 149 6.2 3.4 5.4 2.3 virginica virginica ## 150 5.9 3.0 5.1 1.8 virginica virginica 1.1.12 Combine data sets prepare data sets data1 &lt;- data.frame(ID = 1:4, X1 = c(&quot;a1&quot;, &quot;a2&quot;,&quot;a3&quot;, &quot;a4&quot;), stringsAsFactors = FALSE) data2 &lt;- data.frame(ID = 2:5, X2 = c(&quot;b1&quot;, &quot;b2&quot;,&quot;b3&quot;, &quot;b4&quot;), stringsAsFactors = FALSE) inner join inner_join(data1, data2, by = &quot;ID&quot;) ## ID X1 X2 ## 1 2 a2 b1 ## 2 3 a3 b2 ## 3 4 a4 b3 left join left_join(data1, data2, by = &quot;ID&quot;) ## ID X1 X2 ## 1 1 a1 &lt;NA&gt; ## 2 2 a2 b1 ## 3 3 a3 b2 ## 4 4 a4 b3 right join right_join(data1, data2, by = &quot;ID&quot;) ## ID X1 X2 ## 1 2 a2 b1 ## 2 3 a3 b2 ## 3 4 a4 b3 ## 4 5 &lt;NA&gt; b4 full join full_join(data1, data2, by = &quot;ID&quot;) ## ID X1 X2 ## 1 1 a1 &lt;NA&gt; ## 2 2 a2 b1 ## 3 3 a3 b2 ## 4 4 a4 b3 ## 5 5 &lt;NA&gt; b4 keep cases of left data table without in right data table anti_join(data1, data2, by = &quot;ID&quot;) ## ID X1 ## 1 1 a1 keep cases of left data table in right data table semi_join(data1, data2, by = &quot;ID&quot;) ## ID X1 ## 1 2 a2 ## 2 3 a3 ## 3 4 a4 multiple full join full_join(data1, data2, by = &quot;ID&quot;) %&gt;% full_join(., data2, by = &quot;ID&quot;) ## ID X1 X2.x X2.y ## 1 1 a1 &lt;NA&gt; &lt;NA&gt; ## 2 2 a2 b1 b1 ## 3 3 a3 b2 b2 ## 4 4 a4 b3 b3 ## 5 5 &lt;NA&gt; b4 b4 rbind doesn’t work # df1 &lt;- data.frame(col1 = LETTERS[1:6], # col2a = c(5:10), # col3a = TRUE) # # df2 &lt;- data.frame(col1 = LETTERS[4:8], # col2b= c(4:8), # col3b = FALSE) # rbind(df1,df2) append two data tables by using join and merge data_frame1 &lt;- data.frame(col1 = c(6:8), col2 = letters[1:3], col3 = c(1,4,NA)) data_frame2 &lt;- data.frame(col1 = c(5:6), col5 = letters[7:8]) data_frame_merge &lt;- merge(data_frame1, data_frame2, by = &#39;col1&#39;, all = TRUE) print (data_frame_merge) ## col1 col2 col3 col5 ## 1 5 &lt;NA&gt; NA g ## 2 6 a 1 h ## 3 7 b 4 &lt;NA&gt; ## 4 8 c NA &lt;NA&gt; full_join(data_frame1,data_frame2, by=c(&quot;col1&quot;),) ## col1 col2 col3 col5 ## 1 6 a 1 h ## 2 7 b 4 &lt;NA&gt; ## 3 8 c NA &lt;NA&gt; ## 4 5 &lt;NA&gt; NA g 1.2 How to do aggregation/ summarization 1.2.1 Summarization after grouping library(tidyverse) iris %&gt;% group_by(Species) %&gt;% summarize(Support = mean(Sepal.Length)) %&gt;% # average arrange(-Support) # sort ## # A tibble: 3 × 2 ## Species Support ## &lt;fct&gt; &lt;dbl&gt; ## 1 virginica 6.59 ## 2 versicolor 5.94 ## 3 setosa 5.01 iris %&gt;% group_by(Species) %&gt;% summarize(mean_s = mean(Sepal.Width), meas_p = mean(Petal.Length), diff = mean(Sepal.Width-Petal.Length)) %&gt;% arrange(-diff) ## # A tibble: 3 × 4 ## Species mean_s meas_p diff ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 3.43 1.46 1.97 ## 2 versicolor 2.77 4.26 -1.49 ## 3 virginica 2.97 5.55 -2.58 iris %&gt;% group_by(Species) %&gt;% summarize(n = n(), meas_p = mean(Petal.Length), sd = sd(Petal.Length)) ## # A tibble: 3 × 4 ## Species n meas_p sd ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 50 1.46 0.174 ## 2 versicolor 50 4.26 0.470 ## 3 virginica 50 5.55 0.552 1.2.2 Summarization with upgroup iris %&gt;% ungroup( ) %&gt;% summarize(n = n(), meas_p = mean(Petal.Length), sd = sd(Petal.Length)) ## n meas_p sd ## 1 150 3.758 1.765298 1.2.3 Mutate new variables after grouping iris %&gt;% group_by(Species) %&gt;% mutate(n = n(), meas_p = mean(Petal.Length), sd = sd(Petal.Length)) ## # A tibble: 150 × 8 ## # Groups: Species [3] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species n meas_p sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 setosa 50 1.46 0.174 ## 2 4.9 3 1.4 0.2 setosa 50 1.46 0.174 ## 3 4.7 3.2 1.3 0.2 setosa 50 1.46 0.174 ## 4 4.6 3.1 1.5 0.2 setosa 50 1.46 0.174 ## 5 5 3.6 1.4 0.2 setosa 50 1.46 0.174 ## 6 5.4 3.9 1.7 0.4 setosa 50 1.46 0.174 ## 7 4.6 3.4 1.4 0.3 setosa 50 1.46 0.174 ## 8 5 3.4 1.5 0.2 setosa 50 1.46 0.174 ## 9 4.4 2.9 1.4 0.2 setosa 50 1.46 0.174 ## 10 4.9 3.1 1.5 0.1 setosa 50 1.46 0.174 ## # … with 140 more rows iris %&gt;% group_by(Species) %&gt;% mutate(n = n(), meas_p = mean(Petal.Length, na.rm = T), sd = sd(Petal.Length)) %&gt;% summarize (n_mean = paste (&quot;sample size:&quot;,mean(n)), meas_p = mean(Petal.Length), sd = sd(Petal.Length)) ## # A tibble: 3 × 4 ## Species n_mean meas_p sd ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa sample size: 50 1.46 0.174 ## 2 versicolor sample size: 50 4.26 0.470 ## 3 virginica sample size: 50 5.55 0.552 1.2.4 Recode and generate new variables, then value label irisifelse &lt;- iris %&gt;% mutate(Species2 = ifelse(Species == &quot;setosa&quot;, NA, Species)) # relabel values irisifelse$Species2 &lt;- factor(irisifelse$Species2,labels = c( &quot;versi&quot;,&quot;virg&quot;)) irisifelse ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Species2 ## 1 5.1 3.5 1.4 0.2 setosa &lt;NA&gt; ## 2 4.9 3.0 1.4 0.2 setosa &lt;NA&gt; ## 3 4.7 3.2 1.3 0.2 setosa &lt;NA&gt; ## 4 4.6 3.1 1.5 0.2 setosa &lt;NA&gt; ## 5 5.0 3.6 1.4 0.2 setosa &lt;NA&gt; ## 6 5.4 3.9 1.7 0.4 setosa &lt;NA&gt; ## 7 4.6 3.4 1.4 0.3 setosa &lt;NA&gt; ## 8 5.0 3.4 1.5 0.2 setosa &lt;NA&gt; ## 9 4.4 2.9 1.4 0.2 setosa &lt;NA&gt; ## 10 4.9 3.1 1.5 0.1 setosa &lt;NA&gt; ## 11 5.4 3.7 1.5 0.2 setosa &lt;NA&gt; ## 12 4.8 3.4 1.6 0.2 setosa &lt;NA&gt; ## 13 4.8 3.0 1.4 0.1 setosa &lt;NA&gt; ## 14 4.3 3.0 1.1 0.1 setosa &lt;NA&gt; ## 15 5.8 4.0 1.2 0.2 setosa &lt;NA&gt; ## 16 5.7 4.4 1.5 0.4 setosa &lt;NA&gt; ## 17 5.4 3.9 1.3 0.4 setosa &lt;NA&gt; ## 18 5.1 3.5 1.4 0.3 setosa &lt;NA&gt; ## 19 5.7 3.8 1.7 0.3 setosa &lt;NA&gt; ## 20 5.1 3.8 1.5 0.3 setosa &lt;NA&gt; ## 21 5.4 3.4 1.7 0.2 setosa &lt;NA&gt; ## 22 5.1 3.7 1.5 0.4 setosa &lt;NA&gt; ## 23 4.6 3.6 1.0 0.2 setosa &lt;NA&gt; ## 24 5.1 3.3 1.7 0.5 setosa &lt;NA&gt; ## 25 4.8 3.4 1.9 0.2 setosa &lt;NA&gt; ## 26 5.0 3.0 1.6 0.2 setosa &lt;NA&gt; ## 27 5.0 3.4 1.6 0.4 setosa &lt;NA&gt; ## 28 5.2 3.5 1.5 0.2 setosa &lt;NA&gt; ## 29 5.2 3.4 1.4 0.2 setosa &lt;NA&gt; ## 30 4.7 3.2 1.6 0.2 setosa &lt;NA&gt; ## 31 4.8 3.1 1.6 0.2 setosa &lt;NA&gt; ## 32 5.4 3.4 1.5 0.4 setosa &lt;NA&gt; ## 33 5.2 4.1 1.5 0.1 setosa &lt;NA&gt; ## 34 5.5 4.2 1.4 0.2 setosa &lt;NA&gt; ## 35 4.9 3.1 1.5 0.2 setosa &lt;NA&gt; ## 36 5.0 3.2 1.2 0.2 setosa &lt;NA&gt; ## 37 5.5 3.5 1.3 0.2 setosa &lt;NA&gt; ## 38 4.9 3.6 1.4 0.1 setosa &lt;NA&gt; ## 39 4.4 3.0 1.3 0.2 setosa &lt;NA&gt; ## 40 5.1 3.4 1.5 0.2 setosa &lt;NA&gt; ## 41 5.0 3.5 1.3 0.3 setosa &lt;NA&gt; ## 42 4.5 2.3 1.3 0.3 setosa &lt;NA&gt; ## 43 4.4 3.2 1.3 0.2 setosa &lt;NA&gt; ## 44 5.0 3.5 1.6 0.6 setosa &lt;NA&gt; ## 45 5.1 3.8 1.9 0.4 setosa &lt;NA&gt; ## 46 4.8 3.0 1.4 0.3 setosa &lt;NA&gt; ## 47 5.1 3.8 1.6 0.2 setosa &lt;NA&gt; ## 48 4.6 3.2 1.4 0.2 setosa &lt;NA&gt; ## 49 5.3 3.7 1.5 0.2 setosa &lt;NA&gt; ## 50 5.0 3.3 1.4 0.2 setosa &lt;NA&gt; ## 51 7.0 3.2 4.7 1.4 versicolor versi ## 52 6.4 3.2 4.5 1.5 versicolor versi ## 53 6.9 3.1 4.9 1.5 versicolor versi ## 54 5.5 2.3 4.0 1.3 versicolor versi ## 55 6.5 2.8 4.6 1.5 versicolor versi ## 56 5.7 2.8 4.5 1.3 versicolor versi ## 57 6.3 3.3 4.7 1.6 versicolor versi ## 58 4.9 2.4 3.3 1.0 versicolor versi ## 59 6.6 2.9 4.6 1.3 versicolor versi ## 60 5.2 2.7 3.9 1.4 versicolor versi ## 61 5.0 2.0 3.5 1.0 versicolor versi ## 62 5.9 3.0 4.2 1.5 versicolor versi ## 63 6.0 2.2 4.0 1.0 versicolor versi ## 64 6.1 2.9 4.7 1.4 versicolor versi ## 65 5.6 2.9 3.6 1.3 versicolor versi ## 66 6.7 3.1 4.4 1.4 versicolor versi ## 67 5.6 3.0 4.5 1.5 versicolor versi ## 68 5.8 2.7 4.1 1.0 versicolor versi ## 69 6.2 2.2 4.5 1.5 versicolor versi ## 70 5.6 2.5 3.9 1.1 versicolor versi ## 71 5.9 3.2 4.8 1.8 versicolor versi ## 72 6.1 2.8 4.0 1.3 versicolor versi ## 73 6.3 2.5 4.9 1.5 versicolor versi ## 74 6.1 2.8 4.7 1.2 versicolor versi ## 75 6.4 2.9 4.3 1.3 versicolor versi ## 76 6.6 3.0 4.4 1.4 versicolor versi ## 77 6.8 2.8 4.8 1.4 versicolor versi ## 78 6.7 3.0 5.0 1.7 versicolor versi ## 79 6.0 2.9 4.5 1.5 versicolor versi ## 80 5.7 2.6 3.5 1.0 versicolor versi ## 81 5.5 2.4 3.8 1.1 versicolor versi ## 82 5.5 2.4 3.7 1.0 versicolor versi ## 83 5.8 2.7 3.9 1.2 versicolor versi ## 84 6.0 2.7 5.1 1.6 versicolor versi ## 85 5.4 3.0 4.5 1.5 versicolor versi ## 86 6.0 3.4 4.5 1.6 versicolor versi ## 87 6.7 3.1 4.7 1.5 versicolor versi ## 88 6.3 2.3 4.4 1.3 versicolor versi ## 89 5.6 3.0 4.1 1.3 versicolor versi ## 90 5.5 2.5 4.0 1.3 versicolor versi ## 91 5.5 2.6 4.4 1.2 versicolor versi ## 92 6.1 3.0 4.6 1.4 versicolor versi ## 93 5.8 2.6 4.0 1.2 versicolor versi ## 94 5.0 2.3 3.3 1.0 versicolor versi ## 95 5.6 2.7 4.2 1.3 versicolor versi ## 96 5.7 3.0 4.2 1.2 versicolor versi ## 97 5.7 2.9 4.2 1.3 versicolor versi ## 98 6.2 2.9 4.3 1.3 versicolor versi ## 99 5.1 2.5 3.0 1.1 versicolor versi ## 100 5.7 2.8 4.1 1.3 versicolor versi ## 101 6.3 3.3 6.0 2.5 virginica virg ## 102 5.8 2.7 5.1 1.9 virginica virg ## 103 7.1 3.0 5.9 2.1 virginica virg ## 104 6.3 2.9 5.6 1.8 virginica virg ## 105 6.5 3.0 5.8 2.2 virginica virg ## 106 7.6 3.0 6.6 2.1 virginica virg ## 107 4.9 2.5 4.5 1.7 virginica virg ## 108 7.3 2.9 6.3 1.8 virginica virg ## 109 6.7 2.5 5.8 1.8 virginica virg ## 110 7.2 3.6 6.1 2.5 virginica virg ## 111 6.5 3.2 5.1 2.0 virginica virg ## 112 6.4 2.7 5.3 1.9 virginica virg ## 113 6.8 3.0 5.5 2.1 virginica virg ## 114 5.7 2.5 5.0 2.0 virginica virg ## 115 5.8 2.8 5.1 2.4 virginica virg ## 116 6.4 3.2 5.3 2.3 virginica virg ## 117 6.5 3.0 5.5 1.8 virginica virg ## 118 7.7 3.8 6.7 2.2 virginica virg ## 119 7.7 2.6 6.9 2.3 virginica virg ## 120 6.0 2.2 5.0 1.5 virginica virg ## 121 6.9 3.2 5.7 2.3 virginica virg ## 122 5.6 2.8 4.9 2.0 virginica virg ## 123 7.7 2.8 6.7 2.0 virginica virg ## 124 6.3 2.7 4.9 1.8 virginica virg ## 125 6.7 3.3 5.7 2.1 virginica virg ## 126 7.2 3.2 6.0 1.8 virginica virg ## 127 6.2 2.8 4.8 1.8 virginica virg ## 128 6.1 3.0 4.9 1.8 virginica virg ## 129 6.4 2.8 5.6 2.1 virginica virg ## 130 7.2 3.0 5.8 1.6 virginica virg ## 131 7.4 2.8 6.1 1.9 virginica virg ## 132 7.9 3.8 6.4 2.0 virginica virg ## 133 6.4 2.8 5.6 2.2 virginica virg ## 134 6.3 2.8 5.1 1.5 virginica virg ## 135 6.1 2.6 5.6 1.4 virginica virg ## 136 7.7 3.0 6.1 2.3 virginica virg ## 137 6.3 3.4 5.6 2.4 virginica virg ## 138 6.4 3.1 5.5 1.8 virginica virg ## 139 6.0 3.0 4.8 1.8 virginica virg ## 140 6.9 3.1 5.4 2.1 virginica virg ## 141 6.7 3.1 5.6 2.4 virginica virg ## 142 6.9 3.1 5.1 2.3 virginica virg ## 143 5.8 2.7 5.1 1.9 virginica virg ## 144 6.8 3.2 5.9 2.3 virginica virg ## 145 6.7 3.3 5.7 2.5 virginica virg ## 146 6.7 3.0 5.2 2.3 virginica virg ## 147 6.3 2.5 5.0 1.9 virginica virg ## 148 6.5 3.0 5.2 2.0 virginica virg ## 149 6.2 3.4 5.4 2.3 virginica virg ## 150 5.9 3.0 5.1 1.8 virginica virg str(irisifelse) ## &#39;data.frame&#39;: 150 obs. of 6 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Species2 : Factor w/ 2 levels &quot;versi&quot;,&quot;virg&quot;: NA NA NA NA NA NA NA NA NA NA ... "],["machine-learning.html", "2 Machine learning 2.1 Machine learning workflow 2.2 KNN Classifier 2.3 KNN regression", " 2 Machine learning 2.1 Machine learning workflow 2.1.1 Loading packages and datasets # load the Pima Indians dataset from the mlbench dataset library(mlbench) data(PimaIndiansDiabetes) # rename dataset to have shorter name because lazy diabetes &lt;- PimaIndiansDiabetes look at the data set # install.packages(c(&#39;caret&#39;, &#39;skimr&#39;, &#39;RANN&#39;, &#39;randomForest&#39;, &#39;fastAdaboost&#39;, &#39;gbm&#39;, &#39;xgboost&#39;, &#39;caretEnsemble&#39;, &#39;C50&#39;, &#39;earth&#39;)) # Load the caret package library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift # Structure of the dataframe str(diabetes) ## &#39;data.frame&#39;: 768 obs. of 9 variables: ## $ pregnant: num 6 1 8 1 0 5 3 10 2 8 ... ## $ glucose : num 148 85 183 89 137 116 78 115 197 125 ... ## $ pressure: num 72 66 64 66 40 74 50 0 70 96 ... ## $ triceps : num 35 29 0 23 35 0 32 0 45 0 ... ## $ insulin : num 0 0 0 94 168 0 88 0 543 0 ... ## $ mass : num 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ... ## $ pedigree: num 0.627 0.351 0.672 0.167 2.288 ... ## $ age : num 50 31 32 21 33 30 26 29 53 54 ... ## $ diabetes: Factor w/ 2 levels &quot;neg&quot;,&quot;pos&quot;: 2 1 2 1 2 1 2 1 2 2 ... # See top 6 rows head(diabetes ) ## pregnant glucose pressure triceps insulin mass pedigree age diabetes ## 1 6 148 72 35 0 33.6 0.627 50 pos ## 2 1 85 66 29 0 26.6 0.351 31 neg ## 3 8 183 64 0 0 23.3 0.672 32 pos ## 4 1 89 66 23 94 28.1 0.167 21 neg ## 5 0 137 40 35 168 43.1 2.288 33 pos ## 6 5 116 74 0 0 25.6 0.201 30 neg 2.1.2 Spliting the dataset into training and test data sets # Create the training and test datasets set.seed(100) # Step 1: Get row numbers for the training data trainRowNumbers &lt;- createDataPartition(diabetes$diabetes, p=0.8, list=FALSE) # Step 2: Create the training dataset trainData &lt;- diabetes[trainRowNumbers,] # Step 3: Create the test dataset testData &lt;- diabetes[-trainRowNumbers,] # Store X and Y for later use. # x = trainData[, -1] y = trainData$diabetes have a look training data set library(skimr) skimmed &lt;- skim (trainData) skimmed Table 2.1: Data summary Name trainData Number of rows 615 Number of columns 9 _______________________ Column type frequency: factor 1 numeric 8 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts diabetes 0 1 FALSE 2 neg: 400, pos: 215 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist pregnant 0 1 3.88 3.42 0.00 1.00 3.00 6.00 17.00 ▇▃▂▁▁ glucose 0 1 120.87 32.58 0.00 99.00 116.00 140.00 199.00 ▁▁▇▅▂ pressure 0 1 69.33 19.44 0.00 62.00 72.00 80.00 122.00 ▁▁▇▇▁ triceps 0 1 19.97 15.87 0.00 0.00 22.00 32.00 99.00 ▇▇▂▁▁ insulin 0 1 78.00 114.39 0.00 0.00 18.00 125.00 846.00 ▇▁▁▁▁ mass 0 1 32.08 7.97 0.00 27.50 32.00 36.80 67.10 ▁▂▇▂▁ pedigree 0 1 0.46 0.33 0.08 0.24 0.36 0.61 2.29 ▇▃▁▁▁ age 0 1 33.41 11.77 21.00 24.00 29.00 41.00 81.00 ▇▃▁▁▁ 2.1.3 Implement data imputation compiling knnimpute model # Create the knn imputation model on the training data preProcess_missingdata_model &lt;- preProcess(trainData, method=&#39;knnImpute&#39;) preProcess_missingdata_model ## Created from 615 samples and 9 variables ## ## Pre-processing: ## - centered (8) ## - ignored (1) ## - 5 nearest neighbor imputation (8) ## - scaled (8) check missingness # Use the imputation model to predict the values of missing data points library(RANN) # required for knnInpute trainData &lt;- predict(preProcess_missingdata_model, newdata = trainData) anyNA(trainData) ## [1] FALSE 2.1.4 One-hot-endcoding Y (dependent) will not be encoded as one-hot-encoding # One-Hot Encoding # Creating dummy variables is converting a categorical variable to as many binary variables as here are categories. dummies_model &lt;- dummyVars(diabetes ~ ., data=trainData) # Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat. trainData_mat &lt;- predict(dummies_model, newdata = trainData) ## Warning in model.frame.default(Terms, newdata, na.action = na.action, xlev = ## object$lvls): variable &#39;diabetes&#39; is not a factor # # Convert to dataframe trainData &lt;- data.frame(trainData_mat) # # See the structure of the new dataset str(trainData) ## &#39;data.frame&#39;: 615 obs. of 8 variables: ## $ pregnant: num -0.843 1.205 -0.843 -1.136 0.327 ... ## $ glucose : num -1.101 1.907 -0.978 0.495 -0.15 ... ## $ pressure: num -0.171 -0.274 -0.171 -1.508 0.24 ... ## $ triceps : num 0.569 -1.258 0.191 0.947 -1.258 ... ## $ insulin : num -0.682 -0.682 0.14 0.787 -0.682 ... ## $ mass : num -0.687 -1.101 -0.499 1.382 -0.812 ... ## $ pedigree: num -0.349 0.637 -0.915 5.601 -0.81 ... ## $ age : num -0.205 -0.1201 -1.0548 -0.0351 -0.29 ... 2.1.5 Normalizing features preProcess_range_model &lt;- preProcess(trainData, method=&#39;range&#39;) trainData &lt;- predict(preProcess_range_model, newdata = trainData) # Append the Y variable instead of normalized data trainData$diabetes &lt;- y # Look the dataset apply(trainData[, -1], 2, FUN=function(x){c(&#39;min&#39;=min(x), &#39;max&#39;=max(x))}) ## glucose pressure triceps insulin mass pedigree ## min &quot;0.0000000&quot; &quot;0.0000000&quot; &quot;0.00000000&quot; &quot;0.00000000&quot; &quot;0.0000000&quot; &quot;0.000000000&quot; ## max &quot;1.0000000&quot; &quot;1.0000000&quot; &quot;1.00000000&quot; &quot;1.00000000&quot; &quot;1.0000000&quot; &quot;1.000000000&quot; ## age diabetes ## min &quot;0.00000000&quot; &quot;neg&quot; ## max &quot;1.00000000&quot; &quot;pos&quot; str(trainData) ## &#39;data.frame&#39;: 615 obs. of 9 variables: ## $ pregnant: num 0.0588 0.4706 0.0588 0 0.2941 ... ## $ glucose : num 0.427 0.92 0.447 0.688 0.583 ... ## $ pressure: num 0.541 0.525 0.541 0.328 0.607 ... ## $ triceps : num 0.293 0 0.232 0.354 0 ... ## $ insulin : num 0 0 0.111 0.199 0 ... ## $ mass : num 0.396 0.347 0.419 0.642 0.382 ... ## $ pedigree: num 0.1235 0.2688 0.0403 1 0.0557 ... ## $ age : num 0.167 0.183 0 0.2 0.15 ... ## $ diabetes: Factor w/ 2 levels &quot;neg&quot;,&quot;pos&quot;: 1 2 1 2 1 2 1 2 2 2 ... 2.1.6 Plot features featurePlot(x = trainData[, 1:8], y = trainData$diabetes, plot = &quot;box&quot;, strip=strip.custom(par.strip.text=list(cex=.7)), scales = list(x = list(relation=&quot;free&quot;), y = list(relation=&quot;free&quot;))) featurePlot(x = trainData[, 1:8], y = trainData$diabetes, plot = &quot;density&quot;, strip=strip.custom(par.strip.text=list(cex=.7)), scales = list(x = list(relation=&quot;free&quot;), y = list(relation=&quot;free&quot;))) library(corrplot) ## corrplot 0.92 loaded corrplot(cor((trainData[,-9] ))) 2.1.7 Recursive feature elimination (rfe) In some scenarios, we just have to include the significant features into the following model. A good choice of selecting the important features is the recursive feature elimination (RFE). the final subset model is marked with a starisk in the last column, here it is 8th. though it is not wise to neglect the other predictors. set.seed(100) options(warn=-1) subsets &lt;- c(1:8) ctrl &lt;- rfeControl(functions = rfFuncs, #random forest algorithm method = &quot;repeatedcv&quot;, #k fold cross validation repeated 5 times repeats = 5, verbose = FALSE) lmProfile &lt;- rfe(x=trainData[, 1:8], y=trainData$diabetes, sizes = subsets, rfeControl = ctrl) lmProfile ## ## Recursive feature selection ## ## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) ## ## Resampling performance over subset size: ## ## Variables Accuracy Kappa AccuracySD KappaSD Selected ## 1 0.6952 0.2841 0.04915 0.10605 ## 2 0.7275 0.3815 0.04359 0.09609 ## 3 0.7642 0.4673 0.04216 0.09490 ## 4 0.7620 0.4631 0.04762 0.10945 ## 5 0.7571 0.4534 0.05152 0.11813 ## 6 0.7627 0.4679 0.04949 0.11218 ## 7 0.7620 0.4619 0.05210 0.12019 ## 8 0.7682 0.4728 0.04620 0.10576 * ## ## The top 5 variables (out of 8): ## glucose, mass, age, pregnant, insulin look up features of all models in R # See available algorithms in caret modelnames &lt;- paste(names(getModelInfo()), collapse=&#39;, &#39;) modelLookup(&#39;xgbTree&#39;) ## model parameter label forReg forClass ## 1 xgbTree nrounds # Boosting Iterations TRUE TRUE ## 2 xgbTree max_depth Max Tree Depth TRUE TRUE ## 3 xgbTree eta Shrinkage TRUE TRUE ## 4 xgbTree gamma Minimum Loss Reduction TRUE TRUE ## 5 xgbTree colsample_bytree Subsample Ratio of Columns TRUE TRUE ## 6 xgbTree min_child_weight Minimum Sum of Instance Weight TRUE TRUE ## 7 xgbTree subsample Subsample Percentage TRUE TRUE ## probModel ## 1 TRUE ## 2 TRUE ## 3 TRUE ## 4 TRUE ## 5 TRUE ## 6 TRUE ## 7 TRUE 2.1.8 Training a model Multivariate Adaptive Regression Splines (MARS) # Set the seed for reproducibility set.seed(100) # Train the model using randomForest and predict on the training data itself. model_mars = train(diabetes ~ ., data=trainData, method=&#39;earth&#39;) ## Loading required package: earth ## Loading required package: Formula ## Loading required package: plotmo ## Loading required package: plotrix ## Loading required package: TeachingDemos fitted &lt;- predict(model_mars) the default of resampling (Bootstrapped) is 25 reps model_mars ## Multivariate Adaptive Regression Spline ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 615, 615, 615, 615, 615, 615, ... ## Resampling results across tuning parameters: ## ## nprune Accuracy Kappa ## 2 0.7451922 0.4023855 ## 8 0.7680686 0.4748261 ## 14 0.7603116 0.4581491 ## ## Tuning parameter &#39;degree&#39; was held constant at a value of 1 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were nprune = 8 and degree = 1. plot the Accuracy of various combinations of the hyper parameters - interaction.depth and n.trees. plot(model_mars, main=&quot;Model Accuracies with MARS&quot;) calculate the importance of variable varimp_mars &lt;- varImp(model_mars) plot(varimp_mars, main=&quot;Variable Importance with MARS&quot;) 2.1.9 Prepare the test data set imputation,dummy, and normalization # Step 1: Impute missing values testData2 &lt;- predict(preProcess_missingdata_model, testData) # Step 2: Create one-hot encodings (dummy variables) testData3 &lt;- predict(dummies_model, testData2) # Step 3: Transform the features to range between 0 and 1 testData4 &lt;- predict(preProcess_range_model, testData3) # View head(testData4 ) ## pregnant glucose pressure triceps insulin mass pedigree ## 1 0.35294118 0.7437186 0.5901639 0.3535354 0.0000000 0.5007452 0.24841629 ## 11 0.23529412 0.5527638 0.7540984 0.0000000 0.0000000 0.5603577 0.05113122 ## 21 0.17647059 0.6331658 0.7213115 0.4141414 0.2777778 0.5856930 0.28325792 ## 24 0.52941176 0.5979899 0.6557377 0.3535354 0.0000000 0.4321908 0.08371041 ## 28 0.05882353 0.4874372 0.5409836 0.1515152 0.1654846 0.3457526 0.18506787 ## 37 0.64705882 0.6934673 0.6229508 0.0000000 0.0000000 0.4947839 0.15475113 ## age ## 1 0.48333333 ## 11 0.15000000 ## 21 0.10000000 ## 24 0.13333333 ## 28 0.01666667 ## 37 0.23333333 2.1.10 Prediction uisng testdata # Predict on testData predicted &lt;- predict(model_mars, testData4) head(predicted) ## [1] pos neg neg neg neg pos ## Levels: neg pos 2.1.11 Compute confusion matrix # Compute the confusion matrix confusionMatrix(reference = as.factor(testData$diabetes), data = predicted ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 86 22 ## pos 14 31 ## ## Accuracy : 0.7647 ## 95% CI : (0.6894, 0.8294) ## No Information Rate : 0.6536 ## P-Value [Acc &gt; NIR] : 0.001988 ## ## Kappa : 0.4613 ## ## Mcnemar&#39;s Test P-Value : 0.243345 ## ## Sensitivity : 0.8600 ## Specificity : 0.5849 ## Pos Pred Value : 0.7963 ## Neg Pred Value : 0.6889 ## Prevalence : 0.6536 ## Detection Rate : 0.5621 ## Detection Prevalence : 0.7059 ## Balanced Accuracy : 0.7225 ## ## &#39;Positive&#39; Class : neg ## 2.1.12 Tuning hyperparameter to optimize the model setting up hyper parameter tuneLength, tuneGrid # Define the training control fitControl &lt;- trainControl( method = &#39;cv&#39;, # k-fold cross validation number = 5, # number of folds savePredictions = &#39;final&#39;, # saves predictions for optimal tuning parameter classProbs = T, # should class probabilities be returned summaryFunction=twoClassSummary # results summary function ) # Step 1: Define the tuneGrid marsGrid &lt;- expand.grid(nprune = c(2, 4, 6, 8, 10), degree = c(1, 2, 3)) # Step 2: Tune hyper parameters by setting tuneGrid set.seed(100) model_mars3 = train(diabetes ~ ., data=trainData, method=&#39;earth&#39;, metric=&#39;ROC&#39;, tuneGrid = marsGrid, trControl = fitControl) model_mars3 ## Multivariate Adaptive Regression Spline ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results across tuning parameters: ## ## degree nprune ROC Sens Spec ## 1 2 0.7962500 0.8850 0.4976744 ## 1 4 0.8400581 0.8725 0.6046512 ## 1 6 0.8410465 0.8825 0.6046512 ## 1 8 0.8471512 0.8850 0.5860465 ## 1 10 0.8437209 0.8775 0.6093023 ## 2 2 0.7962500 0.8850 0.4976744 ## 2 4 0.8284593 0.8775 0.6000000 ## 2 6 0.8224419 0.8725 0.5488372 ## 2 8 0.8237209 0.8700 0.5395349 ## 2 10 0.8212209 0.8650 0.5395349 ## 3 2 0.7962500 0.8850 0.4976744 ## 3 4 0.8245058 0.8825 0.6000000 ## 3 6 0.8205814 0.8750 0.5627907 ## 3 8 0.8191860 0.8725 0.5581395 ## 3 10 0.8195349 0.8650 0.5627907 ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were nprune = 8 and degree = 1. # Step 3: Predict on testData and Compute the confusion matrix predicted3 &lt;- predict(model_mars3, testData4) confusionMatrix(reference = as.factor(testData$diabetes), data = predicted3 ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 86 22 ## pos 14 31 ## ## Accuracy : 0.7647 ## 95% CI : (0.6894, 0.8294) ## No Information Rate : 0.6536 ## P-Value [Acc &gt; NIR] : 0.001988 ## ## Kappa : 0.4613 ## ## Mcnemar&#39;s Test P-Value : 0.243345 ## ## Sensitivity : 0.8600 ## Specificity : 0.5849 ## Pos Pred Value : 0.7963 ## Neg Pred Value : 0.6889 ## Prevalence : 0.6536 ## Detection Rate : 0.5621 ## Detection Prevalence : 0.7059 ## Balanced Accuracy : 0.7225 ## ## &#39;Positive&#39; Class : neg ## 2.1.13 Other marchine learning algorithms 2.1.13.1 adaboost algorithm set.seed(100) # Train the model using adaboost model_adaboost = train(diabetes ~ ., data=trainData, method=&#39;adaboost&#39;, tuneLength=2, trControl = fitControl) model_adaboost ## AdaBoost Classification Trees ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results across tuning parameters: ## ## nIter method ROC Sens Spec ## 50 Adaboost.M1 0.7856395 0.8025 0.5906977 ## 50 Real adaboost 0.6250000 0.8350 0.5534884 ## 100 Adaboost.M1 0.7852907 0.8050 0.6325581 ## 100 Real adaboost 0.6051163 0.8450 0.5581395 ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were nIter = 50 and method = Adaboost.M1. 2.1.13.2 random forest set.seed(100) # Train the model using rf model_rf = train(diabetes ~ ., data=trainData, method=&#39;rf&#39;, tuneLength=5, trControl = fitControl) model_rf ## Random Forest ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results across tuning parameters: ## ## mtry ROC Sens Spec ## 2 0.8210756 0.8600 0.5906977 ## 3 0.8217733 0.8575 0.6046512 ## 5 0.8145640 0.8550 0.5906977 ## 6 0.8152616 0.8575 0.6093023 ## 8 0.8145349 0.8500 0.6000000 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 3. 2.1.13.3 xgbDART algorithm # set.seed(100) # # # Train the model using MARS # model_xgbDART = train(Purchase ~ ., data=trainData, method=&#39;xgbDART&#39;, tuneLength=5, trControl = fitControl, verbose=F) # model_xgbDART 2.1.13.4 Support Vector Machines (SVM) set.seed(100) # Train the model using MARS model_svmRadial = train(diabetes ~ ., data=trainData, method=&#39;svmRadial&#39;, tuneLength=15, trControl = fitControl) model_svmRadial ## Support Vector Machines with Radial Basis Function Kernel ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results across tuning parameters: ## ## C ROC Sens Spec ## 0.25 0.8306395 0.8650 0.5813953 ## 0.50 0.8308140 0.8800 0.5720930 ## 1.00 0.8279070 0.8750 0.5348837 ## 2.00 0.8216860 0.8825 0.4976744 ## 4.00 0.8204070 0.8925 0.4883721 ## 8.00 0.8080814 0.8800 0.4790698 ## 16.00 0.7892442 0.8825 0.4651163 ## 32.00 0.7677326 0.8875 0.4093023 ## 64.00 0.7430814 0.8925 0.3674419 ## 128.00 0.7165698 0.8825 0.3255814 ## 256.00 0.7062209 0.8975 0.3255814 ## 512.00 0.7051163 0.9100 0.2930233 ## 1024.00 0.7005814 0.9025 0.3023256 ## 2048.00 0.6955233 0.9000 0.3162791 ## 4096.00 0.6948837 0.8925 0.3348837 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.1161195 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.1161195 and C = 0.5. 2.1.13.5 K-Nearest Neighbors set.seed(100) # Train the model using MARS model_knn = train(diabetes ~ ., data=trainData, method=&#39;knn&#39;, tuneLength=15, trControl = fitControl) model_knn ## k-Nearest Neighbors ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results across tuning parameters: ## ## k ROC Sens Spec ## 5 0.7401744 0.8250 0.4651163 ## 7 0.7655523 0.8425 0.4744186 ## 9 0.7707849 0.8500 0.4930233 ## 11 0.7797384 0.8800 0.4976744 ## 13 0.7876744 0.8725 0.4790698 ## 15 0.7951163 0.8800 0.4837209 ## 17 0.7933721 0.8775 0.4651163 ## 19 0.7997965 0.8825 0.4465116 ## 21 0.8001163 0.8975 0.4418605 ## 23 0.8024709 0.9050 0.4651163 ## 25 0.8037791 0.9050 0.4744186 ## 27 0.8082267 0.9050 0.4790698 ## 29 0.8069767 0.9150 0.4697674 ## 31 0.8083430 0.9100 0.4418605 ## 33 0.8064244 0.9175 0.4372093 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was k = 31. 2.1.14 Comparisons of different models # Compare model performances using resample() models_compare &lt;- resamples(list(ADABOOST=model_adaboost, RF=model_rf, knn=model_knn, MARS=model_mars3, SVM=model_svmRadial)) # Summary of the models performances summary(models_compare) ## ## Call: ## summary.resamples(object = models_compare) ## ## Models: ADABOOST, RF, knn, MARS, SVM ## Number of resamples: 5 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## ADABOOST 0.7436047 0.7750000 0.7851744 0.7856395 0.7889535 0.8354651 0 ## RF 0.7697674 0.8155523 0.8170058 0.8217733 0.8497093 0.8568314 0 ## knn 0.7646802 0.7680233 0.8209302 0.8083430 0.8297965 0.8582849 0 ## MARS 0.8238372 0.8430233 0.8450581 0.8471512 0.8613372 0.8625000 0 ## SVM 0.7648256 0.8279070 0.8436047 0.8308140 0.8441860 0.8735465 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## ADABOOST 0.7500 0.7750 0.8125 0.8025 0.8250 0.8500 0 ## RF 0.8250 0.8375 0.8625 0.8575 0.8750 0.8875 0 ## knn 0.8875 0.9000 0.9000 0.9100 0.9125 0.9500 0 ## MARS 0.8500 0.8625 0.8875 0.8850 0.9125 0.9125 0 ## SVM 0.8625 0.8750 0.8750 0.8800 0.8875 0.9000 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## ADABOOST 0.5348837 0.5348837 0.5581395 0.5906977 0.6046512 0.7209302 0 ## RF 0.5581395 0.5581395 0.5813953 0.6046512 0.6511628 0.6744186 0 ## knn 0.4186047 0.4418605 0.4418605 0.4418605 0.4418605 0.4651163 0 ## MARS 0.5348837 0.5348837 0.5813953 0.5860465 0.6046512 0.6744186 0 ## SVM 0.5116279 0.5581395 0.5813953 0.5720930 0.6046512 0.6046512 0 2.1.15 Plot comparisons of models # Draw box plots to compare models scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) bwplot(models_compare, scales=scales) 2.1.16 Ensemble predictions from multiple models create multiple models library(caretEnsemble) ## ## Attaching package: &#39;caretEnsemble&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## autoplot # Stacking Algorithms - Run multiple algos in one call. trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE) algorithmList &lt;- c(&#39;rf&#39;, &#39;adaboost&#39;, &#39;earth&#39;, &#39;knn&#39;, &#39;svmRadial&#39;) set.seed(100) models &lt;- caretList(diabetes ~ ., data=trainData, trControl=trainControl, methodList=algorithmList) results &lt;- resamples(models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rf, adaboost, earth, knn, svmRadial ## Number of resamples: 30 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rf 0.6774194 0.7540984 0.7704918 0.7723956 0.8056584 0.8548387 0 ## adaboost 0.6612903 0.7224352 0.7540984 0.7539221 0.7868852 0.8387097 0 ## earth 0.6612903 0.7419355 0.7741935 0.7745725 0.8032787 0.8709677 0 ## knn 0.6229508 0.7049180 0.7398202 0.7372466 0.7704918 0.8360656 0 ## svmRadial 0.6935484 0.7387626 0.7805394 0.7712938 0.8000397 0.8387097 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rf 0.2654028 0.4262678 0.4828423 0.4872370 0.5579818 0.6796785 0 ## adaboost 0.2203593 0.3688623 0.4526472 0.4369009 0.4864705 0.6403712 0 ## earth 0.2368113 0.4089641 0.4935398 0.4820558 0.5378447 0.7122970 0 ## knn 0.1553281 0.3014894 0.4019217 0.3890542 0.4644021 0.6007853 0 ## svmRadial 0.3237658 0.3923780 0.5012975 0.4765574 0.5300695 0.6403712 0 comparison by visualization # Box plots to compare models scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) bwplot(results, scales=scales) ensemble predictions on testdata # Create the trainControl set.seed(101) stackControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE) # Ensemble the predictions of `models` to form a new combined prediction based on glm stack.glm &lt;- caretStack(models, method=&quot;glm&quot;, metric=&quot;Accuracy&quot;, trControl=stackControl) print(stack.glm) ## A glm ensemble of 5 base models: rf, adaboost, earth, knn, svmRadial ## ## Ensemble results: ## Generalized Linear Model ## ## 1845 samples ## 5 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 1660, 1660, 1661, 1661, 1661, 1660, ... ## Resampling results: ## ## Accuracy Kappa ## 0.7799569 0.4938476 compute confusion matrix # Predict on testData stack_predicteds &lt;- predict(stack.glm, newdata=testData4) confusionMatrix(reference = as.factor(testData$diabetes), data = stack_predicteds ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 84 22 ## pos 16 31 ## ## Accuracy : 0.7516 ## 95% CI : (0.6754, 0.8179) ## No Information Rate : 0.6536 ## P-Value [Acc &gt; NIR] : 0.005891 ## ## Kappa : 0.4365 ## ## Mcnemar&#39;s Test P-Value : 0.417304 ## ## Sensitivity : 0.8400 ## Specificity : 0.5849 ## Pos Pred Value : 0.7925 ## Neg Pred Value : 0.6596 ## Prevalence : 0.6536 ## Detection Rate : 0.5490 ## Detection Prevalence : 0.6928 ## Balanced Accuracy : 0.7125 ## ## &#39;Positive&#39; Class : neg ## 2.2 KNN Classifier # Loading package # library(e1071) library(caTools) library(class) 2.2.1 Splitting data # load the Pima Indians dataset from the mlbench dataset library(mlbench) data(PimaIndiansDiabetes) # rename dataset to have shorter name because lazy diabetes &lt;- PimaIndiansDiabetes # Splitting data into train and test data set.seed(100) split &lt;- sample.split(diabetes, SplitRatio = 0.8) train_cl &lt;- subset(diabetes, split == &quot;TRUE&quot;) test_cl &lt;- subset(diabetes, split == &quot;FALSE&quot;) # Feature Scaling train_scale &lt;- scale(train_cl[, 1:8]) test_scale &lt;- scale(test_cl[, 1:8]) # train_y &lt;- scale(train_cl[, 5]) # test_y &lt;- scale(test_cl[, 5]) 2.2.2 Creating KNN model # Fitting KNN Model to training dataset classifier_knn &lt;- knn(train = train_scale, cl = train_cl$diabetes, test = test_scale, k = 1) classifier_knn ## [1] pos neg neg neg pos neg neg neg neg neg neg neg pos neg neg pos neg pos ## [19] neg neg neg neg pos neg neg pos neg pos pos neg neg neg neg pos neg pos ## [37] neg neg neg pos neg pos pos neg neg neg pos pos neg pos neg neg neg neg ## [55] pos neg pos neg pos neg neg neg pos pos pos pos neg pos neg pos pos neg ## [73] pos neg neg pos neg neg neg pos pos neg neg pos neg pos pos neg neg neg ## [91] neg neg neg pos pos neg neg neg pos neg neg pos neg neg pos neg pos neg ## [109] neg neg neg neg pos pos pos pos pos pos neg pos pos pos neg neg neg neg ## [127] neg neg neg neg neg neg pos neg pos neg pos pos neg pos neg pos neg pos ## [145] neg neg neg pos neg neg neg pos pos pos neg pos neg pos neg neg neg neg ## [163] neg neg pos pos neg pos neg neg neg ## Levels: neg pos 2.2.3 Model Evaluation Creat confusion matrix # Confusion Matrix cm &lt;- table(test_cl$diabetes, classifier_knn) cm ## classifier_knn ## neg pos ## neg 79 32 ## pos 27 33 2.2.4 Calculate accuracy with different K # Model Evaluation - Choosing K =1 # Calculate out of Sample error misClassError &lt;- mean(classifier_knn != test_cl$diabetes) print(paste(&#39;Accuracy =&#39;, 1-misClassError)) ## [1] &quot;Accuracy = 0.654970760233918&quot; # K = 7 classifier_knn &lt;- knn(train = train_scale, test = test_scale, cl = train_cl$diabetes, k = 23) misClassError &lt;- mean(classifier_knn != test_cl$diabetes) print(paste(&#39;Accuracy =&#39;, 1-misClassError)) ## [1] &quot;Accuracy = 0.795321637426901&quot; 2.2.5 Optimization search better k parameter i=1 k.optm=1 for (i in 1:39){ y_pred = knn(train = train_scale, test = test_scale, cl = train_cl$diabetes, k = i ) k.optm[i] &lt;- 1- mean(y_pred != test_cl$diabetes) k=i cat(k,&#39;=&#39;,k.optm[i],&#39;&#39;) } ## 1 = 0.6549708 2 = 0.6666667 3 = 0.7426901 4 = 0.6900585 5 = 0.7309942 6 = 0.748538 7 = 0.7368421 8 = 0.7309942 9 = 0.7368421 10 = 0.7251462 11 = 0.7602339 12 = 0.748538 13 = 0.7719298 14 = 0.748538 15 = 0.754386 16 = 0.754386 17 = 0.7602339 18 = 0.7192982 19 = 0.7719298 20 = 0.754386 21 = 0.7836257 22 = 0.7777778 23 = 0.7953216 24 = 0.7719298 25 = 0.7777778 26 = 0.7836257 27 = 0.7719298 28 = 0.7660819 29 = 0.7777778 30 = 0.7660819 31 = 0.7660819 32 = 0.7602339 33 = 0.7719298 34 = 0.754386 35 = 0.7602339 36 = 0.7719298 37 = 0.7777778 38 = 0.7836257 39 = 0.7836257 Accuracy plot k=15 plot(k.optm, type=&quot;b&quot;, xlab=&quot;K- Value&quot;,ylab=&quot;RMSE level&quot;) 2.2.6 Visualization # Visualising the Training set results # Install ElemStatLearn if not present 2.3 KNN regression 2.3.1 Data exploring library(&quot;Amelia&quot;) data(&quot;Boston&quot;, package = &quot;MASS&quot;) missmap(Boston,col=c(&#39;yellow&#39;,&#39;black&#39;),y.at=1,y.labels=&#39;&#39;,legend=TRUE) library(corrplot) corrplot(cor((Boston))) library(Hmisc) describe(Boston) ## Boston ## ## 14 Variables 506 Observations ## -------------------------------------------------------------------------------- ## crim ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 504 1 3.614 5.794 0.02791 0.03819 ## .25 .50 .75 .90 .95 ## 0.08204 0.25651 3.67708 10.75300 15.78915 ## ## lowest : 0.00632 0.00906 0.01096 0.01301 0.01311 ## highest: 45.74610 51.13580 67.92080 73.53410 88.97620 ## -------------------------------------------------------------------------------- ## zn ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 26 0.603 11.36 18.77 0.0 0.0 ## .25 .50 .75 .90 .95 ## 0.0 0.0 12.5 42.5 80.0 ## ## lowest : 0.0 12.5 17.5 18.0 20.0, highest: 82.5 85.0 90.0 95.0 100.0 ## -------------------------------------------------------------------------------- ## indus ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 76 0.982 11.14 7.705 2.18 2.91 ## .25 .50 .75 .90 .95 ## 5.19 9.69 18.10 19.58 21.89 ## ## lowest : 0.46 0.74 1.21 1.22 1.25, highest: 18.10 19.58 21.89 25.65 27.74 ## -------------------------------------------------------------------------------- ## chas ## n missing distinct Info Sum Mean Gmd ## 506 0 2 0.193 35 0.06917 0.129 ## ## -------------------------------------------------------------------------------- ## nox ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 81 1 0.5547 0.1295 0.4092 0.4270 ## .25 .50 .75 .90 .95 ## 0.4490 0.5380 0.6240 0.7130 0.7400 ## ## lowest : 0.385 0.389 0.392 0.394 0.398, highest: 0.713 0.718 0.740 0.770 0.871 ## -------------------------------------------------------------------------------- ## rm ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 446 1 6.285 0.7515 5.314 5.594 ## .25 .50 .75 .90 .95 ## 5.886 6.208 6.623 7.152 7.588 ## ## lowest : 3.561 3.863 4.138 4.368 4.519, highest: 8.375 8.398 8.704 8.725 8.780 ## -------------------------------------------------------------------------------- ## age ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 356 0.999 68.57 31.52 17.72 26.95 ## .25 .50 .75 .90 .95 ## 45.02 77.50 94.07 98.80 100.00 ## ## lowest : 2.9 6.0 6.2 6.5 6.6, highest: 98.8 98.9 99.1 99.3 100.0 ## -------------------------------------------------------------------------------- ## dis ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 412 1 3.795 2.298 1.462 1.628 ## .25 .50 .75 .90 .95 ## 2.100 3.207 5.188 6.817 7.828 ## ## lowest : 1.1296 1.1370 1.1691 1.1742 1.1781 ## highest: 9.2203 9.2229 10.5857 10.7103 12.1265 ## -------------------------------------------------------------------------------- ## rad ## n missing distinct Info Mean Gmd ## 506 0 9 0.959 9.549 8.518 ## ## lowest : 1 2 3 4 5, highest: 5 6 7 8 24 ## ## Value 1 2 3 4 5 6 7 8 24 ## Frequency 20 24 38 110 115 26 17 24 132 ## Proportion 0.040 0.047 0.075 0.217 0.227 0.051 0.034 0.047 0.261 ## -------------------------------------------------------------------------------- ## tax ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 66 0.981 408.2 181.7 222 233 ## .25 .50 .75 .90 .95 ## 279 330 666 666 666 ## ## lowest : 187 188 193 198 216, highest: 432 437 469 666 711 ## -------------------------------------------------------------------------------- ## ptratio ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 46 0.978 18.46 2.383 14.70 14.75 ## .25 .50 .75 .90 .95 ## 17.40 19.05 20.20 20.90 21.00 ## ## lowest : 12.6 13.0 13.6 14.4 14.7, highest: 20.9 21.0 21.1 21.2 22.0 ## -------------------------------------------------------------------------------- ## black ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 357 0.986 356.7 65.5 84.59 290.27 ## .25 .50 .75 .90 .95 ## 375.38 391.44 396.23 396.90 396.90 ## ## lowest : 0.32 2.52 2.60 3.50 3.65, highest: 396.28 396.30 396.33 396.42 396.90 ## -------------------------------------------------------------------------------- ## lstat ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 455 1 12.65 7.881 3.708 4.680 ## .25 .50 .75 .90 .95 ## 6.950 11.360 16.955 23.035 26.808 ## ## lowest : 1.73 1.92 1.98 2.47 2.87, highest: 34.37 34.41 34.77 36.98 37.97 ## -------------------------------------------------------------------------------- ## medv ## n missing distinct Info Mean Gmd .05 .10 ## 506 0 229 1 22.53 9.778 10.20 12.75 ## .25 .50 .75 .90 .95 ## 17.02 21.20 25.00 34.80 43.40 ## ## lowest : 5.0 5.6 6.3 7.0 7.2, highest: 46.7 48.3 48.5 48.8 50.0 ## -------------------------------------------------------------------------------- 2.3.2 Prepareing data Boston &lt;- dplyr::select (Boston ,medv , crim , rm , tax , lstat) # Splitting the dataset into # the Training set and Test set # install.packages(&#39;caTools&#39;) library(caTools) set.seed(123) split = sample.split(Boston$medv, SplitRatio = 0.75) training_set_origi = subset(Boston, split == TRUE) test_set_origi = subset(Boston, split == FALSE) # Feature Scaling training_set = scale(training_set_origi[,-1] ) test_set = scale(test_set_origi [,-1]) 2.3.3 Creating model # Fitting K-NN to the Training set # and Predicting the Test set results # library(class) y_pred = knn(train = training_set[, -1], test = test_set[, -1], cl = training_set_origi[, 1], k = 15 ) # 2.3.4 Evaluation # converting factor into character then into numeric error &lt;- test_set_origi[,1]-as.numeric (as.character(y_pred)) head(error) ## [1] -3.5 -0.5 -1.4 -2.6 1.0 -8.8 rmse &lt;- sqrt(mean(error)^2) rmse ## [1] 0.8487179 plot(error) head(cbind(test_set_origi[,1], as.numeric (as.character(y_pred)))) ## [,1] [,2] ## [1,] 18.2 21.7 ## [2,] 19.9 20.4 ## [3,] 17.5 18.9 ## [4,] 15.2 17.8 ## [5,] 14.5 13.5 ## [6,] 15.6 24.4 2.3.5 Optimization search better k parameter i=1 k.optm=1 for (i in 1:29){ y_pred = knn(train = training_set[, -1], test = test_set[, -1], cl = training_set_origi[, 1], k = i ) k.optm[i] &lt;- sqrt(mean( test_set_origi[,1]-as.numeric (as.character(y_pred)) )^2) k=i cat(k,&#39;=&#39;,k.optm[i],&#39;&#39;) } ## 1 = 0.35 2 = 0.5371795 3 = 0.9705128 4 = 1.105128 5 = 1.373077 6 = 0.4512821 7 = 0.6230769 8 = 0.575641 9 = 1.325641 10 = 1.176923 11 = 0.6628205 12 = 0.15 13 = 0.04358974 14 = 0.724359 15 = 0.3551282 16 = 0.07820513 17 = 0.07820513 18 = 0.6346154 19 = 0.2628205 20 = 0.4769231 21 = 0.9294872 22 = 0.6423077 23 = 0.4333333 24 = 0.4320513 25 = 0.3807692 26 = 1.061538 27 = 0.924359 28 = 0.7230769 29 = 0.03461538 Accuracy plot k=15 plot(k.optm, type=&quot;b&quot;, xlab=&quot;K- Value&quot;,ylab=&quot;RMSE level&quot;) "],["deep-learning.html", "3 Deep learning 3.1 Deep neural network 3.2 Deep neural networks for regression 3.3 Convolutional neural netwrok", " 3 Deep learning 3.1 Deep neural network 3.1.1 Load data # load the Pima Indians dataset from the mlbench dataset library(mlbench) data(PimaIndiansDiabetes) # rename dataset to have shorter name because lazy diabetes &lt;- PimaIndiansDiabetes data.set &lt;- diabetes # datatable(data.set[sample(nrow(data.set), # replace = FALSE, # size = 0.005 * nrow(data.set)), ]) summary(data.set) ## pregnant glucose pressure triceps ## Min. : 0.000 Min. : 0.0 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 1.000 1st Qu.: 99.0 1st Qu.: 62.00 1st Qu.: 0.00 ## Median : 3.000 Median :117.0 Median : 72.00 Median :23.00 ## Mean : 3.845 Mean :120.9 Mean : 69.11 Mean :20.54 ## 3rd Qu.: 6.000 3rd Qu.:140.2 3rd Qu.: 80.00 3rd Qu.:32.00 ## Max. :17.000 Max. :199.0 Max. :122.00 Max. :99.00 ## insulin mass pedigree age diabetes ## Min. : 0.0 Min. : 0.00 Min. :0.0780 Min. :21.00 neg:500 ## 1st Qu.: 0.0 1st Qu.:27.30 1st Qu.:0.2437 1st Qu.:24.00 pos:268 ## Median : 30.5 Median :32.00 Median :0.3725 Median :29.00 ## Mean : 79.8 Mean :31.99 Mean :0.4719 Mean :33.24 ## 3rd Qu.:127.2 3rd Qu.:36.60 3rd Qu.:0.6262 3rd Qu.:41.00 ## Max. :846.0 Max. :67.10 Max. :2.4200 Max. :81.00 3.1.2 Process data and variable data.set$diabetes &lt;- as.numeric(data.set$diabetes) data.set$diabetes=data.set$diabetes-1 head(data.set$diabetes) ## [1] 1 0 1 0 1 0 head(data.set) ## pregnant glucose pressure triceps insulin mass pedigree age diabetes ## 1 6 148 72 35 0 33.6 0.627 50 1 ## 2 1 85 66 29 0 26.6 0.351 31 0 ## 3 8 183 64 0 0 23.3 0.672 32 1 ## 4 1 89 66 23 94 28.1 0.167 21 0 ## 5 0 137 40 35 168 43.1 2.288 33 1 ## 6 5 116 74 0 0 25.6 0.201 30 0 str(data.set) ## &#39;data.frame&#39;: 768 obs. of 9 variables: ## $ pregnant: num 6 1 8 1 0 5 3 10 2 8 ... ## $ glucose : num 148 85 183 89 137 116 78 115 197 125 ... ## $ pressure: num 72 66 64 66 40 74 50 0 70 96 ... ## $ triceps : num 35 29 0 23 35 0 32 0 45 0 ... ## $ insulin : num 0 0 0 94 168 0 88 0 543 0 ... ## $ mass : num 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ... ## $ pedigree: num 0.627 0.351 0.672 0.167 2.288 ... ## $ age : num 50 31 32 21 33 30 26 29 53 54 ... ## $ diabetes: num 1 0 1 0 1 0 1 0 1 1 ... transform dataframe into matrix # Cast dataframe as a matrix data.set &lt;- as.matrix(data.set) # Remove column names dimnames(data.set) = NULL head(data.set) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 6 148 72 35 0 33.6 0.627 50 1 ## [2,] 1 85 66 29 0 26.6 0.351 31 0 ## [3,] 8 183 64 0 0 23.3 0.672 32 1 ## [4,] 1 89 66 23 94 28.1 0.167 21 0 ## [5,] 0 137 40 35 168 43.1 2.288 33 1 ## [6,] 5 116 74 0 0 25.6 0.201 30 0 3.1.3 Split data into training and test datasets including xtrain ytrian xtest ytest # Split for train and test data set.seed(100) indx &lt;- sample(2, nrow(data.set), replace = TRUE, prob = c(0.8, 0.2)) # Makes index with values 1 and 2 # Select only the feature variables # Take rows with index = 1 x_train &lt;- data.set[indx == 1, 1:8] x_test &lt;- data.set[indx == 2, 1:8] # Feature Scaling x_train &lt;- scale(x_train ) x_test &lt;- scale(x_test ) y_test_actual &lt;- data.set[indx == 2, 9] transform target as on-hot-coding format # Using similar indices to correspond to the training and test set y_train &lt;- to_categorical(data.set[indx == 1, 9]) ## Loaded Tensorflow version 2.8.0 y_test &lt;- to_categorical(data.set[indx == 2, 9]) head(y_train) ## [,1] [,2] ## [1,] 0 1 ## [2,] 1 0 ## [3,] 0 1 ## [4,] 1 0 ## [5,] 0 1 ## [6,] 1 0 head(data.set[indx == 1, 9],20) ## [1] 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 dimension of four splitting data sets dim(x_train) ## [1] 609 8 dim(y_train) ## [1] 609 2 dim(x_test) ## [1] 159 8 dim(y_test) ## [1] 159 2 3.1.4 Creating neural network model 3.1.4.1 construction of model the output layer contains 3 levels # Creating the model model &lt;- keras_model_sequential() model %&gt;% layer_dense(name = &quot;DeepLayer1&quot;, units = 10, activation = &quot;relu&quot;, input_shape = c(8)) %&gt;% # input 4 features layer_dense(name = &quot;DeepLayer2&quot;, units = 10, activation = &quot;relu&quot;) %&gt;% layer_dense(name = &quot;OutputLayer&quot;, units = 2, activation = &quot;softmax&quot;) # output 4 categories using one-hot-coding summary(model) ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## DeepLayer1 (Dense) (None, 10) 90 ## DeepLayer2 (Dense) (None, 10) 110 ## OutputLayer (Dense) (None, 2) 22 ## ================================================================================ ## Total params: 222 ## Trainable params: 222 ## Non-trainable params: 0 ## ________________________________________________________________________________ 3.1.4.2 Compiling the model # Compiling the model model %&gt;% compile(loss = &quot;categorical_crossentropy&quot;, optimizer = &quot;adam&quot;, metrics = c(&quot;accuracy&quot;)) 3.1.4.3 Fitting the data and plot history &lt;- model %&gt;% fit(x_train, y_train, # adjusting number of epoch epoch = 60, # adjusting number of batch size batch_size = 64, validation_split = 0.15, verbose = 2) plot(history) 3.1.5 Evaluation 3.1.5.1 Output loss and accuracy using xtest and ytest data sets to evaluate the built model directly model %&gt;% evaluate(x_test, y_test) ## loss accuracy ## 0.4688513 0.7924528 3.1.5.2 Output the predicted classes and confusion matrix pred &lt;- model %&gt;% predict(x_test) %&gt;% k_argmax() %&gt;% k_get_value() head(pred) ## [1] 0 1 0 0 0 0 table(Predicted = pred, Actual = y_test_actual) ## Actual ## Predicted 0 1 ## 0 91 20 ## 1 13 35 3.1.5.3 Output the predicted values prob &lt;- model %&gt;% predict(x_test) %&gt;% k_get_value() head(prob) ## [,1] [,2] ## [1,] 0.91165835 0.08834162 ## [2,] 0.09532551 0.90467453 ## [3,] 0.92566639 0.07433363 ## [4,] 0.54701668 0.45298335 ## [5,] 0.96424216 0.03575778 ## [6,] 0.75708139 0.24291863 3.1.5.4 Comparison between prob, pred, and ytest comparison &lt;- cbind(prob , pred , y_test_actual ) head(comparison) ## pred y_test_actual ## [1,] 0.91165835 0.08834162 0 1 ## [2,] 0.09532551 0.90467453 1 1 ## [3,] 0.92566639 0.07433363 0 0 ## [4,] 0.54701668 0.45298335 0 1 ## [5,] 0.96424216 0.03575778 0 0 ## [6,] 0.75708139 0.24291863 0 0 3.2 Deep neural networks for regression 3.2.1 Loading packages and data sets library(readr) library(keras) library(plotly) data(&quot;Boston&quot;, package = &quot;MASS&quot;) data.set &lt;- Boston dim(data.set) ## [1] 506 14 3.2.2 Convert dataframe to matrix without dimnames library(DT) # Cast dataframe as a matrix data.set &lt;- as.matrix(data.set) # Remove column names dimnames(data.set) = NULL head(data.set) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## [2,] 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## [3,] 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## [4,] 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## [5,] 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## [6,] 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## [,13] [,14] ## [1,] 4.98 24.0 ## [2,] 9.14 21.6 ## [3,] 4.03 34.7 ## [4,] 2.94 33.4 ## [5,] 5.33 36.2 ## [6,] 5.21 28.7 summary(data.set[, 14]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.00 17.02 21.20 22.53 25.00 50.00 hist( data.set[, 14]) (#fig:target variable histogram)Fig 1 Histogram of the target variable 3.2.3 Spiting training and test data # Split for train and test data set.seed(123) indx &lt;- sample(2, nrow(data.set), replace = TRUE, prob = c(0.75, 0.25)) # Makes index with values 1 and 2 x_train &lt;- data.set[indx == 1, 1:13] x_test &lt;- data.set[indx == 2, 1:13] y_train &lt;- data.set[indx == 1, 14] y_test &lt;- data.set[indx == 2, 14] 3.2.4 Normalizing xtrain and xtest data x_train &lt;- scale(x_train) x_test &lt;- scale(x_test) 3.2.5 Creating the model model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 25, activation = &quot;relu&quot;, input_shape = c(13)) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(units = 25, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(units = 25, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(units = 1) model %&gt;% summary() ## Model: &quot;sequential_1&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_3 (Dense) (None, 25) 350 ## dropout_2 (Dropout) (None, 25) 0 ## dense_2 (Dense) (None, 25) 650 ## dropout_1 (Dropout) (None, 25) 0 ## dense_1 (Dense) (None, 25) 650 ## dropout (Dropout) (None, 25) 0 ## dense (Dense) (None, 1) 26 ## ================================================================================ ## Total params: 1,676 ## Trainable params: 1,676 ## Non-trainable params: 0 ## ________________________________________________________________________________ model %&gt;% get_config() ## {&#39;name&#39;: &#39;sequential_1&#39;, &#39;layers&#39;: [{&#39;class_name&#39;: &#39;InputLayer&#39;, &#39;config&#39;: {&#39;batch_input_shape&#39;: (None, 13), &#39;dtype&#39;: &#39;float32&#39;, &#39;sparse&#39;: False, &#39;ragged&#39;: False, &#39;name&#39;: &#39;dense_3_input&#39;}}, {&#39;class_name&#39;: &#39;Dense&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dense_3&#39;, &#39;trainable&#39;: True, &#39;batch_input_shape&#39;: (None, 13), &#39;dtype&#39;: &#39;float32&#39;, &#39;units&#39;: 25, &#39;activation&#39;: &#39;relu&#39;, &#39;use_bias&#39;: True, &#39;kernel_initializer&#39;: {&#39;class_name&#39;: &#39;GlorotUniform&#39;, &#39;config&#39;: {&#39;seed&#39;: None}}, &#39;bias_initializer&#39;: {&#39;class_name&#39;: &#39;Zeros&#39;, &#39;config&#39;: {}}, &#39;kernel_regularizer&#39;: None, &#39;bias_regularizer&#39;: None, &#39;activity_regularizer&#39;: None, &#39;kernel_constraint&#39;: None, &#39;bias_constraint&#39;: None}}, {&#39;class_name&#39;: &#39;Dropout&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dropout_2&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;rate&#39;: 0.2, &#39;noise_shape&#39;: None, &#39;seed&#39;: None}}, {&#39;class_name&#39;: &#39;Dense&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dense_2&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;units&#39;: 25, &#39;activation&#39;: &#39;relu&#39;, &#39;use_bias&#39;: True, &#39;kernel_initializer&#39;: {&#39;class_name&#39;: &#39;GlorotUniform&#39;, &#39;config&#39;: {&#39;seed&#39;: None}}, &#39;bias_initializer&#39;: {&#39;class_name&#39;: &#39;Zeros&#39;, &#39;config&#39;: {}}, &#39;kernel_regularizer&#39;: None, &#39;bias_regularizer&#39;: None, &#39;activity_regularizer&#39;: None, &#39;kernel_constraint&#39;: None, &#39;bias_constraint&#39;: None}}, {&#39;class_name&#39;: &#39;Dropout&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dropout_1&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;rate&#39;: 0.2, &#39;noise_shape&#39;: None, &#39;seed&#39;: None}}, {&#39;class_name&#39;: &#39;Dense&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dense_1&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;units&#39;: 25, &#39;activation&#39;: &#39;relu&#39;, &#39;use_bias&#39;: True, &#39;kernel_initializer&#39;: {&#39;class_name&#39;: &#39;GlorotUniform&#39;, &#39;config&#39;: {&#39;seed&#39;: None}}, &#39;bias_initializer&#39;: {&#39;class_name&#39;: &#39;Zeros&#39;, &#39;config&#39;: {}}, &#39;kernel_regularizer&#39;: None, &#39;bias_regularizer&#39;: None, &#39;activity_regularizer&#39;: None, &#39;kernel_constraint&#39;: None, &#39;bias_constraint&#39;: None}}, {&#39;class_name&#39;: &#39;Dropout&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dropout&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;rate&#39;: 0.2, &#39;noise_shape&#39;: None, &#39;seed&#39;: None}}, {&#39;class_name&#39;: &#39;Dense&#39;, &#39;config&#39;: {&#39;name&#39;: &#39;dense&#39;, &#39;trainable&#39;: True, &#39;dtype&#39;: &#39;float32&#39;, &#39;units&#39;: 1, &#39;activation&#39;: &#39;linear&#39;, &#39;use_bias&#39;: True, &#39;kernel_initializer&#39;: {&#39;class_name&#39;: &#39;GlorotUniform&#39;, &#39;config&#39;: {&#39;seed&#39;: None}}, &#39;bias_initializer&#39;: {&#39;class_name&#39;: &#39;Zeros&#39;, &#39;config&#39;: {}}, &#39;kernel_regularizer&#39;: None, &#39;bias_regularizer&#39;: None, &#39;activity_regularizer&#39;: None, &#39;kernel_constraint&#39;: None, &#39;bias_constraint&#39;: None}}]} 3.2.6 Compiling the model model %&gt;% compile(loss = &quot;mse&quot;, optimizer = optimizer_rmsprop(), metrics = c(&quot;mean_absolute_error&quot;)) 3.2.7 Fitting the model history &lt;- model %&gt;% fit(x_train, y_train, epoch = 100, batch_size = 64, validation_split = 0.1, callbacks = c(callback_early_stopping(monitor = &quot;val_mean_absolute_error&quot;, patience = 5)), verbose = 2) c(loss, mae) %&lt;-% (model %&gt;% evaluate(x_test, y_test, verbose = 0)) paste0(&quot;Mean absolute error on test set: &quot;, sprintf(&quot;%.2f&quot;, mae)) ## [1] &quot;Mean absolute error on test set: 4.40&quot; 3.2.8 Plot the training process plot(history) ### Calculating the predicted values on test data pred2 &lt;- model %&gt;% predict(x_test) %&gt;% k_get_value() head(cbind(pred2,y_test)) ## y_test ## [1,] 23.93786 21.6 ## [2,] 31.00385 33.4 ## [3,] 29.89205 36.2 ## [4,] 13.15364 27.1 ## [5,] 13.52814 15.0 ## [6,] 17.96552 19.9 calculating mean absolute error and root mean square error and ploting error &lt;- y_test-pred2 head(error) ## [,1] ## [1,] -2.337864 ## [2,] 2.396149 ## [3,] 6.307948 ## [4,] 13.946360 ## [5,] 1.471858 ## [6,] 1.934481 rmse &lt;- sqrt(mean(error)^2) rmse ## [1] 0.9275442 plot(error) 3.3 Convolutional neural netwrok 3.3.1 Import library library(keras) 3.3.2 Importing the data mnist &lt;- dataset_mnist() mnist is list; it contains trainx, trainy, testx, testy class(mnist) ## [1] &quot;list&quot; the dim of “mnist\\(train\\)x” is 60000 28 28 # head(mnist) 3.3.3 preparing the data randomly sampling 1000 cases for training and 100 for testing set.seed(123) index &lt;- sample(nrow(mnist$train$x), 1000) x_train &lt;- mnist$train$x[index,,] y_train &lt;- (mnist$train$y[index]) index &lt;- sample(nrow(mnist$test$x), 100) x_test &lt;- mnist$test$x[index,,] y_test &lt;- (mnist$test$y[index]) dim of four data sets dim(x_train) ## [1] 1000 28 28 dim(y_train) ## [1] 1000 dim(x_test) ## [1] 100 28 28 dim(y_test) ## [1] 100 3.3.3.1 Generate tensors each image is 28*28 pixel size; pass these values to computer img_rows &lt;- 28 img_cols &lt;- 28 using array_reshape() function to transform list data into tensors x_train &lt;- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1)) x_test &lt;- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1)) input_shape &lt;- c(img_rows, img_cols, 1) this below is tensor data dim(x_train) ## [1] 1000 28 28 1 3.3.3.2 Normalization and one-hot-encoded (dummy) training (features) data is rescaled by dividing the maxmimum to be normalized x_train &lt;- x_train / 255 x_test &lt;- x_test / 255 converse targets into one-hot-encoded (dummy) type using to_categorical() function num_classes = 10 y_train &lt;- to_categorical(y_train, num_classes) y_test &lt;- to_categorical(y_test, num_classes) y_train[1,] ## [1] 0 0 0 0 0 0 1 0 0 0 3.3.4 Creating the model model &lt;- keras_model_sequential() %&gt;% layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = &#39;relu&#39;, input_shape = input_shape) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = &#39;relu&#39;) %&gt;% layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% layer_dropout(rate = 0.25) %&gt;% layer_flatten() %&gt;% layer_dense(units = 128, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_dense(units = num_classes, activation = &#39;softmax&#39;) summary of model model %&gt;% summary() ## Model: &quot;sequential_2&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## conv2d_1 (Conv2D) (None, 26, 26, 32) 320 ## conv2d (Conv2D) (None, 24, 24, 64) 18496 ## max_pooling2d (MaxPooling2D) (None, 12, 12, 64) 0 ## dropout_4 (Dropout) (None, 12, 12, 64) 0 ## flatten (Flatten) (None, 9216) 0 ## dense_5 (Dense) (None, 128) 1179776 ## dropout_3 (Dropout) (None, 128) 0 ## dense_4 (Dense) (None, 10) 1290 ## ================================================================================ ## Total params: 1,199,882 ## Trainable params: 1,199,882 ## Non-trainable params: 0 ## ________________________________________________________________________________ 3.3.4.1 compiling loss function is categorical crossentropy; the gradient descent will be optimized by adadelta; model %&gt;% compile( loss = loss_categorical_crossentropy, optimizer = optimizer_adadelta(), metrics = c(&#39;accuracy&#39;) ) 3.3.5 Training batch_size &lt;- 128 epochs &lt;- 10 # Train model history &lt;- model %&gt;% fit( x_train, y_train, batch_size = batch_size, epochs = epochs, validation_split = 0.2 ) plot(history) 3.3.6 Evaluating the accuracy score &lt;- model %&gt;% evaluate(x_test, y_test) score ## loss accuracy ## 0.2502894 0.9200000 "],["data-visualization.html", "4 Data visualization 4.1 Data visualization introduction 4.2 Scatter plot 4.3 Bar chart 4.4 Line charts 4.5 ggplot2 parameters", " 4 Data visualization 4.1 Data visualization introduction 4.1.1 Summarization library(tidyverse) library(dplyr) mtcars %&gt;% mutate( kml = mpg * 0.42) %&gt;% group_by(cyl) %&gt;% summarise(avg_US = mean(mpg), avg_metric = mean(kml)) ## # A tibble: 3 × 3 ## cyl avg_US avg_metric ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 26.7 11.2 ## 2 6 19.7 8.29 ## 3 8 15.1 6.34 mpg %&gt;% group_by(manufacturer, year) %&gt;% summarise_at(vars(cty, hwy), mean) ## # A tibble: 30 × 4 ## # Groups: manufacturer [15] ## manufacturer year cty hwy ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi 1999 17.1 26.1 ## 2 audi 2008 18.1 26.8 ## 3 chevrolet 1999 15.1 21.6 ## 4 chevrolet 2008 14.9 22.1 ## 5 dodge 1999 13.4 18.4 ## 6 dodge 2008 13.0 17.6 ## 7 ford 1999 13.9 18.6 ## 8 ford 2008 14.1 20.5 ## 9 honda 1999 24.8 31.6 ## 10 honda 2008 24 33.8 ## # … with 20 more rows change layout mpg %&gt;% count(class, year)%&gt;% spread(class, n) ## # A tibble: 2 × 8 ## year `2seater` compact midsize minivan pickup subcompact suv ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1999 2 25 20 6 16 19 29 ## 2 2008 3 22 21 5 17 16 33 change all characters into factors mpg &lt;- mpg %&gt;% mutate_if(is.character, as.factor) #if a column is a character, change to a factor wide to long data mpg1 &lt;- mpg %&gt;% gather(&quot;key&quot;, &quot;value&quot;, cty, hwy) convert wide data to long data using pivot_longer ## Your code here. Naming choices for 1 and 2 are yours dta &lt;- mpg %&gt;% pivot_longer(cty:hwy, names_to = &quot;var&quot;, values_to = &quot;value&quot;) %&gt;% # Both of those are # value label mutate(var = ifelse( var == &#39;cty&#39;, &#39;city&#39;,&#39;highway&#39;)) ggplot(dta, aes(x = displ, y = value)) + geom_point(aes(color = var)) + geom_smooth(aes(color = var), se = F) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; explore distribution library(DataExplorer) library(psych) ## ## Attaching package: &#39;psych&#39; ## The following object is masked from &#39;package:Hmisc&#39;: ## ## describe ## The following object is masked from &#39;package:plotrix&#39;: ## ## rescale ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha library(naniar) ## ## Attaching package: &#39;naniar&#39; ## The following object is masked from &#39;package:skimr&#39;: ## ## n_complete plot_histogram(riskfactors) explore relationship/correlation library(psych) pairs.panels(riskfactors[,1:10]) create a individual theme my_theme &lt;- function(){ theme_bw() + theme(axis.title = element_text(size=16), axis.text = element_text(size=14), text = element_text(size = 14)) } 4.1.2 Explore missing values # install.packages(&quot;naniar&quot;) library(naniar) # head(riskfactors) riskfactors &lt;- riskfactors gg_miss_upset(riskfactors,nsets=10) # install.packages(&quot;DataExplorer&quot;) plot_missing(riskfactors) # take a quick look at the data types of each column visdat::vis_dat(riskfactors) 4.1.3 Add statistical test library(ggpubr) plt &lt;- ggplot( data=mpg, mapping= aes(x = as.factor(year), y = cty, color = as.factor(year) ) )+ geom_boxplot() + geom_jitter(width=0.1)+ labs(x = &#39;Year&#39;, y = &quot;City mpg&quot;) + my_theme()+ facet_wrap( ~ manufacturer,nrow = 2) # add statistical test my_comparisons &lt;- list(c(&#39;1999&#39;,&#39;2008&#39;)) plt + stat_compare_means() + stat_compare_means(comparisons = my_comparisons) 4.1.4 Add texts to dots USArrests &lt;- USArrests %&gt;% rownames_to_column(&#39;State&#39;) ggplot(USArrests, aes( x=UrbanPop,y=Murder))+ geom_point() + labs(x = &quot;Percent of population that is urban&quot;, y = &quot;Murder arrests (per 100,000)&quot;, caption = &quot;McNeil (1997). Interactive Data Analysis&quot;)+ geom_text(aes(label=State),size=3) 4.1.5 Set the legend ggplot(iris, aes(x= Sepal.Length , fill= as.factor( Species)) ) + #whole plot&#39;s option geom_histogram(aes(y=..density..),alpha=0.5, position=&quot;identity&quot; , bins = 50)+ geom_density(aes(linetype=as.factor(Species)),alpha=.1 )+ #aesthetic&#39;s option scale_fill_manual( name = &quot;Groups&quot;,values = c(&quot;grey&quot;, &quot;black&quot;, &quot;skyblue&quot;),labels = c(&quot;setosa&quot;, &quot;versicolor&quot; , &quot;virginica&quot; ))+ scale_linetype_manual( name = &quot;Groups&quot; ,values = c(1,3,5),labels = c(&quot;setosa&quot;, &quot;versicolor&quot; , &quot;virginica&quot;) )+ # common legend labs(x = &quot;Sepal.Length&quot;, y = &quot;Density&quot;, title = &quot;&quot;) 4.1.6 Create a panel of plots combine multiple plots into one p1=ggplot(data=riskfactors,aes(x=age))+ geom_histogram(bins = 30 ) p2=ggplot(data=riskfactors,aes(x=sex))+ geom_bar (aes(x=sex) ) p3=ggplot(riskfactors,aes(x = education, y = bmi))+ geom_boxplot ( ) p4=ggplot(riskfactors, aes(x = marital )) + geom_bar(aes(group = education, y = (..count..)/sum(..count..),fill = education)) + scale_y_continuous(labels=scales::percent) # install.packages(&quot;ggpubr&quot;) library(ggpubr) ggarrange(p1, p2, p3, p4, ncol = 2, nrow=2) 4.1.7 Plots in regression create linear regression model data(&quot;Boston&quot;, package = &quot;MASS&quot;) linear_reg &lt;- glm(medv ~ ., data=Boston , family = gaussian()) summary(linear_reg) ## ## Call: ## glm(formula = medv ~ ., family = gaussian(), data = Boston) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -15.595 -2.730 -0.518 1.777 26.199 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.646e+01 5.103e+00 7.144 3.28e-12 *** ## crim -1.080e-01 3.286e-02 -3.287 0.001087 ** ## zn 4.642e-02 1.373e-02 3.382 0.000778 *** ## indus 2.056e-02 6.150e-02 0.334 0.738288 ## chas 2.687e+00 8.616e-01 3.118 0.001925 ** ## nox -1.777e+01 3.820e+00 -4.651 4.25e-06 *** ## rm 3.810e+00 4.179e-01 9.116 &lt; 2e-16 *** ## age 6.922e-04 1.321e-02 0.052 0.958229 ## dis -1.476e+00 1.995e-01 -7.398 6.01e-13 *** ## rad 3.060e-01 6.635e-02 4.613 5.07e-06 *** ## tax -1.233e-02 3.760e-03 -3.280 0.001112 ** ## ptratio -9.527e-01 1.308e-01 -7.283 1.31e-12 *** ## black 9.312e-03 2.686e-03 3.467 0.000573 *** ## lstat -5.248e-01 5.072e-02 -10.347 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 22.51785) ## ## Null deviance: 42716 on 505 degrees of freedom ## Residual deviance: 11079 on 492 degrees of freedom ## AIC: 3027.6 ## ## Number of Fisher Scoring iterations: 2 summary knitr::kable(broom::tidy(linear_reg)) term estimate std.error statistic p.value (Intercept) 36.4594884 5.1034588 7.1440742 0.0000000 crim -0.1080114 0.0328650 -3.2865169 0.0010868 zn 0.0464205 0.0137275 3.3815763 0.0007781 indus 0.0205586 0.0614957 0.3343100 0.7382881 chas 2.6867338 0.8615798 3.1183809 0.0019250 nox -17.7666112 3.8197437 -4.6512574 0.0000042 rm 3.8098652 0.4179253 9.1161402 0.0000000 age 0.0006922 0.0132098 0.0524024 0.9582293 dis -1.4755668 0.1994547 -7.3980036 0.0000000 rad 0.3060495 0.0663464 4.6128998 0.0000051 tax -0.0123346 0.0037605 -3.2800091 0.0011116 ptratio -0.9527472 0.1308268 -7.2825106 0.0000000 black 0.0093117 0.0026860 3.4667926 0.0005729 lstat -0.5247584 0.0507153 -10.3471458 0.0000000 create logistical regression # load the Pima Indians dataset from the mlbench dataset library(mlbench) data(PimaIndiansDiabetes) # rename dataset to have shorter name because lazy diabetes &lt;- PimaIndiansDiabetes logistic_reg &lt;- glm(diabetes ~ ., data=diabetes, family = binomial) summary(logistic_reg) ## ## Call: ## glm(formula = diabetes ~ ., family = binomial, data = diabetes) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.5566 -0.7274 -0.4159 0.7267 2.9297 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.4046964 0.7166359 -11.728 &lt; 2e-16 *** ## pregnant 0.1231823 0.0320776 3.840 0.000123 *** ## glucose 0.0351637 0.0037087 9.481 &lt; 2e-16 *** ## pressure -0.0132955 0.0052336 -2.540 0.011072 * ## triceps 0.0006190 0.0068994 0.090 0.928515 ## insulin -0.0011917 0.0009012 -1.322 0.186065 ## mass 0.0897010 0.0150876 5.945 2.76e-09 *** ## pedigree 0.9451797 0.2991475 3.160 0.001580 ** ## age 0.0148690 0.0093348 1.593 0.111192 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 993.48 on 767 degrees of freedom ## Residual deviance: 723.45 on 759 degrees of freedom ## AIC: 741.45 ## ## Number of Fisher Scoring iterations: 5 summary knitr::kable(broom::tidy(logistic_reg)) term estimate std.error statistic p.value (Intercept) -8.4046964 0.7166359 -11.7279870 0.0000000 pregnant 0.1231823 0.0320776 3.8401403 0.0001230 glucose 0.0351637 0.0037087 9.4813935 0.0000000 pressure -0.0132955 0.0052336 -2.5404160 0.0110721 triceps 0.0006190 0.0068994 0.0897131 0.9285152 insulin -0.0011917 0.0009012 -1.3223094 0.1860652 mass 0.0897010 0.0150876 5.9453340 0.0000000 pedigree 0.9451797 0.2991475 3.1595780 0.0015800 age 0.0148690 0.0093348 1.5928584 0.1111920 4.1.7.1 Create forest plots for coefficients or OR library(sjPlot) ## Registered S3 methods overwritten by &#39;effectsize&#39;: ## method from ## standardize.Surv datawizard ## standardize.bcplm datawizard ## standardize.clm2 datawizard ## standardize.default datawizard ## standardize.mediate datawizard ## standardize.wbgee datawizard ## standardize.wbm datawizard ## #refugeeswelcome plot_model(linear_reg, show.values = TRUE, value.offset = 0.5) plot_model(logistic_reg, show.values = TRUE, value.offset = .5, vline.color = &quot;black&quot;) another way library(finalfit) explanatory = c( &quot;crim&quot; , &quot;zn&quot; , &quot;indus&quot; , &quot;nox&quot; , &quot;rm&quot; , &quot;age&quot; , &quot;dis&quot; , &quot;rad&quot; , &quot;tax&quot; ,&quot;ptratio&quot; ,&quot;black&quot; , &quot;lstat&quot; ) dependent = &quot;medv&quot; Boston %&gt;% coefficient_plot(dependent, explanatory, table_text_size=3, title_text_size=12, plot_opts=list(xlab(&quot;Beta, 95% CI&quot;), theme(axis.title = element_text(size=12)))) library(finalfit) explanatory = c( &quot;pregnant&quot;, &quot;glucose&quot; , &quot;pressure&quot;, &quot;triceps&quot; ,&quot;insulin&quot; , &quot;mass&quot; , &quot;pedigree&quot;, &quot;age&quot; ) dependent = &quot;diabetes&quot; diabetes %&gt;% or_plot(dependent, explanatory, table_text_size=3, title_text_size=12, plot_opts=list(xlab(&quot;OR, 95% CI&quot;), theme(axis.title = element_text(size=12)))) ## Waiting for profiling to be done... ## Waiting for profiling to be done... ## Waiting for profiling to be done... qq plot ggqqplot( (Boston$medv)) Loading data set library(printr) ## Registered S3 method overwritten by &#39;printr&#39;: ## method from ## knit_print.data.frame rmarkdown library(tidyverse) head(iris) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.2 Scatter plot 4.2.1 Create a empty canvas then create aesthetic mapping tell the function which dataset and variables to use ggplot(data = iris, # which data set? canvas? aes(x=Sepal.Length , y=Petal.Length )) # which variables as aesthetics? x and y are mapped to columns of the data; different geoms can have different aesthetics (different variables). 4.2.2 Add a layer/geom of points to the canvas ggplot(data = iris, mapping = aes(x=Sepal.Length , y=Petal.Length )) + geom_point() # adding the geometrical representation # same plot as above ggplot(data = iris) + geom_point( aes(x=Sepal.Length , y=Petal.Length )) 4.2.3 Add another aesthetic add a curve/straight line to fit these points geom provides the aesthetic to ggplot # Loess curve ggplot(data = iris, mapping = aes(x=Sepal.Length , y=Petal.Length )) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; # Linear regression line ggplot(data = iris, mapping = aes(x=Sepal.Length , y=Petal.Length)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 4.2.4 Add other aesthetic set other aesthetics colour, alpha (transparency), and size of points ggplot(data = iris) + geom_point(aes(x=Sepal.Length , y=Petal.Length, size = Sepal.Width ), alpha = .5, colour = &quot;red&quot;) ggplot(data = iris) + geom_point(aes(x=Sepal.Length , y=Petal.Length, size = Sepal.Width , colour=Species), #white is a variable here alpha=.9) categorize Petal.Width then map colour to this new variable iris &lt;- iris %&gt;% mutate(growth = ifelse(Petal.Width &gt; 1.5, &quot;Wide&quot;, &quot;Normal&quot;)) ggplot(data=iris) + geom_point(aes(x=Sepal.Length , y=Petal.Length, size = Sepal.Width , colour=growth), alpha=.9) 4.3 Bar chart ggplot(data = iris) + geom_bar(aes(x = growth)) bar chart after group_by then use stat='identity' library(dplyr) results &lt;- iris %&gt;% group_by(Species, growth) %&gt;% summarise(Sepal.Length.mean=mean (Sepal.Length )) ## `summarise()` has grouped output by &#39;Species&#39;. You can override using the ## `.groups` argument. gop &lt;- results %&gt;% filter(Species != &quot;setosa_null&quot; ) gop ## # A tibble: 5 × 3 ## # Groups: Species [3] ## Species growth Sepal.Length.mean ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 setosa Normal 5.01 ## 2 versicolor Normal 5.91 ## 3 versicolor Wide 6.18 ## 4 virginica Normal 6.13 ## 5 virginica Wide 6.62 though meaningless below until line chart (just use the mean as the sum for demonstration) # We can also store parts of a plot in an object plot1 &lt;- ggplot(gop) + geom_bar(aes(x=growth , y=Sepal.Length.mean), stat=&#39;identity&#39;) plot1 ### Add some options for the whole ggplot rather than layers - switch the x and y axes plot1 + coord_flip() reorder x categories (-means descending) ggplot( gop) + geom_bar(aes(x=reorder(growth, -Sepal.Length.mean), y=Sepal.Length.mean, fill=growth), stat=&#39;identity&#39;) + coord_flip() add x axis label and a theme ggplot(gop) + geom_bar(aes(x=reorder(growth, -Sepal.Length.mean), y=Sepal.Length.mean, fill=growth), stat=&#39;identity&#39;) + coord_flip() + xlab(&quot;Growth categories&quot;) + guides(fill=F) + theme_minimal() set theme library(ggthemes) ggplot(data = iris) + geom_bar(aes(x = growth)) + theme_economist() 4.3.1 Grouped bar chart -bar chart with different panels ggplot(mpg, aes(x = class)) + geom_bar() + facet_wrap( ~ year) actual number (groups are stacked by default) ggplot(gop) + geom_bar(aes(x=growth, y=Sepal.Length.mean, fill=Species) , stat=&#39;identity&#39; ) ggplot(mpg, aes(x = class )) + geom_bar(aes(group = year, fill = year), position = &quot;stack&quot;) percentage ggplot(gop) + geom_bar(aes(x=growth, y=Sepal.Length.mean, fill=Species), stat=&#39;identity&#39;, position=&#39;fill&#39;) groups are dodge with actual number ggplot(gop) + geom_bar(aes(x=growth, y=Sepal.Length.mean, fill=Species), stat=&#39;identity&#39;, position=&#39;dodge&#39;) - groups are dodge with percentage gop2 &lt;- gop %&gt;% group_by(growth ) %&gt;% mutate(Sepal.Length.prop=Sepal.Length.mean/sum(Sepal.Length.mean)) ggplot(gop2) + geom_bar(aes(x=growth, y=Sepal.Length.prop, fill=Species), stat=&#39;identity&#39;, position=&#39;dodge&#39;) + ylab(&quot;Votes (%)&quot;) 4.4 Line charts ggplot(iris) + geom_line(aes(x=Sepal.Length , y=Petal.Length)) 4.4.1 Grouped by colour variable ggplot(iris) + geom_line(aes(x=Sepal.Length , y=Petal.Length, colour = Species)) grouped by state then set how many rows or columns ggplot(iris) + geom_line(aes(x=Sepal.Length , y=Petal.Length) ) + facet_wrap(~Species, nrow = 1) + #set how many rows coord_flip() 4.4.2 Multiple aesthetics iris &lt;- iris %&gt;% mutate(growth = ifelse(Petal.Width &gt; 1.5, &quot;Wide&quot;, &quot;Normal&quot;)) ggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) + geom_line(size=2,color=&quot;purple&quot;)+ # number format scale_x_log10(labels = scales::label_number())+ geom_point( aes(size = Sepal.Length,colour = as.factor(growth)),show.legend = F)+ facet_wrap(~ Species) 4.5 ggplot2 parameters For detail, please read this article and this one. library(datasauRus) ggplot( )+ geom_point(data=datasaurus_dozen[datasaurus_dozen$dataset==&quot;dino&quot;,], aes(x = x, y = y),color= &quot;#7CAE00&quot; ) + theme_void()+ theme(legend.position = &quot;none&quot;) 4.5.1 Components of plot Components of plot Data: is a data frame Aesthetics: is used to indicate x and y variables and to control the color, the size or the shape … Geometry: the type of graphics (bar plot, line plot, scatter plot…) adjust parameters adjust legend using guide_ adjust color, size, and shape using scale_, guide_ can further adjust scale_ adjust panel, background, axis (font, color, size, angle), title, legend (position), caption using theme types of plots geom_boxplot(): Box plot geom_violin(): Violin plot geom_dotplot(): Dot plot geom_jitter(): Jitter charts geom_line(): Line plot geom_bar(): Bar plot geom_errorbar(): Error bars geom_point(): Scatter plot geom_smooth(): Add smoothed curve geom_quantile(): Add quantile lines geom_text(): Textual annotations geom_density(): Create a smooth density curve geom_histogram(): Histogram example data(&quot;faithful&quot;) # Basic scatterplot ggplot(data = faithful, mapping = aes(x = eruptions, y = waiting)) + geom_point()+ stat_density_2d(aes(fill = ..level..), geom=&quot;polygon&quot;) # Data and mapping can be given both as global (in ggplot()) or per layer # ggplot() + # geom_point(mapping = aes(x = eruptions, y = waiting), # data = faithful) 4.5.2 Create main title, axis labels, caption pay attention whether argument is factor or continuous. ggplot(data = faithful, mapping = aes(x = eruptions, y = waiting)) + geom_point()+ labs(title = &quot;Number of xxx&quot;, caption = &quot;source: http://xxx&quot;, x = &quot;Eruptions&quot; , y = &quot;Waiting time&quot; ) + # customize title, axis, caption theme( plot.title = element_text(color=&quot;red&quot;, size=14, face=&quot;bold.italic&quot;), plot.caption = element_text(color=&quot;red&quot;, size=10, face=&quot;italic&quot;), axis.title.x = element_text(color=&quot;blue&quot;, size=14, face=&quot;bold&quot;), axis.title.y = element_text(color=&quot;#993333&quot;, size=14, face=&quot;bold&quot;) )+ # hide main title theme(plot.title = element_blank() ) 4.5.3 Create legend title, position p &lt;- ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting, colour = eruptions &lt; 3))+ labs(color=&quot;Numbers of eruptions&quot;) + theme(legend.position = &quot;left&quot;)+ # Change the appearance of legend title and labels theme(legend.title = element_text(colour=&quot;blue&quot;), legend.text = element_text(colour=&quot;red&quot;))+ # Change legend box background color theme(legend.background = element_rect(fill=NULL)) print(p) customize legends using scale functions # how to change order of legend? # Set legend title and labels p+ scale_color_discrete(name = &quot;Numbers of eruptions change&quot;, labels = c(&quot;F&quot;, &quot;T&quot; )) customize legend guide_colorbar(): continuous colors guide_legend(): discrete values (shapes, colors) ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting, size = eruptions &lt; 3))+ guides(size = guide_legend( # legend title title = &quot;title is too low&quot;, title.position = &quot;bottom&quot;, title.vjust = -5, # legend label label.position = &quot;left&quot;, label.hjust = 1, label.theme = element_text(size = 15, face = &quot;italic&quot;, colour = &quot;red&quot;, angle = 0), # label reverse reverse = TRUE, # width of bin keywidth = 1, ncol = 4 ) ) delete a legend ggplot(mpg, aes(x = displ, y = hwy, color = class, size = cyl)) + geom_point() + guides( color = guide_legend(&quot;type&quot;), # keep size = &quot;none&quot; # remove ) combine two legends when they use the same variable (mapping) ggplot(mpg, aes(x = displ, y = hwy, color = cyl, size = cyl)) + geom_point() + scale_color_viridis_c() + guides( color = guide_legend(&quot;title&quot;), size = guide_legend(&quot;title&quot;) ) # guide = &quot;legend&quot; 4.5.4 Change plot colors set color into aes() ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting, colour = eruptions &lt; 3)) set color outside of aes() ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting), colour = &#39;steelblue&#39;) Colour the histogram with color and fill ggplot(faithful) + geom_histogram(aes(x = eruptions,color=eruptions &lt; 3, fill=eruptions &lt; 4)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Colour the histogram by waiting and changing position ggplot(faithful) + geom_histogram(aes(x = eruptions,color=waiting&gt;60), position = &#39;dodge&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(faithful) + geom_histogram(aes(x = eruptions,color=waiting&gt;60), position = &#39;identity&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. For fill and stack position, please see position section. change colors manually using scale ggplot(faithful) + geom_histogram(aes(x = eruptions,color=eruptions &lt; 3, fill=eruptions &lt; 4))+ # Box plot scale_fill_manual(values=c(&quot;#999999&quot;, &quot;#E69F00&quot;)) # Scatter plot scale_color_manual(values=c( &quot;#E69F00&quot;, &quot;#56B4E9&quot;)) ## &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt; ## aesthetics: colour ## axis_order: function ## break_info: function ## break_positions: function ## breaks: waiver ## call: call ## clone: function ## dimension: function ## drop: TRUE ## expand: waiver ## get_breaks: function ## get_breaks_minor: function ## get_labels: function ## get_limits: function ## guide: legend ## is_discrete: function ## is_empty: function ## labels: waiver ## limits: NULL ## make_sec_title: function ## make_title: function ## map: function ## map_df: function ## n.breaks.cache: NULL ## na.translate: TRUE ## na.value: grey50 ## name: waiver ## palette: function ## palette.cache: NULL ## position: left ## range: &lt;ggproto object: Class RangeDiscrete, Range, gg&gt; ## range: NULL ## reset: function ## train: function ## super: &lt;ggproto object: Class RangeDiscrete, Range, gg&gt; ## rescale: function ## reset: function ## scale_name: manual ## train: function ## train_df: function ## transform: function ## transform_df: function ## super: &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt; using scale brewer automatically ggplot(faithful) + geom_histogram(aes(x = eruptions,color=eruptions &lt; 3, fill=eruptions &lt; 4))+ # Box plot scale_fill_brewer(palette=&quot;Dark2&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Scatter plot scale_color_brewer(palette=&quot;Set1&quot;) ## &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt; ## aesthetics: colour ## axis_order: function ## break_info: function ## break_positions: function ## breaks: waiver ## call: call ## clone: function ## dimension: function ## drop: TRUE ## expand: waiver ## get_breaks: function ## get_breaks_minor: function ## get_labels: function ## get_limits: function ## guide: legend ## is_discrete: function ## is_empty: function ## labels: waiver ## limits: NULL ## make_sec_title: function ## make_title: function ## map: function ## map_df: function ## n.breaks.cache: NULL ## na.translate: TRUE ## na.value: NA ## name: waiver ## palette: function ## palette.cache: NULL ## position: left ## range: &lt;ggproto object: Class RangeDiscrete, Range, gg&gt; ## range: NULL ## reset: function ## train: function ## super: &lt;ggproto object: Class RangeDiscrete, Range, gg&gt; ## rescale: function ## reset: function ## scale_name: brewer ## train: function ## train_df: function ## transform: function ## transform_df: function ## super: &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt; # using guide to change the color of legend key using gray colors using scale # p + scale_fill_grey() #no fill element # p + scale_color_grey() Gradient or continuous colors (can set the middle point aswhite) # Color by cty values sp2&lt;-ggplot(mpg, aes(x = displ, y = hwy)) + geom_point(aes(color = cty)) sp2 # Change the low and high colors # Sequential color scheme sp2+scale_color_gradient(low=&quot;blue&quot;, high=&quot;red&quot;) # Diverging color scheme mid&lt;-mean(mpg$cty) sp2+scale_color_gradient2(midpoint=mid, low=&quot;blue&quot;, mid=&quot;white&quot;, high=&quot;red&quot; ) 4.5.5 Change points shapes, transparent and size make the points larger and slightly transparent. ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting, shape= eruptions &lt; 3, size=eruptions), color=&quot;steelblue&quot;, alpha=0.5) # hwo to reverse order of legend size ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting, shape= eruptions &lt; 3, size=eruptions), color=&quot;steelblue&quot;, alpha=0.5)+ scale_shape_manual(values=c(10, 23 ))+ theme(legend.position=&quot;top&quot;) 4.5.6 Change bars position p &lt;- ggplot(mpg, aes(fl, fill = drv)) p1 &lt;- p + geom_bar () p2 &lt;- p + geom_bar(position = &quot;dodge&quot;) p3 &lt;-p + geom_bar(position = &quot;fill&quot;) p4 &lt;-p + geom_bar(position = &quot;stack&quot;) library(gridExtra) ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine grid.arrange(p1, p2, p3,p4, ncol=2) 4.5.7 Add text annotations ggplot(data=mpg[(1:100), ], aes(x = displ, y = hwy)) + geom_point(aes(color = cty))+ geom_text(aes(label = manufacturer ), size = 2, vjust = -1) #vjust is site not direction 4.5.8 Add a line that (separates points) ggplot(faithful) + geom_point(aes(x = eruptions, y = waiting))+ geom_abline(slope=-13,intercept = 100,color=&quot;red&quot;, linetype = &quot;dashed&quot;)+ # Add horizontal line at y = 2O; change line type and color geom_hline(yintercept=20, linetype=&quot;dotted&quot;, color = &quot;red&quot;)+ # Add vertical line at x = 3; change line type, color and size geom_vline(xintercept = 3, color = &quot;blue&quot;, size=1.5) # Add regression line add segment and arrow ggplot(mpg, aes(x = displ, y = hwy )) + geom_point() + # Add horizontal line segment geom_segment(aes(x = 2, y = 15, xend = 3, yend = 15, size=3, color=&quot;red&quot;)) + geom_segment(aes(x = 3, y = 33, xend = 2.5 , yend = 30), arrow = arrow(length = unit(0.5, &quot;cm&quot;))) fitted curve ggplot(data=mpg[mpg$fl!=&quot;c&quot;,], aes(x = displ, y = hwy)) + geom_point( ) + geom_smooth(color=&quot;red&quot;) #fitted curve ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(data=mpg[mpg$fl!=&quot;c&quot;,], aes(x = displ, y = hwy)) + geom_point( ) + geom_quantile() + geom_rug()+ theme_minimal() ## Smoothing formula not specified. Using: y ~ x customize line ggplot(mpg, aes(x = displ, y = hwy, color = fl, linetype = fl, size=fl)) + geom_point() + geom_line(aes( ) )+ labs(color=&quot;What is fl&quot;)+ # customize linetype, color, size scale_linetype_manual(values=c(&quot;twodash&quot;, &quot;dotted&quot;,&quot;twodash&quot;, &quot;dotted&quot;,&quot;twodash&quot;))+ scale_color_manual(name = &quot;continents&quot;,breaks = c(&quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;Europe&quot;, &quot;Oceania&quot;),labels = c(&quot;africa&quot;, &quot;americas&quot;, &quot;asia&quot;, &quot;europe&quot;, &quot;oceania&quot;), values=c(&#39;#999999&#39;,&#39;#E69F00&#39;,&#39;#999999&#39;,&#39;#E69F00&#39;,&#39;#999999&#39;) )+ #using breaks define three labels scale_size_manual(values=seq(1,4, 0.2))+ theme(legend.position=&quot;top&quot;) + guides(color=&quot;legend&quot;) 4.5.9 Using scale_ function all mappings have associated scales even if not specified. uisng scale_colour_brewer. ggplot(mpg) + geom_point(aes(x = displ, y = hwy, colour = class)) + scale_colour_brewer(type = &#39;qual&#39;) RColorBrewer RColorBrewer::display.brewer.all() using different palettes ggplot(mpg) + geom_point(aes(x = displ, y = hwy, colour = class)) + scale_colour_brewer (palette = &#39;Paired&#39;) showing cyl with size ggplot(mpg) + geom_point(aes(x = displ, y = hwy, colour = class, size=cyl)) + scale_colour_brewer(palette = &#39;Set1&#39; ) + scale_size (breaks = c(4,6)) using guides to modify the scale_ ggplot(mpg) + geom_point(aes(x = displ, y = hwy, colour = cyl, size=cyl)) + # scale_colour_brewer(palette = &#39;Set1&#39;) + #can not continuous scale_size (breaks = c(4,5,6)) + guides( size = guide_legend( override.aes = list(color = c(&#39;red&#39;, &#39;blue&#39;, &#39;black&#39;)))) unite legends when multiple aesthetics are mapped to the same variable. ggplot(mpg) + geom_point(aes(x = displ, y = hwy, colour = cyl, size=cyl)) + guides(colour=&quot;legend&quot;) category is also ok ggplot(mpg) + geom_point(aes(x = displ, y = hwy, colour = fl, size=fl)) + guides(colour=&quot;legend&quot;) #size is not ok x and y also have associated scales ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + scale_x_continuous(breaks = c(3.5, 5, 6)) + scale_y_continuous(trans = &#39;log2&#39;) 4.5.10 Change coordinates Changing the coordinate system can have dramatic effects coord_polar ggplot(mpg) + geom_bar(aes(x = class)) + coord_polar() ggplot(mpg) + geom_bar(aes(x = class)) + coord_polar(theta = &#39;y&#39;) + expand_limits(y = 70) specify the scale of coordinate require(scales) ## Loading required package: scales ## ## Attaching package: &#39;scales&#39; ## The following objects are masked from &#39;package:psych&#39;: ## ## alpha, rescale ## The following object is masked from &#39;package:plotrix&#39;: ## ## rescale ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor ggplot(mpg) + geom_bar(aes(x = class)) + scale_y_continuous(limits = c(0, 50), breaks = seq(0, 50, 01)) # scale_y_continuous(labels = percent) # labels as percents # + # scale_x_discrete(labels=c(1:7) ) using coord_cartesian zoom in # have been deleted ggplot(mpg) + geom_bar(aes(x = class)) + scale_y_continuous(limits = c(0, 30))+ scale_x_discrete( limit=c(&quot;midsize&quot;,&quot;compact&quot;)) ggplot(mpg) + geom_bar(aes(x = class)) + coord_cartesian( ylim = c(0, 30))+ scale_x_discrete( limit=c(&quot;midsize&quot;,&quot;compact&quot;)) reverse direction of axes ggplot(mpg) + geom_point(aes(x = hwy, y = displ))+ scale_x_continuous(breaks = c(20, 30, 35,40)) + scale_y_reverse()+ scale_y_continuous(trans=&quot;log2&quot;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. # log10, sqrt, reverse, scale_y_continuous(trans=&quot;log2&quot;) 4.5.11 Customize axis ticks change axis text font, color, size, angle using theme # when use theme, scale, guide? ggplot(mpg) + geom_point(aes(x = hwy, y = displ))+ theme(axis.text.x = element_text(face=&quot;bold&quot;, color=&quot;#993333&quot;, size=14, angle=45), axis.text.y = element_text(face=&quot;bold&quot;, color=&quot;blue&quot;, size=7, angle=90)) remove aixs ticks and tick labels ggplot(mpg) + geom_point(aes(x = hwy, y = displ))+ theme( axis.text.x = element_blank(), # Remove x axis tick labels axis.text.y = element_blank(), # Remove y axis tick labels axis.ticks = element_blank()) # Remove ticks 4.5.12 Flip and reverse plot boxplot and violin ggplot(mpg ) + geom_violin( aes(x = as.factor(cyl), y=hwy ,color=as.factor(cyl) ) ,trim = FALSE,width = 4 ) + geom_boxplot( aes(x = as.factor(cyl), y=hwy ), notch = F , width = 0.1) dotplot using stat_ function ggplot(mpg ,aes(x = as.factor(cyl), y=hwy ) )+ geom_dotplot(aes(color =as.factor(cyl), fill = as.factor(cyl)), binaxis = &quot;y&quot;, stackdir = &quot;center&quot;) + stat_summary(fun.data=&quot;mean_sdl&quot; ) ## Bin width defaults to 1/30 of the range of the data. Pick better value with `binwidth`. errorbar df3 &lt;- data_summary(mpg, varname=&quot;hwy&quot;, grps= c(&quot;cyl&quot; )) ## Loading required package: plyr ## ------------------------------------------------------------------------------ ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------------ ## ## Attaching package: &#39;plyr&#39; ## The following object is masked from &#39;package:ggpubr&#39;: ## ## mutate ## The following objects are masked from &#39;package:plotly&#39;: ## ## arrange, mutate, rename, summarise ## The following objects are masked from &#39;package:Hmisc&#39;: ## ## is.discrete, summarize ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ## The following object is masked from &#39;package:purrr&#39;: ## ## compact head(df3) cyl hwy sd 4 28.80247 4.515030 5 28.75000 0.500000 6 22.82278 3.685590 8 17.62857 3.262307 ggplot( df3, aes(as.factor(cyl) , (hwy), ymin = hwy-sd, ymax = hwy+sd) ) + geom_line(aes(group = 0 )) + geom_errorbar(aes(color = as.factor(cyl) ),width = 0.2) ggplot( df3, aes(as.factor(cyl) , (hwy), ymin = hwy-sd, ymax = hwy+sd) ) + geom_bar(aes(fill = as.factor(cyl)), stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar(aes( ),width = 0.2) + coord_flip() using original data ggplot(mpg, aes(cyl, hwy)) + stat_summary(geom = &quot;bar&quot;) + stat_summary(geom = &quot;errorbar&quot;) ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` flip ggplot(mpg) + geom_bar(aes(x = class)) + coord_flip() reverse ggplot(mpg) + geom_bar(aes(x = class)) + scale_y_reverse() 4.5.13 Create stats Every geom has a stat. The stat can be overwritten if we use any additional computations. ggplot(mpg ) + geom_bar (aes(x = cyl ),position = &#39;identity&#39; ) #using original data using transformed variables library(dplyr) library(ggplot2) # mpg_counted &lt;- # count(mpg, cyl ) # head(mpg_counted) # ggplot(mpg_counted) + # geom_smooth(aes(x = cyl , y = n)) + # geom_bar (aes(x = cyl , y = n), stat = &#39;identity&#39;) #using summary data using the after_stat() function inside aes(). require(scales) ggplot(mpg) + geom_bar(aes(x = class, y = after_stat( count / sum(count))))+ scale_y_continuous(labels = percent) # labels decimals as percents using density geometric in histogram ggplot(mpg,aes(x = hwy)) + geom_histogram(aes(y=..density..))+ geom_density( ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Use stat_summary() to add the mean of hwy for each group STAT vs. GEOM p1 &lt;- ggplot(mpg,aes(x = class, y = hwy) ) + stat_summary( geom = &quot;pointrange&quot;, fun.data = mean_se ) p2 &lt;- ggplot(mpg,aes(x = class, y = hwy) ) + stat_summary( ) p3 &lt;- ggplot(mpg,aes(x = class, y = hwy) ) + stat_summary( )+ stat_summary( fun.data = ~mean_se(., mult = 1.96), # Increase `mult` value for bigger interval! geom = &quot;errorbar&quot;, ) library(patchwork) p1+p2+p3 ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ggplot(mpg) + geom_point(aes(x = class, y = hwy), width = 0.2)+ stat_summary(aes( x = class,y = hwy), geom=&quot;point&quot;,color=&quot;red&quot;,size=4) ## No summary function supplied, defaulting to `mean_se()` jitter points ggplot(mpg) + geom_jitter(aes(x = class, y = hwy), width = 0.2)+ stat_summary(aes( x = class,y = hwy), geom=&quot;point&quot;,color=&quot;red&quot;,size=4) ## No summary function supplied, defaulting to `mean_se()` 4.5.14 Facets facet_wrap() allows you to place facet side by side into a rectangular layout. facet_grid() allows you to specify different directions and works on two variables. share the axes between the different panels ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_wrap(~ class) ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_grid(drv~ . ) ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_grid(~ drv ) ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_grid(year ~ drv) do not share the axes between the different panels ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_wrap(~ drv , scales = &quot;free&quot;) only free y axes ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_wrap(~ drv , scales = &quot;free_y&quot;) adjust y scale (space) between the panels ggplot(mpg) + geom_bar(aes(y = manufacturer)) + facet_grid(class ~ .) ggplot(mpg) + geom_bar(aes(y = manufacturer)) + facet_grid(class ~ ., space = &quot;free_y&quot;, scales = &quot;free_y&quot;) display by adding multiple variables together ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) + facet_wrap(year ~ drv) 4.5.15 Theme theme_minimal ggplot(mpg) + geom_bar(aes(y = class)) + facet_wrap(~year) + theme_minimal() Further adjustments theme_bw ggplot(mpg) + geom_bar(aes(y = class)) + facet_wrap(~year) + labs(title = &quot;Number of car models per class&quot;, caption = &quot;source: http://fueleconomy.gov&quot;, x = NULL, y = NULL) + theme_bw() + theme( strip.text = element_text(face = &#39;bold&#39;, hjust = 0), plot.caption = element_text(face = &#39;italic&#39;), panel.grid.major = element_line(&#39;white&#39;, size = 0.5), panel.grid.minor = element_blank(), panel.grid.major.y = element_blank() # , # panel.ontop = TRUE ) theme_classic ggplot(mpg) + geom_bar(aes(y = class, fill = drv) ,position = &quot;dodge&quot;) + facet_wrap(~year) + labs(title = &quot;Number of car models per class&quot;, caption = &quot;source: http://fueleconomy.gov&quot;, x = &#39;Number of cars&#39;, y = NULL)+ scale_fill_manual(name = &quot;Drive Models&quot;,values=c(&quot;black&quot;, &quot;grey50&quot;, &quot;grey80&quot;), labels = c(&quot;4w&quot;,&quot;Fw&quot;,&quot;Rw&quot; )) + # scale_x_continuous(expand = c(0, NA)) + theme_classic() + theme( # text = element_text(&#39;Avenir Next Condensed&#39;), # strip.text = element_text(face = &#39;bold&#39;, hjust = 0), plot.caption = element_text(face = &#39;italic&#39;), panel.grid.major = element_line(&#39;white&#39;, size = 0.5), panel.grid.minor = element_blank(), panel.grid.major.y = element_blank() # panel.ontop = TRUE) ) using ggthemes library(ggthemes) ggplot(mpg) + geom_bar(aes(y = class, fill = drv) ,position = &quot;dodge&quot;) + facet_wrap(~year) + labs(title = &quot;Number of car models per class&quot;, caption = &quot;source: http://fueleconomy.gov&quot;, x = &#39;Number of cars&#39;, y = NULL)+ ggthemes::theme_economist() customized theme ggplot(mpg) + geom_bar(aes(y = class, fill = drv) ,position = &quot;dodge&quot;) + facet_wrap(~year) + labs(title = &quot;Number of car models per class&quot;, caption = &quot;source: http://fueleconomy.gov&quot;, x = &#39;Number of cars&#39;, y = NULL)+ theme( # 1 change legend legend.background = element_rect( fill = &quot;#fff6c2&quot;, color = &quot;black&quot;, linetype = &quot;dashed&quot; ), legend.key = element_rect(fill = &quot;grey&quot;, color = &quot;brown&quot;), legend.position = &quot;bottom&quot;, # 2 change panel (middle erea) background panel.background = element_rect( fill = &quot;#005F59&quot;, color = &quot;red&quot;, size = 3 ), panel.border = element_rect( color = &quot;black&quot;, fill = &quot;transparent&quot;, linetype = &quot;dashed&quot;, size = 3 ), # 3 change plot background plot.background = element_rect( fill = &quot;#a1dce9&quot;, color = &quot;black&quot;, size = 1.3 ), # 4 change axis elements axis.line = element_line(color = &quot;orange&quot;, size = 2), axis.title = element_text(color = &quot;red&quot;, face = &quot;italic&quot;), axis.ticks = element_line(color = &quot;purple&quot;, size = 3), axis.text = element_text(color = &quot;blue&quot;), axis.text.x = element_text(angle = 45, hjust = 1), # 5 change facet panel strip.background = element_rect(fill = &quot;orange&quot;), strip.text = element_text(color = &quot;red&quot;), panel.spacing = unit(0.3, &quot;inch&quot;) ) "],["basic-statistics.html", "5 Basic statistics 5.1 The essentials of R", " 5 Basic statistics 5.1 The essentials of R 5.1.1 Manipulation of vector library(tidyverse) library(dplyr) vec &lt;- c(3,5,2,1,5,&quot;O&quot;,NA) length(unique(vec)) ## [1] 6 num_vec &lt;- as.numeric(vec) log(num_vec) ## [1] 1.0986123 1.6094379 0.6931472 0.0000000 1.6094379 NA NA sum(c(num_vec, NA), na.rm=T) ## [1] 16 sort(num_vec, decreasing = T) ## [1] 5 5 3 2 1 is.na(num_vec) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE num_vec[!is.na(num_vec)] ## [1] 3 5 2 1 5 c(5,6) %in% vec ## [1] TRUE FALSE grepl(&quot;5&quot;, vec) ## [1] FALSE TRUE FALSE FALSE TRUE FALSE FALSE 5.1.2 Generate sequence or repeted sequece seq(from = 0, to = 10, by = 0.5) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 ## [16] 7.5 8.0 8.5 9.0 9.5 10.0 rep(x = 1:3, times = 4) ## [1] 1 2 3 1 2 3 1 2 3 1 2 3 rep(x = 1:3, each = 4) ## [1] 1 1 1 1 2 2 2 2 3 3 3 3 5.1.3 Get directory and write data out and in getwd() ## [1] &quot;C:/Users/hed2/Downloads/mybook2/mybook2&quot; setwd(getwd()) write.csv(cars, &quot;cars.csv&quot;, row.names=F) dataframe &lt;- read.csv(&quot;cars.csv&quot;) 5.1.4 Function my_func &lt;- function(x){ x_mod &lt;- (x + 7) * 4 return(x_mod) } my_func(num_vec) ## [1] 40 48 36 32 48 NA NA 5.1.5 Plot plot(dist ~ speed, data=cars) hist(cars$dist ) ### Build model and plot model &lt;- lm(dist ~ speed, data=cars) plot(dist ~ speed, data=cars) abline(model) abline(v = 25) abline(h = 15) ### Rename names of columns names(cars) ## [1] &quot;speed&quot; &quot;dist&quot; names(cars) &lt;- c(&quot;speed per hour&quot;, &quot;total dist&quot;) 5.1.6 Class of dataframe matrix &lt;- as.matrix(cars) df &lt;- as.data.frame(matrix) class(matrix) ## [1] &quot;matrix&quot; &quot;array&quot; class(df) ## [1] &quot;data.frame&quot; # tranform t(matrix) speed per hour 4 4 7 7 8 9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25 total dist 2 10 4 22 16 10 18 26 34 17 28 14 20 24 28 26 34 34 46 26 36 60 80 20 26 54 32 40 32 40 50 42 56 76 84 36 46 68 32 48 52 56 64 66 54 70 92 93 120 85 5.1.7 Generate new variable for dataframe (character) paste0(&quot;raster_&quot;, 1:10) ## [1] &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_6&quot; ## [7] &quot;raster_7&quot; &quot;raster_8&quot; &quot;raster_9&quot; &quot;raster_10&quot; paste0(&quot;raster_&quot;, rep(x = 1:5, times = 10)) ## [1] &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; ## [7] &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; ## [13] &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; ## [19] &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; ## [25] &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; ## [31] &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; ## [37] &quot;raster_2&quot; &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; ## [43] &quot;raster_3&quot; &quot;raster_4&quot; &quot;raster_5&quot; &quot;raster_1&quot; &quot;raster_2&quot; &quot;raster_3&quot; ## [49] &quot;raster_4&quot; &quot;raster_5&quot; df$group &lt;- paste0(&quot;raster_&quot;, rep(x = 1:5, times = 10)) df$id &lt;- paste0(&quot;raster_&quot;, 1:50) 5.1.8 Create a new dataframe using ‘rnorm’ - random number from distribution sample &lt;- round((rnorm(50,0, 1)),2) group &lt;- paste0(&quot;raster_&quot;, rep(x = 1:5, times = 10)) df_join &lt;- data.frame(sample, group) df_join$id &lt;- paste0(&quot;raster_&quot;, 1:50) 5.1.9 Left join two dataframes library(dplyr) data_all &lt;- left_join(df, df_join, by=&quot;id&quot;) head(data_all) speed per hour total dist group.x id sample group.y 4 2 raster_1 raster_1 -0.01 raster_1 4 10 raster_2 raster_2 -1.78 raster_2 7 4 raster_3 raster_3 -0.78 raster_3 7 22 raster_4 raster_4 0.13 raster_4 8 16 raster_5 raster_5 -0.71 raster_5 9 10 raster_1 raster_6 -0.04 raster_1 5.1.10 Select variables select(data_all, group.x, id ) group.x id raster_1 raster_1 raster_2 raster_2 raster_3 raster_3 raster_4 raster_4 raster_5 raster_5 raster_1 raster_6 raster_2 raster_7 raster_3 raster_8 raster_4 raster_9 raster_5 raster_10 raster_1 raster_11 raster_2 raster_12 raster_3 raster_13 raster_4 raster_14 raster_5 raster_15 raster_1 raster_16 raster_2 raster_17 raster_3 raster_18 raster_4 raster_19 raster_5 raster_20 raster_1 raster_21 raster_2 raster_22 raster_3 raster_23 raster_4 raster_24 raster_5 raster_25 raster_1 raster_26 raster_2 raster_27 raster_3 raster_28 raster_4 raster_29 raster_5 raster_30 raster_1 raster_31 raster_2 raster_32 raster_3 raster_33 raster_4 raster_34 raster_5 raster_35 raster_1 raster_36 raster_2 raster_37 raster_3 raster_38 raster_4 raster_39 raster_5 raster_40 raster_1 raster_41 raster_2 raster_42 raster_3 raster_43 raster_4 raster_44 raster_5 raster_45 raster_1 raster_46 raster_2 raster_47 raster_3 raster_48 raster_4 raster_49 raster_5 raster_50 5.1.11 Filter observations raster_1 &lt;- filter(data_all, group.x == &quot;raster_1&quot;) raster_1 speed per hour total dist group.x id sample group.y 4 2 raster_1 raster_1 -0.01 raster_1 9 10 raster_1 raster_6 -0.04 raster_1 11 28 raster_1 raster_11 -0.31 raster_1 13 26 raster_1 raster_16 2.19 raster_1 14 36 raster_1 raster_21 1.45 raster_1 15 54 raster_1 raster_26 0.85 raster_1 17 50 raster_1 raster_31 1.11 raster_1 19 36 raster_1 raster_36 0.27 raster_1 20 52 raster_1 raster_41 -1.49 raster_1 24 70 raster_1 raster_46 -2.16 raster_1 speed_dist &lt;- filter(data_all, data_all$`speed per hour` &lt; 11 &amp; data_all$`total dist` &gt;= 10) speed_dist speed per hour total dist group.x id sample group.y 4 10 raster_2 raster_2 -1.78 raster_2 7 22 raster_4 raster_4 0.13 raster_4 8 16 raster_5 raster_5 -0.71 raster_5 9 10 raster_1 raster_6 -0.04 raster_1 10 18 raster_2 raster_7 -0.47 raster_2 10 26 raster_3 raster_8 0.61 raster_3 10 34 raster_4 raster_9 1.17 raster_4 5.1.12 Append rows rbind(raster_1,speed_dist) speed per hour total dist group.x id sample group.y 4 2 raster_1 raster_1 -0.01 raster_1 9 10 raster_1 raster_6 -0.04 raster_1 11 28 raster_1 raster_11 -0.31 raster_1 13 26 raster_1 raster_16 2.19 raster_1 14 36 raster_1 raster_21 1.45 raster_1 15 54 raster_1 raster_26 0.85 raster_1 17 50 raster_1 raster_31 1.11 raster_1 19 36 raster_1 raster_36 0.27 raster_1 20 52 raster_1 raster_41 -1.49 raster_1 24 70 raster_1 raster_46 -2.16 raster_1 4 10 raster_2 raster_2 -1.78 raster_2 7 22 raster_4 raster_4 0.13 raster_4 8 16 raster_5 raster_5 -0.71 raster_5 9 10 raster_1 raster_6 -0.04 raster_1 10 18 raster_2 raster_7 -0.47 raster_2 10 26 raster_3 raster_8 0.61 raster_3 10 34 raster_4 raster_9 1.17 raster_4 5.1.13 Create new variables instead of old variables mutate(data_all, sample = round(sample,1)) speed per hour total dist group.x id sample group.y 4 2 raster_1 raster_1 0.0 raster_1 4 10 raster_2 raster_2 -1.8 raster_2 7 4 raster_3 raster_3 -0.8 raster_3 7 22 raster_4 raster_4 0.1 raster_4 8 16 raster_5 raster_5 -0.7 raster_5 9 10 raster_1 raster_6 0.0 raster_1 10 18 raster_2 raster_7 -0.5 raster_2 10 26 raster_3 raster_8 0.6 raster_3 10 34 raster_4 raster_9 1.2 raster_4 11 17 raster_5 raster_10 -0.8 raster_5 11 28 raster_1 raster_11 -0.3 raster_1 12 14 raster_2 raster_12 1.4 raster_2 12 20 raster_3 raster_13 -2.2 raster_3 12 24 raster_4 raster_14 -0.3 raster_4 12 28 raster_5 raster_15 2.1 raster_5 13 26 raster_1 raster_16 2.2 raster_1 13 34 raster_2 raster_17 0.2 raster_2 13 34 raster_3 raster_18 -0.9 raster_3 13 46 raster_4 raster_19 0.2 raster_4 14 26 raster_5 raster_20 -0.7 raster_5 14 36 raster_1 raster_21 1.4 raster_1 14 60 raster_2 raster_22 -0.8 raster_2 14 80 raster_3 raster_23 0.4 raster_3 15 20 raster_4 raster_24 0.6 raster_4 15 26 raster_5 raster_25 -0.8 raster_5 15 54 raster_1 raster_26 0.8 raster_1 16 32 raster_2 raster_27 -0.8 raster_2 16 40 raster_3 raster_28 0.6 raster_3 17 32 raster_4 raster_29 1.1 raster_4 17 40 raster_5 raster_30 -1.0 raster_5 17 50 raster_1 raster_31 1.1 raster_1 18 42 raster_2 raster_32 -0.5 raster_2 18 56 raster_3 raster_33 0.3 raster_3 18 76 raster_4 raster_34 0.2 raster_4 18 84 raster_5 raster_35 -0.4 raster_5 19 36 raster_1 raster_36 0.3 raster_1 19 46 raster_2 raster_37 -1.2 raster_2 19 68 raster_3 raster_38 -0.1 raster_3 20 32 raster_4 raster_39 0.8 raster_4 20 48 raster_5 raster_40 -2.2 raster_5 20 52 raster_1 raster_41 -1.5 raster_1 20 56 raster_2 raster_42 -1.2 raster_2 20 64 raster_3 raster_43 -1.6 raster_3 22 66 raster_4 raster_44 0.4 raster_4 23 54 raster_5 raster_45 -1.0 raster_5 24 70 raster_1 raster_46 -2.2 raster_1 24 92 raster_2 raster_47 -0.6 raster_2 24 93 raster_3 raster_48 -0.4 raster_3 24 120 raster_4 raster_49 0.9 raster_4 25 85 raster_5 raster_50 -1.1 raster_5 5.1.14 summarise statistics summarise(data_all, mean_speed = mean(sample), max_dist = max( &quot;total dist&quot; )) mean_speed max_dist -0.1862 total dist 5.1.15 Group dataframe then summarise statistics data_all_group &lt;- group_by(data_all, group.x) summarise(data_all_group, mean_speed = mean(sample), max_dist = max( &quot;total dist&quot; )) mean_speed max_dist -0.1862 total dist 5.1.16 Ungroup then summarise statistics ungroup_data &lt;- ungroup( data_all_group) summarise( ungroup_data , mean_speed = mean(sample), max_dist = max( &quot;total dist&quot; )) mean_speed max_dist -0.1862 total dist 5.1.17 Summary linear regression model mod1 &lt;- lm(cars$`total dist` ~ cars$`speed per hour` ) summary(mod1) ## ## Call: ## lm(formula = cars$`total dist` ~ cars$`speed per hour`) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## cars$`speed per hour` 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 5.1.18 Create frequency table table(data_all_group$`speed per hour`,data_all_group$group.x ) / raster_1 raster_2 raster_3 raster_4 raster_5 4 1 1 0 0 0 7 0 0 1 1 0 8 0 0 0 0 1 9 1 0 0 0 0 10 0 1 1 1 0 11 1 0 0 0 1 12 0 1 1 1 1 13 1 1 1 1 0 14 1 1 1 0 1 15 1 0 0 1 1 16 0 1 1 0 0 17 1 0 0 1 1 18 0 1 1 1 1 19 1 1 1 0 0 20 1 1 1 1 1 22 0 0 0 1 0 23 0 0 0 0 1 24 1 1 1 1 0 25 0 0 0 0 1 5.1.19 Value and variable label table(iris$Species) setosa versicolor virginica 50 50 50 iris$Species &lt;- factor(iris$Species,labels = c( &quot;setosanew&quot;,&quot;versicolornew&quot;,&quot;virginianew&quot;)) table(iris$Species) setosanew versicolornew virginianew 50 50 50 library(Hmisc) label(iris$Species) &lt;- &quot;Species types&quot; table(iris$Species) setosanew versicolornew virginianew 50 50 50 5.1.20 Recode a variable irisifelse &lt;- iris%&gt;% mutate(Sepal.Length2 = ifelse(Sepal.Length &lt; 6 , &quot;level1&quot;, ifelse(Sepal.Length &lt; 7 , &quot;level2&quot;, Sepal.Length))) table(irisifelse$Sepal.Length2) 7 7.1 7.2 7.3 7.4 7.6 7.7 7.9 level1 level2 1 1 3 1 1 1 4 1 83 54 "],["statistical-models.html", "6 Statistical models 6.1 Simple linear regression 6.2 Multiple linear regression 6.3 Multiple linear regression practice 6.4 Linear mixed model theory 6.5 Linear mixed model practice 6.6 Linear mixed model covariance decomposition with random intercept- lme4 6.7 Linear mixed model covariance decomposition with random slopes- lme4 6.8 Linear mixed model covariance decomposition (nlme)", " 6 Statistical models 6.1 Simple linear regression 6.1.1 Linear regeression assumptions There are four principal assumptions: linearity of the relationship between dependent and independent variables. statistical independence of the errors with y variable. homoscedasticity (constant variance) of the errors for all x. normality of the error distribution. if independent assumption violated, the estimated standard errors tend to underestimate the true standard error. P value associated thus is lower. only the prediction errors need to be normally distributed. but with extremely asymmetric or long-tailed, it may be hard to fit them (x and y) into a linear model whose errors will be normally distributed. 6.1.2 Population regression function Regression is to estimate and/or predict the population mean (expectation) of dependent variable (yi) by a known or a set value of explanatory variables (xi). Population regression line (PRL) is the trajectory of the conditional expectation value given Xi. \\[ E(Y|X_i)=f(X_i)=\\beta_1+\\beta_2X_i \\] This is an unknown but fixed value (can be estimated). 6.1.3 Population regression model the errors \\(u_i=y_i-\\hat{y}_i\\) have equal variance \\[ Y_i=\\beta_1+\\beta_2X_i+u_i \\] 6.1.4 Sample regression model (using hat to indicate sample) \\[ Y_i=\\hat{\\beta}_1+\\hat{\\beta}_2X_i+e_i \\] since \\[ u_i \\sim N(0,\\sigma^2) \\] or \\[ e_i \\sim N(0,\\hat{\\sigma} ^2) \\] and i.i.d., independent identically distribution, the probability distributions are all the same and variables are independent of each other. \\[ \\begin{align} u_i \\sim i.i.d \\ N(0,\\sigma^2) \\end{align} \\] then \\[ \\begin{align} Y_i- \\beta_1+\\beta_2X_i (\\hat{Y_i}) &amp;\\sim i.i.d \\ N(0,\\sigma^2)\\\\ \\end{align} \\] thence, to minimize Q \\(\\sum{(Y_i-\\hat{Y}_i)^2}\\) to solve b0 and b1. \\[ \\begin{align} Min(Q) &amp;=\\sum{(Y_i-\\hat{Y}_i)^2}\\\\ &amp;=\\sum{\\left ( Y_i-(\\hat{\\beta}_1+\\hat{\\beta}_2X_i) \\right )^2}\\\\ &amp;=f(\\hat{\\beta}_1,\\hat{\\beta}_2) \\end{align} \\] 6.1.5 Solve \\(\\hat{\\beta_1},\\hat{\\beta_2}\\) and variance \\[ \\begin{align} \\begin{split} \\hat{\\beta}_2 &amp;=\\frac{\\sum{x_iy_i}}{\\sum{x_i^2}}\\\\ \\hat{\\beta_1} &amp;=\\bar{Y}_i-\\hat{\\beta}_2\\bar{X}_i \\end{split} \\\\ var(\\hat{\\beta}_2) =\\sigma_{\\hat{\\beta}_2}^2&amp;=\\frac{1}{\\sum{x_i^2}}\\cdot\\sigma^2&amp;&amp;\\text{} \\\\ var(\\hat{\\beta}_1) =\\sigma_{\\hat{\\beta}_1}^2 &amp;=\\frac{\\sum{X_i^2}}{n\\sum{x_i^2}}\\cdot\\sigma^2 \\end{align} \\] 6.1.6 Calculate the variance \\(\\hat{\\sigma}^2\\) of error \\(e_i\\) (for sample) \\[ \\begin{align} \\hat{Y}_i &amp;=\\hat{\\beta}_1+\\hat{\\beta}_2X_i \\\\ e_i &amp;=Y_i-\\hat{Y}_i \\\\ \\hat{\\sigma}^2 &amp;=\\frac{\\sum{e_i^2}}{n-1}=\\frac{\\sum{(Y_i-\\hat{Y}_i)^2}}{n-1} \\end{align} \\] 6.1.7 Sum of squares decomposition \\[ \\begin{align} (Y_i-\\bar{Y_i}) &amp;= (\\hat{Y_i}-\\bar{Y_i}) +(Y_i-\\hat{Y_i}) \\\\ \\sum{y_i^2} &amp;= \\sum{\\hat{y_i}^2} +\\sum{e_i^2} \\\\ TSS&amp;=ESS+RSS \\end{align} \\] 6.1.8 Coefficient of determination \\(R^2\\) and goodness of fit \\[ \\begin{align} r^2 &amp;=\\frac{ESS}{TSS}=\\frac{\\sum{(\\hat{Y_i}-\\bar{Y})^2}}{\\sum{(Y_i-\\bar{Y})^2}}\\\\ &amp;=1-\\frac{RSS}{TSS}=1-\\frac{\\sum{(Y_i-\\hat{Y_i})^2}}{\\sum{(Y_i-\\bar{Y})^2}} \\end{align} \\] 6.1.9 Test of regression coefficients since \\[ \\begin{align} \\hat{\\beta_2} &amp;\\sim N(\\beta_2,\\sigma^2_{\\hat{\\beta_2}}) \\\\ \\hat{\\beta_1} &amp;\\sim N(\\beta_1,\\sigma^2_{\\hat{\\beta_1}}) \\end{align} \\] and \\[ \\begin{align} S_{\\hat{\\beta}_2} &amp;=\\sqrt{\\frac{1}{\\sum{x_i^2}}}\\cdot\\hat{\\sigma} \\\\ S_{\\hat{\\beta}_1} &amp;=\\sqrt{\\frac{\\sum{X_i^2}}{n\\sum{x_i^2}}}\\cdot\\hat{\\sigma} \\end{align} \\] therefore \\[ \\begin{align} t_{\\hat{\\beta_2}}^{\\ast}&amp;=\\frac{\\hat{\\beta_2}-\\beta_2}{S_{\\hat{\\beta_2}}} =\\frac{\\hat{\\beta_2}}{S_{\\hat{\\beta_2}}} =\\frac{\\hat{\\beta_2}}{\\sqrt{\\frac{1}{\\sum{x_i^2}}}\\cdot\\hat{\\sigma}} \\sim t(n-2) \\\\ t_{\\hat{\\beta_1}}^{\\ast}&amp;=\\frac{\\hat{\\beta_1}-\\beta_1}{S_{\\hat{\\beta_1}}} =\\frac{\\hat{\\beta_1}}{S_{\\hat{\\beta_1}}} =\\frac{\\hat{\\beta_1}}{\\sqrt{\\frac{\\sum{X_i^2}}{n\\sum{x_i^2}}}\\cdot\\hat{\\sigma}} \\sim t(n-2) \\end{align} \\] 6.1.10 Statistical test of model since \\[ \\begin{align} Y_i&amp;\\sim i.i.d \\ N(\\beta_1+\\beta_2X_i,\\sigma^2)\\\\ \\end{align} \\] and \\[ \\begin{align} ESS&amp;=\\sum{(\\hat{Y_i}-\\bar{Y})^2} \\sim \\chi^2(df_{ESS}) \\\\ RSS&amp;=\\sum{(Y_i-\\hat{Y_i})^2} \\sim \\chi^2(df_{RSS}) \\end{align} \\] therefore \\[ \\begin{align} F^{\\ast}&amp;=\\frac{ESS/df_{ESS}}{RSS/df_{RSS}}=\\frac{MSS_{ESS}}{MSS_{RSS}}\\\\ &amp;=\\frac{\\sum{(\\hat{Y_i}-\\bar{Y})^2}/df_{ESS}}{\\sum{(Y_i-\\hat{Y_i})^2}/df_{RSS}} \\\\ &amp;=\\frac{\\hat{\\beta_2}^2\\sum{x_i^2}}{\\sum{e_i^2}/{(n-2)}}\\\\ &amp;=\\frac{\\hat{\\beta_2}^2\\sum{x_i^2}}{\\hat{\\sigma}^2} \\end{align} \\] 6.1.11 Mean prediction since \\[ \\begin{align} \\mu_{\\hat{Y}_0}&amp;=E(\\hat{Y}_0)\\\\ &amp;=E(\\hat{\\beta}_1+\\hat{\\beta}_2X_0)\\\\ &amp;=\\beta_1+\\beta_2X_0\\\\ &amp;=E(Y|X_0) \\end{align} \\] and \\[ \\begin{align} var(\\hat{Y}_0)&amp;=\\sigma^2_{\\hat{Y}_0}\\\\ &amp;=E(\\hat{\\beta}_1+\\hat{\\beta}_2X_0)\\\\ &amp;=\\sigma^2 \\left( \\frac{1}{n}+ \\frac{(X_0-\\bar{X})^2}{\\sum{x_i^2}} \\right) \\end{align} \\] therefore \\[ \\begin{align} \\hat{Y}_0&amp; \\sim N(\\mu_{\\hat{Y}_0},\\sigma^2_{\\hat{Y}_0})\\\\ \\hat{Y}_0&amp; \\sim N \\left(E(Y|X_0), \\sigma^2 \\left( \\frac{1}{n}+ \\frac{(X_0-\\bar{X})^2}{\\sum{x_i^2}} \\right) \\right) \\end{align} \\] then construct t statistic to estimate CI \\[ \\begin{align} t_{\\hat{Y}_0}&amp; =\\frac{\\hat{Y}_0-E(Y|X_0)}{S_{\\hat{Y}_0}} \\sim t(n-2) \\end{align} \\] \\[ \\begin{align} \\hat{Y}_0-t_{1-\\alpha/2}(n-2) \\cdot S_{\\hat{Y}_0} \\leq E(Y|X_0) \\leq \\hat{Y}_0+t_{1-\\alpha/2}(n-2) \\cdot S_{\\hat{Y}_0} \\end{align} \\] 6.1.12 Individual prediction since \\[ \\begin{align} (Y_0-\\hat{Y}_0)&amp; \\sim N \\left(\\mu_{(Y_0-\\hat{Y}_0)},\\sigma^2_{(Y_0-\\hat{Y}_0)} \\right)\\\\ (Y_0-\\hat{Y}_0)&amp; \\sim N \\left(0, \\sigma^2 \\left(1+ \\frac{1}{n}+ \\frac{(X_0-\\bar{X})^2}{\\sum{x_i^2}} \\right) \\right) \\end{align} \\] and Construct t statistic \\[ \\begin{align} t_{\\hat{Y}_0}&amp; =\\frac{\\hat{Y}_0-E(Y|X_0)}{S_{\\hat{Y}_0}} \\sim t(n-2) \\end{align} \\] and \\[ \\begin{align} S_{\\hat{Y}_0}&amp; = \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n}+ \\frac{(X_0-\\bar{X})^2}{\\sum{x_i^2}} \\right)} \\end{align} \\] therefore \\[ \\begin{align} \\hat{Y}_0-t_{1-\\alpha/2}(n-2) \\cdot S_{\\hat{Y}_0} \\leq E(Y|X_0) \\leq \\hat{Y}_0+t_{1-\\alpha/2}(n-2) \\cdot S_{\\hat{Y}_0} \\end{align} \\] it is harder to predict your weight based on your age than to predict the mean weight of people who are your age. so, the interval of individual prediction is wider than those of mean prediction. 6.2 Multiple linear regression 6.2.1 Matrix format \\[ \\begin{align} Y_i&amp;=\\beta_1+\\beta_2X_{2i}+\\beta_3X_{3i}+\\cdots+\\beta_kX_{ki}+u_i &amp;&amp; \\ \\end{align} \\] \\[ \\begin{equation} \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\cdots \\\\ Y_n \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; X_{21} &amp; X_{31} &amp; \\cdots &amp; X_{k1} \\\\ 1 &amp; X_{22} &amp; X_{32} &amp; \\cdots &amp; X_{k2} \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ 1 &amp; X_{2n} &amp; X_{3n} &amp; \\cdots &amp; X_{kn} \\end{bmatrix} \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\\\ \\end{bmatrix}+ \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\\\ \\end{bmatrix} \\end{equation} \\] \\[ \\begin{alignat}{4} \\mathbf{y} &amp;= &amp;\\mathbf{X}&amp;\\mathbf{\\beta}&amp;+&amp;\\mathbf{u} \\\\ (n \\times 1) &amp; &amp;{(n \\times k)} &amp;{(k \\times 1)}&amp;+&amp;{(n \\times 1)} \\end{alignat} \\] 6.2.2 Variance covariance matrix of random errors because \\[ \\mathbf{u} \\sim N(\\mathbf{0},\\sigma^2\\mathbf{I})\\text{ population}\\\\ \\mathbf{e} \\sim N(\\mathbf{0},\\sigma^2\\mathbf{I})\\text{ sample}\\ \\] therefore \\[ \\begin{align} var-cov(\\mathbf{u})&amp;=E(\\mathbf{uu&#39;})\\\\ &amp;= \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12}^2 &amp;\\cdots &amp;\\sigma_{1n}^2\\\\ \\sigma_{21}^2 &amp; \\sigma_2^2 &amp;\\cdots &amp;\\sigma_{2n}^2\\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp;\\vdots \\\\ \\sigma_{n1}^2 &amp; \\sigma_{n2}^2 &amp;\\cdots &amp;\\sigma_n^2\\\\ \\end{bmatrix} &amp;&amp; \\leftarrow (E{(u_i)}=0)\\\\ &amp;= \\begin{bmatrix} \\sigma^2 &amp; \\sigma_{12}^2 &amp;\\cdots &amp;\\sigma_{1n}^2\\\\ \\sigma_{21}^2 &amp; \\sigma^2 &amp;\\cdots &amp;\\sigma_{2n}^2\\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp;\\vdots \\\\ \\sigma_{n1}^2 &amp; \\sigma_{n2}^2 &amp;\\cdots &amp;\\sigma^2\\\\ \\end{bmatrix} &amp;&amp; \\leftarrow (var{(u_i)}=\\sigma^2)\\\\ &amp;= \\begin{bmatrix} \\sigma^2 &amp; 0 &amp;\\cdots &amp;0\\\\ 0 &amp; \\sigma^2 &amp;\\cdots &amp;0\\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp;\\vdots \\\\ 0 &amp; 0 &amp;\\cdots &amp;\\sigma^2\\\\ \\end{bmatrix} &amp;&amp; \\leftarrow (cov{(u_i,u_j)}=0,i \\neq j)\\\\ &amp;=\\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp;\\cdots &amp;0\\\\ 0 &amp; 1 &amp;\\cdots &amp;0\\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp;\\vdots \\\\ 0 &amp; 0 &amp;\\cdots &amp;1\\\\ \\end{bmatrix}\\\\ &amp;=\\sigma^2\\mathbf{I} \\end{align} \\] 6.2.3 Minimize Q, \\(\\sum{(y-\\hat{y})^2}\\) \\[ \\begin{align} Q&amp;=\\sum{e_i^2}\\\\ &amp;=\\mathbf{e&#39;e}\\\\ &amp;=\\mathbf{(y-X\\hat{\\beta})&#39;(y-X\\hat{\\beta})}\\\\ &amp;=\\mathbf{y&#39;y-2\\hat{\\beta}&#39;X&#39;y+\\hat{\\beta}&#39;X&#39;X\\hat{\\beta}} \\end{align} \\] 6.2.4 Solve \\(\\hat{\\beta}\\) by derivation (population=sample) \\[ \\begin{align} \\frac{\\partial Q}{\\partial \\mathbf{\\hat{\\beta}}}&amp;=0\\\\ \\frac{\\partial(\\mathbf{y&#39;y-2\\hat{\\beta}&#39;X&#39;y+\\hat{\\beta}&#39;X&#39;X\\hat{\\beta}})}{\\partial \\mathbf{\\hat{\\beta}}}&amp;=0\\\\ -2\\mathbf{X&#39;y}+2\\mathbf{X&#39;X\\hat{\\beta}}&amp;=0\\\\ -\\mathbf{X&#39;y}+\\mathbf{X&#39;X\\hat{\\beta}}&amp;=0\\\\ \\mathbf{X&#39;X\\hat{\\beta}} &amp;=\\mathbf{X&#39;y} \\end{align} \\] \\[ \\begin{align} \\mathbf{\\hat{\\beta}} &amp;=\\mathbf{(X&#39;X)^{-1}X&#39;y} \\end{align} \\] 6.2.5 Solve \\(\\hat{\\sigma_\\beta}^2\\) \\[ \\begin{align} var-cov(\\mathbf{\\hat{\\beta}}) &amp;=\\mathbf{E\\left( \\left(\\hat{\\beta}-E(\\hat{\\beta}) \\right) \\left( \\hat{\\beta}-E(\\hat{\\beta}) \\right )&#39; \\right)}\\\\ &amp;=\\mathbf{E\\left( \\left(\\hat{\\beta}-{\\beta} \\right) \\left( \\hat{\\beta}-\\beta \\right )&#39; \\right)} \\\\ &amp;=\\mathbf{E\\left( \\left((X&#39;X)^{-1}X&#39;u \\right) \\left( (X&#39;X)^{-1}X&#39;u \\right )&#39; \\right)} \\\\ &amp;=\\mathbf{E\\left( (X&#39;X)^{-1}X&#39;uu&#39;X(X&#39;X)^{-1} \\right)} \\\\ &amp;= \\mathbf{(X&#39;X)^{-1}X&#39;E(uu&#39;)X(X&#39;X)^{-1}} \\\\ &amp;= \\mathbf{(X&#39;X)^{-1}X&#39;}\\sigma^2\\mathbf{IX(X&#39;X)^{-1}} \\\\ &amp;= \\sigma^2\\mathbf{(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1}} \\\\ &amp;= \\sigma^2\\mathbf{(X&#39;X)^{-1}} \\\\ \\end{align} \\] 6.2.6 Solve \\(S^2(\\mathbf{\\hat{\\beta}})\\) (for sample) where \\[ \\begin{align} \\hat{\\sigma}^2&amp;=\\frac{\\sum{e_i^2}}{n-k}=\\frac{\\mathbf{e&#39;e}}{n-k} \\\\ E(\\hat{\\sigma}^2)&amp;=\\sigma^2 \\end{align} \\] therefore \\[ \\begin{align} S^2_{ij}(\\mathbf{\\hat{\\beta}}) &amp;= \\hat{\\sigma}^2\\mathbf{(X&#39;X)^{-1}} \\\\ &amp;= \\frac{\\mathbf{e&#39;e}}{n-k}\\mathbf{(X&#39;X)^{-1}} \\\\ \\end{align} \\] which is variance-covariance of coefficients 6.2.7 Sum of squares decomposition (matrix format) \\[ \\begin{align} TSS&amp;=\\mathbf{y&#39;y}-n\\bar{Y}^2 \\\\ RSS&amp;=\\mathbf{ee&#39;}=\\mathbf{yy&#39;-\\hat{\\beta}&#39;X&#39;y} \\\\ ESS&amp;=\\mathbf{\\hat{\\beta}&#39;X&#39;y}-n\\bar{Y}^2 \\end{align} \\] 6.2.8 Determination coefficient \\(R^2\\) and goodness of fit \\[ \\begin{align} R^2&amp;=\\frac{ESS}{TSS}\\\\ &amp;=\\frac{\\mathbf{\\hat{\\beta}&#39;X&#39;y}-n\\bar{Y}^2}{\\mathbf{y&#39;y}-n\\bar{Y}^2} \\end{align} \\] 6.2.9 Test of regression coefficients because \\[ \\begin{align} \\mathbf{u}&amp;\\sim N(\\mathbf{0},\\sigma^2\\mathbf{I}) \\\\ \\mathbf{\\hat{\\beta}} &amp;\\sim N\\left(\\mathbf{\\beta},\\sigma^2\\mathbf{X&#39;X}^{-1} \\right) \\\\ \\end{align} \\] therefore (for all coefficients test, vector, see above \\(S_{\\hat{\\beta}}^2\\) ) \\[ \\begin{align} \\mathbf{t_{\\hat{\\beta}}}&amp;=\\mathbf{\\frac{\\hat{\\beta}-\\beta}{S_{\\hat{\\beta}}}} \\sim \\mathbf{t(n-k)} \\end{align} \\] (for individual coefficient test) \\[ \\begin{align} \\mathbf{t_{\\hat{\\beta}}^{\\ast}}&amp;=\\frac{\\mathbf{\\hat{\\beta}}}{\\mathbf{\\sqrt{S^2_{ij}(\\hat{\\beta}_{kk})}}} \\end{align} \\] where \\[ S^2_{ij}(\\hat{\\beta}_{kk})=[s^2_{\\hat{\\beta}_1},s^2_{\\hat{\\beta}_2},\\cdots,s^2_{\\hat{\\beta}_k}]&#39; \\] they are on diagonal line of the matrix of \\(S^2(\\mathbf{\\hat{\\beta}})\\) 6.2.10 Test of model unrestricted model \\[ \\begin{align} u_i &amp;\\sim i.i.d \\ N(0,\\sigma^2)\\\\ Y_i&amp;\\sim i.i.d \\ N(\\beta_1+\\beta_2X_i+\\cdots+\\beta_kX_i,\\sigma^2)\\\\ RSS_U&amp;=\\sum{(Y_i-\\hat{Y_i})^2} \\sim \\chi^2(n-k) \\\\ \\end{align} \\] restricted model \\[ \\begin{align} u_i &amp;\\sim i.i.d \\ N(0,\\sigma^2)\\\\ Y_i&amp;\\sim i.i.d \\ N(\\beta_1,\\sigma^2)\\\\ RSS_R&amp;=\\sum{(Y_i-\\hat{Y_i})^2} \\sim \\chi^2(n-1) \\\\ \\end{align} \\] F test \\[ \\begin{align} F^{\\ast}&amp;=\\frac{(RSS_R-RSS_U)/(k-1)}{RSS_U/(n-k)} \\\\ &amp;=\\frac{ESS_U/df_{ESS_U}}{RSS_U/df_{RSS_U}} \\\\ &amp;\\sim F(df_{ESS_U},df_{RSS_U}) \\end{align} \\] \\[ \\begin{align} F^{\\ast}&amp;=\\frac{ESS_U/df_{ESS_U}}{RSS_U/df_{RSS_U}} =\\frac{\\left(\\mathbf{\\hat{\\beta}&#39;X&#39;y}-n\\bar{Y}^2 \\right)/{(k-1)}}{\\left(\\mathbf{yy&#39;-\\hat{\\beta}&#39;X&#39;y}\\right)/{(n-k)}} \\end{align} \\] 6.2.11 Mean prediction since \\[ \\begin{align} E(\\hat{Y}_0)&amp;=E\\mathbf{(X_0\\hat{\\beta})}=\\mathbf{X_0\\beta}=E\\mathbf{(Y_0)}\\\\ var(\\hat{Y}_0)&amp;=E\\mathbf{(X_0\\hat{\\beta}-X_0\\beta)}^2\\\\ &amp;=E\\mathbf{\\left( X_0(\\hat{\\beta}-\\beta)(\\hat{\\beta}-\\beta)&#39;X_0&#39; \\right)}\\\\ &amp;=E\\mathbf{X_0\\left( (\\hat{\\beta}-\\beta)(\\hat{\\beta}-\\beta)&#39; \\right)X_0&#39;}\\\\ &amp;=\\sigma^2\\mathbf{X_0\\left( X&#39;X \\right)^{-1}X_0&#39;}\\\\ \\end{align} \\] and \\[ \\begin{align} \\hat{Y}_0&amp; \\sim N(\\mu_{\\hat{Y}_0},\\sigma^2_{\\hat{Y}_0})\\\\ \\hat{Y}_0&amp; \\sim N\\left(E(Y_0|X_0), \\sigma^2\\mathbf{X_0(X&#39;X)^{-1}X_0&#39;}\\right) \\end{align} \\] construct t statistic \\[ \\begin{align} t_{\\hat{Y}_0}&amp; =\\frac{\\hat{Y}_0-E(Y|X_0)}{S_{\\hat{Y}_0}} &amp;\\sim t(n-k) \\end{align} \\] therefore \\[ \\begin{align} \\hat{Y}_0-t_{1-\\alpha/2}(n-2) \\cdot S_{\\hat{Y}_0} \\leq E(Y|X_0) \\leq \\hat{Y}_0+t_{1-\\alpha/2}(n-2) \\cdot S_{\\hat{Y}_0} \\end{align} \\] where \\[ \\begin{align} \\mathbf{S_{\\hat{Y}_0}} &amp;=\\sqrt{\\hat{\\sigma}^2X_0(X&#39;X)^{-1}X_0&#39;} \\\\ \\hat{\\sigma}^2&amp;=\\frac{\\mathbf{ee&#39;}}{(n-k)} \\end{align} \\] 6.2.12 Individual prediction since \\[ \\begin{align} e_0&amp;=Y_0-\\hat{Y}_0 \\end{align} \\] and \\[ \\begin{align} E(e_0)&amp;=E(Y_0-\\hat{Y}_0)\\\\ &amp;=E(\\mathbf{X_0\\beta}+u_0-\\mathbf{X_0\\hat{\\beta}})\\\\ &amp;=E\\left(u_0-\\mathbf{X_0 (\\hat{\\beta}- \\beta)} \\right)\\\\ &amp;=E\\left(u_0-\\mathbf{X_0 (X&#39;X)^{-1}X&#39;u} \\right)\\\\ &amp;=0 \\end{align} \\] \\[ \\begin{align} var(e_0)&amp;=E(Y_0-\\hat{Y}_0)^2\\\\ &amp;=E(e_0^2)\\\\ &amp;=E\\left(u_0-\\mathbf{X_0 (X&#39;X)^{-1}X&#39;u} \\right)^2\\\\ &amp;=\\sigma^2\\left( 1+ \\mathbf{X_0(X&#39;X)^{-1}X_0&#39;}\\right) \\end{align} \\] and \\[ \\begin{align} e_0&amp; \\sim N(\\mu_{e_0},\\sigma^2_{e_0})\\\\ e_0&amp; \\sim N\\left(0, \\sigma^2\\left(1+\\mathbf{X_0(X&#39;X)^{-1}X_0&#39;}\\right)\\right) \\end{align} \\] construct a t statistic \\[ \\begin{align} t_{e_0}&amp; =\\frac{\\hat{Y}_0-Y_0}{S_{e_0}} \\sim t(n-k) \\end{align} \\] therefore \\[ \\begin{align} \\hat{Y}_0-t_{1-\\alpha/2}(n-2) \\cdot S_{Y_0-\\hat{Y}_0} \\leq (Y_0|X_0) \\leq \\hat{Y}_0+t_{1-\\alpha/2}(n-2) \\cdot S_{Y_0-\\hat{Y}_0} \\end{align} \\] where \\[ \\begin{align} S_{Y_0-\\hat{Y}_0}=S_{e_0} &amp;=\\sqrt{\\hat{\\sigma}^2 \\left( 1+X_0(X&#39;X)^{-1}X_0&#39; \\right) } \\\\ \\hat{\\sigma}^2&amp;=\\frac{\\mathbf{ee&#39;}}{(n-k)} \\end{align} \\] 6.3 Multiple linear regression practice library(car) library(MASS) library(psych) 6.3.1 Loading and describing data data(Boston) data_ori &lt;- Boston describe(data_ori) vars n mean sd median trimmed mad min max range skew kurtosis se crim 1 506 3.6135236 8.6015451 0.25651 1.6816300 0.3283218 0.00632 88.9762 88.96988 5.1922223 36.5958159 0.3823853 zn 2 506 11.3636364 23.3224530 0.00000 5.0800493 0.0000000 0.00000 100.0000 100.00000 2.2124881 3.9523873 1.0368095 indus 3 506 11.1367787 6.8603529 9.69000 10.9318719 9.3700320 0.46000 27.7400 27.28000 0.2932747 -1.2401949 0.3049799 chas 4 506 0.0691700 0.2539940 0.00000 0.0000000 0.0000000 0.00000 1.0000 1.00000 3.3857377 9.4819703 0.0112914 nox 5 506 0.5546951 0.1158777 0.53800 0.5450601 0.1297275 0.38500 0.8710 0.48600 0.7249897 -0.0874106 0.0051514 rm 6 506 6.2846344 0.7026171 6.20850 6.2528744 0.5122383 3.56100 8.7800 5.21900 0.4012223 1.8418324 0.0312351 age 7 506 68.5749012 28.1488614 77.50000 71.1960591 28.9848300 2.90000 100.0000 97.10000 -0.5954162 -0.9780297 1.2513695 dis 8 506 3.7950427 2.1057101 3.20745 3.5393786 1.9142590 1.12960 12.1265 10.99690 1.0057898 0.4575916 0.0936102 rad 9 506 9.5494071 8.7072594 5.00000 8.7339901 2.9652000 1.00000 24.0000 23.00000 0.9988651 -0.8789291 0.3870849 tax 10 506 408.2371542 168.5371161 330.00000 400.0443350 108.2298000 187.00000 711.0000 524.00000 0.6659891 -1.1503176 7.4923887 ptratio 11 506 18.4555336 2.1649455 19.05000 18.6625616 1.7049900 12.60000 22.0000 9.40000 -0.7975743 -0.3048010 0.0962436 black 12 506 356.6740316 91.2948644 391.44000 383.1695074 8.0949960 0.32000 396.9000 396.58000 -2.8732597 7.1037150 4.0585518 lstat 13 506 12.6530632 7.1410615 11.36000 11.8990394 7.1090670 1.73000 37.9700 36.24000 0.9010929 0.4628171 0.3174589 medv 14 506 22.5328063 9.1971041 21.20000 21.5623153 5.9304000 5.00000 50.0000 45.00000 1.1015373 1.4509837 0.4088611 summary(data_ori) crim zn indus chas nox rm age dis rad tax ptratio black lstat medv Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 Min. : 1.73 Min. : 5.00 1st Qu.: 0.08205 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 1st Qu.: 6.95 1st Qu.:17.02 Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 Median : 5.000 Median :330.0 Median :19.05 Median :391.44 Median :11.36 Median :21.20 Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 Mean :12.65 Mean :22.53 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 3rd Qu.:16.95 3rd Qu.:25.00 Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 Max. :37.97 Max. :50.00 6.3.2 Create table 1 library(boot) library(table1) table1(~ . , data=data_ori) Overall(N=506) crim Mean (SD) 3.61 (8.60) Median [Min, Max] 0.257 [0.00632, 89.0] zn Mean (SD) 11.4 (23.3) Median [Min, Max] 0 [0, 100] indus Mean (SD) 11.1 (6.86) Median [Min, Max] 9.69 [0.460, 27.7] chas Mean (SD) 0.0692 (0.254) Median [Min, Max] 0 [0, 1.00] nox Mean (SD) 0.555 (0.116) Median [Min, Max] 0.538 [0.385, 0.871] rm Mean (SD) 6.28 (0.703) Median [Min, Max] 6.21 [3.56, 8.78] age Mean (SD) 68.6 (28.1) Median [Min, Max] 77.5 [2.90, 100] dis Mean (SD) 3.80 (2.11) Median [Min, Max] 3.21 [1.13, 12.1] rad Mean (SD) 9.55 (8.71) Median [Min, Max] 5.00 [1.00, 24.0] tax Mean (SD) 408 (169) Median [Min, Max] 330 [187, 711] ptratio Mean (SD) 18.5 (2.16) Median [Min, Max] 19.1 [12.6, 22.0] black Mean (SD) 357 (91.3) Median [Min, Max] 391 [0.320, 397] lstat Mean (SD) 12.7 (7.14) Median [Min, Max] 11.4 [1.73, 38.0] medv Mean (SD) 22.5 (9.20) Median [Min, Max] 21.2 [5.00, 50.0] 6.3.3 Missingness checking library(mice) md.pattern(data_ori) ## /\\ /\\ ## { `---&#39; } ## { O O } ## ==&gt; V &lt;== No need for mice. This data set is completely observed. ## \\ \\|/ / ## `-----&#39; crim zn indus chas nox rm age dis rad tax ptratio black lstat medv 506 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.3.4 Exploratory data analysis -correlation matrix library(psych) pairs.panels(data_ori) histogram library(DataExplorer) plot_histogram(data_ori) 6.3.5 Transformations transformation (based on the following rule) then histogram library(tidyverse) data_trans = data_ori %&gt;% mutate(age= sqrt(max(age)+1-age), black= log10(max(black)+1-black), crim= log10(crim), dis= sqrt(dis) ) plot_histogram(data_trans) # pairs.panels(data2) ! How to transform data for normality. check linearity between y and x attach(data_trans) plot(medv, rm) plot(medv,lstat) plot(medv,age) plot(medv, black) plot(medv,crim) 6.3.6 Data imputation and normalization 6.3.6.1 For original”data” compiling knnimpute model (see machine learning section) library(caret) # Create the knn imputation model on the training data y=data_ori$medv preProcess_missingdata_model &lt;- preProcess(data_ori , method=&#39;knnImpute&#39;) preProcess_missingdata_model ## Created from 506 samples and 14 variables ## ## Pre-processing: ## - centered (14) ## - ignored (0) ## - 5 nearest neighbor imputation (14) ## - scaled (14) check missingness # Use the imputation model to predict the values of missing data points library(RANN) # required for knnInpute data_ori &lt;- predict(preProcess_missingdata_model, newdata = data_ori ) anyNA(data_ori ) ## [1] FALSE data_ori$medv &lt;- y 6.3.6.2 For transformed “data2” compiling knnimpute model (see machine learning section) library(caret) y2=data_trans$medv # Create the knn imputation model on the training data preProcess_missingdata_model2 &lt;- preProcess(data_trans , method=&#39;knnImpute&#39;) preProcess_missingdata_model2 ## Created from 506 samples and 14 variables ## ## Pre-processing: ## - centered (14) ## - ignored (0) ## - 5 nearest neighbor imputation (14) ## - scaled (14) check missingness # Use the imputation model to predict the values of missing data points library(RANN) # required for knnInpute data_trans &lt;- predict(preProcess_missingdata_model2, newdata = data_trans ) anyNA(data_trans ) ## [1] FALSE data_trans$medv &lt;- y2 6.3.7 Generate dummy variables also can do using as.factor function for predictors x 6.3.8 Spliting data into trainning data and test data by using caret package (test data for external validation) # Create the training and test datasets set.seed(123) # for original data # Step 1: Get row numbers for the training data trainRowNumbers &lt;- createDataPartition(data_ori$medv, p=0.8, list=FALSE) # Step 2: Create the training dataset data &lt;- data_ori[trainRowNumbers,] # Step 3: Create the test dataset testdata &lt;- data_ori[-trainRowNumbers,] # for transformed data # Step 1: Get row numbers for the training data trainRowNumbers2 &lt;- createDataPartition(data_trans$medv, p=0.8, list=FALSE) # Step 2: Create the training dataset data2 &lt;- data_trans[trainRowNumbers2,] # Step 3: Create the test dataset testdata2 &lt;- data_trans[-trainRowNumbers2,] 6.3.9 Step regression model_o = lm( medv ~. , data=data2) step(model_o,direction = &quot;both&quot;) ## Start: AIC=1281.15 ## medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + ## tax + ptratio + black + lstat ## ## Df Sum of Sq RSS AIC ## - black 1 0.17 8847.0 1279.2 ## - age 1 7.07 8853.9 1279.5 ## - crim 1 14.36 8861.2 1279.8 ## - indus 1 24.08 8870.9 1280.3 ## &lt;none&gt; 8846.8 1281.2 ## - rad 1 103.22 8950.0 1283.9 ## - tax 1 156.33 9003.1 1286.3 ## - zn 1 198.34 9045.2 1288.2 ## - chas 1 251.31 9098.1 1290.5 ## - nox 1 692.00 9538.8 1309.8 ## - ptratio 1 840.04 9686.9 1316.1 ## - rm 1 965.90 9812.7 1321.3 ## - dis 1 1349.41 10196.2 1336.9 ## - lstat 1 2766.14 11613.0 1389.9 ## ## Step: AIC=1279.16 ## medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + ## tax + ptratio + lstat ## ## Df Sum of Sq RSS AIC ## - age 1 7.15 8854.1 1277.5 ## - crim 1 14.32 8861.3 1277.8 ## - indus 1 24.52 8871.5 1278.3 ## &lt;none&gt; 8847.0 1279.2 ## + black 1 0.17 8846.8 1281.2 ## - rad 1 103.72 8950.7 1281.9 ## - tax 1 157.40 9004.4 1284.3 ## - zn 1 198.20 9045.2 1286.2 ## - chas 1 251.25 9098.2 1288.6 ## - nox 1 695.37 9542.4 1308.0 ## - ptratio 1 850.76 9697.7 1314.5 ## - rm 1 966.99 9814.0 1319.4 ## - dis 1 1375.04 10222.0 1336.0 ## - lstat 1 2770.28 11617.3 1388.0 ## ## Step: AIC=1277.49 ## medv ~ crim + zn + indus + chas + nox + rm + dis + rad + tax + ## ptratio + lstat ## ## Df Sum of Sq RSS AIC ## - crim 1 18.36 8872.5 1276.3 ## - indus 1 25.56 8879.7 1276.7 ## &lt;none&gt; 8854.1 1277.5 ## + age 1 7.15 8847.0 1279.2 ## + black 1 0.26 8853.9 1279.5 ## - rad 1 97.20 8951.3 1279.9 ## - tax 1 152.93 9007.1 1282.5 ## - zn 1 196.76 9050.9 1284.4 ## - chas 1 255.17 9109.3 1287.0 ## - nox 1 694.68 9548.8 1306.2 ## - ptratio 1 843.66 9697.8 1312.5 ## - rm 1 1023.40 9877.5 1320.0 ## - dis 1 1633.60 10487.7 1344.4 ## - lstat 1 2978.57 11832.7 1393.5 ## ## Step: AIC=1276.33 ## medv ~ zn + indus + chas + nox + rm + dis + rad + tax + ptratio + ## lstat ## ## Df Sum of Sq RSS AIC ## - indus 1 21.92 8894.4 1275.3 ## &lt;none&gt; 8872.5 1276.3 ## + crim 1 18.36 8854.1 1277.5 ## + age 1 11.19 8861.3 1277.8 ## + black 1 0.13 8872.4 1278.3 ## - tax 1 158.39 9030.9 1281.5 ## - zn 1 180.19 9052.7 1282.5 ## - rad 1 212.19 9084.7 1284.0 ## - chas 1 249.50 9122.0 1285.6 ## - nox 1 689.33 9561.8 1304.8 ## - ptratio 1 873.78 9746.3 1312.6 ## - rm 1 1025.43 9897.9 1318.8 ## - dis 1 1701.37 10573.9 1345.7 ## - lstat 1 2996.77 11869.3 1392.8 ## ## Step: AIC=1275.34 ## medv ~ zn + chas + nox + rm + dis + rad + tax + ptratio + lstat ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 8894.4 1275.3 ## + indus 1 21.92 8872.5 1276.3 ## + crim 1 14.72 8879.7 1276.7 ## + age 1 11.89 8882.5 1276.8 ## + black 1 0.00 8894.4 1277.3 ## - zn 1 206.52 9100.9 1282.7 ## - chas 1 237.50 9131.9 1284.1 ## - tax 1 281.42 9175.8 1286.0 ## - rad 1 293.27 9187.7 1286.5 ## - nox 1 800.54 9695.0 1308.4 ## - ptratio 1 929.05 9823.5 1313.8 ## - rm 1 1083.34 9977.8 1320.1 ## - dis 1 1706.94 10601.4 1344.8 ## - lstat 1 3024.07 11918.5 1392.5 ## ## Call: ## lm(formula = medv ~ zn + chas + nox + rm + dis + rad + tax + ## ptratio + lstat, data = data2) ## ## Coefficients: ## (Intercept) zn chas nox rm dis ## 22.509 1.016 0.759 -2.764 2.176 -3.970 ## rad tax ptratio lstat ## 2.119 -2.317 -1.991 -4.224 # summary(step(model_o,direction = &quot;both&quot;)) 6.3.10 Create a model after selecting variables model_trasf &lt;- lm(formula = medv ~ zn + chas + nox + rm + dis + rad + tax + ptratio + lstat, data = data2) summary(model_trasf) ## ## Call: ## lm(formula = medv ~ zn + chas + nox + rm + dis + rad + tax + ## ptratio + lstat, data = data2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.2923 -2.4690 -0.5086 1.6269 24.5813 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.5090 0.2352 95.718 &lt; 2e-16 *** ## zn 1.0161 0.3347 3.036 0.002554 ** ## chas 0.7590 0.2331 3.256 0.001227 ** ## nox -2.7640 0.4624 -5.978 5.05e-09 *** ## rm 2.1760 0.3129 6.954 1.47e-11 *** ## dis -3.9697 0.4548 -8.729 &lt; 2e-16 *** ## rad 2.1194 0.5858 3.618 0.000335 *** ## tax -2.3171 0.6538 -3.544 0.000441 *** ## ptratio -1.9909 0.3092 -6.440 3.47e-10 *** ## lstat -4.2244 0.3636 -11.618 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.733 on 397 degrees of freedom ## Multiple R-squared: 0.7187, Adjusted R-squared: 0.7123 ## F-statistic: 112.7 on 9 and 397 DF, p-value: &lt; 2.2e-16 6.3.11 Multicollinearity checking vif(model_trasf) ## zn chas nox rm dis rad tax ptratio ## 2.024269 1.044796 4.074241 1.721080 3.759267 6.008751 7.469414 1.743359 ## lstat ## 2.372125 6.3.12 Plot model to check assumptions plot(model_trasf) histogram of residuals resid&lt;- model_trasf$residuals hist(resid) F test of model anova(model_trasf) Df Sum Sq Mean Sq F value Pr(&gt;F) zn 1 4019.0308 4019.03076 179.38831 0e+00 chas 1 1011.5766 1011.57663 45.15144 0e+00 nox 1 2785.8086 2785.80857 124.34378 0e+00 rm 1 8487.0727 8487.07268 378.81810 0e+00 dis 1 951.7774 951.77743 42.48232 0e+00 rad 1 558.8550 558.85503 24.94434 9e-07 tax 1 767.8718 767.87176 34.27374 0e+00 ptratio 1 1119.3945 1119.39453 49.96386 0e+00 lstat 1 3024.0665 3024.06652 134.97836 0e+00 Residuals 397 8894.4215 22.40408 NA NA coefficients coef(summary(model_trasf)) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 22.5089571 0.2351584 95.718273 0.0000000 zn 1.0160975 0.3346703 3.036115 0.0025545 chas 0.7589909 0.2331142 3.255876 0.0012275 nox -2.7639762 0.4623875 -5.977619 0.0000000 rm 2.1760130 0.3129267 6.953747 0.0000000 dis -3.9697228 0.4547944 -8.728609 0.0000000 rad 2.1193795 0.5857829 3.618029 0.0003352 tax -2.3171470 0.6537962 -3.544143 0.0004408 ptratio -1.9908948 0.3091666 -6.439552 0.0000000 lstat -4.2244121 0.3636086 -11.618019 0.0000000 confidence interval confint(model_trasf) 2.5 % 97.5 % (Intercept) 22.0466457 22.971269 zn 0.3581500 1.674045 chas 0.3006983 1.217283 nox -3.6730103 -1.854942 rm 1.5608125 2.791214 dis -4.8638292 -3.075616 rad 0.9677553 3.271004 tax -3.6024824 -1.031812 ptratio -2.5987032 -1.383086 lstat -4.9392512 -3.509573 6.3.13 Add polynomial of quadratic term rm and lstat model_trasf_poly &lt;- lm(formula = medv ~ zn + chas + nox + I(rm^2) + dis + rad + tax + ptratio + I(lstat^2), data = data2) summary(model_trasf_poly) ## ## Call: ## lm(formula = medv ~ zn + chas + nox + I(rm^2) + dis + rad + tax + ## ptratio + I(lstat^2), data = data2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.0736 -3.4029 -0.6212 2.8340 29.4942 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.2859 0.3526 63.212 &lt; 2e-16 *** ## zn 1.5524 0.4096 3.790 0.000174 *** ## chas 0.9570 0.2834 3.377 0.000805 *** ## nox -4.4962 0.5455 -8.242 2.50e-15 *** ## I(rm^2) 1.5313 0.1589 9.637 &lt; 2e-16 *** ## dis -3.3186 0.5644 -5.880 8.69e-09 *** ## rad 3.0529 0.7024 4.346 1.76e-05 *** ## tax -3.6647 0.7861 -4.662 4.28e-06 *** ## ptratio -2.6800 0.3706 -7.232 2.47e-12 *** ## I(lstat^2) -1.3170 0.1918 -6.865 2.56e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.763 on 397 degrees of freedom ## Multiple R-squared: 0.583, Adjusted R-squared: 0.5735 ## F-statistic: 61.67 on 9 and 397 DF, p-value: &lt; 2.2e-16 6.3.14 Add interaction terms rm and lstat R2 &gt;0.7 indicates a good fit of the model model_trasf_term &lt;- lm(formula = medv ~ zn + chas + nox + (rm* lstat) + dis + rad + tax + ptratio , data = data2) summary(model_trasf_term) ## ## Call: ## lm(formula = medv ~ zn + chas + nox + (rm * lstat) + dis + rad + ## tax + ptratio, data = data2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.4595 -2.3458 -0.2389 1.7950 26.6992 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.4297 0.2379 90.060 &lt; 2e-16 *** ## zn 0.5221 0.3046 1.714 0.087296 . ## chas 0.6538 0.2095 3.120 0.001941 ** ## nox -2.0295 0.4218 -4.812 2.13e-06 *** ## rm 1.6459 0.2860 5.754 1.75e-08 *** ## lstat -5.8715 0.3669 -16.002 &lt; 2e-16 *** ## dis -3.1810 0.4161 -7.645 1.60e-13 *** ## rad 1.9251 0.5262 3.658 0.000288 *** ## tax -1.9187 0.5883 -3.261 0.001205 ** ## ptratio -1.5554 0.2811 -5.534 5.70e-08 *** ## rm:lstat -1.8202 0.1852 -9.830 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.249 on 396 degrees of freedom ## Multiple R-squared: 0.7739, Adjusted R-squared: 0.7682 ## F-statistic: 135.5 on 10 and 396 DF, p-value: &lt; 2.2e-16 plot(model_trasf_term) 6.3.15 Robust regression robust_model_term &lt;- rlm(medv ~ zn + chas + nox + (rm* lstat) + dis + rad + tax + ptratio , data = data2) summary(robust_model_term) ## ## Call: rlm(formula = medv ~ zn + chas + nox + (rm * lstat) + dis + rad + ## tax + ptratio, data = data2) ## Residuals: ## Min 1Q Median 3Q Max ## -20.90999 -1.74873 -0.09845 1.92931 33.87244 ## ## Coefficients: ## Value Std. Error t value ## (Intercept) 20.9624 0.1656 126.5763 ## zn 0.1998 0.2120 0.9424 ## chas 0.5198 0.1458 3.5643 ## nox -1.3458 0.2935 -4.5846 ## rm 2.8773 0.1991 14.4524 ## lstat -4.5706 0.2554 -17.8979 ## dis -1.9208 0.2896 -6.6326 ## rad 0.9005 0.3663 2.4586 ## tax -1.7199 0.4095 -4.2004 ## ptratio -1.2451 0.1956 -6.3652 ## rm:lstat -1.9899 0.1289 -15.4398 ## ## Residual standard error: 2.698 on 396 degrees of freedom 6.3.16 Create a model before transforming data model_trasf_orig &lt;- lm(formula = medv ~ zn + chas + nox + rm + dis + rad + tax + ptratio + lstat, data = data) summary(model_trasf_orig) ## ## Call: ## lm(formula = medv ~ zn + chas + nox + rm + dis + rad + tax + ## ptratio + lstat, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.219 -2.729 -0.463 1.920 25.992 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.5094 0.2430 92.619 &lt; 2e-16 *** ## zn 0.8232 0.3754 2.193 0.028904 * ## chas 0.6582 0.2412 2.728 0.006652 ** ## nox -1.9351 0.4727 -4.093 5.15e-05 *** ## rm 2.3985 0.3128 7.668 1.36e-13 *** ## dis -2.9289 0.4565 -6.416 3.99e-10 *** ## rad 2.2109 0.6348 3.483 0.000551 *** ## tax -2.1880 0.6896 -3.173 0.001627 ** ## ptratio -2.0274 0.3254 -6.230 1.19e-09 *** ## lstat -4.3534 0.3844 -11.325 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.895 on 397 degrees of freedom ## Multiple R-squared: 0.7212, Adjusted R-squared: 0.7149 ## F-statistic: 114.1 on 9 and 397 DF, p-value: &lt; 2.2e-16 non nest models comparisons AIC(model_trasf,model_trasf_orig) df AIC model_trasf 11 2432.353 model_trasf_orig 11 2459.780 6.3.17 K-fold cross validation to make sure which model is better # install.packages(&quot;DAAG&quot;) library(DAAG) set.seed(123) model_trasf_term_cv &lt;- glm( medv ~ zn + chas + nox + (rm* lstat) + dis + rad + tax + ptratio , data = data2) cv.err &lt;- cv.glm(data2, model_trasf_term_cv, K = 10)$delta cv.err ## [1] 19.24588 19.15400 6.3.18 Nonnest models comparisons AIC(model_trasf_term,model_trasf,model_trasf_orig,model_trasf_poly) df AIC model_trasf_term 12 2345.492 model_trasf 11 2432.353 model_trasf_orig 11 2459.780 model_trasf_poly 11 2592.584 # interaction, transformation, original, polynomial by order (`data` has been normalized but not `data2`) plot the final model and check the model assumption posterior predictive check, linearity, homogeneity of variance, influential points, colinearity, and normality of residuals # install.packages(&quot;performance&quot;) library(performance) check_model(model_trasf_term) 6.3.19 Create forest plot for coefficients library(sjPlot) plot_model(model_trasf_term, show.values = TRUE, value.offset = 0.4) 6.3.20 Relative Importance provides measures of relative importance library(relaimpo) # calc.relimp(fit,type=c(&quot;lmg&quot;,&quot;last&quot;,&quot;first&quot;,&quot;pratt&quot;), # rela=TRUE) # Bootstrap Measures of Relative Importance (1000 samples) boot &lt;- boot.relimp(model_trasf_term, b = 10, type =c(&quot;lmg&quot; ), rank = TRUE, # type =c(&quot;lmg&quot;,&quot;last&quot;,&quot;first&quot;,&quot;pratt&quot;) diff = TRUE, rela = TRUE) booteval.relimp(boot) # print result ## Response variable: medv ## Total response variance: 77.88147 ## Analysis based on 407 observations ## ## 10 Regressors: ## zn chas nox rm lstat dis rad tax ptratio rm:lstat ## Proportion of variance explained by model: 77.39% ## Metrics are normalized to sum to 100% (rela=TRUE). ## ## Relative importance metrics: ## ## lmg ## zn 0.03220843 ## chas 0.01925573 ## nox 0.05753222 ## rm 0.24841214 ## lstat 0.32582422 ## dis 0.04647715 ## rad 0.03216196 ## tax 0.05940953 ## ptratio 0.09003437 ## rm:lstat 0.08868423 ## ## Average coefficients for different model sizes: ## ## 1X 2Xs 3Xs 4Xs 5Xs 6Xs ## zn 3.1505046 2.1309586 1.5419108 1.1885764 0.9622432 0.8066737 ## chas 1.4317561 1.3828980 1.2922217 1.1827563 1.0710746 0.9666952 ## nox -3.6683140 -2.8590532 -2.3987148 -2.1635246 -2.0628637 -2.0336275 ## rm 5.8715576 5.1812470 4.6686609 4.2214027 3.7902531 3.3584729 ## lstat -6.3961998 -6.2413211 -6.1543204 -6.0962610 -6.0505503 -6.0111059 ## dis 2.3495853 0.6106749 -0.5986185 -1.4554886 -2.0655023 -2.4944977 ## rad -3.2090101 -1.5469225 -0.4505596 0.2850626 0.7884232 1.1408255 ## tax -4.1727995 -3.6061521 -3.1776467 -2.8413999 -2.5683300 -2.3434564 ## ptratio -4.2048171 -3.4233285 -2.9476391 -2.6199136 -2.3633992 -2.1469790 ## rm:lstat -0.7386853 -1.0072327 -1.1975350 -1.3430505 -1.4610987 -1.5602846 ## 7Xs 8Xs 9Xs 10Xs ## zn 0.6942682 0.6122971 0.5558327 0.5221292 ## chas 0.8734919 0.7916003 0.7191770 0.6537613 ## nox -2.0351844 -2.0434312 -2.0444974 -2.0294702 ## rm 2.9235837 2.4886854 2.0599352 1.6458985 ## lstat -5.9756419 -5.9424093 -5.9088868 -5.8714971 ## dis -2.7882468 -2.9825750 -3.1064663 -3.1809633 ## rad 1.3969994 1.5963587 1.7666711 1.9250667 ## tax -2.1625306 -2.0281734 -1.9455332 -1.9186629 ## ptratio -1.9610124 -1.8027959 -1.6694147 -1.5553973 ## rm:lstat -1.6447082 -1.7160362 -1.7745576 -1.8202278 ## ## ## Confidence interval information ( 10 bootstrap replicates, bty= perc ): ## Relative Contributions with confidence intervals: ## ## Lower Upper ## percentage 0.95 0.95 0.95 ## zn.lmg 0.0322 _______HIJ 0.0202 0.0358 ## chas.lmg 0.0193 _____FGHIJ 0.0016 0.0512 ## nox.lmg 0.0575 ____EFG___ 0.0427 0.0707 ## rm.lmg 0.2484 AB________ 0.1929 0.3313 ## lstat.lmg 0.3258 AB________ 0.2627 0.3840 ## dis.lmg 0.0465 _____FGHIJ 0.0287 0.0545 ## rad.lmg 0.0322 ______GHIJ 0.0213 0.0453 ## tax.lmg 0.0594 ___DEFG___ 0.0476 0.0774 ## ptratio.lmg 0.0900 __CD______ 0.0712 0.1166 ## rm:lstat.lmg 0.0887 __CDEF____ 0.0614 0.1114 ## ## Letters indicate the ranks covered by bootstrap CIs. ## (Rank bootstrap confidence intervals always obtained by percentile method) ## CAUTION: Bootstrap confidence intervals can be somewhat liberal. ## ## ## Differences between Relative Contributions: ## ## Lower Upper ## difference 0.95 0.95 0.95 ## zn-chas.lmg 0.0130 -0.0198 0.0270 ## zn-nox.lmg -0.0253 * -0.0505 -0.0125 ## zn-rm.lmg -0.2162 * -0.3057 -0.1571 ## zn-lstat.lmg -0.2936 * -0.3481 -0.2295 ## zn-dis.lmg -0.0143 -0.0246 0.0027 ## zn-rad.lmg 0.0000 -0.0214 0.0087 ## zn-tax.lmg -0.0272 * -0.0520 -0.0173 ## zn-ptratio.lmg -0.0578 * -0.0880 -0.0378 ## zn-rm:lstat.lmg -0.0565 * -0.0828 -0.0315 ## chas-nox.lmg -0.0383 * -0.0584 -0.0019 ## chas-rm.lmg -0.2292 * -0.3173 -0.1765 ## chas-lstat.lmg -0.3066 * -0.3675 -0.2115 ## chas-dis.lmg -0.0272 -0.0377 0.0155 ## chas-rad.lmg -0.0129 -0.0437 0.0215 ## chas-tax.lmg -0.0402 -0.0758 0.0007 ## chas-ptratio.lmg -0.0708 * -0.1150 -0.0250 ## chas-rm:lstat.lmg -0.0694 * -0.1098 -0.0338 ## nox-rm.lmg -0.1909 * -0.2815 -0.1395 ## nox-lstat.lmg -0.2683 * -0.3305 -0.2057 ## nox-dis.lmg 0.0111 -0.0007 0.0275 ## nox-rad.lmg 0.0254 * 0.0047 0.0291 ## nox-tax.lmg -0.0019 -0.0274 0.0065 ## nox-ptratio.lmg -0.0325 * -0.0666 -0.0138 ## nox-rm:lstat.lmg -0.0312 -0.0615 0.0008 ## rm-lstat.lmg -0.0774 -0.1910 0.0509 ## rm-dis.lmg 0.2019 * 0.1388 0.2939 ## rm-rad.lmg 0.2163 * 0.1616 0.3100 ## rm-tax.lmg 0.1890 * 0.1332 0.2837 ## rm-ptratio.lmg 0.1584 * 0.1081 0.2560 ## rm-rm:lstat.lmg 0.1597 * 0.1055 0.2633 ## lstat-dis.lmg 0.2793 * 0.2259 0.3298 ## lstat-rad.lmg 0.2937 * 0.2211 0.3527 ## lstat-tax.lmg 0.2664 * 0.1889 0.3242 ## lstat-ptratio.lmg 0.2358 * 0.1497 0.2991 ## lstat-rm:lstat.lmg 0.2371 * 0.1549 0.3056 ## dis-rad.lmg 0.0143 -0.0084 0.0228 ## dis-tax.lmg -0.0129 * -0.0406 -0.0044 ## dis-ptratio.lmg -0.0436 * -0.0797 -0.0307 ## dis-rm:lstat.lmg -0.0422 * -0.0746 -0.0069 ## rad-tax.lmg -0.0272 * -0.0321 -0.0204 ## rad-ptratio.lmg -0.0579 * -0.0713 -0.0377 ## rad-rm:lstat.lmg -0.0565 * -0.0718 -0.0258 ## tax-ptratio.lmg -0.0306 * -0.0473 -0.0137 ## tax-rm:lstat.lmg -0.0293 -0.0457 0.0021 ## ptratio-rm:lstat.lmg 0.0014 -0.0253 0.0301 ## ## * indicates that CI for difference does not include 0. ## CAUTION: Bootstrap confidence intervals can be somewhat liberal. plot(booteval.relimp(boot,sort=TRUE)) # plot result 6.3.21 Model prediction prediction ci and mean ci library(dplyr) data_pred &lt;- dplyr::select(data2 , zn , chas , nox , rm , dis , rad , tax , ptratio , lstat) data_pred[1:10,] zn chas nox rm dis rad tax ptratio lstat 1 0.2845483 -0.2723291 -0.1440749 0.4132629 0.2785465 -0.9818712 -0.6659492 -1.4575580 -1.0744990 2 -0.4872402 -0.2723291 -0.7395304 0.1940824 0.6786919 -0.8670245 -0.9863534 -0.3027945 -0.4919525 4 -0.4872402 -0.2723291 -0.8344581 1.0152978 1.1314532 -0.7521778 -1.1050216 0.1129203 -1.3601708 5 -0.4872402 -0.2723291 -0.8344581 1.2273620 1.1314532 -0.7521778 -1.1050216 0.1129203 -1.0254866 6 -0.4872402 -0.2723291 -0.8344581 0.2068916 1.1314532 -0.7521778 -1.1050216 0.1129203 -1.0422909 7 0.0487240 -0.2723291 -0.2648919 -0.3880270 0.9295961 -0.5224844 -0.5769480 -1.5037485 -0.0312367 8 0.0487240 -0.2723291 -0.2648919 -0.1603069 1.0872565 -0.5224844 -0.5769480 -1.5037485 0.9097999 9 0.0487240 -0.2723291 -0.2648919 -0.9302853 1.1392843 -0.5224844 -0.5769480 -1.5037485 2.4193794 10 0.0487240 -0.2723291 -0.2648919 -0.3994130 1.3357787 -0.5224844 -0.5769480 -1.5037485 0.6227277 11 0.0487240 -0.2723291 -0.2648919 0.1314594 1.2422167 -0.5224844 -0.5769480 -1.5037485 1.0918456 predy &lt;- predict(model_trasf_term, data_pred[1:10,], interval=&quot;predict&quot;) predy fit lwr upr 1 30.258620 21.8534712 38.66377 2 24.415330 16.0263022 32.80436 4 31.759215 23.3073401 40.21109 5 29.920476 21.4789966 38.36196 6 26.441050 18.0254500 34.85665 7 20.820462 12.3963513 29.24457 8 15.455998 6.9807773 23.93122 9 8.991037 0.4048231 17.57725 10 16.144719 7.6745761 24.61486 11 13.847673 5.3305253 22.36482 confy &lt;- predict(model_trasf_term, data_pred[1:10,], interval=&quot;confidence&quot;) confy fit lwr upr 1 30.258620 29.329961 31.18728 2 24.415330 23.646134 25.18452 4 31.759215 30.474666 33.04376 5 29.920476 28.706202 31.13475 6 26.441050 25.422125 27.45998 7 20.820462 19.733484 21.90744 8 15.455998 14.025877 16.88612 9 8.991037 7.006358 10.97572 10 16.144719 14.745003 17.54443 11 13.847673 12.187044 15.50830 ci width (the lower and upper bonds) confy %*% c(0, -1, 1) 1 1.857318 2 1.538391 4 2.569097 5 2.428550 6 2.037850 7 2.173956 8 2.860241 9 3.969358 10 2.799431 11 3.321259 predy %*% c(0, -1, 1) 1 16.81030 2 16.77805 4 16.90375 5 16.88296 6 16.83120 7 16.84822 8 16.95044 9 17.17243 10 16.94029 11 17.03430 plot the differences of predictions and actual values plot(data2$medv,predict(model_trasf_term) ) fit &lt;- lm(predict(model_trasf_term)~data2$medv) abline(fit) compute y hat and confidence interval data_ci &lt;- dplyr::select(data2, zn ,chas ,tax , medv) model_ci &lt;- lm(formula = medv ~ zn + chas +tax , data = data_ci) summary(model_ci) ## ## Call: ## lm(formula = medv ~ zn + chas + tax, data = data_ci) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.182 -4.715 -1.305 2.345 34.377 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.3073 0.3699 60.307 &lt; 2e-16 *** ## zn 2.1080 0.3919 5.379 1.27e-07 *** ## chas 1.4174 0.3599 3.939 9.66e-05 *** ## tax -3.4463 0.3985 -8.649 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.46 on 403 degrees of freedom ## Multiple R-squared: 0.2907, Adjusted R-squared: 0.2855 ## F-statistic: 55.07 on 3 and 403 DF, p-value: &lt; 2.2e-16 compute y hat and compare with y predict and actual y XCI &lt;- data.frame(intercept=1, data_ci[,1:3]) comp_y &lt;- as.matrix(XCI)%*%as.numeric(model_ci$coefficients) head(cbind(comp_y,predict(model_ci, XCI), data_ci[,4])) 1 24.81617 24.81617 24.0 2 24.29345 24.29345 21.6 4 24.70242 24.70242 33.4 5 24.70242 24.70242 36.2 6 24.70242 24.70242 28.7 7 24.01233 24.01233 22.9 compute ci library(matlib) ## ## Attaching package: &#39;matlib&#39; ## The following object is masked from &#39;package:psych&#39;: ## ## tr var.yhat &lt;- sigma(model_ci)**2* as.matrix(XCI[1 ,]) %*% inv(t(as.matrix(XCI)) %*% as.matrix (XCI))%*%t(as.matrix(XCI[1 ,])) # var.yhat cbind( (predict(model_ci, XCI[1 ,])-1.96 * sqrt(var.yhat)), (predict(model_ci, XCI[1 ,]) ), (predict(model_ci, XCI[1 ,])+1.96 * sqrt(var.yhat)) ) 1 1 23.91998 24.81617 25.71237 predict(model_ci, XCI[1 ,], interval=&quot;confidence&quot;) fit lwr upr 24.81617 23.9173 25.71505 \\[ E(\\hat{Y}_0)=\\sigma^2\\mathbf{X_0\\left( X&#39;X\\right)^{-1}X_0&#39;} \\] 6.3.22 External data validation this is not ture external validation (just for demonstration) Split the data randomly into a training set and a test set by using caret package. library(caret) testdata2_pred &lt;- dplyr::select(testdata2, zn , chas , nox , rm , dis , rad , tax , ptratio , lstat) R_sq &lt;- R2(testdata2$medv,predict(model_trasf_term,testdata2_pred)) RMSE &lt;- RMSE(testdata2$medv,predict(model_trasf_term,testdata2_pred)) MAE &lt;- MAE(testdata2$medv,predict(model_trasf_term,testdata2_pred)) print(c(R_sq, RMSE, MAE)) ## [1] 0.8465718 4.1960036 3.4352611 6.3.23 To do Models comparisons in further and adjusting the final model by a loop 6.4 Linear mixed model theory 6.4.1 Matrix format Linear mixed models is to analyze data that are independent, longitudinal, repeated, multilevel. LMMs allows both fixed and random effects. \\[ \\mathbf{y} = \\boldsymbol{X\\beta} + \\boldsymbol{Zu} + \\boldsymbol{\\varepsilon} \\] Where y is a Nx1 column vector for the dependent variable; x is a Nxr design matrix of independent variables ;\\(\\beta\\) is a rx1 column vector of the fixed-effects regression coefficients ; z is the Nxm design matrix for the random effects ; u is a mx1 vector of random effects ; and e is a Nx1 column vector of the residuals. \\(xb\\) is the fixed effect and \\(zu\\) is the random effect. \\[ \\overbrace{\\mathbf{y}}^{\\mbox{N x 1}} \\quad = \\quad \\overbrace{\\underbrace{\\mathbf{X}}_{\\mbox{N x r}} \\quad \\underbrace{\\boldsymbol{\\beta}}_{\\mbox{r x 1}}}^{\\mbox{N x 1}} \\quad + \\quad \\overbrace{\\underbrace{\\mathbf{Z}}_{\\mbox{N x m}} \\quad \\underbrace{\\boldsymbol{u}}_{\\mbox{m x 1}}}^{\\mbox{N x 1}} \\quad + \\quad \\overbrace{\\boldsymbol{\\varepsilon}}^{\\mbox{N x 1}} \\] 6.4.2 Exmaple: reducing the work stress of nurses For a study, it is to evaluate whether an intervention reduces the work stress for nurses, 1000 nurses were investigated from 25 hospitals. y is the work stress score, and independent variables are age, gender, age of experience, ward type, and intervention. there, only random intercepts (hospital) is considered. \\[ \\overbrace{\\mathbf{y}}^{ 1000 \\times 1} \\quad = \\quad \\overbrace{\\underbrace{\\mathbf{X}}_{ 1000 \\times 5} \\quad \\underbrace{\\boldsymbol{\\beta}}_{5 \\times 1}}^{ 1000 \\times 1} \\quad + \\quad \\overbrace{\\underbrace{\\mathbf{Z}}_{ 1000 \\times 25} \\quad \\underbrace{\\boldsymbol{u}}_{ 25 \\times 1}}^{ 1000 \\times 1} \\quad + \\quad \\overbrace{\\boldsymbol{\\varepsilon}}^{ 1000 \\times 1} \\] 6.4.3 Dependent variable, \\(y\\) \\[ \\mathbf{y} = \\left[ \\begin{array}{l} \\text{stress} \\\\ 7 \\\\ 7 \\\\ \\ldots \\\\ 6 \\end{array} \\right] \\begin{array}{l} n_{ij} \\\\ 1 \\\\ 2 \\\\ \\ldots \\\\ 1000 \\end{array} \\] 6.4.4 Independent variables (fixed effects), \\(Xs\\) \\[ \\quad \\mathbf{X} = \\left[ \\begin{array}{llllll} \\text{Intercept} &amp; \\text{Age} &amp; \\text{Gender} &amp; \\text{Ward type} &amp; \\text{Age of exper} &amp; \\text{Intervention} \\\\ 1 &amp; 34.97 &amp; 0 &amp; 1 &amp; 60 &amp; 1 \\\\ 1 &amp; 33.92 &amp; 0 &amp; 0 &amp; 67 &amp; 1 \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ 1 &amp; 26.07 &amp; 0 &amp; 1 &amp; 64 &amp; 0 \\\\ \\end{array} \\right] \\begin{array}{l} n_{ij} \\\\ 1 \\\\ 2 \\\\ \\ldots \\\\ 1000 \\end{array} \\] 6.4.5 Fixed effect coefficients, \\(\\hat{\\beta}\\) \\[ \\boldsymbol{\\hat{\\beta}} = \\left[ \\begin{array}{l} \\text{beta} \\\\ 72.1 \\\\ 135.6 \\\\ \\ldots \\\\ 37.8 \\end{array} \\right] \\begin{array}{l} n_{} \\\\ intercept \\\\ 1 \\\\ \\ldots \\\\ 5 \\end{array} \\] 6.4.5.1 How to solve \\(\\hat{\\beta}\\) minimize \\(\\ell\\) the likelihood function by letting the derivation = 0 \\[\\ell=-\\log L\\] \\[ \\begin{aligned} \\ell(\\mathbf{y}, \\beta, \\gamma) &amp;=\\frac{1}{2}\\left\\{n \\log (2 \\pi)+\\log |\\mathbf{V}(\\gamma)|+(\\mathbf{y}-\\mathbf{X} \\beta)^{\\prime}(\\mathbf{V}(\\gamma))^{-1}(\\mathbf{y}-\\mathbf{X} \\beta)\\right\\} \\\\ &amp; \\propto \\frac{1}{2}\\left\\{\\log |\\mathbf{V}(\\gamma)|+(\\mathbf{y}-\\mathbf{X} \\beta)^{\\prime}(\\mathbf{V}(\\gamma))^{-1}(\\mathbf{y}-\\mathbf{X} \\beta)\\right\\} \\end{aligned} \\] or (using restricted likelihood for adjust) \\[ \\frac{1}{2}\\left\\{\\log |\\mathbf{V}(\\gamma)|+(\\mathbf{y}-\\mathbf{X} \\beta)^{\\prime}(\\mathbf{V}(\\gamma))^{-1}(\\mathbf{y}-\\mathbf{X} \\beta)+\\log \\left|\\mathbf{X}^{\\prime}(\\mathbf{V}(\\gamma))^{-1} \\mathbf{X}\\right|\\right\\} \\] where \\(\\gamma\\) derived from \\(R=\\sigma^{2} \\mathbf{I}\\). \\[ (\\hat{\\beta}, \\hat{\\gamma})=\\underset{(\\beta, \\gamma)}{\\operatorname{argmin}} \\ell(\\mathbf{y}, \\beta, \\gamma) \\] minimization by three steps The estimate of the fixed effect parameters \\(\\beta\\) is expressed as a function of the random effect parameters \\(\\gamma\\), \\[ \\hat{\\beta}(\\gamma) = \\left(\\mathbf{X}^{\\prime}(\\mathbf{V}(\\gamma))^{-1} \\mathbf{X}\\right)^{-1}\\mathbf{X}^{\\prime}(\\mathbf{V}(\\gamma))^{-1} \\mathbf{y} \\] Minimizing \\(\\ell(\\mathbf{y}, \\hat{\\beta}(\\gamma), \\gamma)\\) as a function of \\(\\gamma\\) to calculate the random effect parameters. \\[\\ell(\\mathbf{y}, \\hat{\\beta}(\\gamma), \\gamma)\\] Go back to solve the fixed effect parameters \\(\\hat{\\beta}=\\hat{\\beta}(\\hat{\\gamma})\\). 6.4.6 Variance- covariance matrix of \\(\\hat{\\beta}\\) \\[ \\boldsymbol{\\hat{\\beta}} \\sim \\mathcal{N}(\\mathbf{\\beta}, \\mathbf{\\sigma^2_{\\beta}}) \\] where \\(\\sigma^2_{\\hat{\\beta}}\\) is a rxr (6x6) variance-covariance matrix of the fixed effects \\(\\boldsymbol{\\hat{\\beta}}\\). therefore \\[ \\quad \\mathbf{\\sigma^2_{\\hat{\\beta}}} = \\left[ \\begin{array}{llllll} \\text{_1} &amp; \\text{_2} &amp; \\text{_3} &amp; \\text{... } &amp; \\text{_5} \\\\ \\sigma^2_{1,1} &amp; \\sigma^2_{1,2} &amp; ... &amp; ... &amp; ... \\\\ \\sigma^2_{2,1} &amp; \\sigma^2_{2,2} &amp; ... &amp; ... &amp; ... \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ ... &amp; ... &amp; \\sigma^2_{3,3} &amp; ... &amp; ... \\\\ \\\\ ... &amp; ... &amp; ... &amp; \\sigma^2_{...} &amp; ... \\\\ ... &amp; ... &amp; ... &amp; ... &amp; \\sigma^2_{5,5} \\\\ \\end{array} \\right] \\begin{array}{l} n_{ij} \\\\ 1 \\\\ 2 \\\\ ... \\\\ 4 \\\\ 5 \\end{array} \\] 6.4.7 Random effects, \\(Z\\) \\[ \\quad \\mathbf{Z} = \\left[ \\begin{array}{llllll} \\text{h_1} &amp; \\text{h_2} &amp; \\text{h_3} &amp; \\text{... } &amp; \\text{h_24} &amp; \\text{h_25} \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{array} \\right] \\begin{array}{l} n_{ij} \\\\ 1 \\\\ 2 \\\\ ... \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ \\ldots \\\\ 1000 \\end{array} \\] h_ represents hospital 6.4.8 Random effects coefficients, \\(u\\) \\[ \\boldsymbol{u} = \\left[ \\begin{array}{l} \\boldsymbol{u} \\\\ 7.1 \\\\ 15.6 \\\\ \\ldots \\\\ 33.8 \\end{array} \\right] \\begin{array}{l} n_{} \\\\ 1 \\\\ 2 \\\\ \\ldots \\\\ 25 \\end{array} \\] if there are two random effect variables, a intercept and a slope \\[ \\boldsymbol{u} = \\left[ \\begin{array}{l} \\boldsymbol{u} \\\\ 7.1 \\\\ 15.6 \\\\ \\ldots \\\\ 33.8 \\\\ slope \\end{array} \\right] \\begin{array}{l} n_{} \\\\ 1 \\\\ 2 \\\\ \\ldots \\\\ 25\\\\ ... \\end{array} \\] How to calculate \\(\\hat{\\mathbf{u}}\\) \\[ \\hat{\\mathbf{u}}=\\mathrm{G}\\mathrm{Z}^{\\prime} \\mathbf{V}^{-1}(\\mathbf{y}-\\mathbf{X} \\hat{\\beta}) \\] e.g. (with one random intercept) \\[ \\begin{aligned} \\hat{\\mathbf{u}}_{i} &amp;=\\left(\\sigma_{B}^{2}, \\sigma_{B}^{2}\\right) U^{-1}\\left(\\begin{array}{l} y_{i 1}-\\bar{y} \\\\ y_{i 2}-\\bar{y} \\end{array}\\right) \\\\ &amp;=\\left(\\sigma^{2} \\sigma_{B}^{2}, \\sigma^{2} \\sigma_{B}^{2}\\right)\\left(\\begin{array}{l} y_{i 1}-\\bar{y} \\\\ y_{i 2}-\\bar{y} \\end{array}\\right) / \\sigma^{2}\\left(\\sigma^{2}+2 \\sigma_{B}^{2}\\right) \\\\ &amp;=\\frac{\\overline{y_{i}}-\\bar{y}}{\\frac{\\sigma^{2}}{2 \\sigma_{B}^{2}}+1} \\end{aligned} \\] 6.4.9 Variance-covariance matrix of the random effects, \\(\\boldsymbol{u}\\) since \\[ \\boldsymbol{u} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{G}) \\] where G is a mxm (25x25) diagonal variance-covariance matrix of the random effects \\(\\boldsymbol{u}\\). therefore \\[ \\quad \\mathbf{G} = \\left[ \\begin{array}{llllll} \\text{_1} &amp; \\text{_2} &amp; \\text{_3} &amp; \\text{... } &amp; \\text{_25} &amp; \\text{_25} \\\\ \\sigma^2_{1,1} &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... \\\\ ... &amp; \\sigma^2_{2,2} &amp; ... &amp; ... &amp; ... &amp; ... \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ ... &amp; ... &amp; \\sigma^2_{3,3} &amp; ... &amp; ... &amp; ... \\\\ \\\\ ... &amp; ... &amp; ... &amp; \\sigma^2_{...} &amp; ... &amp; ... \\\\ ... &amp; ... &amp; ... &amp; ... &amp; \\sigma^2_{24,24} &amp; ... \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; \\sigma^2_{25,25} \\\\ \\end{array} \\right] \\begin{array}{l} n_{ij} \\\\ 1 \\\\ 2 \\\\ ... \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ \\ldots \\\\ 25 \\end{array} \\] Typically G has a very simple diagonal structure. (random intercept) \\[ \\begin{bmatrix} \\sigma^{2}_{u} \\end{bmatrix}=\\mathbf{\\boldsymbol{I\\sigma^2_{u}}} = \\left[ \\begin{array}{llllll} \\text{_1} &amp; \\text{_2} &amp; \\text{_3} &amp; \\text{... } &amp; \\text{_24} &amp; \\text{_25} \\\\ \\sigma^2_{u} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2_{u} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ 0 &amp; 0 &amp; \\sigma^2_{u} &amp; 0 &amp; 0 &amp; 0 \\\\ \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2_{u} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^2_{u} &amp; 0 \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^2_{u} \\\\ \\end{array} \\right] \\begin{array}{l} n_{ij} \\\\ 1 \\\\ 2 \\\\ ... \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ \\ldots \\\\ 25 \\end{array} \\] we know \\[ \\mathbf{G} = \\begin{bmatrix} \\sigma^{2}_{int} \\end{bmatrix}= \\begin{bmatrix} \\sigma^{2}_{u} \\end{bmatrix} \\] if there are two random effect variables, a intercept and a slope \\[ \\mathbf{G} = \\begin{bmatrix} \\sigma^{2}_{int} &amp; \\sigma^{2}_{int,slope} \\\\ \\sigma^{2}_{int,slope} &amp; \\sigma^{2}_{slope} \\end{bmatrix} \\] 6.4.10 How to solve \\(\\sigma^{2}\\) and \\(\\sigma_{u(B)}^{2}\\) we know \\[ \\mathbf{\\mu} = E(\\boldsymbol{X\\beta} + \\boldsymbol{Zu} + \\boldsymbol{\\varepsilon}) =\\boldsymbol{X\\beta} \\quad \\text{other terms have mean zero} \\ \\] \\[ \\begin{aligned} \\mathbf{V} &amp;=\\operatorname{var}(\\mathbf{y})=\\operatorname{var}(\\mathbf{X} \\beta+\\mathbf{Z} \\mathbf{u}+\\varepsilon) \\quad \\text{[from model ]}\\\\ &amp;=\\operatorname{var}(\\mathbf{X} \\beta)+\\operatorname{var}(\\mathbf{Z u})+\\operatorname{var}(\\varepsilon) \\quad \\text{[all terms are independent]}\\\\ &amp;=\\operatorname{var}(\\mathbf{Z u})+\\operatorname{var}(\\varepsilon) \\quad \\text{[variance of fixed effects is zero]}\\\\ &amp;=Z \\operatorname{var}(\\mathbf{u}) Z^{\\prime}+\\operatorname{var}(\\varepsilon) \\quad \\text{[ Z is constant ]}\\\\ &amp;=\\mathrm{ZGZ}^{\\prime}+\\mathrm{R} \\quad \\text{[from model ]}\\\\ \\end{aligned} \\] e.g. the covariance matrix \\(\\mathbf{V}\\) can be calculated in some conditions (with 3 random coefficients). \\[ \\begin{aligned} \\mathbf{V}=&amp; \\operatorname{var}(\\mathbf{y})=\\mathbf{Z G Z}^{\\prime}+\\sigma^{2} \\mathbf{I} \\\\ =&amp;\\left(\\begin{array}{llllll} \\sigma^{2}+\\sigma_{B}^{2} &amp; \\sigma_{B}^{2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\sigma_{B}^{2} &amp; \\sigma^{2}+\\sigma_{B}^{2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^{2}+\\sigma_{B}^{2} &amp; \\sigma_{B}^{2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{B}^{2} &amp; \\sigma^{2}+\\sigma_{B}^{2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^{2}+\\sigma_{B}^{2} &amp; \\sigma_{B}^{2} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_{B}^{2} &amp; \\sigma^{2}+\\sigma_{B}^{2} \\end{array}\\right) \\end{aligned} \\] Remark, \\[ \\operatorname{var}(\\mathbf{A x})=\\operatorname{Avar}(\\mathbf{x}) \\mathbf{A}^{\\prime} \\] Remark, \\[ \\operatorname{var}(\\mathbf{x}+\\mathbf{y})=\\operatorname{var}(\\mathbf{x})+\\operatorname{var}(\\mathbf{y}) \\] generally \\[ \\begin{align} Var(X+Y) &amp;= Cov(X+Y,X+Y) \\\\ &amp;= E((X+Y)^2)-E(X+Y)E(X+Y) \\\\ &amp;\\text{by expanding,} \\\\ &amp;= E(X^2) - (E(X))^2 + E(Y^2) - (E(Y))^2 + 2(E(XY) - E(X)E(Y)) \\\\ &amp;= Var(X) + Var(Y) + 2(E(XY)) - E(X)E(Y)) \\\\ &amp;= Var(X) + Var(Y) + 2Cov(X,Y) \\end{align} \\] therefore, \\[ \\begin{aligned} \\log |\\mathbf{V}(\\gamma)| &amp;=3 \\log \\left(\\sigma^{2}\\right)+3 \\log \\left(\\sigma^{2}+2 \\sigma_{B}^{2}\\right) \\\\ \\log \\left|\\mathbf{X}^{\\prime}(\\mathbf{V}(\\gamma))^{-1} \\mathbf{X}\\right| &amp;=\\log (n)-\\log \\left(\\sigma^{2}+2 \\sigma_{B}^{2}\\right) \\\\ (\\mathbf{y}-\\mathbf{X} \\beta)^{\\prime}(\\mathbf{V}(\\gamma))^{-1}(\\mathbf{y}-\\mathbf{X} \\beta)=&amp; \\frac{1}{\\sigma^{2}\\left(\\sigma^{2}+2 \\sigma_{B}^{2}\\right)} \\times \\\\ &amp;\\left(\\left(\\sigma^{2}+\\sigma_{B}^{2}\\right) \\sum_{i=1}^{3} \\sum_{j=1}^{2}\\left(y_{i j}-\\mu\\right)^{2}-2 \\sigma_{B}^{2} \\sum_{i=1}^{3}\\left(y_{i 1}-\\mu\\right)\\left(y_{i 2}-\\mu\\right)\\right) \\end{aligned} \\] Here, \\(r\\) represents \\(\\sigma_{B}^{2}\\) and \\(\\sigma^{2}\\) Using profile likelihood \\[ \\begin{aligned} \\ell_{R E}\\left(\\mu, \\sigma^{2}, \\sigma_{B}^{2}\\right) \\propto &amp; 3 \\log \\left(\\sigma^{2}\\right)+3 \\log \\left(\\sigma^{2}+2 \\sigma_{B}^{2}\\right)-\\log \\left(\\sigma^{2}+2 \\sigma_{B}^{2}\\right) \\\\ &amp;+\\frac{1}{\\sigma^{2}\\left(\\sigma^{2}+2 \\sigma_{B}^{2}\\right)}\\left(\\left(\\sigma^{2}+\\sigma_{B}^{2}\\right) \\sum_{i=1}^{3} \\sum_{j=1}^{2}\\left(y_{i j}-\\mu\\right)^{2}-2 \\sigma_{B}^{2} \\sum_{i=1}^{3}\\left(y_{i 1}-\\mu\\right)\\left(y_{i 2}-\\mu\\right)\\right) \\end{aligned} \\] then take the derivative with respect to the two variance parameters, equate with zero and solve. \\[ \\begin{aligned} \\hat{\\sigma}^{2} &amp;=M S E \\\\ \\hat{\\sigma}_{B}^{2} &amp;=\\frac{M S E-M S_{B}}{2} \\end{aligned} \\] the REML approach gives the same results as the moments method variance of random effect (in level) &lt; mean square of errors &lt; mean square between levels 6.4.11 Residuals (errors) and their variance- covariance matrix \\[ \\boldsymbol{\\varepsilon} = \\left[ \\begin{array}{l} \\boldsymbol{\\varepsilon} \\\\ \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\ldots \\\\ \\varepsilon_{1000} \\end{array} \\right] \\begin{array}{l} n_{} \\\\ 1 \\\\ 2 \\\\ \\ldots \\\\ 1000 \\end{array} \\] which is independent and identically distributed \\[ \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\boldsymbol{I\\sigma^2_{\\varepsilon}}}) \\] where \\[ \\mathbf{R}=\\mathbf{\\boldsymbol{I\\sigma^2_{\\varepsilon}}} = \\left[ \\begin{array}{llllll} \\text{_1} &amp; \\text{_2} &amp; \\text{_3} &amp; \\text{... } &amp; \\text{_999} &amp; \\text{_1000} \\\\ \\sigma^2_{\\varepsilon} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2_{\\varepsilon} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ 0 &amp; 0 &amp; \\sigma^2_{\\varepsilon} &amp; 0 &amp; 0 &amp; 0 \\\\ \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2_{\\varepsilon} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^2_{\\varepsilon} &amp; 0 \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^2_{\\varepsilon} \\\\ \\end{array} \\right] \\begin{array}{l} n_{ij} \\\\ 1 \\\\ 2 \\\\ ... \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ \\ldots \\\\ 1000 \\end{array} \\] 6.4.12 Estimated parameters So the final estimated elements are \\(\\beta\\), \\(u\\), and \\(R\\). The final distribution of the model is: \\[ (\\mathbf{y} | \\boldsymbol{\\beta}; \\boldsymbol{u} = u) \\sim \\mathcal{N}(\\boldsymbol{X\\beta} + \\boldsymbol{Z}u, \\mathbf{R}) \\] we can not estimate \\(\\varepsilon\\) but its variance \\(\\sigma^2_{\\varepsilon}\\quad or \\quad R\\). 6.4.13 LMM equation \\[ Y_{ij} = (\\gamma_{00} + u_{0j}) + \\gamma_{10}Age_{ij} + \\gamma_{20}Married_{ij} + \\gamma_{30}SEX_{ij} + \\gamma_{40}WBC_{ij} + \\gamma_{50}RBC_{ij} + e_{ij} \\] In this equation for the i-th patient for the j-th doctor, a specific intercept is given to a particular doctor. 6.4.14 Testing of models (also suitable for fixed effect coefficients) test \\(A \\rightarrow B\\) if \\(B\\) is a sub-model of \\(A\\). \\[ G_{A \\rightarrow B}=2 \\ell_{r e}^{(B)}-2 \\ell_{r e}^{(A)} \\] where \\(G_{A \\rightarrow B}\\) is asymptotically \\(\\chi_{f}^{2}\\) distributed, k=number of tested parameters. Additionally, can choose the smallest AIC or BIC when non nested models \\[ \\begin{aligned} &amp;\\mathrm{AIC}=-2 \\cdot \\log L(\\hat{\\theta_i})+2 \\cdot p \\\\ &amp;\\mathrm{BIC}=-2 \\cdot \\log L(\\hat{\\theta_i})+\\log (n) p \\end{aligned} \\] p is the number of parameters 6.4.15 Testing of fixed effect coefficients (only ML) A linear hypothesis \\[ \\mathbf{L}^{\\prime} \\beta=c \\] e.g.  \\[ \\underbrace{\\left(\\begin{array}{llll} 0 &amp; 1 &amp; 0 &amp; 0 \\end{array}\\right)}_{\\mathbf{L}^{\\prime}}\\left(\\begin{array}{c} \\mu \\\\ \\alpha_{1} \\\\ \\alpha_{2} \\\\ \\alpha_{3} \\end{array}\\right)=0 \\] we know \\[ \\mathbf{L}^{\\prime} \\hat{\\beta}=\\mathbf{L}^{\\prime}\\left(\\mathbf{X}^{\\prime} \\mathbf{V}^{-1} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{V}^{-1} \\mathbf{y} \\] \\[ \\mathbf{L}^{\\prime} \\hat{\\beta} \\sim N\\left(\\mathbf{L}^{\\prime} \\beta, \\mathbf{L}^{\\prime}\\left(\\mathbf{X}^{\\prime} \\mathbf{V}^{-1} \\mathbf{X}\\right)^{-1} \\mathbf{L}\\right) \\] \\[ \\left(\\mathbf{L}^{\\prime} \\hat{\\beta}-c\\right) \\sim N\\left(0, \\mathbf{L}^{\\prime}\\left(\\mathbf{X}^{\\prime} \\mathbf{V}^{-1} \\mathbf{X}\\right)^{-1} \\mathbf{L}\\right) \\] therefore, using Wald test \\[ W=\\left(\\mathbf{L}^{\\prime} \\hat{\\beta}-c\\right)^{\\prime}\\left(\\mathbf{L}^{\\prime}\\left(\\mathbf{X}^{\\prime} \\mathbf{V}^{-1} \\mathbf{X}\\right)^{-1} \\mathbf{L}\\right)^{-1}\\left(\\mathbf{L}^{\\prime} \\hat{\\beta}-c\\right)^{\\prime} \\] \\(W\\) is approximately \\(\\chi_{\\operatorname{rank}(\\mathrm{L})^{2}}\\) distributed. But, better approximation: Wald F–test (&amp; Satterthwaite’s) 6.4.16 The \\(95 \\%\\) confidence interval of fixed effect coefficients \\[ \\mathbf{}^{} \\hat{\\beta} \\pm t_{0.975, d f} \\sqrt{\\mathbf{}\\left(\\mathbf{X}^{\\prime} \\mathbf{V}^{-1} \\mathbf{X}\\right)^{-1} \\mathbf{}} \\] 6.4.17 Testing of random effect coefficients (typically variances, REML) test \\(A \\rightarrow B\\) if \\(B\\) is a sub-model of \\(A\\). \\[ G_{A \\rightarrow B}=2 \\ell_{r e}^{(B)}-2 \\ell_{r e}^{(A)} \\] where \\(G_{A \\rightarrow B}\\) is asymptotically \\(\\chi_{0}^{2}\\) and \\(\\chi_{1}^{2}\\) mixture distribution. therefore, p value is \\(P(\\chi_{1}^{2}&gt;=G_{A \\rightarrow B}) \\quad /2\\) 6.4.18 Confidence intervals for random effects parameters (variances) Use a \\(\\chi^{2}\\)-based (Satterthwaite) approximation approach. assuming asymptotically \\[\\quad \\hat{\\sigma}_{b}^{2} \\sim \\frac{\\sigma^{2}}{d f} \\chi_{d f}^{2}\\] The \\(95 \\%\\) confidence interval \\[\\frac{d f \\hat{\\sigma}_{b}^{2}}{\\chi_{0.025 ; d f}^{2}}&lt;\\sigma_{b}^{2}&lt;\\frac{d f \\hat{\\sigma}_{b}^{2}}{\\chi_{0.975 ; d f}^{2}}\\] \\(d f\\) \\[ \\hat{d f}=\\frac{2 \\hat{\\sigma}_{b}^{4}}{\\operatorname{var}\\left(\\hat{\\sigma}_{b}^{2}\\right)} \\] ### Specified covariance structures, 12 lecture) mixed model can be expressed as \\[ \\mathbf{y} \\sim N\\left(\\mathbf{X} \\beta, \\mathbf{Z G Z}^{\\prime}+\\mathbf{R}\\right), \\] The total covariance of all observations \\[ \\mathbf{V}=\\mathbf{Z G Z}{ }^{\\prime}+\\mathbf{R} \\] The ZGZ’ part is specified through the random effects of the model. for the \\(\\mathbf{R}\\) part, we will put some structure into \\(\\mathrm{R}\\) the structure known from the random effects model #### compound symmetry \\(\\operatorname{cov}\\left(y_{i_{1}}, y_{i_{2}}\\right)= \\begin{cases}0 &amp; , \\text { if individual } i_{1} \\neq \\text { individual }_{i_{2}} \\text { and } i_{1} \\neq i_{2} \\\\ \\sigma_{\\text {individual }}^{2} &amp; , \\text { if individual } i_{1}=\\text { individual }_{i_{2}} \\text { and } i_{1} \\neq i_{2} \\\\ \\sigma_{\\text {individual }}^{2}+\\sigma^{2} &amp; , \\text { if } i_{1}=i_{2}\\end{cases}\\) 6.4.18.1 Guassian model of spatial correlation depending on “how far” observations are apart are known as spatial. \\[ V_{i_{1}, i_{2}}= \\begin{cases}0 &amp; , \\text { if individual } i_{1} \\neq \\text { individual } i_{2} \\text { and } i_{1} \\neq i_{2} \\\\ \\nu^{2}+\\tau^{2} \\exp \\left\\{\\frac{-\\left(t_{i_{1}}-t_{i_{2}}\\right)^{2}}{\\rho^{2}}\\right\\} &amp; , \\text { if individual } i_{1}=\\text { individual } i_{2} \\text { and } i_{1} \\neq i_{2} \\\\ \\nu^{2}+\\tau^{2}+\\sigma^{2} &amp; , \\text { if } i_{1}=i_{2}\\end{cases} \\] #### \\(\\mathrm{R}\\) has several build-in correlation structures: \\[ \\begin{array}{ccc} \\text { Write in R } &amp; \\text { Name } &amp; \\text { Correlation term } \\\\ \\hline \\text { corGaus } &amp; \\text { Gaussian } &amp; \\tau^{2} \\exp \\left\\{\\frac{-\\left(t_{i_{1}}-t_{i_{2}}\\right)^{2}}{\\rho^{2}}\\right\\} \\\\ \\text { corExp } &amp; \\text { exponential } &amp; \\tau^{2} \\exp \\left\\{\\frac{-\\left|t_{i_{1}}-t_{i_{2}}\\right|}{\\rho \\rho}\\right\\} \\\\ \\text { corAR1 } &amp; \\text { autoregressive(1) } &amp; \\tau^{2} \\rho^{\\left|i_{1}-i_{2}\\right|} \\\\ \\text { corSymm } &amp; \\text { unstructured } &amp; \\tau_{i_{1}, i_{2}}^{2} \\\\ \\hline \\end{array} \\] Notice: - Keep it simple - Graphical methods - Information criteria: AIC or BIC - Try to cross-validate 6.4.19 Model diagnostics since \\[ \\hat{\\epsilon}_{i}=y_{i}-\\hat{y}_{i} \\] and \\[ \\operatorname{Var}\\left(\\hat{\\epsilon}_{i}\\right)=\\sigma^{2}\\left(1-h_{i}\\right)^{2} \\] where $h_{i}$ is the so-called leverage for observation $i$. therefore, Standardized residuals \\[ \\hat{\\epsilon}_{i}=\\frac{y_{i}-\\hat{y}_{i}}{\\hat{\\sigma}\\left(1-h_{i}\\right)} \\] Studentized residuals \\[ \\hat{\\epsilon}_{i}=\\frac{y_{i}-\\hat{y}_{i}}{\\hat{\\sigma}_{(i)}\\left(1-h_{i}\\right)} \\] 6.5 Linear mixed model practice 6.5.1 Loading data and library library(lme4) # load library library(arm) # convenience functions for regression in R ## ## arm (Version 1.12-2, built: 2021-10-15) ## Working directory is C:/Users/hed2/Downloads/mybook2/mybook2 ## ## Attaching package: &#39;arm&#39; ## The following object is masked from &#39;package:performance&#39;: ## ## display ## The following object is masked from &#39;package:boot&#39;: ## ## logit ## The following object is masked from &#39;package:car&#39;: ## ## logit ## The following object is masked from &#39;package:scales&#39;: ## ## rescale ## The following objects are masked from &#39;package:psych&#39;: ## ## logit, rescale, sim ## The following object is masked from &#39;package:plotrix&#39;: ## ## rescale ## The following object is masked from &#39;package:corrplot&#39;: ## ## corrplot lmm.data &lt;- read.table(&quot;http://bayes.acs.unt.edu:8083/BayesContent/class/Jon/R_SC/Module9/lmm.data.txt&quot;, header=TRUE, sep=&quot;,&quot;, na.strings=&quot;NA&quot;, dec=&quot;.&quot;, strip.white=TRUE) #summary(lmm.data) head(lmm.data) id extro open agree social class school 1 63.69356 43.43306 38.02668 75.05811 d IV 2 69.48244 46.86979 31.48957 98.12560 a VI 3 79.74006 32.27013 40.20866 116.33897 d VI 4 62.96674 44.40790 30.50866 90.46888 c IV 5 64.24582 36.86337 37.43949 98.51873 d IV 6 50.97107 46.25627 38.83196 75.21992 d I 6.5.2 General linear regression OLSexamp &lt;- lm(extro ~ open + agree + social, data = lmm.data) display(OLSexamp) ## lm(formula = extro ~ open + agree + social, data = lmm.data) ## coef.est coef.se ## (Intercept) 57.84 3.15 ## open 0.02 0.05 ## agree 0.03 0.05 ## social 0.01 0.02 ## --- ## n = 1200, k = 4 ## residual sd = 9.34, R-Squared = 0.00 6.5.3 Generalized linear regression MLexamp &lt;- glm(extro ~ open + agree + social, data=lmm.data) display(MLexamp) ## glm(formula = extro ~ open + agree + social, data = lmm.data) ## coef.est coef.se ## (Intercept) 57.84 3.15 ## open 0.02 0.05 ## agree 0.03 0.05 ## social 0.01 0.02 ## --- ## n = 1200, k = 4 ## residual deviance = 104378.2, null deviance = 104432.7 (difference = 54.5) ## overdispersion parameter = 87.3 ## residual sd is sqrt(overdispersion) = 9.34 6.5.4 Varying intercept by adding a stratum variable as fixed effect (glm) MLexamp.2 &lt;- glm(extro ~ open + agree + social + class, data=lmm.data ) display(MLexamp.2) ## glm(formula = extro ~ open + agree + social + class, data = lmm.data) ## coef.est coef.se ## (Intercept) 56.05 3.09 ## open 0.03 0.05 ## agree -0.01 0.05 ## social 0.01 0.02 ## classb 2.06 0.75 ## classc 3.70 0.75 ## classd 5.67 0.75 ## --- ## n = 1200, k = 7 ## residual deviance = 99187.7, null deviance = 104432.7 (difference = 5245.0) ## overdispersion parameter = 83.1 ## residual sd is sqrt(overdispersion) = 9.12 6.5.5 Comparisions of models add class as fixed effect AIC(MLexamp) ## [1] 8774.291 AIC(MLexamp.2) ## [1] 8719.083 anova(MLexamp, MLexamp.2, test=&quot;F&quot;) Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) 1196 104378.15 NA NA NA NA 1193 99187.68 3 5190.471 20.80982 0 6.5.6 Adding class and school as fixed effects (glm) MLexamp.4 &lt;- glm(extro ~ open + agree + social + school:class, data=lmm.data ) display(MLexamp.4) ## glm(formula = extro ~ open + agree + social + school:class, data = lmm.data) ## coef.est coef.se ## (Intercept) 80.36 0.37 ## open 0.01 0.00 ## agree -0.01 0.01 ## social 0.00 0.00 ## schoolI:classa -40.39 0.20 ## schoolII:classa -28.15 0.20 ## schoolIII:classa -23.58 0.20 ## schoolIV:classa -19.76 0.20 ## schoolV:classa -15.50 0.20 ## schoolVI:classa -10.46 0.20 ## schoolI:classb -34.60 0.20 ## schoolII:classb -26.76 0.20 ## schoolIII:classb -22.59 0.20 ## schoolIV:classb -18.71 0.20 ## schoolV:classb -14.31 0.20 ## schoolVI:classb -8.54 0.20 ## schoolI:classc -31.86 0.20 ## schoolII:classc -25.64 0.20 ## schoolIII:classc -21.58 0.20 ## schoolIV:classc -17.58 0.20 ## schoolV:classc -13.38 0.20 ## schoolVI:classc -5.58 0.20 ## schoolI:classd -30.00 0.20 ## schoolII:classd -24.57 0.20 ## schoolIII:classd -20.64 0.20 ## schoolIV:classd -16.60 0.20 ## schoolV:classd -12.04 0.20 ## --- ## n = 1200, k = 27 ## residual deviance = 1135.9, null deviance = 104432.7 (difference = 103296.8) ## overdispersion parameter = 1.0 ## residual sd is sqrt(overdispersion) = 0.98 AIC(MLexamp) ## [1] 8774.291 AIC(MLexamp.4) ## [1] 3395.573 anova(MLexamp, MLexamp.4, test=&quot;F&quot;) Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) 1196 104378.151 NA NA NA NA 1173 1135.896 23 103242.3 4635.419 0 6.5.7 Considering different slopes by stratum (glm) require(plyr) modellist &lt;- dlply(lmm.data, .(school, class), function(x) glm(extro~ open + agree + social, data=x)) strat1 &lt;- display(modellist[[1]]) ## glm(formula = extro ~ open + agree + social, data = x) ## coef.est coef.se ## (Intercept) 35.87 5.90 ## open 0.05 0.09 ## agree 0.02 0.10 ## social 0.01 0.03 ## --- ## n = 50, k = 4 ## residual deviance = 500.2, null deviance = 506.2 (difference = 5.9) ## overdispersion parameter = 10.9 ## residual sd is sqrt(overdispersion) = 3.30 strat2 &lt;- display(modellist[[2]]) ## glm(formula = extro ~ open + agree + social, data = x) ## coef.est coef.se ## (Intercept) 47.96 2.16 ## open -0.01 0.03 ## agree -0.03 0.03 ## social -0.01 0.01 ## --- ## n = 50, k = 4 ## residual deviance = 47.9, null deviance = 49.1 (difference = 1.2) ## overdispersion parameter = 1.0 ## residual sd is sqrt(overdispersion) = 1.02 strat24 &lt;- display(modellist[[24]]) ## glm(formula = extro ~ open + agree + social, data = x) ## coef.est coef.se ## (Intercept) 80.36 4.10 ## open 0.12 0.07 ## agree -0.12 0.06 ## social -0.01 0.02 ## --- ## n = 50, k = 4 ## residual deviance = 323.1, null deviance = 363.1 (difference = 40.0) ## overdispersion parameter = 7.0 ## residual sd is sqrt(overdispersion) = 2.65 AIC(MLexamp) ## [1] 8774.291 # AIC(strat1) # AIC(strat2) # AIC(strat24) how to combine these models? 6.5.8 Varying intercept with LMM add class as random effect MLexamp.6 &lt;- lmer(extro ~ open + agree + social + (1|class), data=lmm.data) display(MLexamp.6) ## lmer(formula = extro ~ open + agree + social + (1 | class), data = lmm.data) ## coef.est coef.se ## (Intercept) 58.86 3.29 ## open 0.03 0.05 ## agree 0.00 0.05 ## social 0.01 0.02 ## ## Error terms: ## Groups Name Std.Dev. ## class (Intercept) 2.35 ## Residual 9.12 ## --- ## number of obs: 1200, groups: class, 4 ## AIC = 8742.8, DIC = 8705.4 ## deviance = 8718.1 AIC(MLexamp.2) ## [1] 8719.083 AIC(MLexamp.6) ## [1] 8742.792 add class and school as random effects MLexamp.7 &lt;- lmer(extro ~ open + agree + social + (1|school) + (1|class), data=lmm.data) display(MLexamp.7) ## lmer(formula = extro ~ open + agree + social + (1 | school) + ## (1 | class), data = lmm.data) ## coef.est coef.se ## (Intercept) 60.20 4.21 ## open 0.01 0.01 ## agree -0.01 0.01 ## social 0.00 0.00 ## ## Error terms: ## Groups Name Std.Dev. ## school (Intercept) 9.79 ## class (Intercept) 2.41 ## Residual 1.67 ## --- ## number of obs: 1200, groups: school, 6; class, 4 ## AIC = 4737.9, DIC = 4683.3 ## deviance = 4703.6 fit nested terms school/class MLexamp.8 &lt;- lmer(extro ~ open + agree + social + (1|school/class), data=lmm.data) display(MLexamp.8) ## lmer(formula = extro ~ open + agree + social + (1 | school/class), ## data = lmm.data) ## coef.est coef.se ## (Intercept) 60.24 4.01 ## open 0.01 0.00 ## agree -0.01 0.01 ## social 0.00 0.00 ## ## Error terms: ## Groups Name Std.Dev. ## class:school (Intercept) 2.86 ## school (Intercept) 9.69 ## Residual 0.98 ## --- ## number of obs: 1200, groups: class:school, 24; school, 6 ## AIC = 3568.6, DIC = 3507.6 ## deviance = 3531.1 6.5.9 Varying slope with LMM MLexamp.9 &lt;- lmer(extro ~ open + agree + social + (1+open|school/class), data=lmm.data) ## boundary (singular) fit: see help(&#39;isSingular&#39;) display(MLexamp.9) ## lmer(formula = extro ~ open + agree + social + (1 + open | school/class), ## data = lmm.data) ## coef.est coef.se ## (Intercept) 60.26 3.81 ## open 0.01 0.01 ## agree -0.01 0.01 ## social 0.00 0.00 ## ## Error terms: ## Groups Name Std.Dev. Corr ## class:school (Intercept) 2.67 ## open 0.01 1.00 ## school (Intercept) 9.20 ## open 0.00 1.00 ## Residual 0.98 ## --- ## number of obs: 1200, groups: class:school, 24; school, 6 ## AIC = 3574.8, DIC = 3505.8 ## deviance = 3529.3 AIC(MLexamp.4) #using the interaction of class and school as fixed effect ## [1] 3395.573 AIC(MLexamp.7) #using class and school as random intercepts ## [1] 4737.868 AIC(MLexamp.8) #using nested term of class and school as random intercepts ## [1] 3568.571 AIC(MLexamp.9) #adding open as random slope based on MLexamp.8 ## [1] 3574.754 6.5.10 Summary the model 8 summary(MLexamp.8) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: extro ~ open + agree + social + (1 | school/class) ## Data: lmm.data ## ## REML criterion at convergence: 3554.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -9.9949 -0.3348 0.0057 0.3394 10.6476 ## ## Random effects: ## Groups Name Variance Std.Dev. ## class:school (Intercept) 8.2043 2.8643 ## school (Intercept) 93.8419 9.6872 ## Residual 0.9684 0.9841 ## Number of obs: 1200, groups: class:school, 24; school, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 60.2378227 4.0117874 15.015 ## open 0.0061065 0.0049636 1.230 ## agree -0.0076659 0.0056986 -1.345 ## social 0.0005404 0.0018524 0.292 ## ## Correlation of Fixed Effects: ## (Intr) open agree ## open -0.049 ## agree -0.049 -0.012 ## social -0.045 -0.006 -0.009 in random effect variances, the nested effect of class within the higher level variable and the random effect of the higher level variable. If all the percentages for each random effect (each part/ the total) are very small, then the random effects are not present and linear mixed modeling is not appropriate. testing random effect # W2 will have to build the test statistic by hand because we cannot rely on asymptotics L_Model= logLik(MLexamp.8)[1] L_Model1=logLik(MLexamp.9)[1] LRT_statistic=-2*(L_Model1-L_Model) # Model has 3 parameters associated with the Random Effects # Model1 has 1 parameter associated with the Random Effects # DF=2 # We will look at a mixture of chi-squared distributions with 1 and 2 degrees of freedom 0.5*pchisq(LRT_statistic,df=2,lower.tail = F)+0.5*pchisq(LRT_statistic,df=1,lower.tail = F) ## [1] 1 plot a x and y by grouping variables library(ggplot2) ggplot(lmm.data,aes(x= open,y=extro ,group=school,color= school))+geom_line() Plot of Residuals by grouping variables plot(MLexamp.8, school~resid(.)) plot(MLexamp.8, class~resid(.)) Plot Residuals by explanatory variables plot(MLexamp.8, resid(.)~open) # plot(MLexamp.8,resid(.,type=&quot;p&quot;)~fitted(.) | BMIGRP_fm002) QQ plot of residuals qqnorm(resid(MLexamp.8) ) Random effect coefficients normality test= ((ranef(MLexamp.8)[1])[[&quot;class:school&quot;]]) qqnorm ((test$`(Intercept)`) ) 6.5.11 Extracting elements (parameters) fixed effect coefficients fixef(MLexamp.8) ## (Intercept) open agree social ## 60.2378227199 0.0061065139 -0.0076659234 0.0005404066 random effect coefficients ranef(MLexamp.8) ## $`class:school` ## (Intercept) ## a:I -6.4667512 ## a:II -1.9994013 ## a:III -1.5196312 ## a:IV -1.5496936 ## a:V -1.5551474 ## a:VI -4.0069544 ## b:I -0.6892956 ## b:II -0.6113909 ## b:III -0.5356311 ## b:IV -0.5050668 ## b:V -0.3674627 ## b:VI -2.0891795 ## c:I 2.0467886 ## c:II 0.5096079 ## c:III 0.4718805 ## c:IV 0.6227982 ## c:V 0.5644201 ## c:VI 0.8638052 ## d:I 3.9033142 ## d:II 1.5740706 ## d:III 1.4138302 ## d:IV 1.5992023 ## d:V 1.8981080 ## d:VI 6.4277799 ## ## $school ## (Intercept) ## I -13.793761 ## II -6.029202 ## III -1.939355 ## IV 1.912916 ## V 6.175660 ## VI 13.673742 ## ## with conditional variances for &quot;class:school&quot; &quot;school&quot; total coefficients for model (random + fixed) coef(MLexamp.8) ## $`class:school` ## (Intercept) open agree social ## a:I 53.77107 0.006106514 -0.007665923 0.0005404066 ## a:II 58.23842 0.006106514 -0.007665923 0.0005404066 ## a:III 58.71819 0.006106514 -0.007665923 0.0005404066 ## a:IV 58.68813 0.006106514 -0.007665923 0.0005404066 ## a:V 58.68268 0.006106514 -0.007665923 0.0005404066 ## a:VI 56.23087 0.006106514 -0.007665923 0.0005404066 ## b:I 59.54853 0.006106514 -0.007665923 0.0005404066 ## b:II 59.62643 0.006106514 -0.007665923 0.0005404066 ## b:III 59.70219 0.006106514 -0.007665923 0.0005404066 ## b:IV 59.73276 0.006106514 -0.007665923 0.0005404066 ## b:V 59.87036 0.006106514 -0.007665923 0.0005404066 ## b:VI 58.14864 0.006106514 -0.007665923 0.0005404066 ## c:I 62.28461 0.006106514 -0.007665923 0.0005404066 ## c:II 60.74743 0.006106514 -0.007665923 0.0005404066 ## c:III 60.70970 0.006106514 -0.007665923 0.0005404066 ## c:IV 60.86062 0.006106514 -0.007665923 0.0005404066 ## c:V 60.80224 0.006106514 -0.007665923 0.0005404066 ## c:VI 61.10163 0.006106514 -0.007665923 0.0005404066 ## d:I 64.14114 0.006106514 -0.007665923 0.0005404066 ## d:II 61.81189 0.006106514 -0.007665923 0.0005404066 ## d:III 61.65165 0.006106514 -0.007665923 0.0005404066 ## d:IV 61.83702 0.006106514 -0.007665923 0.0005404066 ## d:V 62.13593 0.006106514 -0.007665923 0.0005404066 ## d:VI 66.66560 0.006106514 -0.007665923 0.0005404066 ## ## $school ## (Intercept) open agree social ## I 46.44406 0.006106514 -0.007665923 0.0005404066 ## II 54.20862 0.006106514 -0.007665923 0.0005404066 ## III 58.29847 0.006106514 -0.007665923 0.0005404066 ## IV 62.15074 0.006106514 -0.007665923 0.0005404066 ## V 66.41348 0.006106514 -0.007665923 0.0005404066 ## VI 73.91157 0.006106514 -0.007665923 0.0005404066 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; predicted values based on the model yhat &lt;- fitted(MLexamp.8) head(yhat) ## 1 2 3 4 5 6 ## 63.76422 70.00245 80.29104 62.85973 63.74128 50.37281 summary(yhat) Min. 1st Qu. Median Mean 3rd Qu. Max. 39.90131 54.42539 60.16062 60.26744 66.35197 80.4983 residuals of model residuals &lt;- resid(MLexamp.8) head(residuals) ## 1 2 3 4 5 6 ## -0.0706615 -0.5200163 -0.5509793 0.1070113 0.5045414 0.5982642 summary(residuals) Min. 1st Qu. Median Mean 3rd Qu. Max. -9.835528 -0.3294215 0.0056099 0 0.3340266 10.47782 hist(residuals,50) Fitted lines (less variation) lmm.data$Fitted=predict(MLexamp.8) # subjects=unique(lmm.data$school) #Taking the identifiers and saving it # sam=&quot;V&quot; # Taking a random sample of 10 Subjects ggplot(lmm.data ,aes(x=open ,y=Fitted,group=school,color=school ) )+geom_line(se=F) 6.5.12 Model diagnostics 6.5.12.1 Residuals normality library(haven) nurse &lt;- read_sav(file=&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%202/Nurses/SPSS/Nurses.sav?raw=true&quot;) MLexamp.18 &lt;- lmer(stress ~ experien+ wardtype+ expcon + (1|hospital/ward), data=nurse) summary(MLexamp.18) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: stress ~ experien + wardtype + expcon + (1 | hospital/ward) ## Data: nurse ## ## REML criterion at convergence: 1865.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.85886 -0.69376 -0.01469 0.68749 3.12171 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ward:hospital (Intercept) 0.3272 0.5720 ## hospital (Intercept) 0.2129 0.4614 ## Residual 0.2811 0.5302 ## Number of obs: 1000, groups: ward:hospital, 100; hospital, 25 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 5.738157 0.147508 38.901 ## experien -0.024015 0.002932 -8.190 ## wardtype 0.055586 0.119280 0.466 ## expcon -0.711551 0.119281 -5.965 ## ## Correlation of Fixed Effects: ## (Intr) expern wrdtyp ## experien -0.343 ## wardtype -0.406 0.005 ## expcon -0.407 0.007 0.000 outcome is ordinal variable residual vs. fitted values lmerresid &lt;- resid(MLexamp.18) par(mfrow = c(1,2)) plot(sqrt(abs(lmerresid)) ~ predict(MLexamp.18)) qqnorm(lmerresid) qqline(lmerresid) 6.5.12.2 Checking for variance homogeneity residual vs. explanatory variables par(mfrow = c(2,2)) plot(lmerresid ~ fitted(MLexamp.18)) with(nurse, plot(lmerresid ~ experien , col = heat.colors(20))) with(nurse, plot(lmerresid ~ wardtype , col = rainbow(3))) with(nurse, plot(lmerresid ~ expcon , col = rainbow(5))) # with(lmm.data, plot(lmerresid ~ class, col = rainbow(5))) 6.5.12.3 Influence points Cook’s distance library(influence.ME) ## ## Attaching package: &#39;influence.ME&#39; ## The following object is masked from &#39;package:arm&#39;: ## ## se.fixef ## The following object is masked from &#39;package:stats&#39;: ## ## influence lmer3.infl &lt;- influence(MLexamp.18, obs = TRUE) par(mfrow = c(1,1)) plot(cooks.distance(lmer3.infl), type = &quot;p&quot;, pch = 20) 6.5.12.4 Random effect normality par(mfrow = c(1,3)) qqnorm(ranef(MLexamp.18)$`ward:hospital`$`(Intercept)`) 6.5.12.5 Post hoc analysis ci: sd of random effect coeffcient confint(MLexamp.18, parm = 1:4, oldNames = FALSE) ## Computing profile confidence intervals ... 2.5 % 97.5 % sd_(Intercept)|ward:hospital 0.4761842 0.6761929 sd_(Intercept)|hospital 0.2874774 0.6693897 sigma 0.5063298 0.5553624 (Intercept) 5.4497316 6.0266362 lsmeans (ideally means) plot mylsmeans &lt;- emmeans::emmeans(MLexamp.18, c(&quot;wardtype&quot;, &quot;expcon&quot; )) sum_mylsmeans &lt;- summary(mylsmeans) with(sum_mylsmeans, interaction.plot(wardtype , expcon ,emmean, col = 2:4)) paired lsmeans sum_difflsmeans &lt;- summary(pairs(mylsmeans)) head(sum_difflsmeans) contrast estimate SE df t.ratio p.value wardtype0 expcon0 - wardtype1 expcon0 -0.0555860 0.1192798 72.98116 -0.4660131 0.9662337 wardtype0 expcon0 - wardtype0 expcon1 0.7115505 0.1192816 72.98594 5.9652983 0.0000005 wardtype0 expcon0 - wardtype1 expcon1 0.6559646 0.1687026 73.00821 3.8882899 0.0012380 wardtype1 expcon0 - wardtype0 expcon1 0.7671365 0.1686743 72.95889 4.5480350 0.0001222 wardtype1 expcon0 - wardtype1 expcon1 0.7115505 0.1192816 72.98594 5.9652983 0.0000005 wardtype0 expcon1 - wardtype1 expcon1 -0.0555860 0.1192798 72.98116 -0.4660131 0.9662337 barplot(sum_difflsmeans$SE) barplot(sum_difflsmeans$estimate) A plot function performance::check_model(MLexamp.18) 6.5.12.6 Intra class correlation It allows us to assess whether or not the random effect is present in the data. lmm.null &lt;- lmer(extro ~ 1 + (1|school), data = lmm.data) summary(lmm.null) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: extro ~ 1 + (1 | school) ## Data: lmm.data ## ## REML criterion at convergence: 5806.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.9773 -0.5315 0.0059 0.5298 6.2109 ## ## Random effects: ## Groups Name Variance Std.Dev. ## school (Intercept) 95.87 9.791 ## Residual 7.14 2.672 ## Number of obs: 1200, groups: school, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 60.267 3.998 15.07 indicates that 93.07% (95.8720 / 103.0119 ) of the variance in 'extro' can be \"explained\" by school group membership. 6.5.13 Notice Back transformed expected values (lsmeans) not ci Box-Cox transformations 6.6 Linear mixed model covariance decomposition with random intercept- lme4 6.6.1 Load data data(Orthodont, package=&quot;nlme&quot;) Data &lt;- Orthodont plot means and variances by higher level variable (grouping) barplot(with(Data, tapply(distance, list(subject = Subject), mean))) barplot(with(Data, tapply(distance, list(subject = Subject), var))) 6.6.2 Using glm fit.lm &lt;- lm(distance ~ age+ Sex + Subject, data = Data) anova(fit.lm) Df Sum Sq Mean Sq F value Pr(&gt;F) age 1 235.3560 235.356019 114.838287 0 Sex 1 140.4649 140.464857 68.537629 0 Subject 25 377.9148 15.116591 7.375904 0 Residuals 80 163.9565 2.049456 NA NA 6.6.3 Using glm with random slopes fit.aov &lt;- aov(distance ~ age+ Sex + Error(Subject), data = Data) summary(fit.aov) ## ## Error: Subject ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Sex 1 140.5 140.46 9.292 0.00538 ** ## Residuals 25 377.9 15.12 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 235.4 235.36 114.8 &lt;2e-16 *** ## Residuals 80 164.0 2.05 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 how can we compute \\(σ^2\\) a from this output? note that \\(σ^2\\)= MSE , and E(MSA) = \\((σ_a)^2\\) +n·\\(σ^2\\) ,therefore\\((σ_a)^2\\) = (MSA−MSE)/n 6.6.4 Using lmm library(lme4) fit.lmer &lt;- lmer(distance ~ age + Sex +(1 | Subject), data = Data) summary(fit.lmer) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: distance ~ age + Sex + (1 | Subject) ## Data: Data ## ## REML criterion at convergence: 437.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7489 -0.5503 -0.0252 0.4534 3.6575 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Subject (Intercept) 3.267 1.807 ## Residual 2.049 1.432 ## Number of obs: 108, groups: Subject, 27 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 17.70671 0.83392 21.233 ## age 0.66019 0.06161 10.716 ## SexFemale -2.32102 0.76142 -3.048 ## ## Correlation of Fixed Effects: ## (Intr) age ## age -0.813 ## SexFemale -0.372 0.000 default is compound symmetry 6.6.5 Get x,y,z X=(getME(fit.lmer, &quot;X&quot;)) head(X) (Intercept) age SexFemale 1 8 0 1 10 0 1 12 0 1 14 0 1 8 0 1 10 0 y=getME(fit.lmer, &quot;y&quot;) head(y) ## [1] 26.0 25.0 29.0 31.0 21.5 22.5 Z &lt;- getME(fit.lmer, &quot;Z&quot;) head(Z) ## 6 x 27 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 27 column names &#39;M16&#39;, &#39;M05&#39;, &#39;M02&#39; ... ]] ## ## 1 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . ## 5 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . 6.6.6 Get fixed effect and random effect coefficients bhat &lt;- getME(fit.lmer, &quot;fixef&quot;) #getME(fit.lmer, &quot;beta&quot;) # fixef(fit.lmer) bhat ## (Intercept) age SexFemale ## 17.7067130 0.6601852 -2.3210227 uhat &lt;- ranef(fit.lmer) uhat ## $Subject ## (Intercept) ## M16 -1.70183357 ## M05 -1.70183357 ## M02 -1.37767479 ## M11 -1.16156894 ## M07 -1.05351602 ## M08 -0.94546309 ## M03 -0.62130432 ## M12 -0.62130432 ## M13 -0.62130432 ## M14 -0.08103969 ## M09 0.13506616 ## M15 0.78338371 ## M06 1.21559540 ## M04 1.43170125 ## M01 2.40417758 ## M10 3.91691853 ## F10 -3.58539251 ## F09 -1.31628108 ## F06 -1.31628108 ## F01 -1.10017524 ## F05 -0.01964599 ## F07 0.30451279 ## F02 0.30451279 ## F08 0.62867156 ## F03 0.95283034 ## F04 1.92530666 ## F11 3.22194176 ## ## with conditional variances for &quot;Subject&quot; coef(fit.lmer) ## $Subject ## (Intercept) age SexFemale ## M16 16.00488 0.6601852 -2.321023 ## M05 16.00488 0.6601852 -2.321023 ## M02 16.32904 0.6601852 -2.321023 ## M11 16.54514 0.6601852 -2.321023 ## M07 16.65320 0.6601852 -2.321023 ## M08 16.76125 0.6601852 -2.321023 ## M03 17.08541 0.6601852 -2.321023 ## M12 17.08541 0.6601852 -2.321023 ## M13 17.08541 0.6601852 -2.321023 ## M14 17.62567 0.6601852 -2.321023 ## M09 17.84178 0.6601852 -2.321023 ## M15 18.49010 0.6601852 -2.321023 ## M06 18.92231 0.6601852 -2.321023 ## M04 19.13841 0.6601852 -2.321023 ## M01 20.11089 0.6601852 -2.321023 ## M10 21.62363 0.6601852 -2.321023 ## F10 14.12132 0.6601852 -2.321023 ## F09 16.39043 0.6601852 -2.321023 ## F06 16.39043 0.6601852 -2.321023 ## F01 16.60654 0.6601852 -2.321023 ## F05 17.68707 0.6601852 -2.321023 ## F07 18.01123 0.6601852 -2.321023 ## F02 18.01123 0.6601852 -2.321023 ## F08 18.33538 0.6601852 -2.321023 ## F03 18.65954 0.6601852 -2.321023 ## F04 19.63202 0.6601852 -2.321023 ## F11 20.92865 0.6601852 -2.321023 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; 6.6.7 Get random effect covariance structure (covariance matrix expandation) # theta &lt;- getME(fit.lmer, &quot;theta&quot;) # $r$ # theta**2+ getME(fit.lmer, &quot;sigma&quot;)**2 # # theta**2 # lwr &lt;- getME(fit.lmer, &quot;lower&quot;) # lwr # Lambda &lt;- getME(fit.lmer, &quot;Lambda&quot;) # matrix # # head(Lambda) # # dim(Lambda) # Lambda%*%t(Lambda) the relationship $\\Sigma = TSST'$ the relationship of theta and covariance $$ = ( \\[\\begin{array}{cc} \\theta_1 &amp; 0 \\\\ \\theta_2 &amp; \\theta_3 \\end{array}\\] ) ( \\[\\begin{array}{cc} \\theta_1 &amp; \\theta_2 \\\\ 0 &amp; \\theta_3 \\end{array}\\] ) = ( \\[\\begin{array}{cc} \\theta_1^2 &amp; \\theta_1 \\theta_2 \\\\ \\theta_1 \\theta_2 &amp; \\theta_2^2 + \\theta_3^2 \\end{array}\\] ) = ( \\[\\begin{array}{cc} \\sigma_1^2 &amp; \\sigma_{12} \\\\ \\sigma_{12} &amp; \\sigma_2^2 \\end{array}\\] ) $$ we can equate θ1 with σ1 (the standard deviation of the intercept), θ2 with σ12/σ1 (the covariance scaled by the SD) ... θ3 is a little more complicated. in particular, in a model with only random-intercept terms, the θ vector is just the vector of RE standard deviations. The way lme4 works is that, given a value of θ, we can determine Σ vc &lt;- VarCorr(fit.lmer) Lambda_new &lt;-vc[[&quot;Subject&quot;]][1]*diag(length(levels(Data$Subject))) head(Lambda_new) 3.266784 0.000000 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 3.266784 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 3.266784 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.266784 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 0.000000 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(Lambda_new) ## [1] 27 27 “theta”: random-effects parameter estimates: these are parameterized as the relative Cholesky factors of each random effect term use other cov structure by nlme lme4 does not allow to specify it, and it can only fit LMMs with independent residual errors. However, the range of available variance-covariance matrices for the random effects are restricted to diagonal or general matrices. 6.6.8 Get residual variance and its identity matrix sigma &lt;- getME(fit.lmer, &quot;sigma&quot;)**2 sigma ## [1] 2.049456 head(sigma*diag(nrow(Data))) 2.049456 0.000000 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 2.049456 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 2.049456 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 2.049456 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 2.049456 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 0.000000 2.049456 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.6.9 Get y covariance matrix VM &lt;- Z%*%Lambda_new%*%t(Z)+sigma*diag(nrow(Data)) head(VM) ## 6 x 108 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 108 column names &#39;1&#39;, &#39;2&#39;, &#39;3&#39; ... ]] ## ## 1 5.316240 3.266784 3.266784 3.266784 . . . . . . . ## 2 3.266784 5.316240 3.266784 3.266784 . . . . . . . ## 3 3.266784 3.266784 5.316240 3.266784 . . . . . . . ## 4 3.266784 3.266784 3.266784 5.316240 . . . . . . . ## 5 . . . . 5.316240 3.266784 3.266784 3.266784 . . . ## 6 . . . . 3.266784 5.316240 3.266784 3.266784 . . . ## ## 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## 1 . . . . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . dim(VM) ## [1] 108 108 6.6.10 Get fixed effect coefficients covariance matrix vcov &lt;- vcov(fit.lmer) #fixed cov vcov ## 3 x 3 Matrix of class &quot;dpoMatrix&quot; ## (Intercept) age SexFemale ## (Intercept) 0.69542669 -4.174818e-02 -2.361967e-01 ## age -0.04174818 3.795289e-03 -3.051416e-17 ## SexFemale -0.23619673 -3.051416e-17 5.797556e-01 # computer correlation coefficients vcov@x[2]/prod(sqrt( diag(vcov(fit.lmer))[-3] )) ## [1] -0.8126236 6.6.11 Get random effect coefficients covariance matrix (standard deviation) vc &lt;- VarCorr(fit.lmer) vc ## default print method: standard dev and corr ## Groups Name Std.Dev. ## Subject (Intercept) 1.8074 ## Residual 1.4316 as.matrix(Matrix::bdiag(vc)) #random effect covariance matrix (Intercept) (Intercept) 3.266784 # https://stackoverflow.com/questions/47307340/extracting-the-i-estimated-variance-covariance-matrix-of-random-effects-and-or # https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect022.htm approximate above sd uintercept &lt;- (uhat[[&quot;Subject&quot;]]) sd(uintercept[,1]) ## [1] 1.647809 6.6.12 Computer fixed effect coefficients library(matlib) inv(t(as.matrix(X))%*%inv(as.matrix(VM))%*%as.matrix(X))%*%t(as.matrix(X))%*%inv(as.matrix(VM))%*%y 17.7067076 0.6601871 -2.3210230 bhat ## (Intercept) age SexFemale ## 17.7067130 0.6601852 -2.3210227 6.6.13 Computer covariance of fixed efffect coefficients inv(t(as.matrix(X))%*%inv(as.matrix(VM))%*%as.matrix(X)) 0.6954267 -0.0417482 -0.2361967 -0.0417482 0.0037953 0.0000000 -0.2361967 0.0000000 0.5797556 # standard error sqrt(inv(t(as.matrix(X))%*%inv(as.matrix(VM))%*%as.matrix(X))) 0.8339225 NaN NaN NaN 0.0616059 0.0000000 NaN 0.0000000 0.7614169 # the following equals lmm summary vcov(fit.lmer) ## 3 x 3 Matrix of class &quot;dpoMatrix&quot; ## (Intercept) age SexFemale ## (Intercept) 0.69542669 -4.174818e-02 -2.361967e-01 ## age -0.04174818 3.795289e-03 -3.051416e-17 ## SexFemale -0.23619673 -3.051416e-17 5.797556e-01 sqrt(vcov(fit.lmer)) ## 3 x 3 Matrix of class &quot;dsyMatrix&quot; ## (Intercept) age SexFemale ## (Intercept) 0.8339225 NaN NaN ## age NaN 0.06160592 NaN ## SexFemale NaN NaN 0.7614168 default is compound symmetry 6.6.14 Computer random effect coefficients comput_uhat &lt;- (as.matrix(Lambda_new))%*%t(Z)%*%inv(as.matrix(VM))%*%(y-as.matrix(X)%*%(bhat)) cbind((comput_uhat@x),(uhat[[&quot;Subject&quot;]])) (comput_uhat@x) (Intercept) M16 -1.7018335 -1.7018336 M05 -1.7018335 -1.7018336 M02 -1.3776748 -1.3776748 M11 -1.1615689 -1.1615689 M07 -1.0535160 -1.0535160 M08 -0.9454631 -0.9454631 M03 -0.6213043 -0.6213043 M12 -0.6213043 -0.6213043 M13 -0.6213043 -0.6213043 M14 -0.0810397 -0.0810397 M09 0.1350662 0.1350662 M15 0.7833837 0.7833837 M06 1.2155954 1.2155954 M04 1.4317012 1.4317013 M01 2.4041775 2.4041776 M10 3.9169184 3.9169185 F10 -3.5853924 -3.5853925 F09 -1.3162811 -1.3162811 F06 -1.3162811 -1.3162811 F01 -1.1001752 -1.1001752 F05 -0.0196460 -0.0196460 F07 0.3045128 0.3045128 F02 0.3045128 0.3045128 F08 0.6286716 0.6286716 F03 0.9528303 0.9528303 F04 1.9253066 1.9253067 F11 3.2219417 3.2219418 covariance matrix of random effect coefficients head(Lambda_new) 3.266784 0.000000 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 3.266784 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 3.266784 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.266784 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 0.000000 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.6.15 Computer predicted values yhat &lt;- X%*%(bhat)+Z%*%(uhat[[&quot;Subject&quot;]][[&quot;(Intercept)&quot;]]) head(yhat) ## 6 x 1 Matrix of class &quot;dgeMatrix&quot; ## [,1] ## 1 25.39237 ## 2 26.71274 ## 3 28.03311 ## 4 29.35348 ## 5 21.61052 ## 6 22.93089 head(fitted(fit.lmer)) ## 1 2 3 4 5 6 ## 25.39237 26.71274 28.03311 29.35348 21.61052 22.93089 6.7 Linear mixed model covariance decomposition with random slopes- lme4 6.7.1 Load data data(Orthodont, package=&quot;nlme&quot;) Data &lt;- Orthodont 6.7.2 Using lmm library(lme4) fit.lmer.slope &lt;- lmer(distance ~ age + Sex +(1+ age | Subject), data = Data) summary(fit.lmer.slope) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: distance ~ age + Sex + (1 + age | Subject) ## Data: Data ## ## REML criterion at convergence: 435.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.0815 -0.4568 0.0155 0.4471 3.8944 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 7.82249 2.7969 ## age 0.05126 0.2264 -0.77 ## Residual 1.71621 1.3100 ## Number of obs: 108, groups: Subject, 27 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 17.63518 0.88623 19.899 ## age 0.66019 0.07125 9.265 ## SexFemale -2.14544 0.75746 -2.832 ## ## Correlation of Fixed Effects: ## (Intr) age ## age -0.838 ## SexFemale -0.348 0.000 default is compound symmetry 6.7.3 Get x,y,z X=(getME(fit.lmer.slope, &quot;X&quot;)) head(X) (Intercept) age SexFemale 1 8 0 1 10 0 1 12 0 1 14 0 1 8 0 1 10 0 y=getME(fit.lmer.slope, &quot;y&quot;) head(y) ## [1] 26.0 25.0 29.0 31.0 21.5 22.5 Z &lt;- getME(fit.lmer.slope, &quot;Z&quot;) head(Z) ## 6 x 54 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 54 column names &#39;M16&#39;, &#39;M16&#39;, &#39;M05&#39; ... ]] ## ## 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 8 . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 10 . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 12 . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 14 . . . . . . . . ## 5 . . . . 1 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . 1 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## 1 . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . dim(Z) ## [1] 108 54 6.7.4 Get fixed effect and random effect coefficients bhat &lt;- getME(fit.lmer.slope, &quot;fixef&quot;) #getME(fit.lmer, &quot;beta&quot;) # fixef(fit.lmer) bhat ## (Intercept) age SexFemale ## 17.6351805 0.6601852 -2.1454431 uhat &lt;- ranef(fit.lmer.slope) uhat ## $Subject ## (Intercept) age ## M16 -0.96698456 -0.06544230 ## M05 -2.15621081 0.04447130 ## M02 -1.58143839 0.02194874 ## M11 0.38737823 -0.13961777 ## M07 -1.40307472 0.03606404 ## M08 0.37415109 -0.11799495 ## M03 -0.83491587 0.02435289 ## M12 -1.82593774 0.11594755 ## M13 -5.59182087 0.46400729 ## M14 0.51944692 -0.04982259 ## M09 -1.07941522 0.11835170 ## M15 -1.11909663 0.18322017 ## M06 1.03469722 0.02495756 ## M04 3.20171821 -0.15492789 ## M01 0.96194797 0.14388309 ## M10 3.04960613 0.09373458 ## F10 -2.31273350 -0.13319695 ## F09 0.32324282 -0.16262237 ## F06 -0.07316594 -0.12598450 ## F01 0.11181130 -0.12268061 ## F05 1.43310624 -0.14279903 ## F07 0.62044804 -0.03708906 ## F02 -0.37057384 0.05450561 ## F08 2.38444671 -0.16952522 ## F03 -0.01384650 0.08273621 ## F04 2.30508388 -0.03978828 ## F11 2.62212981 0.05331079 ## ## with conditional variances for &quot;Subject&quot; 6.7.5 Get random effect covariance structure (covariance matrix expandation) notice z matrix structure vc &lt;- VarCorr(fit.lmer.slope) Lambda_new &lt;- bdiag(replicate(length(levels(Data$Subject)), vc[[&quot;Subject&quot;]], simplify = FALSE)) head(Lambda_new) ## 6 x 54 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] 7.822487 -0.48495997 . . . . . . . . ## [2,] -0.484960 0.05126495 . . . . . . . . ## [3,] . . 7.822487 -0.48495997 . . . . . . ## [4,] . . -0.484960 0.05126495 . . . . . . ## [5,] . . . . 7.822487 -0.48495997 . . . . ## [6,] . . . . -0.484960 0.05126495 . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . ## [2,] . . . . . . . ## [3,] . . . . . . . ## [4,] . . . . . . . ## [5,] . . . . . . . ## [6,] . . . . . . . dim(Lambda_new) ## [1] 54 54 “theta”: random-effects parameter estimates: these are parameterized as the relative Cholesky factors of each random effect term use other cov structure by nlme lme4 does not allow to specify it, and it can only fit LMMs with independent residual errors. However, the range of available variance-covariance matrices for the random effects are restricted to diagonal or general matrices. 6.7.6 Get residual variance and its identity matrix sigma &lt;- getME(fit.lmer.slope, &quot;sigma&quot;)**2 sigma ## [1] 1.71621 head(sigma*diag(nrow(Data))) 1.71621 0.00000 0.00000 0.00000 0.00000 0.00000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.00000 1.71621 0.00000 0.00000 0.00000 0.00000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.00000 0.00000 1.71621 0.00000 0.00000 0.00000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.00000 0.00000 0.00000 1.71621 0.00000 0.00000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.00000 0.00000 0.00000 0.00000 1.71621 0.00000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.00000 0.00000 0.00000 0.00000 0.00000 1.71621 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.7.7 Get y covariance matrix VM &lt;- Z%*%Lambda_new%*%t(Z)+sigma*diag(nrow(Data)) head(VM) ## 6 x 108 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 108 column names &#39;1&#39;, &#39;2&#39;, &#39;3&#39; ... ]] ## ## 1 5.060294 3.194403 3.044722 2.895042 . . . . . . . ## 2 3.194403 4.965992 3.305161 3.360540 . . . . . . . ## 3 3.044722 3.305161 5.281810 3.826039 . . . . . . . ## 4 2.895042 3.360540 3.826039 6.007747 . . . . . . . ## 5 . . . . 5.060294 3.194403 3.044722 2.895042 . . . ## 6 . . . . 3.194403 4.965992 3.305161 3.360540 . . . ## ## 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## 1 . . . . . . . . . . . . . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . dim(VM) ## [1] 108 108 6.7.8 Get fixed effect coefficients covariance matrix vcov &lt;- vcov(fit.lmer.slope) #fixed cov vcov ## 3 x 3 Matrix of class &quot;dpoMatrix&quot; ## (Intercept) age SexFemale ## (Intercept) 0.78540227 -5.292131e-02 -2.337501e-01 ## age -0.05292131 5.076869e-03 -5.668086e-16 ## SexFemale -0.23375009 -5.668086e-16 5.737502e-01 # computer correlation coefficients vcov@x[2]/prod(sqrt( diag(vcov(fit.lmer.slope))[-3] )) ## [1] -0.8380822 6.7.9 Get random effect coefficients covariance matrix (standard deviation) vc &lt;- VarCorr(fit.lmer.slope) vc ## default print method: standard dev and corr ## Groups Name Std.Dev. Corr ## Subject (Intercept) 2.79687 ## age 0.22642 -0.766 ## Residual 1.31004 as.matrix(Matrix::bdiag(vc)) #random effect covariance matrix (Intercept) age (Intercept) 7.822487 -0.4849600 age -0.484960 0.0512649 # https://stackoverflow.com/questions/47307340/extracting-the-i-estimated-variance-covariance-matrix-of-random-effects-and-or # https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect022.htm 6.7.10 Computer fixed effect coefficients library(matlib) inv(t(as.matrix(X))%*%inv(as.matrix(VM))%*%as.matrix(X))%*%t(as.matrix(X))%*%inv(as.matrix(VM))%*%y 17.635190 0.660189 -2.145444 bhat ## (Intercept) age SexFemale ## 17.6351805 0.6601852 -2.1454431 6.7.11 Computer covariance of fixed efffect coefficients inv(t(as.matrix(X))%*%inv(as.matrix(VM))%*%as.matrix(X)) 0.7854023 -0.0529213 -0.2337501 -0.0529213 0.0050769 0.0000000 -0.2337501 0.0000000 0.5737502 # standard error sqrt(inv(t(as.matrix(X))%*%inv(as.matrix(VM))%*%as.matrix(X))) 0.8862292 NaN NaN NaN 0.0712522 0.000000 NaN 0.0000000 0.757463 # the following equals lmm summary vcov(fit.lmer.slope) ## 3 x 3 Matrix of class &quot;dpoMatrix&quot; ## (Intercept) age SexFemale ## (Intercept) 0.78540227 -5.292131e-02 -2.337501e-01 ## age -0.05292131 5.076869e-03 -5.668086e-16 ## SexFemale -0.23375009 -5.668086e-16 5.737502e-01 sqrt(vcov(fit.lmer.slope)) ## 3 x 3 Matrix of class &quot;dsyMatrix&quot; ## (Intercept) age SexFemale ## (Intercept) 0.8862292 NaN NaN ## age NaN 0.07125215 NaN ## SexFemale NaN NaN 0.757463 default is compound symmetry 6.7.12 Computer random effect coefficients comput_uhat &lt;- (as.matrix(Lambda_new))%*%t(Z)%*%inv(as.matrix(VM))%*%(y-as.matrix(X)%*%(bhat)) cbind((comput_uhat@x),(uhat[[&quot;Subject&quot;]])) (comput_uhat@x) (Intercept) age -0.9669845 -0.9669846 -0.0654423 -0.0654423 -2.1562108 0.0444713 -2.1562108 -1.5814384 0.0219487 0.0444713 0.3873782 -0.1396178 -1.5814383 -1.4030747 0.0360640 0.0219487 0.3741511 -0.1179949 0.3873782 -0.8349159 0.0243529 -0.1396178 -1.8259377 0.1159476 -1.4030747 -5.5918209 0.4640073 0.0360640 0.5194469 -0.0498226 0.3741511 -1.0794152 0.1183517 -0.1179950 -1.1190966 0.1832202 -0.8349158 1.0346972 0.0249576 0.0243529 3.2017182 -0.1549279 -1.8259377 0.9619480 0.1438831 0.1159476 3.0496061 0.0937346 -5.5918208 -2.3127335 -0.1331970 0.4640073 0.3232428 -0.1626224 0.5194469 -0.0731659 -0.1259845 -0.0498226 0.1118113 -0.1226806 -1.0794152 1.4331062 -0.1427990 0.1183517 0.6204480 -0.0370891 -1.1190966 -0.3705738 0.0545056 0.1832202 2.3844467 -0.1695252 1.0346972 -0.0138465 0.0827362 0.0249576 2.3050839 -0.0397883 3.2017181 2.6221298 0.0533108 -0.1549279 -0.9669846 -0.0654423 0.9619480 -2.1562108 0.0444713 0.1438831 -1.5814384 0.0219487 3.0496061 0.3873782 -0.1396178 0.0937346 -1.4030747 0.0360640 -2.3127334 0.3741511 -0.1179949 -0.1331970 -0.8349159 0.0243529 0.3232428 -1.8259377 0.1159476 -0.1626224 -5.5918209 0.4640073 -0.0731659 0.5194469 -0.0498226 -0.1259845 -1.0794152 0.1183517 0.1118113 -1.1190966 0.1832202 -0.1226806 1.0346972 0.0249576 1.4331062 3.2017182 -0.1549279 -0.1427990 0.9619480 0.1438831 0.6204480 3.0496061 0.0937346 -0.0370891 -2.3127335 -0.1331970 -0.3705738 0.3232428 -0.1626224 0.0545056 -0.0731659 -0.1259845 2.3844467 0.1118113 -0.1226806 -0.1695252 1.4331062 -0.1427990 -0.0138465 0.6204480 -0.0370891 0.0827362 -0.3705738 0.0545056 2.3050838 2.3844467 -0.1695252 -0.0397883 -0.0138465 0.0827362 2.6221297 2.3050839 -0.0397883 0.0533108 2.6221298 0.0533108 covariance matrix of random effect coefficients head(Lambda_new) ## 6 x 54 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] 7.822487 -0.48495997 . . . . . . . . ## [2,] -0.484960 0.05126495 . . . . . . . . ## [3,] . . 7.822487 -0.48495997 . . . . . . ## [4,] . . -0.484960 0.05126495 . . . . . . ## [5,] . . . . 7.822487 -0.48495997 . . . . ## [6,] . . . . -0.484960 0.05126495 . . . . ## ## [1,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## ## [1,] . . . . . . . ## [2,] . . . . . . . ## [3,] . . . . . . . ## [4,] . . . . . . . ## [5,] . . . . . . . ## [6,] . . . . . . . 6.7.13 Computer predicted values yhat &lt;- X%*%(bhat)+Z%*%comput_uhat # comput_uhat= uhat head(yhat) ## 6 x 1 Matrix of class &quot;dgeMatrix&quot; ## [,1] ## 1 25.02967 ## 2 26.63781 ## 3 28.24595 ## 4 29.85408 ## 5 21.51081 ## 6 22.87508 head(fitted(fit.lmer.slope)) ## 1 2 3 4 5 6 ## 25.02967 26.63781 28.24595 29.85408 21.51081 22.87508 6.7.14 Using lmm with two grouping factors library(lme4) fit.lmer.slope2 &lt;- lmer(distance ~ age + Sex +(1+ age | Subject)+ (1 | age), data = Data) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(fit.lmer.slope2) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: distance ~ age + Sex + (1 + age | Subject) + (1 | age) ## Data: Data ## ## REML criterion at convergence: 435.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.0821 -0.4568 0.0154 0.4471 3.8942 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 7.81589 2.7957 ## age 0.05122 0.2263 -0.77 ## age (Intercept) 0.00000 0.0000 ## Residual 1.71643 1.3101 ## Number of obs: 108, groups: Subject, 27; age, 4 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 17.63511 0.88612 19.902 ## age 0.66019 0.07124 9.267 ## SexFemale -2.14527 0.75745 -2.832 ## ## Correlation of Fixed Effects: ## (Intr) age ## age -0.838 ## SexFemale -0.348 0.000 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) 6.7.15 Get z2 58= 54+4 ages Z2 &lt;- getME(fit.lmer.slope2, &quot;Z&quot;) head(Z2,10) ## 10 x 58 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 58 column names &#39;M16&#39;, &#39;M16&#39;, &#39;M05&#39; ... ]] ## ## 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 8 . . . . . . . ## 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 10 . . . . . . . ## 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 12 . . . . . . . ## 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 14 . . . . . . . ## 5 . . . . 1 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . 1 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 7 . . . . 1 12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 8 . . . . 1 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 9 . . . . . . . . . . . . 1 8 . . . . . . . . . . . . . . . . . . . . . . . ## 10 . . . . . . . . . . . . 1 10 . . . . . . . . . . . . . . . . . . . . . . . ## ## 1 . . . . . . . . . . . . . . . . . 1 . . . ## 2 . . . . . . . . . . . . . . . . . . 1 . . ## 3 . . . . . . . . . . . . . . . . . . . 1 . ## 4 . . . . . . . . . . . . . . . . . . . . 1 ## 5 . . . . . . . . . . . . . . . . . 1 . . . ## 6 . . . . . . . . . . . . . . . . . . 1 . . ## 7 . . . . . . . . . . . . . . . . . . . 1 . ## 8 . . . . . . . . . . . . . . . . . . . . 1 ## 9 . . . . . . . . . . . . . . . . . 1 . . . ## 10 . . . . . . . . . . . . . . . . . . 1 . . dim(Z2) ## [1] 108 58 6.7.16 Using lmm with nested or crossed random effects library(lme4) fit.lmer.slope3 &lt;- lmer(distance ~ age + Sex +(1 | Sex/Subject), data = Data) summary(fit.lmer.slope3) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: distance ~ age + Sex + (1 | Sex/Subject) ## Data: Data ## ## REML criterion at convergence: 437.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7489 -0.5503 -0.0252 0.4534 3.6575 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Subject:Sex (Intercept) 3.2668 1.8074 ## Sex (Intercept) 0.9392 0.9691 ## Residual 2.0495 1.4316 ## Number of obs: 108, groups: Subject:Sex, 27; Sex, 2 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 17.70671 1.27852 13.85 ## age 0.66019 0.06161 10.72 ## SexFemale -2.32102 1.56785 -1.48 ## ## Correlation of Fixed Effects: ## (Intr) age ## age -0.530 ## SexFemale -0.586 0.000 ## optimizer (nloptwrap) convergence code: 0 (OK) ## Model is nearly unidentifiable: large eigenvalue ratio ## - Rescale variables? 6.7.17 Get z3 29= 27+2 ages Z3 &lt;- getME(fit.lmer.slope3, &quot;Z&quot;) head(Z3,10) ## 10 x 29 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 29 column names &#39;M16:Male&#39;, &#39;M05:Male&#39;, &#39;M02:Male&#39; ... ]] ## ## 1 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . 1 . ## 2 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . 1 . ## 3 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . 1 . ## 4 . . . . . . . . . . . . . . 1 . . . . . . . . . . . . 1 . ## 5 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . 1 . ## 6 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . 1 . ## 7 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . 1 . ## 8 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . 1 . ## 9 . . . . . . 1 . . . . . . . . . . . . . . . . . . . . 1 . ## 10 . . . . . . 1 . . . . . . . . . . . . . . . . . . . . 1 . dim(Z3) ## [1] 108 29 Remark nested random effects occur when one factor (grouping variable) appears only within a particular level of another factor (grouping variable). lmer (1|group1/group2) Crossed random effects are simply: not nested. individual observations are nested separately within the two or more factors. lmer (1|group1) + (1|group2) 6.8 Linear mixed model covariance decomposition (nlme) 6.8.1 Load data data(Orthodont, package=&quot;nlme&quot;) Data &lt;- Orthodont library(ggplot2) ggplot(Data, aes(y = distance, x = age, color = Sex)) + geom_smooth(method = &quot;lm&quot;,se=F) + geom_point() + theme_classic() ## `geom_smooth()` using formula &#39;y ~ x&#39; library(ggplot2) ggplot(Data, aes(y = distance, x = age, fill = Subject, color = Sex)) + geom_smooth(method = &quot;lm&quot;,se=F) + geom_point() + theme_classic() ## `geom_smooth()` using formula &#39;y ~ x&#39; 6.8.2 Using lme (nlme) library(nlme) ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmList ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse fit.lmer2 &lt;- lme(distance ~ age + Sex , random = ~1 | Subject, data = Data) summary(fit.lmer2) ## Linear mixed-effects model fit by REML ## Data: Data ## AIC BIC logLik ## 447.5125 460.7823 -218.7563 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.807425 1.431592 ## ## Fixed effects: distance ~ age + Sex ## Value Std.Error DF t-value p-value ## (Intercept) 17.706713 0.8339225 80 21.233044 0.0000 ## age 0.660185 0.0616059 80 10.716263 0.0000 ## SexFemale -2.321023 0.7614168 25 -3.048294 0.0054 ## Correlation: ## (Intr) age ## age -0.813 ## SexFemale -0.372 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.74889609 -0.55034466 -0.02516628 0.45341781 3.65746539 ## ## Number of Observations: 108 ## Number of Groups: 27 library(lme4) # lme4 fit.lmer &lt;- lmer(distance ~ age + Sex +(1 | Subject), data = Data) # compound symmetry t test tstat &lt;- fixef(fit.lmer2)/sqrt(diag(vcov(fit.lmer2))) pval &lt;- 2*pt(-abs(tstat), df = fit.lmer2$fixDF$X) Tresult &lt;- data.frame(t = tstat, p = round(pval, 4)) print(Tresult) ## t p ## (Intercept) 21.233044 0.0000 ## age 10.716263 0.0000 ## SexFemale -3.048294 0.0054 F test anova(fit.lmer2 ) numDF denDF F-value p-value (Intercept) 1 80 4123.155691 0.0000000 age 1 80 114.838287 0.0000000 Sex 1 25 9.292099 0.0053751 restricted model test anova(fit.lmer2, L = c(0, 1, 0)) numDF denDF F-value p-value 1 80 114.8383 0 likelihood test fit.lmer2.2 &lt;- lme(distance ~ Sex , random = ~1 | Subject, data = Data) anova(fit.lmer2, fit.lmer2.2) call Model df AIC BIC logLik Test L.Ratio p-value fit.lmer2 lme.formula(fixed = distance ~ age + Sex, data = Data, random = ~1 | Subject) 1 5 447.5125 460.7823 -218.7563 NA NA fit.lmer2.2 lme.formula(fixed = distance ~ Sex, data = Data, random = ~1 | Subject) 2 4 513.8718 524.5255 -252.9359 1 vs 2 68.35926 0 And now by hand logLik &lt;- logLik(fit.lmer2) logLik0 &lt;- logLik(fit.lmer2.2) LR &lt;- 2 * (logLik - logLik0) pval &lt;- pchisq(LR, df = 2, lower.tail = FALSE) LRresult &lt;- data.frame(LR = LR, p = round(pval, 4), row.names = &quot;age&quot;) print(LRresult) ## LR p ## age 68.35926 0 6.8.3 Model diagnosing library(nlme) plot.lme(fit.lmer2) 6.8.4 Get x, y, z matrixs (using lme4) X=(getME(fit.lmer, &quot;X&quot;)) y=getME(fit.lmer, &quot;y&quot;) Z &lt;- getME(fit.lmer, &quot;Z&quot;) Z2 &lt;- model.matrix(~Subject-1,data=Data) check z matrix dummyz&lt;- as.matrix (Z)==Z2 table(dummyz) TRUE 2916 dim(Z) ## [1] 108 27 check y and X matrixs dummyx&lt;- Data[,c(2)]==X[,2] table(dummyx) TRUE 108 6.8.5 Fixed effect coefficient bhat &lt;- fit.lmer2$coef$fixed bhat ## (Intercept) age SexFemale ## 17.7067130 0.6601852 -2.3210227 # fixef() fixed effect coefficient confidence intervals intervals(fit.lmer2) ## Approximate 95% confidence intervals ## ## Fixed effects: ## lower est. upper ## (Intercept) 16.0471544 17.7067130 19.3662716 ## age 0.5375855 0.6601852 0.7827849 ## SexFemale -3.8891901 -2.3210227 -0.7528554 ## ## Random Effects: ## Level: Subject ## lower est. upper ## sd((Intercept)) 1.31035 1.807425 2.493061 ## ## Within-group standard error: ## lower est. upper ## 1.226102 1.431592 1.671522 6.8.5.1 Plot fixed effect coefficients and theri ci tab &lt;- cbind(Est = intervals(fit.lmer2)[[&quot;fixed&quot;]][,2], LL = intervals(fit.lmer2)[[&quot;fixed&quot;]][,1], UL= intervals(fit.lmer2)[[&quot;fixed&quot;]][,3]) #Extracting the fixed effect estimates and manually calculating the 95% confidence limits #qnorm extracts the standard normal critical value for 1-alpha/2= 1-0.05/2=0.975 Odds= (tab) round(Odds,4) Est LL UL (Intercept) 17.7067 16.0472 19.3663 age 0.6602 0.5376 0.7828 SexFemale -2.3210 -3.8892 -0.7529 Odds=as.data.frame(Odds) Odds$Label=rownames(Odds) ggplot(Odds[-1,],aes(x=Est,y=Label))+geom_point()+ geom_errorbarh(aes(xmax = UL, xmin = LL))+ theme_bw()+geom_vline(xintercept=0,col=&quot;darkgray&quot;,size=1.2,linetype=2)+ theme(axis.title.y = element_blank(),axis.text = element_text(size=10), axis.title.x = element_text(size=12))+labs(x=&quot;Beta coefficients&quot;) And now by hand print(tquants &lt;- qt(0.975, df = fit.lmer2$fixDF$X)) ## (Intercept) age SexFemale ## 1.990063 1.990063 2.059539 Low &lt;- fixef(fit.lmer2) - tquants * sqrt(diag(vcov(fit.lmer2))) Upp &lt;- fixef(fit.lmer2) + tquants * sqrt(diag(vcov(fit.lmer2))) EstInt &lt;- data.frame(Lower = Low, Estimate = fixef(fit.lmer2), Upper = Upp) print(EstInt) ## Lower Estimate Upper ## (Intercept) 16.0471544 17.7067130 19.3662716 ## age 0.5375855 0.6601852 0.7827849 ## SexFemale -3.8891901 -2.3210227 -0.7528554 6.8.6 Random effect coefficient fit.lmer2$coef$random ## $Subject ## (Intercept) ## M16 -1.70183357 ## M05 -1.70183357 ## M02 -1.37767479 ## M11 -1.16156894 ## M07 -1.05351602 ## M08 -0.94546309 ## M03 -0.62130432 ## M12 -0.62130432 ## M13 -0.62130432 ## M14 -0.08103969 ## M09 0.13506616 ## M15 0.78338371 ## M06 1.21559540 ## M04 1.43170125 ## M01 2.40417758 ## M10 3.91691853 ## F10 -3.58539251 ## F09 -1.31628108 ## F06 -1.31628108 ## F01 -1.10017523 ## F05 -0.01964599 ## F07 0.30451279 ## F02 0.30451279 ## F08 0.62867156 ## F03 0.95283034 ## F04 1.92530666 ## F11 3.22194176 6.8.7 Fixed effect coefficients covariance fit.lmer2$varFix (Intercept) age SexFemale (Intercept) 0.6954267 -0.0417482 -0.2361967 age -0.0417482 0.0037953 0.0000000 SexFemale -0.2361967 0.0000000 0.5797556 # vcov(fit.lmer2) sqrt(diag(vcov(fit.lmer2))) ## (Intercept) age SexFemale ## 0.83392247 0.06160592 0.76141685 fixed effect coefficients correlation -0.04174818/prod(sqrt( diag(fit.lmer2$varFix)[-3] )) ## [1] -0.8126236 6.8.8 Random effect covariance and correlation pdMatrix(fit.lmer2$modelStruct$reStruct[[1]])*fit.lmer2$sigma**2 (Intercept) (Intercept) 3.266784 getVarCov(fit.lmer2) #using default ## Random effects variance covariance matrix ## (Intercept) ## (Intercept) 3.2668 ## Standard Deviations: 1.8074 getVarCov(fit.lmer2, individual = &quot;F01&quot;, type = &quot;marginal&quot;) ## Subject F01 ## Marginal variance covariance matrix ## 1 2 3 4 ## 1 5.3162 3.2668 3.2668 3.2668 ## 2 3.2668 5.3162 3.2668 3.2668 ## 3 3.2668 3.2668 5.3162 3.2668 ## 4 3.2668 3.2668 3.2668 5.3162 ## Standard Deviations: 2.3057 2.3057 2.3057 2.3057 getVarCov(fit.lmer2, type = &quot;conditional&quot;) ## Subject M01 ## Conditional variance covariance matrix ## 1 2 3 4 ## 1 2.0495 0.0000 0.0000 0.0000 ## 2 0.0000 2.0495 0.0000 0.0000 ## 3 0.0000 0.0000 2.0495 0.0000 ## 4 0.0000 0.0000 0.0000 2.0495 ## Standard Deviations: 1.4316 1.4316 1.4316 1.4316 vc &lt;- as.numeric(as.matrix(VarCorr(fit.lmer2))[,1]) vc/sum(vc) ## [1] 0.6144914 0.3855086 VarCorr(fit.lmer2) ## Subject = pdLogChol(1) ## Variance StdDev ## (Intercept) 3.266784 1.807425 ## Residual 2.049456 1.431592 6.8.9 Get y covariance directly = Z REcov Zt + sigma**2 require(mgcv) ## Loading required package: mgcv ## This is mgcv 1.8-40. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. library(nlme) cov &lt;- extract.lme.cov(fit.lmer2,Data) head(cov) 5.316240 3.266784 3.266784 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 5.316240 3.266784 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 3.266784 5.316240 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 3.266784 3.266784 5.316240 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 5.316240 3.266784 3.266784 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.266784 5.316240 3.266784 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(cov) ## [1] 108 108 6.8.10 Residual variance fit.lmer2$sigma**2 ## [1] 2.049456 6.8.11 Compute fixed effect coefficients library(matlib) inv(t(as.matrix(X))%*%inv(as.matrix(cov))%*%as.matrix(X))%*%t(as.matrix(X))%*%inv(as.matrix(cov))%*%y 17.7067076 0.6601871 -2.3210230 6.8.12 Compute covariance of fixed efffect coefficients inv(t(as.matrix(X))%*%inv(as.matrix(cov))%*%as.matrix(X)) 0.6954267 -0.0417482 -0.2361967 -0.0417482 0.0037953 0.0000000 -0.2361967 0.0000000 0.5797556 6.8.13 Compute random effect coefficients Lambda_new &lt;-as.numeric(VarCorr(fit.lmer2)[1])*diag(length(levels(Data$Subject))) # head(Lambda_new) uhat &lt;- fit.lmer2$coef$random comput_uhat &lt;- (as.matrix(Lambda_new))%*%t(Z)%*%inv(as.matrix(cov))%*%(y-as.matrix(X)%*%(bhat)) cbind((comput_uhat@x),(uhat[[&quot;Subject&quot;]])) (Intercept) M16 -1.7018337 -1.7018336 M05 -1.7018337 -1.7018336 M02 -1.3776749 -1.3776748 M11 -1.1615690 -1.1615689 M07 -1.0535161 -1.0535160 M08 -0.9454632 -0.9454631 M03 -0.6213044 -0.6213043 M12 -0.6213044 -0.6213043 M13 -0.6213044 -0.6213043 M14 -0.0810397 -0.0810397 M09 0.1350662 0.1350662 M15 0.7833838 0.7833837 M06 1.2155955 1.2155954 M04 1.4317013 1.4317013 M01 2.4041777 2.4041776 M10 3.9169188 3.9169185 F10 -3.5853927 -3.5853925 F09 -1.3162812 -1.3162811 F06 -1.3162812 -1.3162811 F01 -1.1001753 -1.1001752 F05 -0.0196460 -0.0196460 F07 0.3045128 0.3045128 F02 0.3045128 0.3045128 F08 0.6286716 0.6286716 F03 0.9528304 0.9528303 F04 1.9253068 1.9253067 F11 3.2219420 3.2219418 6.8.14 Compute predicted values usually computing mean and prediction ci by using bootstrapping and simulated methods approximate at below: This webpage calculates the sd like above, but I think the following is also correct. 6.8.14.1 the conditional distribution \\[ \\mathbf{y}_{} \\mid \\mathbf{b}_{} \\sim \\mathbf{N}\\left(\\mathbf{X}_{} \\boldsymbol{\\beta}+\\mathbf{Z}_{} \\mathbf{b}_{}, \\boldsymbol{\\Sigma}_{}\\right) \\] therefore conditional mean prediction \\[ \\begin{align} E(\\hat{Y}_0)&amp;=E\\mathbf{(X_0\\hat{\\beta} + Z_0\\hat{u})}=\\mathbf{X_0\\beta + Z_0{u}}=E\\mathbf{(Y_0)}\\\\ var(\\hat{Y}_0)&amp;=E\\mathbf{((X_0\\hat{\\beta}-X_0\\beta)+ (Z_0\\hat{u} - Z_0{u}))}^2\\\\&amp;=E\\mathbf{((X_0\\hat{\\beta}-X_0\\beta)^2+ (Z_0\\hat{u} - Z_0{u})^2+ 2 ((X_0\\hat{\\beta}-X_0\\beta)* (Z_0\\hat{u} - Z_0{u})) )} \\\\ &amp;=E\\mathbf{\\left( X_0(\\hat{\\beta}-\\beta)(\\hat{\\beta}-\\beta)&#39;X_0&#39; \\right)}+ E\\mathbf{\\left( Z_0(\\hat{u}-u)(\\hat{u}-u)&#39;Z_0&#39; \\right)}+2E ((X_0(\\hat{\\beta}-\\beta))* (Z_0(\\hat{u} - {u}))) ? \\\\ &amp;=\\sigma^2\\mathbf{X_0\\left( X&#39;X \\right)^{-1}X_0&#39;}+ \\sigma^2\\mathbf{Z_0\\left( Z&#39;Z \\right)^{-1}Z_0&#39;} + 2 *0*0 \\quad \\text {if independent}\\\\ &amp;= \\mathbf{X_0 cov (X) X_0&#39;}+ \\mathbf{Z_0 cov (Z) Z_0&#39;} \\\\ \\end{align} \\] generally \\[ \\begin{align} Var(X+Y) &amp;= Cov(X+Y,X+Y) \\\\ &amp;= E((X+Y)^2)-E(X+Y)E(X+Y) \\\\ &amp;\\text{by expanding,} \\\\ &amp;= E(X^2) - (E(X))^2 + E(Y^2) - (E(Y))^2 + 2(E(XY) - E(X)E(Y)) \\\\ &amp;= Var(X) + Var(Y) + 2(E(XY)) - E(X)E(Y)) \\\\ \\end{align} \\] conditional individual prediction \\[ \\begin{align} E(\\hat{Y}_0)&amp;= E\\mathbf{(Y_0)}\\\\ var(\\hat{Y}_0) &amp;= \\mathbf{X_0 cov (X) X_0&#39;}+ \\mathbf{Z_0 cov (Z) Z_0&#39;} + \\sigma^2 \\\\ \\end{align} \\] \\[ \\begin{align} \\hat{Y}_0&amp; \\sim N(\\mu_{\\hat{Y}_0},\\sigma^2_{\\hat{Y}_0})\\\\ \\end{align} \\] lower level (conditional prediction) in R # how to calculate predicted values yhat &lt;- X%*%(fit.lmer2$coef$fixed)+Z%*% as.numeric ( uhat[[&quot;Subject&quot;]]) head(cbind (yhat,predict(fit.lmer2),y)) #create individual trajectory curve ## 6 x 3 Matrix of class &quot;dgeMatrix&quot; ## y ## 1 25.39237 25.39237 26.0 ## 2 26.71274 26.71274 25.0 ## 3 28.03311 28.03311 29.0 ## 4 29.35348 29.35348 31.0 ## 5 21.61052 21.61052 21.5 ## 6 22.93089 22.93089 22.5 #compute standard error for marginal predictions predvar_rand &lt;- diag(X %*% fit.lmer2$varFix %*% t(X)) + diag(Z %*% diag(getVarCov(fit.lmer2)[1] ,27) %*% t(Z)) SE_rand &lt;- sqrt (predvar_rand) #mean prediction SE_rand2 &lt;- sqrt(predvar_rand+fit.lmer2$sigma^2) #individual prediction head(SE_rand,20) ## 1 2 3 4 5 6 7 8 ## 1.880728 1.872639 1.872639 1.880728 1.880728 1.872639 1.872639 1.880728 ## 9 10 11 12 13 14 15 16 ## 1.880728 1.872639 1.872639 1.880728 1.880728 1.872639 1.872639 1.880728 ## 17 18 19 20 ## 1.880728 1.872639 1.872639 1.880728 head(SE_rand2,20) ## 1 2 3 4 5 6 7 8 ## 2.363598 2.357166 2.357166 2.363598 2.363598 2.357166 2.357166 2.363598 ## 9 10 11 12 13 14 15 16 ## 2.363598 2.357166 2.357166 2.363598 2.363598 2.357166 2.357166 2.363598 ## 17 18 19 20 ## 2.363598 2.357166 2.357166 2.363598 see below 6.8.14.2 The marginal distribution \\[ \\mathbf{y}_{} \\sim \\mathrm{N}\\left(\\mathbf{X}_{} \\boldsymbol{\\beta}, \\mathbf{V}_{}\\right) \\] \\[ \\operatorname{Cov}[\\mathbf{y}]=\\mathbf{V}=\\mathbf{Z D Z}^{\\prime}+\\Sigma \\] \\(\\mathbf{Z}_{} \\mathbf{D Z}_{}^{\\prime}\\) represents the random-effects structure therefore marginal mean prediction \\[ \\begin{align} E(\\hat{Y}_0)&amp;=E\\mathbf{(X_0\\hat{\\beta})}=\\mathbf{X_0\\beta}=E\\mathbf{(Y_0)}\\\\ var(\\hat{Y}_0)&amp;=E\\mathbf{(X_0\\hat{\\beta}-X_0\\beta)}^2\\\\ &amp;=E\\mathbf{\\left( X_0(\\hat{\\beta}-\\beta)(\\hat{\\beta}-\\beta)&#39;X_0&#39; \\right)}\\\\ &amp;=E\\mathbf{X_0\\left( (\\hat{\\beta}-\\beta)(\\hat{\\beta}-\\beta)&#39; \\right)X_0&#39;}\\\\ &amp;=\\mathbf{X_0Cov(X)X_0&#39;}\\\\ \\end{align} \\] marginal individual prediction \\[ \\begin{align} E(\\hat{Y}_0)&amp;= E\\mathbf{(Y_0)}\\\\ var(\\hat{Y}_0) &amp;= \\mathbf{X_0 cov (X) X_0&#39;}+ \\sigma^2 \\\\ \\end{align} \\] higher level (marginal) in R #compute standard error for marginal predictions predvar &lt;- diag(X %*% fit.lmer2$varFix %*% t(X)) SE &lt;- sqrt (predvar) #mean prediction SE2 &lt;- sqrt(predvar+fit.lmer2$sigma^2) #individual prediction head(SE,10) ## 1 2 3 4 5 6 7 8 ## 0.5199561 0.4898898 0.4898898 0.5199561 0.5199561 0.4898898 0.4898898 0.5199561 ## 9 10 ## 0.5199561 0.4898898 head(SE2,10) ## 1 2 3 4 5 6 7 8 ## 1.523092 1.513092 1.513092 1.523092 1.523092 1.513092 1.513092 1.523092 ## 9 10 ## 1.523092 1.513092 up=predict(fit.lmer2, newdata=Data, level=0) +1.96 *SE #mean prediction up2=predict(fit.lmer2, newdata=Data, level=0) +1.96 *SE2 #individual prediction head(up) ## 1 2 3 4 5 6 ## 24.00731 25.26875 26.58912 27.96842 24.00731 25.26875 head(up2) ## 1 2 3 4 5 6 ## 25.97346 27.27423 28.59460 29.93457 25.97346 27.27423 library(tidyverse) library(ggeffects) ggpredict(fit.lmer2,terms=c(&quot;age&quot; )) x predicted std.error conf.low conf.high group 8 22.98819 0.5199561 21.96910 24.00729 1 10 24.30856 0.4898898 23.34840 25.26873 1 12 25.62894 0.4898898 24.66877 26.58910 1 14 26.94931 0.5199561 25.93021 27.96840 1 ggpredict(fit.lmer2,terms=c(&quot;age&quot; )) %&gt;% plot(rawdata = T, dot.alpha = 0.2) # ggpredict(fit.lmer2,&quot;age&quot;, type = &quot;re&quot; ) %&gt;% plot(rawdata = T, dot.alpha = 0.2) Compute SE by hand and compare with above SE: 0.520408163 0.489795918 0.489795918 0.520408163 head(SE,10) ## 1 2 3 4 5 6 7 8 ## 0.5199561 0.4898898 0.4898898 0.5199561 0.5199561 0.4898898 0.4898898 0.5199561 ## 9 10 ## 0.5199561 0.4898898 # head(predict(fit.lmer2)) # head (predict(fit.lmer2, newdata=Data, level=1)) #predicted value # head (predict(fit.lmer2, newdata=Data, level=0)) #mean and ci 6.8.15 Using lme with Gaussian it means the correlation in a group but not correlation in variables (different covariances) library(nlme) fit.lmer.gaus &lt;- lme(distance ~ age + Sex , random = ~1 | Subject, correlation=corGaus(form= ~ age|Subject, nugget=TRUE), data = Data) summary(fit.lmer.gaus) ## Linear mixed-effects model fit by REML ## Data: Data ## AIC BIC logLik ## 451.4311 470.0089 -218.7156 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.795504 1.446192 ## ## Correlation Structure: Gaussian spatial correlation ## Formula: ~age | Subject ## Parameter estimate(s): ## range nugget ## 1.1324270 0.1039537 ## Fixed effects: distance ~ age + Sex ## Value Std.Error DF t-value p-value ## (Intercept) 17.717033 0.8448813 80 20.969848 0.0000 ## age 0.659620 0.0628344 80 10.497756 0.0000 ## SexFemale -2.325355 0.7612563 25 -3.054629 0.0053 ## Correlation: ## (Intr) age ## age -0.818 ## SexFemale -0.367 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.70618679 -0.54458678 -0.01435195 0.45606804 3.62915585 ## ## Number of Observations: 108 ## Number of Groups: 27 Plotting of the empirical covariation (of residuals) versus “age” when describing the spatial pattern of a measured variable, a common way of visualizing the spatial autocorrelation of a variable is a variogram plot. checking the semi-variance change by the distance of spatial autocorrelation. print(plot(Variogram(fit.lmer.gaus, form =~as.numeric(age)|Subject, data = Data))) require(mgcv) library(nlme) cov.gaus &lt;- extract.lme.cov(fit.lmer.gaus,Data) head(cov.gaus) 5.315307 3.306657 3.223843 3.223836 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.306657 5.315307 3.306657 3.223843 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.223843 3.306657 5.315307 3.306657 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.223836 3.223843 3.306657 5.315307 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 5.315307 3.306657 3.223843 3.223836 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.306657 5.315307 3.306657 3.223843 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(cov.gaus) ## [1] 108 108 Compute fixed effect coefficients using Gaussian library(matlib) inv(t(as.matrix(X))%*%inv(as.matrix(cov.gaus))%*%as.matrix(X))%*%t(as.matrix(X))%*%inv(as.matrix(cov.gaus))%*%y 17.7170272 0.6596201 -2.3253552 fit.lmer.gaus$coef$fixed ## (Intercept) age SexFemale ## 17.717033 0.659620 -2.325355 the same below 6.8.16 Using lme with autoregressive fit.lmer.AR1 &lt;- lme(distance ~ age + Sex , random = ~1 | Subject, correlation=corAR1(form= ~ as.numeric(age)|Subject ), data = Data) summary(fit.lmer.AR1) ## Linear mixed-effects model fit by REML ## Data: Data ## AIC BIC logLik ## 449.5125 465.4363 -218.7563 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.807425 1.431592 ## ## Correlation Structure: ARMA(1,0) ## Formula: ~as.numeric(age) | Subject ## Parameter estimate(s): ## Phi1 ## 0 ## Fixed effects: distance ~ age + Sex ## Value Std.Error DF t-value p-value ## (Intercept) 17.706713 0.8339225 80 21.233044 0.0000 ## age 0.660185 0.0616059 80 10.716263 0.0000 ## SexFemale -2.321023 0.7614168 25 -3.048294 0.0054 ## Correlation: ## (Intr) age ## age -0.813 ## SexFemale -0.372 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.74889609 -0.55034466 -0.02516628 0.45341781 3.65746539 ## ## Number of Observations: 108 ## Number of Groups: 27 Plotting of the empirical covariation (of residuals) versus “age” print(plot(Variogram(fit.lmer.AR1, form =~as.numeric(age)|Subject, data = Data))) require(mgcv) library(nlme) cov.AR1 &lt;- extract.lme.cov(fit.lmer.AR1,Data) head(cov.AR1) 5.316240 3.266784 3.266784 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 5.316240 3.266784 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 3.266784 5.316240 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 3.266784 3.266784 5.316240 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 5.316240 3.266784 3.266784 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.266784 5.316240 3.266784 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(cov.AR1) ## [1] 108 108 6.8.17 Using lme with exponential fit.lmer.Exp &lt;- lme(distance ~ age + Sex , random = ~1 | Subject, correlation=corExp(form= ~ as.numeric(age)|Subject ), data = Data) summary(fit.lmer.Exp) ## Linear mixed-effects model fit by REML ## Data: Data ## AIC BIC logLik ## 449.3968 465.3206 -218.6984 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.788899 1.454494 ## ## Correlation Structure: Exponential spatial correlation ## Formula: ~as.numeric(age) | Subject ## Parameter estimate(s): ## range ## 0.7045117 ## Fixed effects: distance ~ age + Sex ## Value Std.Error DF t-value p-value ## (Intercept) 17.721416 0.8500193 80 20.848250 0.0000 ## age 0.659405 0.0634074 80 10.399499 0.0000 ## SexFemale -2.327485 0.7611852 25 -3.057711 0.0053 ## Correlation: ## (Intr) age ## age -0.821 ## SexFemale -0.365 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.683026667 -0.540915318 -0.008097445 0.461167542 3.612579065 ## ## Number of Observations: 108 ## Number of Groups: 27 Plotting of the empirical covariation (of residuals) versus “age” print(plot(Variogram(fit.lmer.Exp, form =~as.numeric(age)|Subject, data = Data))) require(mgcv) library(nlme) cov.Exp &lt;- extract.lme.cov(fit.lmer.Exp,Data) head(cov.Exp) 5.315710 3.323904 3.207397 3.200582 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.323904 5.315710 3.323904 3.207397 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.207397 3.323904 5.315710 3.323904 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.200582 3.207397 3.323904 5.315710 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 5.315710 3.323904 3.207397 3.200582 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.323904 5.315710 3.323904 3.207397 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(cov.Exp) ## [1] 108 108 6.8.18 Using lme with unstructured fit.lmer.corSymm &lt;- lme(distance ~ age + Sex , random = ~1 | (Subject), correlation=corSymm(form= ~ 1| (Subject) ), weights = varIdent(form=~1|age), data = Data) summary(fit.lmer.corSymm) ## Linear mixed-effects model fit by REML ## Data: Data ## AIC BIC logLik ## 456.6945 493.85 -214.3473 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.783141 1.481553 ## ## Correlation Structure: General ## Formula: ~1 | (Subject) ## Parameter estimate(s): ## Correlation: ## 1 2 3 ## 2 -0.260 ## 3 0.238 -0.149 ## 4 -0.251 -0.007 0.426 ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | age ## Parameter estimates: ## 8 10 12 14 ## 1.0000000 0.6868412 1.1990772 1.0004135 ## Fixed effects: distance ~ age + Sex ## Value Std.Error DF t-value p-value ## (Intercept) 17.417592 0.8657149 80 20.119316 0.0000 ## age 0.674651 0.0702289 80 9.606462 0.0000 ## SexFemale -2.045167 0.7361374 25 -2.778241 0.0102 ## Correlation: ## (Intr) age ## age -0.840 ## SexFemale -0.346 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.33886286 -0.56222407 0.01374192 0.52235930 3.99060002 ## ## Number of Observations: 108 ## Number of Groups: 27 Plotting of the empirical covariation (of residuals) versus “age” print(plot(Variogram(fit.lmer.corSymm, form =~as.numeric(age)|Subject, data = Data))) require(mgcv) library(nlme) cov.corSymm &lt;- extract.lme.cov(fit.lmer.corSymm,Data) head(cov.corSymm) 5.374591 2.786933 3.807033 2.628359 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.786933 4.215084 2.909655 3.168383 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.807033 2.909655 6.335531 4.301450 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.628359 3.168383 4.301450 5.376406 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 5.374591 2.786933 3.807033 2.628359 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 2.786933 4.215084 2.909655 3.168383 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(cov.corSymm) ## [1] 108 108 6.8.19 Using lme with compound symm fit.lmer.Symm &lt;- lme(distance ~ age + Sex , random = ~1 | (Subject), correlation=corCompSymm(form= ~ as.numeric(age)| (Subject) ), data = Data) summary(fit.lmer.Symm) ## Linear mixed-effects model fit by REML ## Data: Data ## AIC BIC logLik ## 449.5125 465.4363 -218.7563 ## ## Random effects: ## Formula: ~1 | Subject ## (Intercept) Residual ## StdDev: 1.807425 1.431592 ## ## Correlation Structure: Compound symmetry ## Formula: ~as.numeric(age) | (Subject) ## Parameter estimate(s): ## Rho ## 0 ## Fixed effects: distance ~ age + Sex ## Value Std.Error DF t-value p-value ## (Intercept) 17.706713 0.8339225 80 21.233044 0.0000 ## age 0.660185 0.0616059 80 10.716263 0.0000 ## SexFemale -2.321023 0.7614168 25 -3.048294 0.0054 ## Correlation: ## (Intr) age ## age -0.813 ## SexFemale -0.372 0.000 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.74889609 -0.55034466 -0.02516628 0.45341781 3.65746539 ## ## Number of Observations: 108 ## Number of Groups: 27 Plotting of the empirical covariation (of residuals) versus “age” print(plot(Variogram(fit.lmer.Symm, form =~as.numeric(age)|Subject, data = Data))) require(mgcv) library(nlme) cov.Symm &lt;- extract.lme.cov(fit.lmer.Symm,Data) head(cov.Symm) 5.316240 3.266784 3.266784 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 5.316240 3.266784 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 3.266784 5.316240 3.266784 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.266784 3.266784 3.266784 5.316240 0.000000 0.000000 0.000000 0.000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 5.316240 3.266784 3.266784 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.000000 0.000000 0.000000 0.000000 3.266784 5.316240 3.266784 3.266784 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dim(cov.Symm) ## [1] 108 108 6.8.20 Using gls with unstructured, corSymm gls.corsymm &lt;- gls(distance ~ Sex +age, Orthodont, correlation = corSymm(form = ~ 1 | Subject), weights = varIdent(form=~1|age)) summary(gls.corsymm) ## Generalized least squares fit by REML ## Model: distance ~ Sex + age ## Data: Orthodont ## AIC BIC logLik ## 454.6945 489.196 -214.3473 ## ## Correlation Structure: General ## Formula: ~1 | Subject ## Parameter estimate(s): ## Correlation: ## 1 2 3 ## 2 0.586 ## 3 0.652 0.563 ## 4 0.489 0.666 0.737 ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | age ## Parameter estimates: ## 8 10 12 14 ## 1.0000000 0.8855835 1.0857261 1.0001634 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 17.417594 0.8657135 20.119350 0.0000 ## SexFemale -2.045174 0.7361405 -2.778238 0.0065 ## age 0.674651 0.0702285 9.606514 0.0000 ## ## Correlation: ## (Intr) SexFml ## SexFemale -0.346 ## age -0.840 0.000 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.508189030 -0.584409277 0.003646472 0.531072913 2.179754376 ## ## Residual standard error: 2.318327 ## Degrees of freedom: 108 total; 105 residual 6.8.21 Using gls with compound symm, which = lmer model default = lme default = lme compound symm =lme auto reg in this example gls.comsys &lt;- gls(distance ~ Sex +age, Orthodont, correlation = corCompSymm(form = ~ 1 | Subject)) summary(gls.comsys) ## Generalized least squares fit by REML ## Model: distance ~ Sex + age ## Data: Orthodont ## AIC BIC logLik ## 447.5125 460.7823 -218.7563 ## ## Correlation Structure: Compound symmetry ## Formula: ~1 | Subject ## Parameter estimate(s): ## Rho ## 0.6144914 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 17.706713 0.8339225 21.233044 0.0000 ## SexFemale -2.321023 0.7614169 -3.048294 0.0029 ## age 0.660185 0.0616059 10.716263 0.0000 ## ## Correlation: ## (Intr) SexFml ## SexFemale -0.372 ## age -0.813 0.000 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.59712955 -0.64544226 -0.02540005 0.51680604 2.32947531 ## ## Residual standard error: 2.305697 ## Degrees of freedom: 108 total; 105 residual lme4 does not allow to specify it, and it can only fit LMMs with independent residual errors. However, the range of available variance-covariance matrices for the random effects are restricted to diagonal or general matrices. "],["probability.html", "7 Probability 7.1 Probability basics 7.2 Probability R practice", " 7 Probability 7.1 Probability basics 7.1.1 Events Probability is the likelihood of an outcome. e.g. {any combination of two dice} Event is a collection of outcomes. e.g. {both dice show the same face} The outcome space is all the possible outcomes. e.g. {all the possible outcomes die show} 7.1.2 Probability formulas discrete variable \\[ P(E) = \\frac{number\\ of\\ outcomes\\ in\\ E}{number\\ of\\ possible\\ outcomes} \\] continuous varable \\[ P\\begin{pmatrix}a\\leq X \\leq b \\end{pmatrix} = \\int_a^b f(x) dx \\] Probabilities of continuous random variables (X) are the area under the curve. The probability of any value is always zero. when X = k, \\[ P\\begin{pmatrix}X = k \\end{pmatrix} = 0 \\] 7.1.3 Calculation of probability (operations) union probability, addition rule + \\[ \\begin{aligned} &amp;P(A \\cup B)=P(A)+P(B)-P(A \\cap B) \\\\ &amp;P(A \\cup B)=P(A)+P(B) \\quad \\text { if } \\mathrm{A} \\text { and } \\mathrm{B} \\text { are mutually exclusive } \\end{aligned} \\] joint probability, multiple rule x \\[ \\begin{aligned} &amp;P(A \\cap B)=P(A \\mid B) P(B)=P(B \\mid A) P(A) \\\\ &amp;P(A \\cap B)=P(A) P(B) \\quad \\text { if } \\mathrm{A} \\text { and } \\mathrm{B} \\text { are independent } \\end{aligned} \\] Marginal Probability is without reference to any other event or events \\[ P(A) or P(B) \\] conditional Probability \\[ P(A\\mid B)=\\dfrac{P(A \\: \\cap\\: B)}{P(B)} \\] p-values are conditional probabilities. 7.1.4 Bayes’s theorem multiple law \\[ P(A \\cap B) = P(A\\ |\\ B) P(B) = P(B\\ |\\ A) P(A) \\] bayes’s formula \\[ P(B_j\\ |\\ A) = \\frac{P(A\\ |\\ B_j) P(B_j)}{P(A)}\\ \\] law of total probability \\(P(A)\\) \\[ P(A) = P(A\\ |\\ B_1) P(B_1) + \\cdots + P(A\\ |\\ B_n) P(B_n).\\notag \\] 7.1.5 Random variables and distribution functions Random variable takes on different values determined by chance. we can use random variables’ mathematical (distribution) function to find their probability. probability mass function (PMF, discrete), e.g. Binomial Distribution. upon some conditions are satisfied, the sampling distribution of the sample proportion is approximately normal. Probability Density Function (PDF, continuous), e.g. normal distribution, t, chi-squre, f… Cumulative Distribution Function (CDF). 7.1.6 Probability distribution joint distribution, discrete variables \\[ P\\left(X=x_{i}, Y=y_{j}\\right)=p_{i j}, i, j=1,2, \\ldots \\] Marginal distribution, discrete variables \\[ P\\left(X=x_{i}\\right)=\\sum_{j=1}^{\\infty} p_{i j}=p_{i}, i=1,2, \\ldots \\] conditional probability, discrete variables \\[ P\\left(Y=y_{j} \\mid X=x_{i}\\right)=\\frac{p_{i j}}{p_{i}}, j=1,2, \\ldots \\] 7.1.7 Conditional expectation Conditional expectation is the mathematical expectation of a conditional distribution. The discrete variable \\[ E(Y|X_i)=\\sum_{i=1}^{N}{(Y_i|X_i)}\\cdot p(Y_i|X_i) \\] The continuous variable \\[ E(Y|X)=\\int{(Y|X)}\\cdot g(Y|X)dY \\] expectation formula, discrete variable \\[ \\mu=E(X)=\\sum x_if(x_i) \\] 7.1.8 Conditional variance variance formula, discrete variable \\[ \\sigma^2=\\text{Var}(X)=\\sum (x_i-\\mu)^2f(x_i) \\] Conditional expectation and conditional variance exist and can be estimated by regression models. 7.1.9 Sampling We make inferences about the population based on the sample (inference) after summarizing data (description). The error is resulting from using a sample characteristic (statistic) to estimate a population characteristic (parameter). Standard Error \\[ SD(\\bar{X})=SE(\\bar{X})=\\dfrac{\\sigma}{\\sqrt{n}} \\] Central limit theorem and law of large numbers For a large sample size, x mean is approximately normally distributed, regardless of the distribution of the population one samples from. so, the population parameter can be estimated using the sample. With large samples, The mean of the sampling distribution is very close to the population mean. 7.1.10 Confidence interval The higher the confidence level, the wider the width of the interval and thus the poorer the precision. \\[ \\text{point estimate }\\pm M\\times \\hat{SE}(\\text{estimate}) \\] the margin of error \\[ E=z_{\\alpha/2}\\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}} \\] 7.1.11 Introduction to Hypothesis Testing -Set up the hypotheses and decide on the significance level. -Construct and calculate the test statistic. -Calculate probability value (p-value). -Make a decision and state an overall conclusion. 7.2 Probability R practice pros: don’t need complicated probability theory, easy (simulation) cons: hard to get the exact solution 7.2.1 Integrate integrate(function(x){x^2},0,2)$value ## [1] 2.666667 7.2.2 Derivation fxy = expression(2*x^2+y+3*x*y^2) dxy = deriv(fxy, c(&quot;x&quot;, &quot;y&quot;), func = TRUE) dxy(1,2) ## [1] 16 ## attr(,&quot;gradient&quot;) ## x y ## [1,] 16 13 7.2.3 Create random variables with specific distributions dnorm(0)# density at a number ## [1] 0.3989423 pnorm(1.28)# cumulative possibility ## [1] 0.8997274 qnorm(0.95)# quantile ## [1] 1.644854 rnorm(10)# random numbers ## [1] -0.1960954 -0.6226706 1.9647829 -0.3028613 0.6063094 -0.3114346 ## [7] 0.9203732 -1.3255641 0.3996609 0.2477700 using covariance matrix to generate Gaussian multiple variables library(MASS) Sigma &lt;- matrix(c(10,3,3,2),2,2) mvrnorm(n=20, rep(0, 2), Sigma) 1.1966051 0.7503701 0.5866985 -1.2388653 -1.4909270 1.0553603 -1.5332388 -0.9982745 4.4648343 1.2246606 -1.2666429 -0.2527817 5.6416318 0.8990023 0.9651718 0.3110785 -2.3932273 -1.6725040 -1.9635546 -1.7538956 -3.6660685 -3.7862973 -1.0203916 -1.5480827 -1.5638779 1.5222195 -4.7849416 -1.8680270 0.0683867 1.3832235 4.3041787 0.3343417 -0.4470743 -1.0450497 0.3749548 -0.4158789 1.6331491 0.0503997 -3.3541999 1.6051076 7.2.4 Prob function pnorm(1.96, 0,1) ## [1] 0.9750021 qnorm(0.025, 0,1) ## [1] -1.959964 pchisq(3.84,1,lower.tail=F) ## [1] 0.05004352 mean(rchisq(10000,1)&gt;3.84) #simulation ## [1] 0.0508 7.2.5 Vector and operations seq(1,10, 2) ## [1] 1 3 5 7 9 x=rep(1:3,6) x ## [1] 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 y=rep(1:3, each = 6) y ## [1] 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 3 x+y ## [1] 2 3 4 2 3 4 3 4 5 3 4 5 4 5 6 4 5 6 x-y ## [1] 0 1 2 0 1 2 -1 0 1 -1 0 1 -2 -1 0 -2 -1 0 x*y ## [1] 1 2 3 1 2 3 2 4 6 2 4 6 3 6 9 3 6 9 x/y ## [1] 1.0000000 2.0000000 3.0000000 1.0000000 2.0000000 3.0000000 0.5000000 ## [8] 1.0000000 1.5000000 0.5000000 1.0000000 1.5000000 0.3333333 0.6666667 ## [15] 1.0000000 0.3333333 0.6666667 1.0000000 x%*%y 72 7.2.6 Select and substitute elements of vector x[c(2,3)] ## [1] 2 3 x[-1] ## [1] 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 x[x&gt;2] ## [1] 3 3 3 3 3 3 x[x==2] ## [1] 2 2 2 2 2 2 # substitute x[x == 2] &lt;- 0.5 x ## [1] 1.0 0.5 3.0 1.0 0.5 3.0 1.0 0.5 3.0 1.0 0.5 3.0 1.0 0.5 3.0 1.0 0.5 3.0 7.2.7 Matrix and operations matrix(1:10,2,5) 1 3 5 7 9 2 4 6 8 10 matrix(1:10,5,2) 1 6 2 7 3 8 4 9 5 10 a &lt;- matrix(12:20,3,3) a[2,] ## [1] 13 16 19 a[,2] ## [1] 15 16 17 a[-2,] 12 15 18 14 17 20 a[,-2] 12 18 13 19 14 20 a[2,1]=21 a 12 15 18 21 16 19 14 17 20 7.2.8 Compute inverse, determinant and eigen values of matrix a&lt;-matrix(c(11,21,31,21,32,43,12,32,54),3,3) solve(a) -1.9775281 3.471910 -1.6179775 0.7977528 -1.247191 0.5617978 0.5000000 -1.000000 0.5000000 det(a) ## [1] -178 solve(a)*det(a) 352 -618 288 -142 222 -100 -89 178 -89 t(a) 11 21 31 21 32 43 12 32 54 eigen(a) ## eigen() decomposition ## $values ## [1] 91.6892193 5.6541299 -0.3433491 ## ## $vectors ## [,1] [,2] [,3] ## [1,] -0.2573423 -0.7530908 -0.9049786 ## [2,] -0.5253459 -0.1712782 0.3538153 ## [3,] -0.8110405 0.6352306 0.2362806 7.2.9 Dataframe name&lt;-c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;) chinese&lt;-c(92,96,95) math&lt;-c(86, 85, 92) score&lt;-data.frame(name, chinese, math) score name chinese math A 92 86 B 96 85 C 95 92 score[2] chinese 92 96 95 score$math ## [1] 86 85 92 7.2.10 Solve problems using simulation for loop sim&lt;-10000 p&lt;-numeric(sim) # numeric=NULL for (i in 1:sim){ p[i]&lt;- abs(mean(rnorm(10,20,sqrt(3)))-mean(rnorm(15,20,sqrt(3))))&lt;0.1 } mean(p) ## [1] 0.1115 using replicate mean(replicate(10000,abs(mean(rnorm(10,20,sqrt(3)))-mean(rnorm(15,20,sqrt(3))))&lt;0.1)) ## [1] 0.1151 using apply function A&lt;-matrix(rnorm(250000, 20,sqrt(3)),10000,25) head(A) 19.06126 20.81637 21.21042 19.95590 21.69846 18.29432 21.82770 18.76974 21.75469 21.10167 20.44127 19.08609 21.82262 20.92953 19.95200 21.11685 21.71785 18.60129 17.37932 18.43603 16.98873 18.16106 19.47008 16.81116 19.04474 21.43252 18.66127 20.71977 17.67351 20.65143 18.21753 16.08327 17.76197 21.77523 20.16426 21.63607 21.76520 16.32750 21.32716 19.39818 18.67702 19.14233 22.04621 21.73406 18.01110 20.33225 17.57646 20.82280 18.94702 21.05311 20.32921 19.50025 19.27749 22.84692 23.77889 20.12394 21.56154 21.56257 21.46917 18.15418 22.87068 22.25317 19.48977 21.47643 22.71880 20.01259 20.38784 20.49415 17.87627 19.00707 23.81232 19.00066 17.29624 21.35546 19.44918 19.72061 23.10872 19.42933 22.42458 22.48210 21.00183 21.99800 21.51217 19.30167 20.10484 20.25181 19.96251 22.74550 17.23501 20.78454 19.72268 19.32037 18.86909 17.19450 18.44990 21.10844 23.41835 19.47133 18.40928 22.09972 20.69447 22.24489 20.82483 19.62620 19.29607 19.15197 19.83745 21.07063 19.15482 17.75430 19.20044 18.26481 20.24717 16.00542 20.14961 21.53620 19.16235 17.06883 20.50845 20.52393 18.86387 21.14049 21.54363 23.30348 20.16277 20.26250 21.74331 19.55805 18.49223 23.49000 20.41913 19.05200 20.50605 19.33760 22.77407 19.79208 19.97451 18.47463 20.98751 22.06745 21.64389 24.27560 19.18268 21.74913 19.90141 17.20166 15.12476 20.79505 20.05493 19.15534 f&lt;-function(x) {abs(mean(x[1:10])-mean(x[11:25]))} # solve the mean by apply mean(apply(A,1,f)&gt;0.1) ## [1] 0.89 using probability method pnorm(0.1,0,sqrt(0.5))-pnorm(-0.1,0,sqrt(0.5)) ## [1] 0.1124629 7.2.11 Permutations and combinations choose(10,2) ## [1] 45 # combn(10,2) factorial(10) ## [1] 3628800 prod(1:10) ## [1] 3628800 7.2.12 Search value position in vector a&lt;-c(1,2,3,5,0,9) which(a==min(a)) ## [1] 5 sum(a) ## [1] 20 unique(a) ## [1] 1 2 3 5 0 9 length(a) ## [1] 6 min(a) ## [1] 0 max(a) ## [1] 9 all(c(3,4) %in% a) ## [1] FALSE 7.2.13 Solve directly and optimize plot and find the range of solve f&lt;-function(x){x^2-exp(x)} uniroot(f,c(-0.8,-0.6)) $root ## [1] -0.7034781 f2&lt;-function(x){abs(x^2-exp(x))} optimize (f2,lower=-0.8,upper=-0.6)$minimum ## [1] -0.703479 7.2.14 Calculate probability using simulation method questions: randomly select 3 numbers out of 1:10, the sum is 9. badge&lt;-1:10 sim&lt;-10000 p&lt;-numeric(sim) for (i in 1:sim){ a&lt;-sample(badge,3,replace=F) p[i]&lt;-sum(a)==9 } mean(p) ## [1] 0.0241 questions: eat three flavors tangyuan. Tangyuan&lt;-c(rep(&#39;A&#39;,8),rep(&#39;B&#39;,8),rep(&#39;C&#39;,8)) sim&lt;-10000 p&lt;-numeric(sim) # how to do it according to the condition for (i in 1:sim){ a&lt;-sample(Tangyuan,24,replace=F) p[i]&lt;-(length(unique(a[1:6]))==3)&amp;(length(unique(a[7:12]))==3)&amp;(length(unique(a[13:18]))==3)&amp;(length(unique(a[19:24]))==3) } mean(p) ## [1] 0.4842 question: select 2 balls when they are the same color. box1&lt;-c(rep(&#39;white&#39;,5), rep(&quot;black&quot;,11), rep(&#39;red&#39;,8)) box2&lt;-c(rep(&#39;white&#39;,10), rep(&quot;black&quot;,8), rep(&#39;red&#39;,6)) sim&lt;-10000 p&lt;-numeric(sim) for (i in 1:sim){ a&lt;-sample(box1, 1) b&lt;-sample(box2, 1) p[i]&lt;- a==b } mean(p) ## [1] 0.3275 select after putting them back box&lt;-c(rep(&quot;white&quot;,4),rep(&quot;red&quot;,2)) sim&lt;-10000 t&lt;-numeric(sim) for (i in 1:sim){ a&lt;-sample(box, 2 ,replace=T) # there are two white balls t[i]&lt;-length(a[a==&quot;white&quot;])==2 } mean(t) ## [1] 0.4447 question: two students have the same birthday out of 30 students n&lt;-30 sim&lt;-10000 t&lt;-numeric(sim) for (i in 1:sim){ a&lt;-sample(1:365, n, replace=T) t[i]&lt;-n-length(unique(a)) } 1-mean(t==0) ## [1] 0.7056 # probability 1-prod(365:(365-30+1))/365^30 ## [1] 0.7063162 An event is a set of outcomes. You can describe certain events using random variables (x, distribution). the distribution of random variable function. 7.2.15 Discrete random variable question: choose correct one out of four answers x&lt;-0:5 y&lt;-dbinom(x,5,1/4) plot(x,y,col=2,type=&#39;h&#39;) using plot the probability of shooting is 0.02, what is the most likelihood of hit with 400 shootings k&lt;-0:400 p&lt;-dbinom(k,400,0.02) plot(k,p,type=&#39;h&#39;,col=2) plot(k,p,type=&#39;h&#39;,col=2,xlim=c(0,20)) dbinom(7,400,0.02) ## [1] 0.1406443 dbinom(8,400,0.02) ## [1] 0.1410031 7.2.16 Exponent distribution question: lifetime of a light (lamda=1/2000) integrate(dexp,rate=1/2000,1000,Inf)$value ## [1] 0.6065307 f&lt;-function(x){dexp(x,rate=1/2000)} integrate(f,1000,Inf) $value ## [1] 0.6065307 1-pexp(1000,rate=1/2000) ## [1] 0.6065307 mean(rexp(10000,rate=1/2000)&gt;1000) ## [1] 0.6009 7.2.17 Normal distribution plot continue variable x&lt;-seq(-3,3,0.01) plot(x, dnorm(x,mean=0, sd=2),type=&quot;l&quot;,xlab=&quot;x&quot;,ylab = &quot;f(x)&quot;, col=1,lwd=2,ylim=c(0,1)) #density function lines(x, dnorm(x,mean=0, sd=1),lty=2, col=2,lwd=2) lines(x, dnorm(x,mean=0, sd=0.5), lty=3,col=3,lwd=2) exbeta&lt;-c(expression(paste(mu,&quot;=0,&quot;, sigma,&quot;=2&quot;)), expression(paste(mu,&quot;=0,&quot;,sigma,&quot;=1&quot;)), expression(paste(mu,&quot;=0,&quot;, sigma,&quot;=0.5&quot;))) legend(&quot;topright&quot;, exbeta, lty = c(1, 2,3),col=c(1,2,3),lwd=2) x&lt;-seq(-3,3,0.01) plot(x, dnorm(x,mean=-1, sd=1),type=&quot;l&quot;,xlab=&quot;x&quot;,ylab = &quot;f(x)&quot;, col=1,lwd=2,ylim=c(0,0.6)) lines(x, dnorm(x,mean=0, sd=1),lty=2, col=2,lwd=2) lines(x, dnorm(x,mean=1, sd=1), lty=3,col=3,lwd=2) exbeta&lt;-c(expression(paste(mu,&quot;=-1,&quot;, sigma,&quot;=1&quot;)), expression(paste(mu,&quot;=0,&quot;,sigma,&quot;=1&quot;)), expression(paste(mu,&quot;=1,&quot;, sigma,&quot;=1&quot;))) legend(&quot;topright&quot;, exbeta, lty = c(1, 2,3),col=c(1,2,3),lwd=2) question: solve sigma using nomoral distribution sigma&lt;-1 repeat{ sigma&lt;-sigma+0.01 if (pnorm(200,160,sigma)-pnorm(120,160,sigma)&lt;0.80) break } sigma ## [1] 31.22 # alternative sigma&lt;-1 while( pnorm(200,160,sigma)-pnorm(120,160,sigma)&gt;=0.80){sigma&lt;-sigma+0.01} sigma ## [1] 31.22 7.2.18 Distribution of random variable function qestion: x^2 and 2x distributions x&lt;-c(-1,0,1,2,2.5) weight&lt;-c(0.2,0.1,0.1,0.3,0.3) toss&lt;-sample(x,10000,replace=T,weight) table(toss^2)/length(toss^2) 0 1 4 6.25 0.1029 0.2966 0.2994 0.3011 table(2*toss)/length(2*toss) -2 0 2 4 5 0.2008 0.1029 0.0958 0.2994 0.3011 quetsion: continous vairable density x &lt;- seq(0,5,0.01) truth&lt;-rep(0,length(x)) truth[0&lt;=x&amp;x&lt;1]&lt;-2/3 truth[1&lt;=x&amp;x&lt;2]&lt;-1/3 plot(density(abs(runif(1000000,-1,2))),main=NA, ylim=c(0,1),lwd=3,lty=3) lines(x,truth,col=&quot;red&quot;,lwd=2) legend(&quot;topright&quot;,c(&quot;True Density&quot;,&quot;Estimated Density&quot;), col=c(&quot;red&quot;,&quot;black&quot;),lwd=3,lty=c(1,3)) 7.2.19 Join and margin probability question: x is randomly selected from 1:4, y randomly select from x p&lt;-function(x,y) { sim&lt;-10000 t&lt;-numeric(sim) for (i in 1:sim) { a&lt;-sample(1:4,1) b&lt;-sample(1:a,1) t[i]&lt;-(a==x)&amp;(b==y) } mean(t) } PF&lt;-matrix(0,4,4) for (i in 1:4) { for (j in 1:4) { PF[i,j]&lt;-p(i, j) } } PF 0.2439 0.0000 0.0000 0.0000 0.1217 0.1228 0.0000 0.0000 0.0838 0.0831 0.0857 0.0000 0.0638 0.0644 0.0634 0.0625 apply(PF,1,sum) ## [1] 0.2439 0.2445 0.2526 0.2541 apply(PF,2,sum) ## [1] 0.5132 0.2703 0.1491 0.0625 7.2.20 Multiple random variables plots 2 discrete variables distribution x&lt;-sample(1:4, 10000, replace=T, prob=c(1/4, 1/4, 1/4, 1/4)) y&lt;-numeric(10000) for(i in 1:10000) { if(x[i]==1) {y[i]&lt;-sample(1:4,1,replace=T, prob=c(1,0,0,0))} if(x[i]==2) {y[i]&lt;-sample(1:4,1,replace=T, prob=c(1/2,1/2,0,0))} if(x[i]==3) {y[i]&lt;-sample(1:4,1,replace=T, prob=c(1/3,1/3,1/3,0))} if(x[i]==4) {y[i]&lt;-sample(1:4,1,replace=T, prob=c(1/4,1/4,1/4,1/4))} } z1&lt;-x+y table(z1)/length(z1) 2 3 4 5 6 7 8 0.2496 0.1224 0.2142 0.1421 0.1472 0.0644 0.0601 z2&lt;-x*y table(z2)/length(z2) 1 2 3 4 6 8 9 12 16 0.2496 0.1224 0.0867 0.1896 0.08 0.0661 0.0811 0.0644 0.0601 z3&lt;-pmax(x,y) table(z3)/length(z3) 1 2 3 4 0.2496 0.2499 0.2478 0.2527 z4&lt;-x/y table(z4)/length(z4) 1 1.33333333333333 1.5 2 3 4 0.5183 0.0644 0.08 0.1885 0.0867 0.0621 two normal distributions X～N(0,1),Y～N(0,1), Z=X+Y, therefre Z~N(0,2) Z&lt;-function(n){ x&lt;-seq(-4,4,0.01) truth&lt;-dnorm(x,0,sqrt(2)) plot(density(rnorm(n)+rnorm(n)),main=&quot;Density Estimate of the Normal Addition Model&quot;,ylim=c(0,0.4),lwd=2,lty=2) lines(x,truth,col=&quot;red&quot;,lwd=2) legend(&quot;topright&quot;,c(&quot;True&quot;,&quot;Estimated&quot;),col=c(&quot;red&quot;,&quot;black&quot;),lwd=2,lty=c(1,2)) } Z(10000) 7.2.21 Generate a circle using simulated random dots D={(x,y)|x^2 + y^2 &lt;= 1} x&lt;-runif(10000,-1,1) y&lt;-runif(10000,-1,1) a&lt;-x[x^2+y^2&lt;=1] b&lt;-y[x^2+y^2&lt;=1] plot(a,b,col=4) oval a&lt;-3 b&lt;-1 x&lt;-runif(10000,-a,a) y&lt;-runif(10000,-b,b) x1&lt;-x[x^2/a^2+y^2/b^2&lt;=1] y1&lt;-y[x^2/a^2+y^2/b^2&lt;=1] plot(x1,y1,col=3) 7.2.22 Expectation discrete variable question: the benefit of products is different. sim&lt;-10000 t&lt;-numeric(sim) for (i in 1:sim) { Y&lt;-1500 X&lt;-rexp(1,rate=1/10) Y[1&lt;X&amp;X&lt;=2]&lt;-2000 Y[2&lt;X&amp;X&lt;=3]&lt;-2500 Y[3&lt;X]&lt;-3000 t[i]&lt;-Y } mean(t) ## [1] 2734.35 continue variable 7.2.23 Central Limit Theorem ###Central Limit Theorem for Expotential distribution layout(matrix(c(1,3,2,4 ),ncol=2)) r&lt;-1000 lambda&lt;-1/100 for (n in c(1,5,10,30)){ mu&lt;-1/lambda xbar&lt;-numeric(r) sxbar&lt;-1/(sqrt(n)*lambda) for(i in 1:r){ xbar[i]&lt;-mean(rexp(n,rate=lambda)) } hist(xbar,prob=T,main=paste(&#39;SampDist.Xbar,n=&#39;,n),col=gray(.8)) Npdf&lt;-dnorm(seq(mu-3*sxbar,mu+3*sxbar,0.01),mu,sxbar) lines(seq(mu-3*sxbar,mu+3*sxbar,0.01),Npdf,lty=2,col=2) box() } #####The central limit theorem for uniform distribution layout(matrix(c(1,3,2,4),ncol=2)) r&lt;-10000 mu&lt;-5 sigma&lt;-10/sqrt(12) for (n in c(1,5,10,30)){ xbar&lt;-numeric(r) sxbar&lt;-sigma/sqrt(n) for (i in 1:r){ xbar[i]&lt;-mean(runif(n,0,10))} hist(xbar,prob=T,main=paste(&#39;SampDist.Xbar,n=&#39;,n),col=gray(0.8),ylim=c(0,1/(sqrt(1*pi)*sxbar))) XX&lt;-seq(mu-3*sxbar,mu+3*sxbar,0.01) Npdf&lt;-dnorm(XX,mu,sxbar) lines(XX,Npdf,lty=2,col=2) box()} 7.2.24 Law of large numbers N &lt;- 5000 set.seed(123) x &lt;- sample(1:10, N, replace = T) s &lt;- cumsum(x) r.avg &lt;- s/(1:N) options(scipen = 10) plot(r.avg, ylim=c(1, 10), type = &quot;l&quot;, xlab = &quot;Observations&quot; ,ylab = &quot;Probability&quot;, lwd = 2) lines(c(0,N), c(5.5,5.5),col=&quot;red&quot;, lwd = 2) 7.2.25 Empirical distribution x&lt;-c(-2,-1.2,1.5,2.3,3.5) plot(ecdf(x),col=2) abline(v=0,col=3) question: three numbers are from N(2,9), and what is the prob of their mean &gt;3? A&lt;-matrix(rnorm(30000,2,3),10000,3) mean(apply(A,1,mean)&gt;3) ## [1] 0.2789 7.2.26 Maximum likelihood estimate question: eatimate mean and variance sample&lt;-c(1.38, 3.96, -0.16, 8.12, 6.30, 2.61, -1.35, 0.03, 3.94, 1.11) n&lt;-length(sample) muhat&lt;-mean(sample) sigsqhat&lt;-sum((sample-muhat)^2)/n muhat ## [1] 2.594 sigsqhat ## [1] 8.133884 loglike&lt;-function(theta){ a&lt;--n/2*log(2*pi)-n/2*log(theta[2])-sum((sample-theta[1])^2)/(2*theta[2]) return(-a) } optim(c(2,2),loglike,method=&quot;BFGS&quot;)$par ## [1] 2.593942 8.130340 7.2.27 t distribution, F distribution plots, and common distributions n&lt;-30 x&lt;-seq(-6,6,0.01) y&lt;-seq(-6,6,1) Truth&lt;-df(x,1,n) plot(density(rt(10000,n)^2),main=&quot;PDF&quot;,ylim=c(0,1),lty=2,xlim=c(-6,6)) #simulation lines(x, dt(x,n), col=3) #t dist lines(x, dchisq(x,2), col=4) #chisq dist lines(x,Truth,col=2) #f dist abline (v=0 ,col=7) points(y,dbinom(y, size=12, prob=0.2),col=1) #binomial dist points(y,dpois(y, 6),col=2) #poisson dist lines(y,dunif(-6:6,min=-6,max=6 ),col=5) #uniform "],["algorithms.html", "8 Algorithms 8.1 Maximum likelihood estimation 8.2 Gradient descent", " 8 Algorithms 8.1 Maximum likelihood estimation 8.1.1 Likelihood estimation (without random effects) With the normal distribution of errors, likelihood can be expressed explicitly as the product of the densities of each of the \\(n\\) independent normal observations. \\[\\ell=-\\log L\\] the negative log-likelihood \\[ \\begin{aligned} \\ell\\left(\\left(y_{1}, \\ldots, y_{n}\\right), \\mu, \\sigma^{2}\\right) &amp;=-\\log \\left[\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left(-\\frac{\\left(y_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}\\right)\\right] \\\\ &amp;=-\\sum_{i=1}^{n}\\left[\\log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right)+\\left(-\\frac{\\left(y_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}\\right)\\right] \\end{aligned} \\] then, for matrix format \\[ \\begin{aligned} \\ell(\\mathbf{y}, \\beta, \\gamma) &amp;=\\frac{1}{2}\\left\\{n \\log (2 \\pi)+\\log \\left|\\sigma^{2} \\mathbf{I}\\right|+(\\mathbf{y}-\\mathbf{X} \\beta)^{\\prime}\\left(\\sigma^{2} \\mathbf{I}\\right)^{-1}(\\mathbf{y}-\\mathbf{X} \\beta)\\right\\} \\\\ &amp;=\\frac{1}{2}\\left\\{n \\log (2 \\pi)+\\log \\left(\\prod_{i=1}^{n} \\sigma^{2}\\right)+(\\mathbf{y}-\\mathbf{X} \\beta)^{\\prime}(\\mathbf{y}-\\mathbf{X} \\beta) / \\sigma^{2}\\right\\} \\\\ &amp;=\\frac{1}{2}\\left\\{n \\log (2 \\pi)+n \\log \\left(\\sigma^{2}\\right)+(\\mathbf{y}-\\mu)^{\\prime}(\\mathbf{y}-\\mu) / \\sigma^{2}\\right\\} \\\\ &amp;=\\frac{1}{2}\\left\\{n \\log (2 \\pi)+n \\log \\left(\\sigma^{2}\\right)+\\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2} / \\sigma^{2}\\right\\} \\end{aligned} \\] where \\(\\gamma\\) derived from \\(\\sigma^{2} \\mathbf{I}\\). minimize \\(\\ell\\) by taking the derivative \\[ argmin(\\ell(\\mathbf{y}, \\beta, \\gamma)) \\] taking the derivatives of the negative log-likelihood function. \\[ \\begin{aligned} \\frac{\\partial \\ell\\left(\\mu, \\sigma^{2}\\right)}{\\partial \\mu} &amp;=\\frac{1}{2}\\left[\\sum_{i=1}^{n}(-2)\\left(y_{i}-\\mu\\right) / \\sigma^{2}\\right] \\\\ &amp;=\\left(n \\mu-\\sum_{i=1}^{n} y_{i}\\right) / \\sigma^{2}=0 \\end{aligned} \\] \\[ \\frac{\\partial \\ell\\left(\\mu, \\sigma^{2}\\right)}{\\partial \\sigma^{2}}=\\frac{1}{2}\\left[\\frac{n}{\\sigma^{2}}-\\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}}{\\left(\\sigma^{2}\\right)^{2}}\\right]=0 \\] setting the derivatives equal to zero and solving for the parameters \\[ \\begin{aligned} \\hat{\\mu} &amp;=\\bar{y} \\\\ \\hat{\\sigma}^{2} &amp;=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2} \\\\ &amp;=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2} \\end{aligned} \\] but for REML \\[ \\begin{aligned} \\hat{\\sigma}^{2} &amp;=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2} \\\\ &amp;=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2} \\end{aligned} \\] 8.1.2 R demonstration toy data set.seed(123) n &lt;- 20000 x &lt;- rnorm(n, 2, sqrt(2)) s &lt;- rnorm(n, 0, 0.8) y &lt;- 1.5+x*3+s mydata &lt;- data.frame(y,x) using linear regression lmfit &lt;- lm(y~., data=mydata) # logLik(lmfit) coefficients(lmfit) ## (Intercept) x ## 1.487701 3.003695 (summary(lmfit)$sigma**2) ## [1] 0.6397411 using -log max likelihood estimate formula notice, using vector and matrix notation ## Using the mathematical expression: minusloglik &lt;- function(param){ beta &lt;- param[-1] #Regression Coefficients sigma &lt;- param[1] #Variance y &lt;- as.vector(mydata$y) #DV x &lt;- cbind(1, mydata$x) #IV mu &lt;- x%*%beta #multiply matrices 0.5*(n*log(2*pi) + n*log(sigma) + sum((y-mu)^2)/sigma) } MLoptimize &lt;- optim( c (1, 1, 1 ), minusloglik) ## The results: MLoptimize$par ## [1] 0.6397201 1.4876186 3.0038675 using max likelihood estimate directly (normal distribution) # max library(maxLik) ols.lf &lt;- function(param) { beta &lt;- param[-1] #Regression Coefficients sigma &lt;- param[1] #Variance y &lt;- as.vector(mydata$y) #DV x &lt;- cbind(1, mydata$x) #IV mu &lt;- x%*%beta #multiply matrices sum(dnorm(y, mu, sqrt(sigma), log = TRUE)) #normal distribution(vector of observations, mean, sd) } mle_ols &lt;- maxLik(logLik = ols.lf, start = c(sigma = 1, beta1 = 1, beta2 = 1 )) summary(mle_ols) ## -------------------------------------------- ## Maximum Likelihood estimation ## Newton-Raphson maximisation, 11 iterations ## Return code 8: successive function values within relative tolerance limit (reltol) ## Log-Likelihood: -23910.85 ## 3 free parameters ## Estimates: ## Estimate Std. error t value Pr(&gt; t) ## sigma 0.639677 0.006396 100.0 &lt;2e-16 *** ## beta1 1.487701 0.009768 152.3 &lt;2e-16 *** ## beta2 3.003695 0.003999 751.2 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## -------------------------------------------- another example ols.lf &lt;- function(param) { beta &lt;- param[-1] #Regression Coefficients sigma &lt;- param[1] #Variance y &lt;- as.vector(mtcars$mpg) #DV x &lt;- cbind(1, mtcars$cyl, mtcars$disp) #IV mu &lt;- x%*%beta #multiply matrices sum(dnorm(y, mu, sqrt(sigma), log = TRUE)) #normal distribution(vector of observations, mean, sd) } mle_ols &lt;- maxLik(logLik = ols.lf, start = c(sigma = 1, beta1 = 1, beta2 = 1, beta3=1)) summary(mle_ols) ## -------------------------------------------- ## Maximum Likelihood estimation ## Newton-Raphson maximisation, 28 iterations ## Return code 2: successive function values within tolerance limit (tol) ## Log-Likelihood: -79.57282 ## 4 free parameters ## Estimates: ## Estimate Std. error t value Pr(&gt; t) ## sigma 8.460632 2.037359 4.153 0.0000329 *** ## beta1 34.661013 2.395871 14.467 &lt; 2e-16 *** ## beta2 -1.587281 0.673675 -2.356 0.0185 * ## beta3 -0.020584 0.009757 -2.110 0.0349 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## -------------------------------------------- #Checking against linear regression lmfit2 &lt;- (lm(mpg~cyl+disp, data=mtcars)) lmfit2 ## ## Call: ## lm(formula = mpg ~ cyl + disp, data = mtcars) ## ## Coefficients: ## (Intercept) cyl disp ## 34.66099 -1.58728 -0.02058 (summary(lmfit2)$sigma**2) ## [1] 9.335872 8.1.3 Estimate confidence intervals using the likelihood Most classical confidence intervals for parameters are estimated using the likelihood approach, the Wald interval (or the asymptotic normality property). \\[ \\hat{\\theta}_{i} \\pm z_{1-\\alpha / 2} S E_{\\hat{\\theta}_{i}} \\] where the standard error is from the second derivative of the log-likelihood function. This is the Hessian matrix/(observed) Information matrix if there is more than one single model parameter. take second derivative of the log-likelihood function \\[ I(\\theta)=\\ell^{\\prime \\prime}(\\theta) \\] e.g. for one independent variable (with parameters: \\(\\beta\\) and \\(\\sigma^2\\) in the model). Here, giving the four entries of the \\(2 \\times 2\\) Hessian matrix \\[ \\frac{\\partial^{2} \\ell\\left(\\mu, \\sigma^{2}\\right)}{\\partial \\mu^{2}}=\\frac{n}{\\sigma^{2}} \\] \\[ \\frac{\\partial^{2} \\ell\\left(\\mu, \\sigma^{2}\\right)}{ \\partial\\left(\\sigma^{2}\\right) }=\\sqrt{\\frac{2\\left(\\hat{\\sigma}^{2}\\right)^{2}}{n}} \\] \\[ \\frac{\\partial^{2} \\ell\\left(\\mu, \\sigma^{2}\\right)}{\\partial \\mu \\partial\\left(\\sigma^{2}\\right)}=0\\\\ \\frac{\\partial^{2} \\ell\\left(\\mu, \\sigma^{2}\\right)}{ \\partial\\left(\\sigma^{2}\\right)\\partial \\mu}=0 \\] therefore \\[ S E_{\\hat{\\theta}_{i}}=\\sqrt{\\left(I(\\hat{\\theta})^{-1}\\right)_{i}} \\] the inverse of the Fisher information is just each diagonal element, so For \\(\\beta\\) \\[ S E_{\\hat{\\mu}}=\\sqrt{\\left(I\\left(\\hat{\\mu}, \\hat{\\sigma}^{2}\\right)^{-1}\\right)_{11}}=\\sqrt{\\frac{\\hat{\\sigma}^{2}}{n}} \\] Thus, the Wald confidence interval for the mean would be \\[ \\hat{u}^{2} \\pm z_{1-\\alpha / 2} \\sqrt{\\frac{\\hat{\\sigma}^{2}}{n}} \\] For variance \\[ S E_{\\hat{\\sigma}^{2}}=\\sqrt{\\left(I\\left(\\hat{\\mu}, \\hat{\\sigma}^{2}\\right)^{-1}\\right)_{22}}=\\sqrt{\\frac{2\\left(\\hat{\\sigma}^{2}\\right)^{2}}{n}} \\] Thus, the Wald confidence interval for the variance would be \\[ \\hat{\\sigma}^{2} \\pm z_{1-\\alpha / 2} \\hat{\\sigma}^{2} \\sqrt{\\frac{2}{n}} \\] However, the following approach confidence interval will generally have much better small sample properties than the Wald interval. \\[\\left\\{\\theta \\mid \\frac{L(\\theta)}{L(\\hat{\\theta})}&gt;\\exp (-3.84 / 2)\\right\\}\\] 8.1.4 The profile likelihood It can be profiled by maximizing the likelihood function with respect to all the other parameters. In the following equation, \\(\\sigma^{2}\\) expressed by \\(\\mu\\) \\[ \\begin{aligned} L_{p}(\\mu) &amp;=L\\left(\\mu, \\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}\\right) \\\\ &amp;=\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}}} \\exp \\left(-\\frac{\\left(y_{i}-\\mu\\right)^{2}}{2 \\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}}\\right) \\end{aligned} \\] Similarly, the profile likelihood for the variance \\(\\sigma^{2}\\) can be expressed \\[ \\begin{aligned} L_{p}\\left(\\sigma^{2}\\right) &amp;=L\\left(\\hat{\\mu}\\left(\\sigma^{2}\\right), \\sigma^{2}\\right) \\\\ &amp;=L\\left(\\bar{y}, \\sigma^{2}\\right) \\end{aligned} \\] This becomes particularly simple, as the \\(u\\)-estimate does not depend on the \\(\\sigma\\). 8.1.5 Maximum likelihood estimate practice question: eatimate mean and variance sample&lt;-c(1.38, 3.96, -0.16, 8.12, 6.30, 2.61, -1.35, 0.03, 3.94, 1.11) n&lt;-length(sample) muhat&lt;-mean(sample) sigsqhat&lt;-sum((sample-muhat)^2)/n muhat ## [1] 2.594 sigsqhat ## [1] 8.133884 loglike&lt;-function(theta){ a&lt;--n/2*log(2*pi)-n/2*log(theta[2])-sum((sample-theta[1])^2)/(2*theta[2]) return(-a) } optim(c(2,2),loglike,method=&quot;BFGS&quot;)$par ## [1] 2.593942 8.130340 8.2 Gradient descent linear regression library(&quot;ggplot2&quot;) # fit a linear model res &lt;- lm( hwy ~ cty ,data=mpg) summary(res) ## ## Call: ## lm(formula = hwy ~ cty, data = mpg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3408 -1.2790 0.0214 1.0338 4.0461 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.89204 0.46895 1.902 0.0584 . ## cty 1.33746 0.02697 49.585 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.752 on 232 degrees of freedom ## Multiple R-squared: 0.9138, Adjusted R-squared: 0.9134 ## F-statistic: 2459 on 1 and 232 DF, p-value: &lt; 2.2e-16 compute regression coefficients # squared error cost function cost &lt;- function(X, y, theta) { sum( (X %*% theta - y)^2 ) / (2*length(y)) } # learning rate and iteration limit alpha &lt;- 0.005 num_iters &lt;- 20000 # keep history cost_history &lt;- double(num_iters) theta_history &lt;- list(num_iters) # initialize coefficients theta &lt;- matrix(c(0,0), nrow=2) x=mpg$cty y=mpg$hwy # add a column of 1&#39;s for the intercept coefficient X &lt;- cbind(1, matrix(x)) # gradient descent for (i in 1:num_iters) { error &lt;- (X %*% theta - y) delta &lt;- t(X) %*% error / length(y) #derivation theta &lt;- theta - alpha * delta cost_history[i] &lt;- cost(X, y, theta) theta_history[[i]] &lt;- theta } print(theta) ## [,1] ## [1,] 0.8899161 ## [2,] 1.3375742 tail(cost_history) ## [1] 1.522137 1.522137 1.522137 1.522137 1.522137 1.522137 plot the cost function plot(cost_history, type=&#39;line&#39;, col=&#39;red&#39;, lwd=2, main=&#39;Cost function&#39;, ylab=&#39;cost&#39;, xlab=&#39;Iterations&#39;) - compare two ways (linear regresion vs. gradient descent) x=mpg$cty y=mpg$cty*theta[2] + theta[1] plot(x,y, main=&#39;Linear regression by gradient descent&#39;) # line(x,y ,col=3) abline(lm(mpg$hwy ~ mpg$cty),col=&quot;blue&quot;,lwd = 4) abline(res, col=&#39;red&#39;) "],["sasmarkdown.html", "9 SASmarkdown 9.1 How to install sasmarkdown 9.2 Common statements", " 9 SASmarkdown 9.1 How to install sasmarkdown sasmarkdown Use an Engine # knitr::opts_chunk$set(echo = TRUE) require(SASmarkdown) ## Loading required package: SASmarkdown ## sas, saslog, sashtml, and sashtmllog engines ## are now ready to use. saspath &lt;- &quot;C:/Program Files/SASHome/SASFoundation/9.4/sas.exe&quot; sasopts &lt;- &quot;-nosplash -ls 75&quot; knitr::opts_chunk$set(engine=&#39;sashtml&#39;, engine.path=saspath, engine.opts=sasopts, comment=&quot;&quot;) 9.2 Common statements Generally, can not use r syntax in these chunks Data can not be used in different chunks compute mean and freqency proc means data=sashelp.class ; run; proc freq data=sashelp.class ; table sex; run; proc contents data=sashelp.class varnum ; run; The MEANS Procedure Variable N Mean Std Dev Minimum Maximum ------------------------------------------------------------------------- Age 19 13.3157895 1.4926722 11.0000000 16.0000000 Height 19 62.3368421 5.1270752 51.3000000 72.0000000 Weight 19 100.0263158 22.7739335 50.5000000 150.0000000 ------------------------------------------------------------------------- The FREQ Procedure Cumulative Cumulative Sex Frequency Percent Frequency Percent -------------------------------------------------------- F 9 47.37 9 47.37 M 10 52.63 19 100.00 The CONTENTS Procedure Data Set Name SASHELP.CLASS Observations 19 Member Type DATA Variables 5 Engine V9 Indexes 0 Created 09/06/2017 22:55:32 Observation Length 40 Last Modified 09/06/2017 22:55:32 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Student Data Data Representation WINDOWS_64 Encoding us-ascii ASCII (ANSI) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 1632 Obs in First Data Page 19 Number of Data Set Repairs 0 ExtendObsCounter YES Filename C:\\Program Files\\SASHome\\SASFoundation\\9. 4\\core\\sashelp\\class.sas7bdat Release Created 9.0401M5 Host Created X64_SR12R2 Owner Name BUILTIN\\Administrators File Size 128KB File Size (bytes) 131072 Variables in Creation Order # Variable Type Len 1 Name Char 8 2 Sex Char 1 3 Age Num 8 4 Height Num 8 5 Weight Num 8 sort a data set proc sort data=sashelp.class Out= name ; by name; Run; proc print data= name (obs=10) ; run; Obs Name Sex Age Height Weight 1 Alfred M 14 69.0 112.5 2 Alice F 13 56.5 84.0 3 Barbara F 13 65.3 98.0 4 Carol F 14 62.8 102.5 5 Henry M 14 63.5 102.5 6 James M 12 57.3 83.0 7 Jane F 12 59.8 84.5 8 Janet F 15 62.5 112.5 9 Jeffrey M 13 62.5 84.0 10 John M 12 59.0 99.5 transpose wide format data to long format data proc transpose data= sashelp.class out= Field; by name; Run; proc print data= field (obs=10) ; run; Obs Name _NAME_ COL1 1 Alfred Age 14.0 2 Alfred Height 69.0 3 Alfred Weight 112.5 4 Alice Age 13.0 5 Alice Height 56.5 6 Alice Weight 84.0 7 Barbara Age 13.0 8 Barbara Height 65.3 9 Barbara Weight 98.0 10 Carol Age 14.0 conditional statement DATA ab; set sashelp.class; IF sex=&quot;F&quot; then message=&#39;A is greater&#39;; Else message=&#39;B is greater&#39;; Run; proc print data=ab (obs=10); run; Obs Name Sex Age Height Weight message 1 Alfred M 14 69.0 112.5 B is greater 2 Alice F 13 56.5 84.0 A is greater 3 Barbara F 13 65.3 98.0 A is greater 4 Carol F 14 62.8 102.5 A is greater 5 Henry M 14 63.5 102.5 B is greater 6 James M 12 57.3 83.0 B is greater 7 Jane F 12 59.8 84.5 A is greater 8 Janet F 15 62.5 112.5 A is greater 9 Jeffrey M 13 62.5 84.0 B is greater 10 John M 12 59.0 99.5 B is greater change format of the variable proc format; value AGEnew 11 = &#39;1: NEW&#39; 12 = &#39;2: NEW&#39; 13 = &#39;3: NEW&#39; 14 = &#39;4: NEW&#39; 15 = &#39;5: NEW&#39; 16 = &#39;6: NEW&#39; ; run; DATA ab; set sashelp.class; Format AGE AGEnew.; Run; proc freq data=ab; table AGE; run; The FREQ Procedure Cumulative Cumulative Age Frequency Percent Frequency Percent ----------------------------------------------------------- 1: NEW 2 10.53 2 10.53 2: NEW 5 26.32 7 36.84 3: NEW 3 15.79 10 52.63 4: NEW 4 21.05 14 73.68 5: NEW 4 21.05 18 94.74 6: NEW 1 5.26 19 100.00 basic operations Data Mathdata; A= 10.12345; B=20; C= mean (a,b); D= Min(a,b); E= Max(a,b); F = log(a); G= round(a,0.02); H= floor(a ); Run; proc print data=Mathdata; run; Data mathdata; Set sashelp.Iris; Sum = sum (of SepalLength, SepalWIDTH); Diff = SepalLength- SepalWIDTH; Mult = SepalLength* SepalWIDTH; Div= SepalLength/ SepalWIDTH; Run; proc print data=mathdata (obs=10); run; Obs A B C D E F G H 1 10.1235 20 15.0617 10.1235 20 2.31485 10.12 10 Sepal Sepal Petal Petal Obs Species Length Width Length Width Sum Diff Mult Div 1 Setosa 50 33 14 2 83 17 1650 1.51515 2 Setosa 46 34 14 3 80 12 1564 1.35294 3 Setosa 46 36 10 2 82 10 1656 1.27778 4 Setosa 51 33 17 5 84 18 1683 1.54545 5 Setosa 55 35 13 2 90 20 1925 1.57143 6 Setosa 48 31 16 2 79 17 1488 1.54839 7 Setosa 52 34 14 2 86 18 1768 1.52941 8 Setosa 49 36 14 1 85 13 1764 1.36111 9 Setosa 44 32 13 2 76 12 1408 1.37500 10 Setosa 50 35 16 6 85 15 1750 1.42857 rename a variable Data AB; set sashelp.class; Rename AGE= AGENEW; Run; proc print data=AB; run; Obs Name Sex AGENEW Height Weight 1 Alfred M 14 69.0 112.5 2 Alice F 13 56.5 84.0 3 Barbara F 13 65.3 98.0 4 Carol F 14 62.8 102.5 5 Henry M 14 63.5 102.5 6 James M 12 57.3 83.0 7 Jane F 12 59.8 84.5 8 Janet F 15 62.5 112.5 9 Jeffrey M 13 62.5 84.0 10 John M 12 59.0 99.5 11 Joyce F 11 51.3 50.5 12 Judy F 14 64.3 90.0 13 Louise F 12 56.3 77.0 14 Mary F 15 66.5 112.0 15 Philip M 16 72.0 150.0 16 Robert M 12 64.8 128.0 17 Ronald M 15 67.0 133.0 18 Thomas M 11 57.5 85.0 19 William M 15 66.5 112.0 extract text from a character value Data Mathdata; Text = &quot;Hello World&quot;; Text1= substr(Text, 6, 2); Run; proc print data=mathdata ; run; Obs Text Text1 1 Hello World W convert the character value into a numeric value and reverse Data ABC; set sashelp.class; agenew= input (age, best.); Run; proc print data=abc ; run; Obs Name Sex Age Height Weight agenew 1 Alfred M 14 69.0 112.5 14 2 Alice F 13 56.5 84.0 13 3 Barbara F 13 65.3 98.0 13 4 Carol F 14 62.8 102.5 14 5 Henry M 14 63.5 102.5 14 6 James M 12 57.3 83.0 12 7 Jane F 12 59.8 84.5 12 8 Janet F 15 62.5 112.5 15 9 Jeffrey M 13 62.5 84.0 13 10 John M 12 59.0 99.5 12 11 Joyce F 11 51.3 50.5 11 12 Judy F 14 64.3 90.0 14 13 Louise F 12 56.3 77.0 12 14 Mary F 15 66.5 112.0 15 15 Philip M 16 72.0 150.0 16 16 Robert M 12 64.8 128.0 12 17 Ronald M 15 67.0 133.0 15 18 Thomas M 11 57.5 85.0 11 19 William M 15 66.5 112.0 15 Data ABC; set sashelp.class; agenew= put (age, best.); Run; proc contents data=abc ; run; The CONTENTS Procedure Data Set Name WORK.ABC Observations 19 Member Type DATA Variables 6 Engine V9 Indexes 0 Created 07/08/2022 10:34:45 Observation Length 48 Last Modified 07/08/2022 10:34:45 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation WINDOWS_64 Encoding wlatin1 Western (Windows) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 1361 Obs in First Data Page 19 Number of Data Set Repairs 0 ExtendObsCounter YES Filename C:\\Users\\hed2\\AppData\\Local\\Temp\\1\\SAS Temporary Files\\_TD20168_HDW02234493_\\abc.sas7bdat Release Created 9.0401M5 Host Created X64_10PRO Owner Name NIH\\hed2 File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 3 Age Num 8 4 Height Num 8 1 Name Char 8 2 Sex Char 1 5 Weight Num 8 6 agenew Char 12 change the length of the variable Data ABC; set sashelp.class; Length agenew $10.; Label agenew=“New age”; Run; proc contents data=abc ; run; The CONTENTS Procedure Data Set Name WORK.ABC Observations 19 Member Type DATA Variables 6 Engine V9 Indexes 0 Created 07/08/2022 10:34:45 Observation Length 48 Last Modified 07/08/2022 10:34:45 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation WINDOWS_64 Encoding wlatin1 Western (Windows) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 1361 Obs in First Data Page 19 Number of Data Set Repairs 0 ExtendObsCounter YES Filename C:\\Users\\hed2\\AppData\\Local\\Temp\\1\\SAS Temporary Files\\_TD14728_HDW02234493_\\abc.sas7bdat Release Created 9.0401M5 Host Created X64_10PRO Owner Name NIH\\hed2 File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len Label 3 Age Num 8 4 Height Num 8 1 Name Char 8 2 Sex Char 1 5 Weight Num 8 6 agenew Char 10 “New age” create a report proc report data=sashelp.class; Column age; Define age / display; Run; Age 14 13 13 14 14 12 12 15 13 12 11 14 12 15 16 12 15 11 15 create a random variable data ab; set sashelp.class; num=rand(&quot;normal&quot;); run; proc print data=ab (obs=10); run; Obs Name Sex Age Height Weight num 1 Alfred M 14 69.0 112.5 -0.28119 2 Alice F 13 56.5 84.0 -1.22290 3 Barbara F 13 65.3 98.0 0.63220 4 Carol F 14 62.8 102.5 -0.53108 5 Henry M 14 63.5 102.5 -1.52793 6 James M 12 57.3 83.0 -1.03686 7 Jane F 12 59.8 84.5 -0.03976 8 Janet F 15 62.5 112.5 0.09698 9 Jeffrey M 13 62.5 84.0 -0.49037 10 John M 12 59.0 99.5 -1.63931 combine two texts Data Mathdata; Text = &quot;Hello&quot;; Text1= &quot;World&quot;; Text2= text || &quot; &quot; ||text1; Run; proc print data=Mathdata ; run; Obs Text Text1 Text2 1 Hello World Hello World compress spaces Data Mathdata; Text = &quot;Hello World &quot;; Text1= trim(text); Text2= compress(text); Run; proc print data=Mathdata ; run; Obs Text Text1 Text2 1 Hello World Hello World HelloWorld identify the position of a specified text Data Mathdata; Text = &quot;Hello World&quot;; indextext= index(text, &quot;or&quot;); Run; proc print data=Mathdata ; run; Obs Text indextext 1 Hello World 8 convert upcase, lower case and propcase Data Mathdata; Text = &quot;Hello World&quot;; upcase= upcase(text ); lowcase= lowcase(text ); propcase= propcase(text ); Run; proc print data=Mathdata ; run; Obs Text upcase lowcase propcase 1 Hello World HELLO WORLD hello world Hello World deduplication /* Dedup, original data has 19 observations */ proc sort data = sashelp.class out = dedup nodupkeys; by height; run; proc print data= dedup; run; Obs Name Sex Age Height Weight 1 Joyce F 11 51.3 50.5 2 Louise F 12 56.3 77.0 3 Alice F 13 56.5 84.0 4 James M 12 57.3 83.0 5 Thomas M 11 57.5 85.0 6 John M 12 59.0 99.5 7 Jane F 12 59.8 84.5 8 Janet F 15 62.5 112.5 9 Carol F 14 62.8 102.5 10 Henry M 14 63.5 102.5 11 Judy F 14 64.3 90.0 12 Robert M 12 64.8 128.0 13 Barbara F 13 65.3 98.0 14 Mary F 15 66.5 112.0 15 Ronald M 15 67.0 133.0 16 Alfred M 14 69.0 112.5 17 Philip M 16 72.0 150.0 select sub data set data where; set sashelp.class; where sex ne &quot;F&quot;; /*if */ run; proc print data= where; run; Obs Name Sex Age Height Weight 1 Alfred M 14 69.0 112.5 2 Henry M 14 63.5 102.5 3 James M 12 57.3 83.0 4 Jeffrey M 13 62.5 84.0 5 John M 12 59.0 99.5 6 Philip M 16 72.0 150.0 7 Robert M 12 64.8 128.0 8 Ronald M 15 67.0 133.0 9 Thomas M 11 57.5 85.0 10 William M 15 66.5 112.0 create macro using do loop statement 2 /*create a macro to calcualte descriptive stats */ 3 %macro means(var_avg) ; 4 5 /*calculate means*/ 6 proc means data=sashelp.class StackODSOutput n mean std min p5 6 ! p95 max nmiss; 7 var &amp;var_avg; 8 class sex; 9 ods output summary=result2; 10 run; 11 12 /*append then output*/ 13 data masterresult2; * combine results; 14 set masterresult2 result2; 15 run; 16 17 %mend means; 18 19 /*use macro to merge all descriptive stats */ 20 data masterresult2 ; 21 set _null_; 22 run; NOTE: The data set WORK.MASTERRESULT2 has 0 observations and 0 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 23 24 %let vars= 25 age 26 height 27 weight 28 ; 29 30 %macro model ; 31 %do i=1 %to %sysfunc(countw(&amp;vars)); 32 33 %let x=%scan(&amp;vars,&amp;i); 34 %means( &amp;x ) 35 36 %end; 37 %mend model; 38 39 %model; NOTE: The data set WORK.RESULT2 has 2 observations and 12 variables. NOTE: There were 19 observations read from the data set SASHELP.CLASS. NOTE: The PROCEDURE MEANS printed page 1. NOTE: PROCEDURE MEANS used (Total process time): real time 0.10 seconds cpu time 0.04 seconds NOTE: There were 0 observations read from the data set WORK.MASTERRESULT2. NOTE: There were 2 observations read from the data set WORK.RESULT2. NOTE: The data set WORK.MASTERRESULT2 has 2 observations and 12 variables. NOTE: DATA statement used (Total process time): real time 0.06 seconds cpu time 0.00 seconds NOTE: The data set WORK.RESULT2 has 2 observations and 12 variables. NOTE: There were 19 observations read from the data set SASHELP.CLASS. NOTE: The PROCEDURE MEANS printed page 2. NOTE: PROCEDURE MEANS used (Total process time): real time 0.00 seconds cpu time 0.01 seconds WARNING: Multiple lengths were specified for the variable Variable by input data set(s). This can cause truncation of data. NOTE: There were 2 observations read from the data set WORK.MASTERRESULT2. NOTE: There were 2 observations read from the data set WORK.RESULT2. NOTE: The data set WORK.MASTERRESULT2 has 4 observations and 12 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds NOTE: The data set WORK.RESULT2 has 2 observations and 12 variables. NOTE: There were 19 observations read from the data set SASHELP.CLASS. NOTE: The PROCEDURE MEANS printed page 3. NOTE: PROCEDURE MEANS used (Total process time): real time 0.01 seconds cpu time 0.00 seconds WARNING: Multiple lengths were specified for the variable Variable by input data set(s). This can cause truncation of data. NOTE: There were 4 observations read from the data set WORK.MASTERRESULT2. NOTE: There were 2 observations read from the data set WORK.RESULT2. NOTE: The data set WORK.MASTERRESULT2 has 6 observations and 12 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 40 41 proc print data= masterresult2; 42 run; NOTE: There were 6 observations read from the data set WORK.MASTERRESULT2. NOTE: The PROCEDURE PRINT printed page 4. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 43 The MEANS Procedure Analysis Variable : Age N Sex Obs N Mean Std Dev Minimum 5th Pctl -------------------------------------------------------------------------- F 9 9 13.222222 1.394433 11.000000 11.000000 M 10 10 13.400000 1.646545 11.000000 11.000000 -------------------------------------------------------------------------- Analysis Variable : Age N N Sex Obs 95th Pctl Maximum Miss ------------------------------------------------ F 9 15.000000 15.000000 0 M 10 16.000000 16.000000 0 ------------------------------------------------ The MEANS Procedure Analysis Variable : Height N Sex Obs N Mean Std Dev Minimum 5th Pctl -------------------------------------------------------------------------- F 9 9 60.588889 5.018328 51.300000 51.300000 M 10 10 63.910000 4.937937 57.300000 57.300000 -------------------------------------------------------------------------- Analysis Variable : Height N N Sex Obs 95th Pctl Maximum Miss ------------------------------------------------ F 9 66.500000 66.500000 0 M 10 72.000000 72.000000 0 ------------------------------------------------ The MEANS Procedure Analysis Variable : Weight N Sex Obs N Mean Std Dev Minimum 5th Pctl -------------------------------------------------------------------------- F 9 9 90.111111 19.383914 50.500000 50.500000 M 10 10 108.950000 22.727186 83.000000 83.000000 -------------------------------------------------------------------------- Analysis Variable : Weight N N Sex Obs 95th Pctl Maximum Miss ------------------------------------------------ F 9 112.500000 112.500000 0 M 10 150.000000 150.000000 0 ------------------------------------------------ Obs Sex NObs _control_ Variable N Mean StdDev 1 F 9 Age 9 13.222222 1.394433 2 M 10 1 Age 10 13.400000 1.646545 3 F 9 Hei 9 60.588889 5.018328 4 M 10 1 Hei 10 63.910000 4.937937 5 F 9 Wei 9 90.111111 19.383914 6 M 10 1 Wei 10 108.950000 22.727186 Obs Min P5 P95 Max NMiss 1 11.000000 11.000000 15.000000 15.000000 0 2 11.000000 11.000000 16.000000 16.000000 0 3 51.300000 51.300000 66.500000 66.500000 0 4 57.300000 57.300000 72.000000 72.000000 0 5 50.500000 50.500000 112.500000 112.500000 0 6 83.000000 83.000000 150.000000 150.000000 0 output intermediate tables ods listing close; ods trace on; ods output ParameterEstimates= ParameterEstimates ; proc glm data=sashelp.class; model height=age; run; ods trace off; ods listing; proc print data=ParameterEstimates; run; Obs Dependent Parameter Estimate StdErr tValue Probt 1 Height Intercept 25.22388451 6.52168912 3.87 0.0012 2 Height Age 2.78713911 0.48688163 5.72 &lt;.0001 merge data sets right, left join 2 data class2; 3 input name $ score; 4 datalines; NOTE: The data set WORK.CLASS2 has 3 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 8 ; 9 run; 10 11 proc sql; 12 create table class_c as 13 select * 14 from sashelp.class as a 15 left join class2 as b 16 on a.name = b.name; WARNING: Variable name already exists on file WORK.CLASS_C. NOTE: Table WORK.CLASS_C created, with 19 rows and 6 columns. 17 quit; NOTE: PROCEDURE SQL used (Total process time): real time 0.16 seconds cpu time 0.01 seconds 18 19 proc print data=class_c; 20 run; NOTE: There were 19 observations read from the data set WORK.CLASS_C. NOTE: The PROCEDURE PRINT printed page 1. NOTE: PROCEDURE PRINT used (Total process time): real time 0.02 seconds cpu time 0.03 seconds 21 22 proc sql; 23 create table class_d as 24 select * 25 from class2 as a 26 right join sashelp.class as b 27 on a.name = b.name; WARNING: Variable Name already exists on file WORK.CLASS_D. NOTE: Table WORK.CLASS_D created, with 19 rows and 6 columns. 28 quit; NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 29 30 proc print data=class_d; 31 run; NOTE: There were 19 observations read from the data set WORK.CLASS_D. NOTE: The PROCEDURE PRINT printed page 2. NOTE: PROCEDURE PRINT used (Total process time): real time 0.01 seconds cpu time 0.00 seconds 32 Obs Name Sex Age Height Weight score 1 Alfred M 14 69.0 112.5 85 2 Alice F 13 56.5 84.0 89 3 Barbara F 13 65.3 98.0 . 4 Carol F 14 62.8 102.5 . 5 Henry M 14 63.5 102.5 . 6 James M 12 57.3 83.0 . 7 Jane F 12 59.8 84.5 . 8 Janet F 15 62.5 112.5 . 9 Jeffrey M 13 62.5 84.0 . 10 John M 12 59.0 99.5 . 11 Joyce F 11 51.3 50.5 . 12 Judy F 14 64.3 90.0 . 13 Louise F 12 56.3 77.0 . 14 Mary F 15 66.5 112.0 . 15 Philip M 16 72.0 150.0 . 16 Robert M 12 64.8 128.0 . 17 Ronald M 15 67.0 133.0 . 18 Thomas M 11 57.5 85.0 . 19 William M 15 66.5 112.0 . Obs name score Sex Age Height Weight 1 Alfred 85 M 14 69.0 112.5 2 Alice 89 F 13 56.5 84.0 3 . F 13 65.3 98.0 4 . F 14 62.8 102.5 5 . M 14 63.5 102.5 6 . M 12 57.3 83.0 7 . F 12 59.8 84.5 8 . F 15 62.5 112.5 9 . M 13 62.5 84.0 10 . M 12 59.0 99.5 11 . F 11 51.3 50.5 12 . F 14 64.3 90.0 13 . F 12 56.3 77.0 14 . F 15 66.5 112.0 15 . M 16 72.0 150.0 16 . M 12 64.8 128.0 17 . M 15 67.0 133.0 18 . M 11 57.5 85.0 19 . M 15 66.5 112.0 full join 2 data class2; 3 input name $ score; 4 datalines; NOTE: The data set WORK.CLASS2 has 3 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 8 ; 9 run; 10 11 proc sql; 12 create table class_e as 13 select * 14 from sashelp.class as a 15 full join class2 as b 16 on a.name = b.name; WARNING: Variable name already exists on file WORK.CLASS_E. NOTE: Table WORK.CLASS_E created, with 20 rows and 6 columns. 17 quit; NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 18 19 proc print data=class_e; 20 run; NOTE: There were 20 observations read from the data set WORK.CLASS_E. NOTE: The PROCEDURE PRINT printed page 1. NOTE: PROCEDURE PRINT used (Total process time): real time 0.02 seconds cpu time 0.03 seconds 21 Obs Name Sex Age Height Weight score 1 Alfred M 14 69.0 112.5 85 2 Alice F 13 56.5 84.0 89 3 Barbara F 13 65.3 98.0 . 4 Carol F 14 62.8 102.5 . 5 . . . 99 6 Henry M 14 63.5 102.5 . 7 James M 12 57.3 83.0 . 8 Jane F 12 59.8 84.5 . 9 Janet F 15 62.5 112.5 . 10 Jeffrey M 13 62.5 84.0 . 11 John M 12 59.0 99.5 . 12 Joyce F 11 51.3 50.5 . 13 Judy F 14 64.3 90.0 . 14 Louise F 12 56.3 77.0 . 15 Mary F 15 66.5 112.0 . 16 Philip M 16 72.0 150.0 . 17 Robert M 12 64.8 128.0 . 18 Ronald M 15 67.0 133.0 . 19 Thomas M 11 57.5 85.0 . 20 William M 15 66.5 112.0 . or using merge for appending data class2; input name $ score; datalines; Alfred 85 Alice 89 Daniel 99 ; run; data class_f; merge sashelp.class class2; by name; run; proc print data=class_f; run; Obs Name Sex Age Height Weight score 1 Alfred M 14 69.0 112.5 85 2 Alice F 13 56.5 84.0 89 3 Barbara F 13 65.3 98.0 . 4 Carol F 14 62.8 102.5 . 5 Daniel . . . 99 6 Henry M 14 63.5 102.5 . 7 James M 12 57.3 83.0 . 8 Jane F 12 59.8 84.5 . 9 Janet F 15 62.5 112.5 . 10 Jeffrey M 13 62.5 84.0 . 11 John M 12 59.0 99.5 . 12 Joyce F 11 51.3 50.5 . 13 Judy F 14 64.3 90.0 . 14 Louise F 12 56.3 77.0 . 15 Mary F 15 66.5 112.0 . 16 Philip M 16 72.0 150.0 . 17 Robert M 12 64.8 128.0 . 18 Ronald M 15 67.0 133.0 . 19 Thomas M 11 57.5 85.0 . 20 William M 15 66.5 112.0 . inner join 2 data class2; 3 input name $ score; 4 datalines; NOTE: The data set WORK.CLASS2 has 3 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 8 ; 9 run; 10 11 proc sql; 12 create table class_e as 13 select * 14 from sashelp.class as a 15 inner join class2 as b 16 on a.name = b.name; WARNING: Variable name already exists on file WORK.CLASS_E. NOTE: Table WORK.CLASS_E created, with 2 rows and 6 columns. 17 quit; NOTE: PROCEDURE SQL used (Total process time): real time 0.05 seconds cpu time 0.00 seconds 18 19 proc print data=class_e; 20 run; NOTE: There were 2 observations read from the data set WORK.CLASS_E. NOTE: The PROCEDURE PRINT printed page 1. NOTE: PROCEDURE PRINT used (Total process time): real time 0.02 seconds cpu time 0.03 seconds 21 Obs Name Sex Age Height Weight score 1 Alfred M 14 69.0 112.5 85 2 Alice F 13 56.5 84.0 89 minus join 2 data class2; 3 input name $ score; 4 datalines; NOTE: The data set WORK.CLASS2 has 3 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 8 ; 9 run; 10 11 proc sql; 12 create table class_e as 13 select * 14 from sashelp.class as a 15 left join class2 as b 16 on a.name = b.name 17 where b.name is NULL; WARNING: Variable name already exists on file WORK.CLASS_E. NOTE: Table WORK.CLASS_E created, with 17 rows and 6 columns. 18 quit; NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 19 20 proc print data=class_e; 21 run; NOTE: There were 17 observations read from the data set WORK.CLASS_E. NOTE: The PROCEDURE PRINT printed page 1. NOTE: PROCEDURE PRINT used (Total process time): real time 0.02 seconds cpu time 0.03 seconds 22 Obs Name Sex Age Height Weight score 1 Barbara F 13 65.3 98.0 . 2 Carol F 14 62.8 102.5 . 3 Henry M 14 63.5 102.5 . 4 James M 12 57.3 83.0 . 5 Jane F 12 59.8 84.5 . 6 Janet F 15 62.5 112.5 . 7 Jeffrey M 13 62.5 84.0 . 8 John M 12 59.0 99.5 . 9 Joyce F 11 51.3 50.5 . 10 Judy F 14 64.3 90.0 . 11 Louise F 12 56.3 77.0 . 12 Mary F 15 66.5 112.0 . 13 Philip M 16 72.0 150.0 . 14 Robert M 12 64.8 128.0 . 15 Ronald M 15 67.0 133.0 . 16 Thomas M 11 57.5 85.0 . 17 William M 15 66.5 112.0 . create table 1 *** Load utility macros; %include &quot;C:\\Users\\hed2\\Downloads\\mybook2\\mybook2\\sasmacro\\create_table1.sas&quot;; *** Specify input and output data sets, and the column variable.; %let INPUT_DATA = sashelp.class; %let OUTPUT_DATA = Table1 ; %let COLVAR = sex; /*chort*/ *** %AddText(text=Height); *** %CategoricalRowVar2(rowVar=); %ContinuousRowVar2(rowVar=height ); *** %AddText(text=); %ContinuousRowVar2(rowVar=weight ); %ContinuousRowVar2(rowVar=Age ); proc print data= table1; run; /* Export Table 1 as a CSV file*/ proc export data=Table1 replace label outfile=&quot;C:\\Users\\hed2\\Downloads\\mybook2\\mybook2\\sasmacro\\table1.csv&quot; dbms=csv; run; Obs Variable freqpct_meansd 1 Height : Mean (SD) [N] 62.3 (5.1) [19] 2 Weight : Mean (SD) [N] 100.0 (22.8) [19] 3 Age : Mean (SD) [N] 13.3 (1.5) [19] freqpct_meansd_ Obs g1 freqpct_meansd_g2 Prob 1 60.6 (5.0) [9] 63.9 (4.9) [10] 0.1943 2 90.1 (19.4) [9] 109.0 (22.7) [10] 0.1476 3 13.2 (1.4) [9] 13.4 (1.6) [10] 0.8695 create table 1 without statistical test *** Load utility macros; %include &quot;C:\\Users\\hed2\\Downloads\\mybook2\\mybook2\\sasmacro/Utility Macros_updated_NG.sas&quot;; /* By counts %let yourdata = sashelp.class; %let output_data= By_sex; %let formatsfolder= ; %let yourfolder = ; %let varlist_cat = ; %let varlist_cont = age heigt weight; %let output_order = height weight age; %let decimal_max =3; %let group_by = sex; %let group_by_missing=0; %Table_summary; */ perform a regression proc glm data= sashelp.class; model height=age; Run; The GLM Procedure Number of Observations Read 19 Number of Observations Used 19 The GLM Procedure Dependent Variable: Height Sum of Source DF Squares Mean Square F Value Pr &gt; F Model 1 311.5434756 311.5434756 32.77 &lt;.0001 Error 17 161.6207349 9.5071021 Corrected Total 18 473.1642105 R-Square Coeff Var Root MSE Height Mean 0.658426 4.946287 3.083359 62.33684 Source DF Type I SS Mean Square F Value Pr &gt; F Age 1 311.5434756 311.5434756 32.77 &lt;.0001 Source DF Type III SS Mean Square F Value Pr &gt; F Age 1 311.5434756 311.5434756 32.77 &lt;.0001 Standard Parameter Estimate Error t Value Pr &gt; |t| Intercept 25.22388451 6.52168912 3.87 0.0012 Age 2.78713911 0.48688163 5.72 &lt;.0001 "],["miscellaneous.html", "10 Miscellaneous 10.1 Linear algebra 10.2 Calculus 10.3 Sample size calculation 10.4 How to evaluate z score 10.5 How to create a bookdown 10.6 How to create a blogdown", " 10 Miscellaneous 10.1 Linear algebra 10.1.1 Matrix basics dimensions of matrix seed=123 X=matrix(1:12,ncol=3,nrow=4) X 1 5 9 2 6 10 3 7 11 4 8 12 dim(X) [1] 4 3 dim(X)[1] [1] 4 ncol(X) [1] 3 change dimensions of matrix dim(X)=c(2,6) X 1 3 5 7 9 11 2 4 6 8 10 12 change names of matrix a &lt;- matrix(1:6,nrow=2,ncol=3,byrow=FALSE) b &lt;- matrix(1:6,nrow=3,ncol=2,byrow=T) c &lt;- matrix(1:6,nrow=3,ncol=2,byrow=T,dimnames=list(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;),c(&quot;boy&quot;,&quot;girl&quot;))) c boy girl A 1 2 B 3 4 C 5 6 rownames(c) [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; colnames(c) [1] &quot;boy&quot; &quot;girl&quot; rownames(c)=c(&quot;E&quot;,&quot;F&quot;,&quot;G&quot;) c boy girl E 1 2 F 3 4 G 5 6 replace elements of matrix X=matrix(1:12,nrow=4,ncol=3) X[2,3] [1] 10 X[2,3]=1000 X 1 5 9 2 6 1000 3 7 11 4 8 12 extract diagonal elements and replace them diag(X) [1] 1 6 11 diag(X)=c(0,0,1) X 0 5 9 2 0 1000 3 7 1 4 8 12 create diagonal identity matrix diag(c(1,2,3)) 1 0 0 0 2 0 0 0 3 diag(3) 1 0 0 0 1 0 0 0 1 extract lower/upper triangle elements X 0 5 9 2 0 1000 3 7 1 4 8 12 X[lower.tri(X)] [1] 2 3 4 7 8 12 X[upper.tri(X)] [1] 5 9 1000 create lower/upper triangle matrix X[lower.tri(X)]=0 X 0 5 9 0 0 1000 0 0 1 0 0 0 10.1.2 Operations transform t(X) 0 0 0 0 5 0 0 0 9 1000 1 0 summary by row or column A=matrix(1:12,3,4) A 1 4 7 10 2 5 8 11 3 6 9 12 rowSums(A) [1] 22 26 30 rowMeans(A) [1] 5.5 6.5 7.5 determinant of matrix X=matrix(rnorm(9),nrow=3,ncol=3) det(X) [1] 0.7165293 Addition A=matrix(1:12,nrow=3,ncol=4) B=matrix(1:12,nrow=3,ncol=4) A+B #same dimensions (non-conformable arrays) 2 8 14 20 4 10 16 22 6 12 18 24 addition by scale A=matrix(1:12,nrow=3,ncol=4) 2+A 3 6 9 12 4 7 10 13 5 8 11 14 multiple by scale A=matrix(1:12,nrow=3,ncol=4) 2*A 2 8 14 20 4 10 16 22 6 12 18 24 dot multiple A=matrix(1:12,nrow=2,ncol=4) B=matrix(1:12,nrow=4,ncol=3) A%*%B 50 114 178 60 140 220 kronecker multiple A=matrix(1:4,2,2) B=matrix(rep(1,4),2,2) kronecker(A,B) 1 1 3 3 1 1 3 3 2 2 4 4 2 2 4 4 inverse matrix must be square A=matrix(rnorm(9),nrow=3,ncol=3) A -1.3843051 -1.310550 0.740038 0.0789580 1.051403 -0.711770 -0.5636427 0.738301 2.260475 AI=solve(A) AI -0.7582116 -0.9167069 -0.0404247 -0.0581822 0.7085465 0.2421523 -0.1700547 -0.4599988 0.3532150 # identity matrix (A%*%AI) 1 0 0 0 1 0 0 0 1 library(matlib) inv(A) -0.7582116 -0.9167069 -0.0404247 -0.0581822 0.7085465 0.2421523 -0.1700547 -0.4599988 0.3532150 generalized inverse when it is not square library(MASS) A2=matrix(1:12,nrow=3,ncol=4) A%*%ginv(A) 1 0 0 0 1 0 0 0 1 A%*%ginv(A)%*%A -1.3843051 -1.310550 0.740038 0.0789580 1.051403 -0.711770 -0.5636427 0.738301 2.260475 crossprod B=matrix(1:12,nrow=4,ncol=3) B 1 5 9 2 6 10 3 7 11 4 8 12 crossprod(B) 30 70 110 70 174 278 110 278 446 t(B)%*%B 30 70 110 70 174 278 110 278 446 B=matrix(1:12,nrow=4,ncol=3) ginv(B) -0.375 -0.1458333 0.0833333 0.3125 -0.100 -0.0333333 0.0333333 0.1000 0.175 0.0791667 -0.0166667 -0.1125 ginv(crossprod(B)) 0.2664931 0.0763889 -0.1137153 0.0763889 0.0222222 -0.0319444 -0.1137153 -0.0319444 0.0498264 # solve(crossprod(B)) #is singular # solve((B)) #is not square eigen values (decomposition) mxn matrix A=UΛU A=matrix(1:9,nrow=3,ncol=3) Aeigen=eigen(A) Aeigen$values [1] 1.611684e+01 -1.116844e+00 -5.700691e-16 val &lt;- diag(Aeigen$values) val 16.11684 0.000000 0 0.00000 -1.116844 0 0.00000 0.000000 0 eigen vectors Aeigen$vectors -0.4645473 -0.8829060 0.4082483 -0.5707955 -0.2395204 -0.8164966 -0.6770438 0.4038651 0.4082483 round(Aeigen$vectors%*%val%*%t(Aeigen$vectors)) 3 4 5 4 5 6 5 6 7 A 1 4 7 2 5 8 3 6 9 10.1.3 Advanced operations Choleskey factor positive definite matrix (symmetric square), A=P’P covariance matrix is semi positive definite matrix A=diag(3)+1 A 2 1 1 1 2 1 1 1 2 chol(A) 1.414214 0.7071068 0.7071068 0.000000 1.2247449 0.4082483 0.000000 0.0000000 1.1547005 t(chol(A))%*%chol(A) 2 1 1 1 2 1 1 1 2 inverse using Choleshey chol2inv(chol(A)) 0.75 -0.25 -0.25 -0.25 0.75 -0.25 -0.25 -0.25 0.75 inv(A) 0.75 -0.25 -0.25 -0.25 0.75 -0.25 -0.25 -0.25 0.75 singular value (svd) decomposition m×n matrix A=UDV A=matrix(1:18,3,6) svd(A) $d [1] 4.589453e+01 1.640705e+00 1.366522e-15 t(svd(A) $u)%*%svd(A) $u 1 0 0 0 1 0 0 0 1 t(svd(A) $v)%*%svd(A) $v 1 0 0 0 1 0 0 0 1 svd(A) $u %*%diag(svd(A) $d)%*% t(svd(A) $v) 1 4 7 10 13 16 2 5 8 11 14 17 3 6 9 12 15 18 A 1 4 7 10 13 16 2 5 8 11 14 17 3 6 9 12 15 18 QR decomposition m×n matrix A=QR，where Q’Q=I, Q is orthogonal matrix A=matrix(1:12,4,3) qr(A) $qr [,1] [,2] [,3] [1,] -5.4772256 -12.7801930 -2.008316e+01 [2,] 0.3651484 -3.2659863 -6.531973e+00 [3,] 0.5477226 -0.3781696 1.601186e-15 [4,] 0.7302967 -0.9124744 -5.547002e-01 $rank [1] 2 $qraux [1] 1.182574 1.156135 1.832050 $pivot [1] 1 2 3 attr(,&quot;class&quot;) [1] &quot;qr&quot; qr.Q(qr(A)) -0.1825742 -0.8164966 -0.4000874 -0.3651484 -0.4082483 0.2546329 -0.5477226 0.0000000 0.6909965 -0.7302967 0.4082483 -0.5455419 qr.R(qr(A)) -5.477226 -12.780193 -20.083160 0.000000 -3.265986 -6.531973 0.000000 0.000000 0.000000 10.1.4 Solve linear equations X=matrix(c(2, 2, 2, 0, 1, 1 ,2, 2, 0),ncol=3,nrow=3) X 2 0 2 2 1 2 2 1 0 b=1:3 b [1] 1 2 3 solve(X,b) # whether it is singular [1] 1.0 1.0 -0.5 10.1.5 Summary Eigen values (values and vectors, UΛU), singular values (s v “\" d), and QR (orthogonal and upper triangle, QR) for any matrix. Choleskey (P’ P, lower and upper triangle) factor for positive definite matrix (cov matrix is semi). 10.2 Calculus 10.2.1 Derivation dx &lt;- deriv(y ~ x^3, &quot;x&quot;); dx expression({ .value &lt;- x^3 .grad &lt;- array(0, c(length(.value), 1L), list(NULL, c(&quot;x&quot;))) .grad[, &quot;x&quot;] &lt;- 3 * x^2 attr(.value, &quot;gradient&quot;) &lt;- .grad .value }) mode(dx) [1] &quot;expression&quot; x&lt;-1:2 eval(dx) [1] 1 8 attr(,&quot;gradient&quot;) x [1,] 3 [2,] 12 dx &lt;- deriv(y ~ sin(x), &quot;x&quot;, func= TRUE) ; mode(dx) [1] &quot;function&quot; dx(c(pi,4*pi)) [1] 1.224606e-16 -4.898425e-16 attr(,&quot;gradient&quot;) x [1,] -1 [2,] 1 a&lt;-2 dx&lt;-deriv(y~a*cos(a*x),&quot;x&quot;,func = TRUE) dx(pi/3) [1] -1 attr(,&quot;gradient&quot;) x [1,] -3.464102 fxy = expression(2*x^2+y+3*x*y^2) dxy = deriv(fxy, c(&quot;x&quot;, &quot;y&quot;), func = TRUE) dxy(1,2) [1] 16 attr(,&quot;gradient&quot;) x y [1,] 16 13 10.2.2 Integration integrate(dnorm, -1.96, 1.96) 0.9500042 with absolute error &lt; 0.00000000001 integrate(dnorm, -Inf, Inf) 1 with absolute error &lt; 0.000094 integrand &lt;- function(x) {1/((x+1)*sqrt(x))} integrate(integrand, lower = 0, upper = Inf) 3.141593 with absolute error &lt; 0.000027 integrand &lt;- function(x) {sin(x)} integrate(integrand, lower = 0, upper = pi/2) 1 with absolute error &lt; 0.000000000000011 10.3 Sample size calculation #given proportion px3=0.11 px4=0.07 px5=0.08 px6=0.06 px7=0.05 ################### Outcome0 = NULL for (t in seq(350, 600, by=2)){ # change the possible sample size graudally from 400 to 700. n=t count=0 M=500 # times of simulation for (i in 1:M){ # for a given sample size (400), for the first simulation, # we use rbinom function simulate the contingencey table as below with above given proportions, # and calculate the p value by using Chi square test for the simulation x3=rbinom(1,n,px3) x4=rbinom(1,n,px4) x5=rbinom(1,n,px5) x6=rbinom(1,n,px6) x7=rbinom(1,n,px7) data=matrix(c(x3,n-x3,x4,n-x4,x5,n-x5,x6,n-x6,x7,n-x7 ),ncol=2,byrow=T) # if the p value of this simulation is less than 0.05, it means we get the significant result for the given this sample size this time. it means we detected the difference in this simulation experiment when the difference is true. pv=prop.test(data)$p.value count=as.numeric(pv&lt;0.05)+count #sum of pv&lt;0.05 } #end loop of p value power0=count/M temp &lt;- data.frame(size=t,power=power0) Outcome0 = rbind(Outcome0, temp) } #end loop of power from different sample size # generate a new variable named &quot;power_loess&quot; by loess method because the curve of sample size and power is not enough smooth power_loess &lt;- round(predict(loess(Outcome0$power ~ Outcome0$size,data=Outcome0,span=0.6)),3) Outcome0 = data.frame(Outcome0, power_loess) # plot a line chart of the power and sample size, and smooth the curve by loess method plot (Outcome0$size, Outcome0$power,type = &quot;n&quot;, ylab = &quot;Power&quot;,xlab=&quot;Sample size&quot;, main = expression(paste(&quot;Power analysis for GDM&quot;))) abline(h=0.9,col=&#39;red&#39;,lwd=2,lty=2) abline(h=0.85, col=&#39;red&#39;,lwd=2,lty=2) abline(h=0.8,col=&#39;red&#39;,lwd=2,lty=2) lines(Outcome0$size,power_loess,col=&quot;blue&quot;,lwd=2) 10.4 How to evaluate z score Call the parameter estimates file PE= matrix(PE,nrow=5,byrow = T) index= Ultra-3; info= PE[index,] fcoef=info[3:9]; sigma=info[10];Sigmab= matrix(info[11:26],4,4);Zeta=info[27:29];varfixed= matrix(info[30:78],7,7) #abstract specific parameters to calculate mean and SD calculate mean and std Set a value,like 28.29 i = 28.29 int &lt;- 1 t1 &lt;- i t2 &lt;- i**2 t3 &lt;- i**3 tt1 &lt;- (i - Zeta[1])**3 * (i &gt; Zeta[1]) tt2 &lt;- (i - Zeta[2])**3 * (i &gt; Zeta[2]) tt3 &lt;- (i - Zeta[3])**3 * (i &gt; Zeta[3]) Fxxi = cbind(int, t1, t2, t3, tt1, tt2, tt3) Rxxi = cbind(int, t1, t2, t3) mean &lt;- Fxxi%*%fcoef var &lt;- sigma**2 + Rxxi%*%Sigmab%*%t(Rxxi) + Fxxi%*%varfixed%*%t(Fxxi) std &lt;- sqrt(var) Output the calculated mean cat(&quot;Actual Mean=&quot;,exp(mean)) Actual Mean= 9.358071 Calculate Z score cat(&quot;Z score=&quot;,((log(289 ))-(mean))/std) Z score= 0.4419324 Result checking pnorm(0.4465193, mean=0, sd=1) [1] 0.6723889 10.5 How to create a bookdown create a github rep create a new project using version control whit Git and SVN create a directory in p drive create a new project using bookdown install bookdown package in R delete and add markdown document in this bookdown project (bookdown rank chapters according to the names of markdown documents, it is necessary to put a charpter in one markdown) modified some places to config it as customized profile built to see the html version hit git then select all updated documents commit and push open netlify using github account then deploy the rep from github change your website 10.5.0.1 How to git up a project into github see youtube tutorial hit tools- version control- project setup git- commit create a new rep in github, do not select readme. copy three code… in terminal window, enter git git git push 10.5.0.2 How to git down a project into your pc create a new project by using version control then pull 10.5.0.3 How to return previous version in github one of ways download then copy and substitute all documents in previous folder then git up 10.6 How to create a blogdown it is a little more compliaced to create a blogdown than to create a book because you need to config more parameters.like index file see youtube tutorial and see my blog "],["english.html", "11 English", " 11 English Stay tuned! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
