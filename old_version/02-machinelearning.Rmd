# Machine learning

## Machine learning workflow
<!-- ### Loading packages and datasets  -->
<!-- ```{r} -->
<!-- # load the Pima Indians dataset from the mlbench dataset -->
<!-- library(mlbench) -->
<!-- data(PimaIndiansDiabetes)  -->
<!-- # rename dataset to have shorter name because lazy -->
<!-- diabetes <- PimaIndiansDiabetes -->
<!-- ``` -->
<!-- - look at the data set -->
<!-- ```{r} -->
<!-- # install.packages(c('caret', 'skimr', 'RANN', 'randomForest', 'fastAdaboost', 'gbm', 'xgboost', 'caretEnsemble', 'C50', 'earth')) -->

<!-- # Load the caret package -->
<!-- library(caret) -->

<!-- # Structure of the dataframe -->
<!-- str(diabetes) -->

<!-- # See top 6 rows  -->
<!-- head(diabetes ) -->
<!-- ``` -->
<!-- ### Spliting the dataset into training and test data sets -->
<!-- ```{r} -->
<!-- # Create the training and test datasets -->
<!-- set.seed(100) -->

<!-- # Step 1: Get row numbers for the training data -->
<!-- trainRowNumbers <- createDataPartition(diabetes$diabetes, p=0.8, list=FALSE) -->

<!-- # Step 2: Create the training  dataset -->
<!-- trainData <- diabetes[trainRowNumbers,] -->

<!-- # Step 3: Create the test dataset -->
<!-- testData <- diabetes[-trainRowNumbers,] -->

<!-- # Store X and Y for later use. -->
<!-- # x = trainData[, -1] -->
<!-- y = trainData$diabetes -->
<!-- ``` -->
<!-- - have a look training data set -->
<!-- ```{r} -->
<!-- library(skimr) -->
<!-- skimmed <- skim (trainData) -->
<!-- skimmed  -->

<!-- ``` -->
<!-- ### Implement data imputation  -->
<!-- - compiling knnimpute model -->
<!-- ```{r} -->
<!-- # Create the knn imputation model on the training data -->
<!-- preProcess_missingdata_model <- preProcess(trainData, method='knnImpute') -->
<!-- preProcess_missingdata_model -->
<!-- ``` -->
<!-- - check missingness -->
<!-- ```{r} -->
<!-- # Use the imputation model to predict the values of missing data points -->
<!-- library(RANN)  # required for knnInpute -->
<!-- trainData <- predict(preProcess_missingdata_model, newdata = trainData) -->
<!-- anyNA(trainData) -->
<!-- ``` -->
<!-- ### One-hot-endcoding   -->
<!-- - Y (dependent) will not be encoded as one-hot-encoding -->
<!-- ```{r} -->
<!-- # One-Hot Encoding -->
<!-- # Creating dummy variables is converting a categorical variable to as many binary variables as here are categories. -->
<!-- dummies_model <- dummyVars(diabetes ~ ., data=trainData) -->

<!-- # Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat. -->
<!-- trainData_mat <- predict(dummies_model, newdata = trainData) -->

<!-- # # Convert to dataframe -->
<!-- trainData <- data.frame(trainData_mat) -->

<!-- # # See the structure of the new dataset -->
<!-- str(trainData) -->
<!-- ``` -->
<!-- ### Normalizing features -->
<!-- ```{r} -->
<!-- preProcess_range_model <- preProcess(trainData, method='range') -->
<!-- trainData <- predict(preProcess_range_model, newdata = trainData) -->

<!-- # Append the Y variable instead of normalized data -->
<!-- trainData$diabetes <- y -->

<!-- # Look the dataset -->
<!-- apply(trainData[, -1], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))}) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- str(trainData) -->
<!-- ``` -->
<!-- ### Plot features -->
<!-- ```{r} -->
<!-- featurePlot(x = trainData[, 1:8],  -->
<!--             y = trainData$diabetes,  -->
<!--             plot = "box", -->
<!--             strip=strip.custom(par.strip.text=list(cex=.7)), -->
<!--             scales = list(x = list(relation="free"),  -->
<!--                           y = list(relation="free"))) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- featurePlot(x = trainData[, 1:8],  -->
<!--             y = trainData$diabetes,  -->
<!--             plot = "density", -->
<!--             strip=strip.custom(par.strip.text=list(cex=.7)), -->
<!--             scales = list(x = list(relation="free"),  -->
<!--                           y = list(relation="free"))) -->
<!-- ``` -->

<!-- ```{r} -->
<!--  library(corrplot) -->
<!-- corrplot(cor((trainData[,-9] ))) -->

<!-- ``` -->

<!-- ### **Recursive feature elimination (rfe)**  -->

<!-- - In some scenarios, we just have to include the significant features into the following model. A good choice of selecting the important features is the recursive feature elimination (RFE).       -->
<!-- - the final subset model is marked with a `starisk` in the last column, here it is 8th.        -->
<!-- - though it is not wise to neglect the other predictors.    -->
<!-- ```{r} -->
<!-- set.seed(100) -->

<!-- options(warn=-1) -->

<!-- subsets <- c(1:8) -->

<!-- ctrl <- rfeControl(functions = rfFuncs,  #random forest algorithm -->
<!--                    method = "repeatedcv", #k fold cross validation repeated 5 times -->
<!--                    repeats = 5, -->
<!--                    verbose = FALSE) -->

<!-- lmProfile <- rfe(x=trainData[, 1:8], y=trainData$diabetes, -->
<!--                  sizes = subsets, -->
<!--                  rfeControl = ctrl) -->

<!-- lmProfile -->
<!-- ``` -->
<!-- `look up features of all models in R`  -->
<!-- ```{r} -->
<!-- # See available algorithms in caret -->
<!-- modelnames <- paste(names(getModelInfo()), collapse=',  ') -->

<!-- ``` -->

<!-- ```{r} -->
<!-- modelLookup('xgbTree') -->
<!-- ``` -->
<!-- ### Training a model `Multivariate Adaptive Regression Splines (MARS)` -->
<!-- ```{r} -->
<!-- # Set the seed for reproducibility -->
<!-- set.seed(100) -->

<!-- # Train the model using randomForest and predict on the training data itself. -->
<!-- model_mars = train(diabetes ~ ., data=trainData, method='earth') -->
<!-- fitted <- predict(model_mars) -->
<!-- ``` -->
<!-- - the default of resampling (Bootstrapped) is 25 reps -->
<!-- ```{r} -->
<!-- model_mars -->
<!-- ``` -->
<!-- - plot the Accuracy of various combinations of the hyper parameters - `interaction.depth and n.trees`.  -->
<!-- ```{r} -->
<!-- plot(model_mars, main="Model Accuracies with MARS") -->
<!-- ``` -->

<!-- - calculate the importance of variable -->
<!-- ```{r} -->
<!-- varimp_mars <- varImp(model_mars) -->
<!-- plot(varimp_mars, main="Variable Importance with MARS") -->
<!-- ``` -->

<!-- ### Prepare the test data set -->
<!-- - `imputation,dummy, and normalization` -->
<!-- ```{r} -->
<!-- # Step 1: Impute missing values  -->
<!-- testData2 <- predict(preProcess_missingdata_model, testData)   -->

<!-- # Step 2: Create one-hot encodings (dummy variables) -->
<!-- testData3 <- predict(dummies_model, testData2) -->

<!-- # Step 3: Transform the features to range between 0 and 1 -->
<!-- testData4 <- predict(preProcess_range_model, testData3) -->

<!-- # View -->
<!-- head(testData4 ) -->
<!-- ``` -->
<!-- ### Prediction uisng testdata -->
<!-- ```{r} -->
<!-- # Predict on testData -->
<!-- predicted <- predict(model_mars, testData4) -->
<!-- head(predicted) -->
<!-- ``` -->
<!-- ### Compute confusion matrix -->
<!-- ```{r} -->
<!-- # Compute the confusion matrix -->
<!-- confusionMatrix(reference = as.factor(testData$diabetes), data = predicted ) -->
<!-- ``` -->
<!-- ### Tuning hyperparameter to optimize the model -->
<!-- - setting up hyper parameter `tuneLength, tuneGrid`   -->
<!-- ```{r} -->
<!-- # Define the training control -->
<!-- fitControl <- trainControl( -->
<!--     method = 'cv',                   # k-fold cross validation  -->
<!--     number = 5,                      # number of folds  -->
<!--     savePredictions = 'final',       # saves predictions for optimal tuning parameter -->
<!--     classProbs = T,                  # should class probabilities be returned -->
<!--     summaryFunction=twoClassSummary  # results summary function -->
<!-- )  -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # Step 1: Define the tuneGrid -->
<!-- marsGrid <-  expand.grid(nprune = c(2, 4, 6, 8, 10),  -->
<!--                          degree = c(1, 2, 3)) -->

<!-- # Step 2: Tune hyper parameters by setting tuneGrid -->
<!-- set.seed(100) -->
<!-- model_mars3 = train(diabetes ~ ., data=trainData, method='earth', metric='ROC', tuneGrid = marsGrid, trControl = fitControl) -->
<!-- model_mars3 -->

<!-- ``` -->
<!-- ```{r} -->
<!-- # Step 3: Predict on testData and Compute the confusion matrix -->
<!-- predicted3 <- predict(model_mars3, testData4) -->
<!-- confusionMatrix(reference = as.factor(testData$diabetes), data = predicted3   ) -->
<!-- ``` -->
<!-- ### Other marchine learning algorithms -->
<!-- #### **adaboost algorithm** -->

<!-- ```{r} -->
<!-- set.seed(100) -->

<!-- # Train the model using adaboost -->
<!-- model_adaboost = train(diabetes ~ ., data=trainData, method='adaboost', tuneLength=2, trControl = fitControl) -->
<!-- model_adaboost -->
<!-- ``` -->
<!-- #### **random forest**  -->
<!-- ```{r} -->
<!-- set.seed(100) -->

<!-- # Train the model using rf -->
<!-- model_rf = train(diabetes ~ ., data=trainData, method='rf', tuneLength=5, trControl = fitControl) -->
<!-- model_rf -->
<!-- ``` -->

<!-- #### **xgbDART algorithm**  -->
<!-- ```{r} -->
<!-- # set.seed(100) -->
<!-- #  -->
<!-- # # Train the model using MARS -->
<!-- # model_xgbDART = train(Purchase ~ ., data=trainData, method='xgbDART', tuneLength=5, trControl = fitControl, verbose=F) -->
<!-- # model_xgbDART -->
<!-- ``` -->
<!-- #### **Support Vector Machines (SVM)** -->
<!-- ```{r} -->
<!-- set.seed(100) -->

<!-- # Train the model using MARS -->
<!-- model_svmRadial = train(diabetes ~ ., data=trainData, method='svmRadial', tuneLength=15, trControl = fitControl) -->
<!-- model_svmRadial -->
<!-- ``` -->
<!-- #### **K-Nearest Neighbors**   -->
<!-- ```{r} -->
<!-- set.seed(100) -->

<!-- # Train the model using MARS -->
<!-- model_knn = train(diabetes ~ ., data=trainData, method='knn', tuneLength=15, trControl = fitControl) -->
<!-- model_knn -->
<!-- ``` -->
<!-- ### Comparisons of different models -->
<!-- ```{r} -->
<!-- # Compare model performances using resample() -->
<!-- models_compare <- resamples(list(ADABOOST=model_adaboost, RF=model_rf, knn=model_knn, MARS=model_mars3, SVM=model_svmRadial)) -->

<!-- # Summary of the models performances -->
<!-- summary(models_compare) -->
<!-- ``` -->
<!-- ### Plot comparisons of models -->
<!-- ```{r} -->
<!-- # Draw box plots to compare models -->
<!-- scales <- list(x=list(relation="free"), y=list(relation="free")) -->
<!-- bwplot(models_compare, scales=scales) -->
<!-- ``` -->

<!-- ### Ensemble predictions from multiple models  -->
<!-- - create multiple models -->
<!-- ```{r} -->
<!-- library(caretEnsemble) -->
<!-- # Stacking Algorithms - Run multiple algos in one call. -->
<!-- trainControl <- trainControl(method="repeatedcv",  -->
<!--                              number=10,  -->
<!--                              repeats=3, -->
<!--                              savePredictions=TRUE,  -->
<!--                              classProbs=TRUE) -->

<!-- algorithmList <- c('rf', 'adaboost', 'earth', 'knn', 'svmRadial') -->

<!-- set.seed(100) -->
<!-- models <- caretList(diabetes ~ ., data=trainData, trControl=trainControl, methodList=algorithmList)  -->
<!-- results <- resamples(models) -->
<!-- summary(results) -->
<!-- ``` -->
<!-- - comparison by visualization -->
<!-- ```{r} -->
<!-- # Box plots to compare models -->
<!-- scales <- list(x=list(relation="free"), y=list(relation="free")) -->
<!-- bwplot(results, scales=scales) -->
<!-- ``` -->

<!-- - ensemble predictions on testdata   -->
<!-- ```{r} -->
<!-- # Create the trainControl -->
<!-- set.seed(101) -->
<!-- stackControl <- trainControl(method="repeatedcv",  -->
<!--                              number=10,  -->
<!--                              repeats=3, -->
<!--                              savePredictions=TRUE,  -->
<!--                              classProbs=TRUE) -->

<!-- # Ensemble the predictions of `models` to form a new combined prediction based on glm -->
<!-- stack.glm <- caretStack(models, method="glm", metric="Accuracy", trControl=stackControl) -->
<!-- print(stack.glm) -->
<!-- ``` -->

<!-- - compute confusion matrix -->
<!-- ```{r} -->
<!-- # Predict on testData -->
<!-- stack_predicteds <- predict(stack.glm, newdata=testData4) -->
<!-- confusionMatrix(reference = as.factor(testData$diabetes), data = stack_predicteds   ) -->
<!-- ``` -->

<!-- <!-- =================== --> -->

<!-- ##  KNN Classifier -->

<!-- ```{r,message=FALSE} -->
<!-- # Loading package -->
<!-- # library(e1071) -->
<!-- library(caTools) -->
<!-- library(class) -->
<!-- ``` -->
<!-- ### Splitting data -->
<!-- ```{r} -->
<!-- # load the Pima Indians dataset from the mlbench dataset -->
<!-- library(mlbench) -->
<!-- data(PimaIndiansDiabetes)  -->
<!-- # rename dataset to have shorter name because lazy -->
<!-- diabetes <- PimaIndiansDiabetes -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # Splitting data into train and test data -->
<!-- set.seed(100) -->

<!-- split <- sample.split(diabetes, SplitRatio = 0.8) -->
<!-- train_cl <- subset(diabetes, split == "TRUE") -->
<!-- test_cl <- subset(diabetes, split == "FALSE") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # Feature Scaling -->
<!-- train_scale <- scale(train_cl[, 1:8]) -->
<!-- test_scale <- scale(test_cl[, 1:8]) -->

<!-- # train_y <- scale(train_cl[, 5]) -->
<!-- # test_y <- scale(test_cl[, 5]) -->
<!-- ``` -->

<!-- ### Creating KNN model -->
<!-- ```{r} -->
<!-- # Fitting KNN Model to training dataset -->
<!-- classifier_knn <- knn(train = train_scale, -->
<!--                       cl = train_cl$diabetes, -->

<!--                       test = test_scale, -->
<!--                       k = 1) -->
<!-- classifier_knn -->
<!-- ``` -->
<!-- ### Model Evaluation -->
<!-- - Creat confusion matrix -->
<!-- ```{r} -->
<!-- # Confusion Matrix -->
<!-- cm <- table(test_cl$diabetes, classifier_knn) -->
<!-- cm -->
<!-- ``` -->
<!-- ### Calculate accuracy with different K -->
<!-- ```{r} -->
<!-- # Model Evaluation - Choosing K =1 -->
<!-- # Calculate out of Sample error -->
<!-- misClassError <- mean(classifier_knn != test_cl$diabetes) -->
<!-- print(paste('Accuracy =', 1-misClassError)) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- # K = 7 -->
<!-- classifier_knn <- knn(train = train_scale, -->
<!--                       test = test_scale, -->
<!--                       cl = train_cl$diabetes, -->
<!--                       k = 23) -->

<!-- misClassError <- mean(classifier_knn != test_cl$diabetes) -->
<!-- print(paste('Accuracy =', 1-misClassError)) -->
<!-- ``` -->
<!-- ###  Optimization -->
<!-- - search better k parameter -->

<!-- ```{r} -->
<!-- i=1 -->
<!-- k.optm=1 -->

<!-- for (i in 1:39){ -->
<!--  y_pred = knn(train = train_scale, -->
<!--              test = test_scale, -->

<!--              cl = train_cl$diabetes, -->
<!--              k = i ) -->

<!--  k.optm[i] <-   1- mean(y_pred != test_cl$diabetes) -->

<!--  k=i -->
<!--  cat(k,'=',k.optm[i],'') -->
<!--  } -->
<!-- ``` -->
<!-- - Accuracy plot `k=15` -->

<!-- ```{r} -->
<!-- plot(k.optm, type="b", xlab="K- Value",ylab="RMSE level") -->
<!-- ``` -->

<!-- ### Visualization -->
<!-- ```{r} -->
<!-- # Visualising the Training set results -->
<!-- # Install ElemStatLearn if not present  -->
<!-- ``` -->


<!-- ## KNN regression -->
<!-- ### Data exploring -->
<!-- ```{r, message=FALSE} -->
<!-- library("Amelia") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- data("Boston", package = "MASS") -->
<!-- missmap(Boston,col=c('yellow','black'),y.at=1,y.labels='',legend=TRUE) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library(corrplot) -->
<!-- corrplot(cor((Boston))) -->
<!-- ``` -->
<!-- ```{r,message=F} -->
<!-- library(Hmisc) -->
<!-- describe(Boston) -->
<!-- ``` -->

<!-- ### Prepareing data -->
<!-- ```{r} -->
<!-- Boston <-    dplyr::select (Boston ,medv , crim , rm , tax , lstat) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # Splitting the dataset into  -->
<!-- # the Training set and Test set -->
<!-- # install.packages('caTools') -->
<!-- library(caTools) -->
<!-- set.seed(123) -->
<!-- split = sample.split(Boston$medv,  -->
<!--                      SplitRatio = 0.75) -->
<!-- training_set_origi = subset(Boston,  -->
<!--                       split == TRUE) -->
<!-- test_set_origi = subset(Boston,  -->
<!--                   split == FALSE) -->


<!-- ``` -->


<!-- ```{r} -->
<!-- # Feature Scaling -->

<!-- training_set = scale(training_set_origi[,-1] ) -->
<!-- test_set = scale(test_set_origi [,-1]) -->
<!-- ``` -->


<!-- ### Creating model -->
<!-- ```{r} -->
<!-- # Fitting K-NN to the Training set  -->
<!-- # and Predicting the Test set results -->
<!-- # library(class) -->
<!-- y_pred = knn(train = training_set[, -1], -->
<!--              test = test_set[, -1], -->

<!--              cl = training_set_origi[, 1], -->
<!--              k = 15 ) -->

<!-- #  -->


<!-- ``` -->

<!-- ### Evaluation -->
<!-- ```{r} -->
<!-- # converting factor into character then into numeric -->
<!-- error <- test_set_origi[,1]-as.numeric (as.character(y_pred)) -->
<!-- head(error) -->
<!-- rmse <- sqrt(mean(error)^2) -->
<!-- rmse -->
<!-- ``` -->
<!-- ```{r} -->
<!-- plot(error) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- head(cbind(test_set_origi[,1], as.numeric (as.character(y_pred)))) -->
<!-- ``` -->
<!-- ###  Optimization -->
<!-- - search better k parameter -->

<!-- ```{r} -->
<!-- i=1 -->
<!-- k.optm=1 -->

<!-- for (i in 1:29){ -->
<!--  y_pred = knn(train = training_set[, -1], -->
<!--              test = test_set[, -1], -->

<!--              cl = training_set_origi[, 1], -->
<!--              k = i ) -->

<!--  k.optm[i] <-  sqrt(mean(   test_set_origi[,1]-as.numeric (as.character(y_pred))   )^2) -->

<!--  k=i -->
<!--  cat(k,'=',k.optm[i],'') -->
<!--  } -->
<!-- ``` -->

<!-- - Accuracy plot `k=15` -->

<!-- ```{r} -->
<!-- plot(k.optm, type="b", xlab="K- Value",ylab="RMSE level") -->
<!-- ``` -->